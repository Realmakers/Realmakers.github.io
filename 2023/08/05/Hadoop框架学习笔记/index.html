<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://example.com">
  <title>Hadoop框架学习笔记 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="第一部分 大数据概论第一章 大数据概念大数据：指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程化能力的海量、高增长和多样化的信息资产。 大数据主要解决的问题：海量数据的采集、存储和分析计算问题。 第二章 大数据特点（4V） Volume（大量）：一些大型企业的数据量已经接近EB量级。 Velocity（高速） Variety">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop框架学习笔记">
<meta property="og:url" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="第一部分 大数据概论第一章 大数据概念大数据：指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程化能力的海量、高增长和多样化的信息资产。 大数据主要解决的问题：海量数据的采集、存储和分析计算问题。 第二章 大数据特点（4V） Volume（大量）：一些大型企业的数据量已经接近EB量级。 Velocity（高速） Variety">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/webwxgetmsgimg%20(54).jpg">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-08-05_22-11-25.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-08-05_22-57-07.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/webwxgetmsgimg.jpg">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-14_18-47-42.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-14_22-08-06.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-14_22-35-15.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-14_22-36-43.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_11-10-08.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_11-13-41.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_11-23-16.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_11-28-33.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_11-30-37.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_11-34-32.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_18-29-51.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_18-31-15.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_18-34-16.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_19-35-15.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_19-37-55.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_19-38-39.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_19-42-31.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_20-13-29.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_20-14-55.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-15_21-10-19.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_14-28-08.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_14-30-31.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_14-39-46.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_14-41-29.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_15-51-45.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_15-52-06.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_16-46-42.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_16-47-10.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_20-14-14.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_20-28-33.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_20-35-08.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_20-44-17.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_21-07-12.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_21-29-03.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_21-32-51.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_21-36-47.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_21-38-11.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_21-44-09.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_21-48-18.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-17_22-13-23.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_12-44-58.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_12-45-42.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_12-47-31.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_20-39-28.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_20-47-23.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_20-49-38.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_20-52-56.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_20-59-31.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_21-20-46.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_21-21-09.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_21-30-37.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_21-46-11.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_21-55-29.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-18_21-59-07.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-19_11-06-51.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-19_11-29-38.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-19_11-33-29.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-19_11-20-38.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-19_20-13-52.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-22_18-24-09.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-22_20-58-59.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-25_09-09-40.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-25_09-30-13.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-25_14-39-40.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-26_12-05-19.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-26_12-51-41.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-26_13-05-13.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-26_13-05-45.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-25_09-30-13.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-26_15-20-12.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/webwxgetmsgi666666mg.jpg">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-26_21-41-49.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-27_12-20-44.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/webwxgetmsgimg%20(1).jpg">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/webwxgetmsgimg%20(2).jpg">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-27_14-07-39.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-27_15-33-02.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-27_15-35-37.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-27_15-36-35.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6(2).jpg">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_13-48-05.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_14-04-43.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_14-06-33.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-26_15-20-12.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_14-50-52.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_15-12-49.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_15-14-17.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_15-21-02.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_15-23-33.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_20-22-50.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_21-10-40.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_21-24-14.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_21-29-13.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_21-50-07.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_21-52-00.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_21-52-27.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_21-52-51.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_21-53-12.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_21-53-36.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-01_12-57-54.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-01_14-02-50.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-01_20-29-39.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-03_13-45-51.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-03_13-46-25.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-03_13-48-51.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-03_14-11-03.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20231003141616898.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-03_14-26-16.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-09-28_14-50-52.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-03_14-46-27.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-03_14-51-30.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-03_14-53-30.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-03_14-57-26.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-24-24.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-25-04.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-26-53.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-28-34.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-40-33.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-45-57.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-46-13.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-46-38.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-47-52.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-49-09.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-49-34.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-51-35.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-54-02.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-55-36.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-57-14.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-58-21.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_13-59-56.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_14-02-49.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_14-04-42.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_14-09-40.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_14-46-29.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_16-24-57.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_16-44-41.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_21-16-30.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-04_21-49-35.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-05_14-00-19.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-05_14-06-57.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-05_15-05-32.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-05_15-06-04.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-05_15-06-18.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-05_15-21-45.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-05_15-34-32.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-05_16-12-45.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-05_21-23-16.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_12-05-22.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_12-43-37.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_12-55-03.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_12-58-40.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_13-25-36.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_13-26-09.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_13-26-45.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_13-30-29.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_13-35-55.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_13-36-38.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_13-55-55.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_14-33-37.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_15-12-00.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_15-13-15.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_15-13-44.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_15-53-21.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_19-28-39.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_20-17-09.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_20-52-10.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_20-53-49.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_21-21-40.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-06_21-25-54.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-07_12-56-34.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-07_12-57-57.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-07_13-08-37.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-10-07_13-09-10.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-06_19-13-03.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-06_19-14-06.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-06_20-05-13.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-06_20-05-23.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-06_20-05-36.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-06_20-27-43.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-06_20-35-08.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-06_21-26-10.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-06_21-26-20.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-06_21-26-27.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-06_22-05-32.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-07_12-45-56.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-07_12-48-36.png">
<meta property="og:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Snipaste_2023-11-07_13-24-21.png">
<meta property="article:published_time" content="2023-08-05T11:31:39.000Z">
<meta property="article:modified_time" content="2023-11-27T11:32:29.195Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/webwxgetmsgimg%20(54).jpg">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div> 
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/123.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/categories">分类</a></li>
	        
			</ul>
		</nav>
		<nav>
			总文章数 48
		</nav>		
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/Realmakers" title="github"><i class="icon-github"></i></a>
		        
					<a class="qq" target="_blank" href="/3558084726" title="qq"><i class="icon-qq"></i></a>
		        
					<a class="mail" target="_blank" href="mailto: 17745182605@163.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>



    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/img/123.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/Realmakers" title="github"><i class="icon-github"></i></a>
			        
						<a class="qq" target="_blank" href="/3558084726" title="qq"><i class="icon-qq"></i></a>
			        
						<a class="mail" target="_blank" href="mailto: 17745182605@163.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 50%">
				
				
					<li style="width: 50%"><a href="/">主页</a></li>
		        
					<li style="width: 50%"><a href="/categories">分类</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-Hadoop框架学习笔记" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Hadoop框架学习笔记
    </h1>
  

        
        <a href="/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="archive-article-date">
  	<time datetime="2023-08-05T11:31:39.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-08-05</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第一部分-大数据概论"><a href="#第一部分-大数据概论" class="headerlink" title="第一部分 大数据概论"></a>第一部分 大数据概论</h1><h2 id="第一章-大数据概念"><a href="#第一章-大数据概念" class="headerlink" title="第一章 大数据概念"></a>第一章 大数据概念</h2><p>大数据：指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程化能力的<strong>海量、高增长和多样化的信息资产</strong>。</p>
<p>大数据主要解决的问题：<strong>海量数据的采集、存储和分析计算问题</strong>。</p>
<h2 id="第二章-大数据特点（4V）"><a href="#第二章-大数据特点（4V）" class="headerlink" title="第二章 大数据特点（4V）"></a>第二章 大数据特点（4V）</h2><ol>
<li>Volume（大量）：一些大型企业的数据量已经接近EB量级。</li>
<li>Velocity（高速）</li>
<li>Variety（多样）：结构化数据和非结构化数据</li>
<li>Value（价值密度低）：价值密度的高低与数据总量的大小成反比</li>
</ol>
<h2 id="第三章-大数据部门内组织结构"><a href="#第三章-大数据部门内组织结构" class="headerlink" title="第三章 大数据部门内组织结构"></a>第三章 大数据部门内组织结构</h2><ol>
<li>平台组</li>
</ol>
<ul>
<li>Hadoop、Flume、Kafka、HBase、Spark等框架平台搭建</li>
<li>集群性能监控</li>
<li>集群性能调优</li>
</ul>
<ol start="2">
<li>数据仓库组</li>
</ol>
<ul>
<li>ETL工程师（数据清洗）</li>
<li>数据分析、数据仓库建模（<strong>建模是灵魂</strong>）</li>
</ul>
<ol start="3">
<li>实时组</li>
</ol>
<ul>
<li>实时指标分析性能调优</li>
</ul>
<ol start="4">
<li>数据挖掘组</li>
</ol>
<ul>
<li>算法工程师</li>
<li>推荐系统工程师</li>
<li>用户画像工程师</li>
</ul>
<ol start="5">
<li>报表开发组</li>
</ol>
<ul>
<li>JavaEE工程师</li>
<li>前端工程师</li>
</ul>
<h1 id="第二部分-Hadoop入门"><a href="#第二部分-Hadoop入门" class="headerlink" title="第二部分 Hadoop入门"></a>第二部分 Hadoop入门</h1><h2 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h2><h2 id="第二章-Hadoop运行环境搭建（开发重点）"><a href="#第二章-Hadoop运行环境搭建（开发重点）" class="headerlink" title="第二章 Hadoop运行环境搭建（开发重点）"></a>第二章 Hadoop运行环境搭建（开发重点）</h2><h3 id="2-1-模板虚拟机环境准备"><a href="#2-1-模板虚拟机环境准备" class="headerlink" title="2.1 模板虚拟机环境准备"></a>2.1 模板虚拟机环境准备</h3><p>0）安装虚拟机，IP地址192.168.255.100、主机名称hadoop100、内存4G、硬盘50G</p>
<p>1）hadoop虚拟机配置要求如下</p>
<p>①测试可以上网</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# ping www.baidu.com</span><br><span class="line">PING www.a.shifen.com (110.242.68.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=1 ttl=128 time=29.9 ms</span><br><span class="line">64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=2 ttl=128 time=30.4 ms</span><br><span class="line">64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=3 ttl=128 time=30.4 ms</span><br><span class="line">64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=4 ttl=128 time=30.3 ms</span><br><span class="line">^C</span><br><span class="line">--- www.a.shifen.com ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3008ms</span><br><span class="line">rtt min/avg/max/mdev = 29.936/30.301/30.491/0.276 ms</span><br></pre></td></tr></table></figure>

<p>②安装epel-release</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# yum install -y epel-release</span><br><span class="line">已加载插件：fastestmirror, langpacks</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line"> * base: mirrors.aliyun.com</span><br><span class="line"> * extras: mirrors.aliyun.com</span><br><span class="line"> * updates: mirrors.bupt.edu.cn</span><br><span class="line">正在解决依赖关系</span><br><span class="line"><span class="meta prompt_">--&gt; </span><span class="language-bash">正在检查事务</span></span><br><span class="line"><span class="meta prompt_">---&gt; </span><span class="language-bash">软件包 epel-release.noarch.0.7-11 将被 安装</span></span><br><span class="line"><span class="meta prompt_">--&gt; </span><span class="language-bash">解决依赖关系完成</span></span><br><span class="line"></span><br><span class="line">依赖关系解决</span><br><span class="line"></span><br><span class="line">=============================================</span><br><span class="line"> Package       架构    版本    源       大小</span><br><span class="line">=============================================</span><br><span class="line">正在安装:</span><br><span class="line"> epel-release  noarch  7-11    extras   15 k</span><br><span class="line"></span><br><span class="line">事务概要</span><br><span class="line">=============================================</span><br><span class="line">安装  1 软件包</span><br><span class="line"></span><br><span class="line">总下载量：15 k</span><br><span class="line">安装大小：24 k</span><br><span class="line">Downloading packages:</span><br><span class="line">epel-release-7-11.noarc |  15 kB   00:00     </span><br><span class="line">Running transaction check</span><br><span class="line">Running transaction test</span><br><span class="line">Transaction test succeeded</span><br><span class="line">Running transaction</span><br><span class="line">  正在安装    : epel-release-7-11.noar   1/1 </span><br><span class="line">  验证中      : epel-release-7-11.noar   1/1 </span><br><span class="line"></span><br><span class="line">已安装:</span><br><span class="line">  epel-release.noarch 0:7-11                 </span><br><span class="line"></span><br><span class="line">完毕！</span><br></pre></td></tr></table></figure>

<p>2）关闭防火墙，关闭防火墙开机自启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# systemctl stop firewalld</span><br><span class="line">[root@hadoop100 ~]# systemctl disable firewalld.service</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br></pre></td></tr></table></figure>

<p>3）创建atguigu用户</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# useradd atguigu</span><br><span class="line">[root@hadoop100 ~]# passwd atguigu</span><br></pre></td></tr></table></figure>

<p>密码为123456</p>
<p>4）配置atguigu用户具有root权限，方便后期加sudo执行root权限的命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim /etc/sudoers</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Allows people in group wheel to run all commands</span></span></span><br><span class="line"><span class="meta prompt_">%</span><span class="language-bash">wheel  ALL=(ALL)       ALL</span></span><br><span class="line">atguigu     ALL=(ALL)       NOPASSWD:ALL</span><br></pre></td></tr></table></figure>

<p>即添加atguigu这一行即可</p>
<p>5）在&#x2F;opt目录下创建文件夹，并修改所属主和所属组</p>
<p>①在&#x2F;opt目录下创建module、software文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# mkdir /opt/module</span><br><span class="line">[root@hadoop100 ~]# mkdir /opt/software</span><br></pre></td></tr></table></figure>

<p>②修改module、software文件夹的所有者和所属组均为tom用户</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# chown atguigu:atguigu /opt/module</span><br><span class="line">[root@hadoop100 ~]# chown atguigu:atguigu /opt/software</span><br></pre></td></tr></table></figure>

<p>③查看module，software文件夹的所有者和所属组</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# cd /opt/</span><br><span class="line">[root@hadoop100 opt]# ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 6 8月   5 21:09 module</span><br><span class="line">drwxr-xr-x. 2 root    root    6 10月 31 2018 rh</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 6 8月   5 21:09 software</span><br></pre></td></tr></table></figure>

<p>6）卸载虚拟机自带的JDK</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 opt]# rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps</span><br></pre></td></tr></table></figure>

<img src="webwxgetmsgimg (54).jpg" alt="webwxgetmsgimg (54)" style="zoom:50%;">

<p>7）重启虚拟机</p>
<h3 id="2-2-克隆虚拟机"><a href="#2-2-克隆虚拟机" class="headerlink" title="2.2 克隆虚拟机"></a>2.2 克隆虚拟机</h3><p>1）利用模板机hadoop100，克隆三台虚拟机：hadoop102  hadoop103  hadoop104</p>
<p>2）克隆详细内容参考Linux和Shell基础知识</p>
<h3 id="2-3-在hadoop102安装JDK"><a href="#2-3-在hadoop102安装JDK" class="headerlink" title="2.3 在hadoop102安装JDK"></a>2.3 在hadoop102安装JDK</h3><p>1）卸载现有JDK</p>
<p>2）用Xftp传输工具将JDK导入到opt目录下面的software文件夹下面</p>
<img src="Snipaste_2023-08-05_22-11-25.png" alt="Snipaste_2023-08-05_22-11-25" style="zoom:43%;">

<p>3）在Linux系统下的opt目录中查看软件包是否导入成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# ls /opt/software</span><br><span class="line">jdk-8u212-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>

<p>4）解压JDK到&#x2F;opt&#x2F;module目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>5）配置JDK环境变量</p>
<p>①新建&#x2F;etc&#x2F;profile.d&#x2F;my_env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>②保存后退出</p>
<p>③source一下&#x2F;etc&#x2F;profile文件，让新的环境变量PATH生效</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# source /etc/profile</span><br></pre></td></tr></table></figure>

<p>6）测试JDK是否安装成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# java -version</span><br><span class="line">java version &quot;1.8.0_212&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_212-b10)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)</span><br></pre></td></tr></table></figure>

<h3 id="2-4-在hadoop102安装Hadoop"><a href="#2-4-在hadoop102安装Hadoop" class="headerlink" title="2.4 在hadoop102安装Hadoop"></a>2.4 在hadoop102安装Hadoop</h3><p>1）用Xftp传输工具将hadoop文件导入到opt目录下面的software文件夹下面</p>
<p>2）进入到Hadoop安装包路径下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/software/</span><br></pre></td></tr></table></figure>

<p>3）解压安装文件到&#x2F;opt&#x2F;module下面</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>4）查看是否解压成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop102 software]# ls /opt/module/</span><br><span class="line">hadoop-3.1.3  jdk1.8.0_212</span><br></pre></td></tr></table></figure>

<p>5）将Hadoop添加到环境变量</p>
<p>①获取Hadoop安装路径</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# pwd</span><br><span class="line">/opt/module/hadoop-3.1.3</span><br></pre></td></tr></table></figure>

<p>②打开&#x2F;etc&#x2F;profile.d&#x2F;my_env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# sudo vim /etc/profile.d/my_env.sh </span><br></pre></td></tr></table></figure>

<p>在文件末尾添加如下语句</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<p>③让修改后的文件生效</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# source /etc/profile</span><br></pre></td></tr></table></figure>

<p>6）测试是否安装成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop version</span><br><span class="line">Hadoop 3.1.3</span><br><span class="line">Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579</span><br><span class="line">Compiled by ztang on 2019-09-12T02:47Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum ec785077c385118ac91aadde5ec9799</span><br><span class="line">This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar</span><br></pre></td></tr></table></figure>

<h3 id="2-5-Hadoop目录结构"><a href="#2-5-Hadoop目录结构" class="headerlink" title="2.5 Hadoop目录结构"></a>2.5 Hadoop目录结构</h3><p>1）查看Hadoop目录结构</p>
<img src="Snipaste_2023-08-05_22-57-07.png" alt="Snipaste_2023-08-05_22-57-07" style="zoom:43%;">

<img src="webwxgetmsgimg.jpg" alt="webwxgetmsgimg" style="zoom:50%;">

<p>bin目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# cd bin/</span><br><span class="line">[root@hadoop102 bin]# ll</span><br><span class="line">总用量 996</span><br><span class="line">-rwxr-xr-x. 1 wyh wyh 441936 9月  12 2019 container-executor</span><br><span class="line">-rwxr-xr-x. 1 wyh wyh   8707 9月  12 2019 hadoop</span><br><span class="line">-rwxr-xr-x. 1 wyh wyh  11265 9月  12 2019 hadoop.cmd</span><br><span class="line">-rwxr-xr-x. 1 wyh wyh  11026 9月  12 2019 hdfs</span><br><span class="line">-rwxr-xr-x. 1 wyh wyh   8081 9月  12 2019 hdfs.cmd</span><br><span class="line">-rwxr-xr-x. 1 wyh wyh   6237 9月  12 2019 mapred</span><br><span class="line">-rwxr-xr-x. 1 wyh wyh   6311 9月  12 2019 mapred.cmd</span><br><span class="line">-rwxr-xr-x. 1 wyh wyh 483728 9月  12 2019 test-container-executor</span><br><span class="line">-rwxr-xr-x. 1 wyh wyh  11888 9月  12 2019 yarn</span><br><span class="line">-rwxr-xr-x. 1 wyh wyh  12840 9月  12 2019 yarn.cmd</span><br></pre></td></tr></table></figure>

<h2 id="第三章-Hadoop运行模式"><a href="#第三章-Hadoop运行模式" class="headerlink" title="第三章 Hadoop运行模式"></a>第三章 Hadoop运行模式</h2><p>Hadoop的运行模式包括：<strong>本地模式</strong>，<strong>伪分布式模式</strong>，<strong>完全分布式模式</strong></p>
<ul>
<li>本地模式：单机运行，只是用来演示一下官方案例。生产环境不用</li>
<li>伪分布式模式：也是单机运行，但是具备Hadoop集群的所有功能，一台服务器模拟一个分布式的环境。个别缺钱的公司用来测试，生产环境不用</li>
<li>完全分布式模式：多台服务器组成分布式环境。生产环境使用</li>
</ul>
<h3 id="3-1-本地运行模式（官方WordCount）"><a href="#3-1-本地运行模式（官方WordCount）" class="headerlink" title="3.1 本地运行模式（官方WordCount）"></a>3.1 本地运行模式（官方WordCount）</h3><p>1）进入hadoop-3.1.3文件夹下，创建一个wcinput文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/module/hadoop-3.1.3</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# mkdir wcinput</span><br></pre></td></tr></table></figure>

<p>2）在wcinput文件下创建一个word.txt文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# cd wcinput/</span><br><span class="line">[root@hadoop102 wcinput]# vim word.txt</span><br></pre></td></tr></table></figure>

<p>3）编辑word.txt文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hadoop HDFS</span><br><span class="line">hadoop mapreduce</span><br><span class="line">hadoop yarn</span><br><span class="line">spark core</span><br><span class="line">spark MLlib</span><br><span class="line">spark RDD</span><br><span class="line">flink core</span><br><span class="line">flink alink</span><br></pre></td></tr></table></figure>

<p>4）回到&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 wcinput]# cd /opt/module/hadoop-3.1.3</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# </span><br></pre></td></tr></table></figure>

<p>5）执行程序</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput/ wcoutput</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ul>
<li>hadoop-mapreduce-examples-3.1.3.jar包下有多个案例，我们现在只用一个案例，案例名为wordcount</li>
<li>wcinput为输入路径，wcoutput为输出路径</li>
<li>mapreduce程序必须指定输入路径和输出路径，且输出路径是不存在的，如果存在，会<strong>抛出异常</strong></li>
</ul>
<p>6）查看结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# cd wcoutput/</span><br><span class="line">[root@hadoop102 wcoutput]# ll</span><br><span class="line">总用量 4</span><br><span class="line">-rw-r--r--. 1 root root 80 8月   7 10:45 part-r-00000</span><br><span class="line">-rw-r--r--. 1 root root  0 8月   7 10:45 _SUCCESS</span><br></pre></td></tr></table></figure>

<p>可以看到输出路径文件夹wcoutput中有两个文件，part-r-00000为真正存储的数据，查看结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 wcoutput]# cat part-r-00000 </span><br><span class="line">HDFS	1</span><br><span class="line">MLlib	1</span><br><span class="line">RDD	1</span><br><span class="line">alink	1</span><br><span class="line">core	2</span><br><span class="line">flink	2</span><br><span class="line">hadoop	3</span><br><span class="line">mapreduce	1</span><br><span class="line">spark	3</span><br><span class="line">yarn	1</span><br></pre></td></tr></table></figure>

<p>小插曲：切换用户</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 wcoutput]# su atguigu</span><br><span class="line">[atguigu@hadoop102 wcoutput]$ </span><br></pre></td></tr></table></figure>

<h3 id="3-2-完全分布式运行模式（开发重点。至少自己安装三遍以上）"><a href="#3-2-完全分布式运行模式（开发重点。至少自己安装三遍以上）" class="headerlink" title="3.2 完全分布式运行模式（开发重点。至少自己安装三遍以上）"></a>3.2 完全分布式运行模式（开发重点。至少自己安装三遍以上）</h3><p>1）准备三台客户机（关闭防火墙、静态IP、主机名称）</p>
<p>2）安装JDK</p>
<p>3）配置环境变量</p>
<p>4）安装Hadoop</p>
<p>5）配置集群</p>
<p>6）单点启动</p>
<p>7）配置ssh</p>
<p>8）群起并测试集群</p>
<h4 id="3-2-1-虚拟机准备"><a href="#3-2-1-虚拟机准备" class="headerlink" title="3.2.1 虚拟机准备"></a>3.2.1 虚拟机准备</h4><p>分别为hadoop_copy2—–hadoop102</p>
<p>hadoop_copy3—–hadoop103</p>
<p>hadoop_copy4—–hadoop104</p>
<p>目前的情况是：<strong>hadoop102上已经安装了jdk和hadoop，但是hadoop103和hadoop104还没有安装</strong></p>
<h4 id="3-2-2-编写一些Shell脚本"><a href="#3-2-2-编写一些Shell脚本" class="headerlink" title="3.2.2 编写一些Shell脚本"></a>3.2.2 编写一些Shell脚本</h4><p>这个集群分发脚本要做的事情就是将hadoop102上的jdk和hadoop拷贝到hadoop103和hadoop104上</p>
<h5 id="1）scp（secure-copy）安全拷贝"><a href="#1）scp（secure-copy）安全拷贝" class="headerlink" title="1）scp（secure copy）安全拷贝"></a><strong>1）scp（secure copy）安全拷贝</strong></h5><p>①scp定义</p>
<p>scp可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）</p>
<p>②基本语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp     -r      $pdir/$fname                $user@$host:$pdir/$fname</span><br><span class="line"></span><br><span class="line">命令    递归     要拷贝的文件路径/名称        目的地用户@主机：目的地路径/名称</span><br></pre></td></tr></table></figure>

<p>③案例实操</p>
<p>前提：在hadoop102，hadoop103，hadoop104都已经创建好&#x2F;opt&#x2F;module，&#x2F;opt&#x2F;software两个目录，并且已经把这两个目录修改为atguigu:atguigu</p>
<p>（a）在hadoop102上，将hadoop102中&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_212目录拷贝到hadoop103上（推文件）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ scp -r /opt/module/jdk1.8.0_212/ atguigu@hadoop103:/opt/module/</span><br></pre></td></tr></table></figure>

<p>此时在hadoop103上可以看到jdk</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 opt]$ cd /opt/module/</span><br><span class="line">[atguigu@hadoop103 module]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 7 atguigu atguigu 245 8月   7 17:06 jdk1.8.0_212</span><br></pre></td></tr></table></figure>

<p>（b）在hadoop103上，将hadoop102中&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3目录拷贝到hadoop103上（拉文件）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ scp -r atguigu@hadoop102:/opt/module/hadoop-3.1.3 /opt/module/</span><br></pre></td></tr></table></figure>

<p>此时：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ cd /opt/module/</span><br><span class="line">[atguigu@hadoop103 module]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 11 atguigu atguigu 180 8月   7 17:14 hadoop-3.1.3</span><br><span class="line">drwxr-xr-x.  7 atguigu atguigu 245 8月   7 17:06 jdk1.8.0_212</span><br></pre></td></tr></table></figure>

<p>（c）在hadoop103上操作，将hadoop102中&#x2F;opt&#x2F;module目录下所有的目录拷贝到hadoop104上</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ scp -r atguigu@hadoop102:/opt/module/* atguigu@hadoop104:/opt/module/</span><br></pre></td></tr></table></figure>

<p>复制成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 opt]$ cd /opt/module/</span><br><span class="line">[atguigu@hadoop104 module]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 11 atguigu atguigu 180 8月   7 17:23 hadoop-3.1.3</span><br><span class="line">drwxr-xr-x.  7 atguigu atguigu 245 8月   7 17:24 jdk1.8.0_212</span><br></pre></td></tr></table></figure>

<h5 id="2）rsync远程同步工具"><a href="#2）rsync远程同步工具" class="headerlink" title="2）rsync远程同步工具"></a><strong>2）rsync远程同步工具</strong></h5><p>rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。</p>
<p>rsync和scp区别：用raync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去</p>
<p>①基本语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rsync       -av       $pdir/$fname              $user@$host:$pdir/$fname</span><br><span class="line">命令        选项参数   要拷贝的文件/路径          目的地用户@主机：目的地路径/名称</span><br><span class="line"></span><br><span class="line">选项参数说明：</span><br><span class="line">-a：归档拷贝</span><br><span class="line">-v：显示复杂过程</span><br></pre></td></tr></table></figure>

<p>②案例实操</p>
<p>（a）删除hadoop103中&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;wcinput</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf wcinput/</span><br></pre></td></tr></table></figure>

<p>（b）同步hadoop102中的&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3到hadoop103</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ rsync -av hadoop-3.1.3/ atguigu@hadoop103:/opt/module/hadoop-3.1.3/</span><br></pre></td></tr></table></figure>

<p>同步成功，在hadoop103中可以找到被删除的wcinput</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ ll</span><br><span class="line">总用量 176</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu    183 9月  12 2019 bin</span><br><span class="line">drwxr-xr-x. 3 atguigu atguigu     20 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu    106 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 atguigu atguigu     20 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 atguigu atguigu    288 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 147145 9月   4 2019 LICENSE.txt</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu   1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 atguigu atguigu   4096 9月  12 2019 sbin</span><br><span class="line">drwxr-xr-x. 4 atguigu atguigu     31 9月  12 2019 share</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu     22 8月   7 10:36 wcinput</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu     88 8月   7 10:45 wcoutput</span><br></pre></td></tr></table></figure>

<h5 id="3）xsync集群分发脚本"><a href="#3）xsync集群分发脚本" class="headerlink" title="3）xsync集群分发脚本"></a><strong>3）xsync集群分发脚本</strong></h5><p>①需求：循环复制文件到所有服务器节点的相同目录下（比如我在hadoop101节点的根目录下新建了一个文件，执行xsync命令后，将该文件复制到hadoop102和hadoop103节点的根目录下）</p>
<p>②需求分析</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">rsync命令原始拷贝</span> </span><br><span class="line">rsync -av /opt/module   atguigu@hadoop103:/opt/</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">期望脚本</span></span><br><span class="line">xsync 要同步的文件名称</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">期望脚本在任何路径都能使用（脚本放在声明了全局环境变量的路径）</span></span><br><span class="line">[atguigu@hadoop102 ~]$ echo $PATH</span><br><span class="line">/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/module/jdk1.8.0_212/bin:/opt/module/hadoop-3.1.3/bin:/opt/module/hadoop-3.1.3/sbin:/root/bin:/opt/module/jdk1.8.0_212/bin:/opt/module/hadoop-3.1.3/bin:/opt/module/hadoop-3.1.3/sbin</span><br></pre></td></tr></table></figure>

<p>③脚本实现</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在/bin目录下创建xsync文件</span></span><br><span class="line">[root@hadoop102 ~]$ cd /bin</span><br><span class="line">[root@hadoop102 bin]$ vim xsync</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在文件中编写如下代码</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1.判断参数格式</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo Not Enough Arguement!</span><br><span class="line">    exit;</span><br><span class="line">fi</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2.遍历集群所有机器</span></span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">    echo ============ $host ==============</span><br><span class="line">    # 3.遍历所有目录，挨个发送</span><br><span class="line"></span><br><span class="line">    for file in $@</span><br><span class="line">    do</span><br><span class="line">       # 4.判断文件是否存在</span><br><span class="line">       if [ -e $file ]</span><br><span class="line">           then</span><br><span class="line">               # 5.获取父目录</span><br><span class="line">               pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line">               # 6.获取当前文件的名称</span><br><span class="line">               fname=$(basename $file)</span><br><span class="line">               ssh $host &quot;mkdir -P $pdir&quot;</span><br><span class="line">               rsync -av $pdir/$fname $host:$pdir</span><br><span class="line">           else</span><br><span class="line">               echo $file does not exists!</span><br><span class="line">       fi</span><br><span class="line">    done</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改脚本文件xsync，使其具有执行权限</span></span><br><span class="line">[root@hadoop102 bin]$ chmod 777 xsync</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用脚本文件xsync进行集群分发测试，将hadoop102节点下的 /bin目录 复制到hadoop103和hadoop104中</span></span><br><span class="line">[root@hadoop102 bin]# su atguigu</span><br><span class="line">[atguigu@hadoop102 bin]$ xsync /bin</span><br><span class="line">============ hadoop102 ==============</span><br><span class="line">The authenticity of host &#x27;hadoop102 (192.168.255.102)&#x27; can&#x27;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:U30fwJUGD+M1SYtW9uchyBhmZZcfSFfbYtFZCKRI0b4.</span><br><span class="line">ECDSA key fingerprint is MD5:40:4d:49:d3:f0:83:12:0a:af:13:d6:9d:de:61:1f:77.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added &#x27;hadoop102,192.168.255.102&#x27; (ECDSA) to the list of known hosts.</span><br><span class="line">atguigu@hadoop102&#x27;s password: </span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">atguigu@hadoop102&#x27;s password: </span><br><span class="line">sending incremental file list</span><br><span class="line"></span><br><span class="line">sent 49 bytes  received 12 bytes  17.43 bytes/sec</span><br><span class="line">total size is 7  speedup is 0.11</span><br><span class="line">============ hadoop103 ==============</span><br><span class="line">atguigu@hadoop103&#x27;s password: </span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">atguigu@hadoop103&#x27;s password: </span><br><span class="line">sending incremental file list</span><br><span class="line"></span><br><span class="line">sent 49 bytes  received 12 bytes  17.43 bytes/sec</span><br><span class="line">total size is 7  speedup is 0.11</span><br><span class="line">============ hadoop104 ==============</span><br><span class="line">atguigu@hadoop104&#x27;s password: </span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">atguigu@hadoop104&#x27;s password: </span><br><span class="line">sending incremental file list</span><br><span class="line"></span><br><span class="line">sent 49 bytes  received 12 bytes  24.40 bytes/sec</span><br><span class="line">total size is 7  speedup is 0.11</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下面正式分发环境变量，由于会出现环境拒绝，所以要在命令前加上sudo：</span></span><br><span class="line">[atguigu@hadoop102 bin]$ sudo xsync /etc/profile.d/my_env.sh </span><br><span class="line">============ hadoop102 ==============</span><br><span class="line">root@hadoop102&#x27;s password: </span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">root@hadoop102&#x27;s password: </span><br><span class="line">sending incremental file list</span><br><span class="line"></span><br><span class="line">sent 48 bytes  received 12 bytes  17.14 bytes/sec</span><br><span class="line">total size is 215  speedup is 3.58</span><br><span class="line">============ hadoop103 ==============</span><br><span class="line">root@hadoop103&#x27;s password: </span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">root@hadoop103&#x27;s password: </span><br><span class="line">sending incremental file list</span><br><span class="line">my_env.sh</span><br><span class="line"></span><br><span class="line">sent 310 bytes  received 35 bytes  98.57 bytes/sec</span><br><span class="line">total size is 215  speedup is 0.62</span><br><span class="line">============ hadoop104 ==============</span><br><span class="line">root@hadoop104&#x27;s password: </span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">root@hadoop104&#x27;s password: </span><br><span class="line">sending incremental file list</span><br><span class="line">my_env.sh</span><br><span class="line"></span><br><span class="line">sent 310 bytes  received 35 bytes  98.57 bytes/sec</span><br><span class="line">total size is 215  speedup is 0.62</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">目前为止，所有的服务器上都成功安装了java和hadoop（要在atguigu用户下查看）</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hadoop102</span></span><br><span class="line">[atguigu@hadoop102 ~]$ java -version</span><br><span class="line">java version &quot;1.8.0_212&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_212-b10)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)</span><br><span class="line">[atguigu@hadoop102 ~]$ hadoop version</span><br><span class="line">Hadoop 3.1.3</span><br><span class="line">Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579</span><br><span class="line">Compiled by ztang on 2019-09-12T02:47Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum ec785077c385118ac91aadde5ec9799</span><br><span class="line">This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hadoop103</span></span><br><span class="line">[atguigu@hadoop103 ~]$ java -version</span><br><span class="line">java version &quot;1.8.0_212&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_212-b10)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)</span><br><span class="line">[atguigu@hadoop103 ~]$ hadoop version</span><br><span class="line">Hadoop 3.1.3</span><br><span class="line">Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579</span><br><span class="line">Compiled by ztang on 2019-09-12T02:47Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum ec785077c385118ac91aadde5ec9799</span><br><span class="line">This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hadoop104</span></span><br><span class="line">[atguigu@hadoop104 root]$ java -version</span><br><span class="line">java version &quot;1.8.0_212&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_212-b10)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)</span><br><span class="line">[atguigu@hadoop104 root]$ hadoop version</span><br><span class="line">Hadoop 3.1.3</span><br><span class="line">Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579</span><br><span class="line">Compiled by ztang on 2019-09-12T02:47Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum ec785077c385118ac91aadde5ec9799</span><br><span class="line">This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar</span><br></pre></td></tr></table></figure>

<h5 id="4）集群命令同时执行脚本"><a href="#4）集群命令同时执行脚本" class="headerlink" title="4）集群命令同时执行脚本"></a><strong>4）集群命令同时执行脚本</strong></h5><p>在启动集群后，用户需要使用jps命令查看各节点服务器进程的启动情况，操作起来比较麻烦，所以我们编写一个集群命令同时执行脚本，达到使用一个脚本查看所有节点上的所有进程的目的。使用该脚本，还可以执行一些需要同时在集群不同节点上运行的命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在/bin目录下创建xsync文件</span></span><br><span class="line">[root@hadoop102 ~]$ cd /bin</span><br><span class="line">[root@hadoop102 bin]$ vim xcall.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在文件中编写如下代码</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">  echo -----------$i---------------</span><br><span class="line">  ssh $i &quot;$*&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改脚本文件，使其具有执行权限</span></span><br><span class="line">[root@hadoop102 bin]# chmod 777 xcall.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动xcall.sh脚本</span></span><br><span class="line">[root@hadoop102 bin]# su atguigu</span><br><span class="line">[atguigu@hadoop102 ~]$ xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">atguigu@hadoop102&#x27;s password: </span><br><span class="line">5290 Jps</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">atguigu@hadoop103&#x27;s password: </span><br><span class="line">4580 Jps</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">atguigu@hadoop104&#x27;s password: </span><br><span class="line">4322 Jps</span><br></pre></td></tr></table></figure>

<h4 id="3-2-3-SSH免密登录"><a href="#3-2-3-SSH免密登录" class="headerlink" title="3.2.3 SSH免密登录"></a>3.2.3 SSH免密登录</h4><p>①有密登录（需要输入密码）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ ssh hadoop103</span><br><span class="line">atguigu@hadoop103&#x27;s password: </span><br><span class="line">Last login: Thu Sep 14 11:17:37 2023</span><br><span class="line">[atguigu@hadoop103 ~]$ </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">退回到hadoop102</span></span><br><span class="line">[atguigu@hadoop103 ~]$ exit</span><br><span class="line">登出</span><br><span class="line">Connection to hadoop103 closed.</span><br><span class="line">[atguigu@hadoop102 ~]$ </span><br></pre></td></tr></table></figure>

<p>②免密登录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hadoop102可以免密登录hadoop103和hadoop104</span></span><br><span class="line">[atguigu@hadoop102 ~]$ cd .ssh/</span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa # 按三次回车</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/home/atguigu/.ssh/id_rsa): </span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /home/atguigu/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /home/atguigu/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:oGyOphAWb7dzBrQcfHLKMRNjhgt6WyVB7o9Qrq77LrM atguigu@hadoop102</span><br><span class="line">The key&#x27;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">|   .+*           |</span><br><span class="line">| . o= +          |</span><br><span class="line">|... +@.o         |</span><br><span class="line">|. +*=.@.         |</span><br><span class="line">|.o.=*B  S        |</span><br><span class="line">|..o*.oo          |</span><br><span class="line">|. + oo.o         |</span><br><span class="line">|++    +          |</span><br><span class="line">|EO+              |</span><br><span class="line">+----[SHA256]-----+</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将hadoop102上的公钥拷贝到hadoop103和hadoop104上</span></span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/home/atguigu/.ssh/id_rsa.pub&quot;</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys</span><br><span class="line">atguigu@hadoop103&#x27;s password: </span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#x27;hadoop103&#x27;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">现在可以免密登录hadoop103了</span></span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh hadoop103</span><br><span class="line">Last login: Thu Sep 14 17:46:12 2023 from hadoop102</span><br><span class="line">[atguigu@hadoop103 ~]$ </span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/home/atguigu/.ssh/id_rsa.pub&quot;</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys</span><br><span class="line">atguigu@hadoop104&#x27;s password: </span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#x27;hadoop104&#x27;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">现在可以免密登录hadoop104了</span></span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh hadoop104</span><br><span class="line">Last login: Thu Sep 14 11:21:54 2023</span><br><span class="line">[atguigu@hadoop104 ~]$ </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">当然了对自己hadoop102也需要配置一次</span></span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/home/atguigu/.ssh/id_rsa.pub&quot;</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys</span><br><span class="line">atguigu@hadoop102&#x27;s password: </span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#x27;hadoop102&#x27;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh hadoop102</span><br><span class="line">Last login: Thu Sep 14 18:07:11 2023</span><br><span class="line">[atguigu@hadoop102 ~]$ </span><br></pre></td></tr></table></figure>

<p>同理，对hadoop103和hadoop104做同样的操作，使得这三个服务器之间的atguigu用户可以互相免密登录。</p>
<p>再将hadoop102上的root用户配置对103和104的免密登录。</p>
<h4 id="3-2-4-集群配置"><a href="#3-2-4-集群配置" class="headerlink" title="3.2.4 集群配置"></a>3.2.4 集群配置</h4><h5 id="①集群部署规划"><a href="#①集群部署规划" class="headerlink" title="①集群部署规划"></a>①集群部署规划</h5><p>HDFS主要角色：NameNode，DataNode，SecindaryNameNode和Client。DataNode主要负责数据的存储工作，需要在每一台节点服务器上部署，SecindaryNameNode主要负责在集群遇到故障时候，协助NameNode进行故障恢复，所以SecindaryNameNode和NameNode不要安装在同一台服务器。</p>
<p>YARN主要角色：ResourceManager和NodeManager。NodeManager是单个节点服务器上的资源和任务管理器，需要在每一台节点服务器上部署，ResourceManager主要负责集群整体的资源调度工作，非常消耗内存，所以不要将其与同样消耗内存的NameNode配置在同一个节点服务器上。</p>
<table>
<thead>
<tr>
<th>节点服务器</th>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td>NameNode<br>DateNode</td>
<td>DateNode</td>
<td>DateNode<br>SecindaryNameNode</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>NodeManager<br>ResourceManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<h5 id="②配置文件说明"><a href="#②配置文件说明" class="headerlink" title="②配置文件说明"></a>②配置文件说明</h5><p>Hadoop配置文件有两类：<strong>默认配置文件</strong>和<strong>自定义配置文件</strong>，默认配置文件是Hadoop源码中自带的，提供了所有参数的默认值，如果用户想要修改默认值，则需要在<strong>自定义配置文件</strong>中修改。<strong>自定义配置文件的优先级高于默认配置文件</strong>。</p>
<img src="Snipaste_2023-09-14_18-47-42.png" alt="Snipaste_2023-09-14_18-47-42" style="zoom:50%;">

<p>自定义配置文件：<strong>core-site.xml、</strong>hdfs-site.xml<strong>、</strong>yarn-site.xml<strong>、</strong>mapred-site.xml** 四个配置文件存放在</p>
<p>$HADOOP_HOME&#x2F;etc&#x2F;hadoop 这个路径上，用户可以根据项目需求重新进行修改配置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cd /opt/module/hadoop-3.1.3/</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ ll</span><br><span class="line">总用量 176</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     183 9月  12 2019 bin</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     106 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh     288 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh  147145 9月   4 2019 LICENSE.txt</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh   21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh    1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh    4096 9月  12 2019 sbin</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh      31 9月  12 2019 share</span><br><span class="line">drwxr-xr-x. 2 root root     22 8月   7 10:36 wcinput</span><br><span class="line">drwxr-xr-x. 2 root root     88 8月   7 10:45 wcoutput</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ cd etc/hadoop</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">就可以找到四个自定义配置文件了</span></span><br><span class="line">[atguigu@hadoop102 hadoop]$ ll</span><br><span class="line">总用量 172</span><br><span class="line">-rw-r--r--. 1 wyh wyh  8260 9月  12 2019 capacity-scheduler.xml</span><br><span class="line">-rw-r--r--. 1 wyh wyh  1335 9月  12 2019 configuration.xsl</span><br><span class="line">-rw-r--r--. 1 wyh wyh  1940 9月  12 2019 container-executor.cfg</span><br><span class="line">-rw-r--r--. 1 wyh wyh   774 9月  12 2019 core-site.xml  # 自定义配置文件</span><br><span class="line">-rw-r--r--. 1 wyh wyh  3999 9月  12 2019 hadoop-env.cmd</span><br><span class="line">-rw-r--r--. 1 wyh wyh 15903 9月  12 2019 hadoop-env.sh</span><br><span class="line">-rw-r--r--. 1 wyh wyh  3323 9月  12 2019 hadoop-metrics2.properties</span><br><span class="line">-rw-r--r--. 1 wyh wyh 11392 9月  12 2019 hadoop-policy.xml</span><br><span class="line">-rw-r--r--. 1 wyh wyh  3414 9月  12 2019 hadoop-user-functions.sh.example</span><br><span class="line">-rw-r--r--. 1 wyh wyh   775 9月  12 2019 hdfs-site.xml  # 自定义配置文件</span><br><span class="line">-rw-r--r--. 1 wyh wyh  1484 9月  12 2019 httpfs-env.sh</span><br><span class="line">-rw-r--r--. 1 wyh wyh  1657 9月  12 2019 httpfs-log4j.properties</span><br><span class="line">-rw-r--r--. 1 wyh wyh    21 9月  12 2019 httpfs-signature.secret</span><br><span class="line">-rw-r--r--. 1 wyh wyh   620 9月  12 2019 httpfs-site.xml</span><br><span class="line">-rw-r--r--. 1 wyh wyh  3518 9月  12 2019 kms-acls.xml</span><br><span class="line">-rw-r--r--. 1 wyh wyh  1351 9月  12 2019 kms-env.sh</span><br><span class="line">-rw-r--r--. 1 wyh wyh  1747 9月  12 2019 kms-log4j.properties</span><br><span class="line">-rw-r--r--. 1 wyh wyh   682 9月  12 2019 kms-site.xml</span><br><span class="line">-rw-r--r--. 1 wyh wyh 13326 9月  12 2019 log4j.properties</span><br><span class="line">-rw-r--r--. 1 wyh wyh   951 9月  12 2019 mapred-env.cmd</span><br><span class="line">-rw-r--r--. 1 wyh wyh  1764 9月  12 2019 mapred-env.sh</span><br><span class="line">-rw-r--r--. 1 wyh wyh  4113 9月  12 2019 mapred-queues.xml.template</span><br><span class="line">-rw-r--r--. 1 wyh wyh   758 9月  12 2019 mapred-site.xml  # 自定义配置文件</span><br><span class="line">drwxr-xr-x. 2 wyh wyh    24 9月  12 2019 shellprofile.d</span><br><span class="line">-rw-r--r--. 1 wyh wyh  2316 9月  12 2019 ssl-client.xml.example</span><br><span class="line">-rw-r--r--. 1 wyh wyh  2697 9月  12 2019 ssl-server.xml.example</span><br><span class="line">-rw-r--r--. 1 wyh wyh  2642 9月  12 2019 user_ec_policies.xml.template</span><br><span class="line">-rw-r--r--. 1 wyh wyh    10 9月  12 2019 workers</span><br><span class="line">-rw-r--r--. 1 wyh wyh  2250 9月  12 2019 yarn-env.cmd</span><br><span class="line">-rw-r--r--. 1 wyh wyh  6056 9月  12 2019 yarn-env.sh</span><br><span class="line">-rw-r--r--. 1 wyh wyh  2591 9月  12 2019 yarnservice-log4j.properties</span><br><span class="line">-rw-r--r--. 1 wyh wyh   690 9月  12 2019 yarn-site.xml  # 自定义配置文件</span><br></pre></td></tr></table></figure>

<h5 id="③配置集群"><a href="#③配置集群" class="headerlink" title="③配置集群"></a>③配置集群</h5><p>1）配置core-site.xml文件</p>
<p>主要用于将分布式文件系统HDFS的NameNode的<strong>入口地址</strong>和分布式文件系统中的<strong>数据</strong> 存储于服务器本地磁盘中进行配置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找到core-site.xml文件</span></span><br><span class="line">[atguigu@hadoop102 hadoop]$ sudo vim core-site.xml </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改配置如下</span></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">     &lt;!-- 指定 NameNode 的地址 --&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">     &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;hdfs://hadoop102:8020&lt;/value&gt;</span><br><span class="line">     &lt;/property&gt;</span><br><span class="line">     &lt;!-- 指定 hadoop 数据的存储目录 --&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">     &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;/opt/module/hadoop-3.1.3/data&lt;/value&gt;</span><br><span class="line">     &lt;/property&gt;</span><br><span class="line">     &lt;!-- 配置 HDFS 网页登录使用的静态用户为 atguigu --&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">     &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;atguigu&lt;/value&gt;</span><br><span class="line">     &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>2）配置hdfs-site.xml文件</p>
<p>我们主要对HDFS的属性进行配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找到hdfs-site.xml文件</span></span><br><span class="line">[atguigu@hadoop102 hadoop]$ sudo vim hdfs-site.xml </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改配置如下</span></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- nn web 端访问地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;hadoop102:9870&lt;/value&gt;</span><br><span class="line">     &lt;/property&gt;</span><br><span class="line">    &lt;!-- 2nn web 端访问地址--&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;hadoop104:9868&lt;/value&gt;</span><br><span class="line">     &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>3）配置yarn-site.xml文件（<strong>注意：这里的配置文件按照书中的来，不按照视频的来</strong>）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找到yarn-site.xml文件</span></span><br><span class="line">[atguigu@hadoop102 hadoop]$ sudo vim yarn-site.xml</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改配置如下</span></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定 MR 走 shuffle --&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;!-- 指定 ResourceManager 的地址--&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;hadoop103&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;!-- 环境变量的继承 --&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">&lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CO</span><br><span class="line">NF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAP</span><br><span class="line">RED_HOME&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.application.classpath&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop:/opt/module/hadoop-3.1.3/share/hadoop/common/lib/*:/opt/module/hadoop-3.1.3/share/hadoop/common/*:/opt/module/hadoop-3.1.3/share/hadoop/hdfs:/opt/module/hadoop-3.1.3/share/hadoop/hdfs/lib/*:/opt/module/hadoop-3.1.3/share/hadoop/hdfs/*:/opt/module/hadoop-3.1.3/share/hadoop/mapreduce/lib/*:/opt/module/hadoop-3.1.3/share/hadoop/mapreduce/*:/opt/module/hadoop-3.1.3/share/hadoop/yarn:/opt/module/hadoop-3.1.3/share/hadoop/yarn/lib/*:/opt/module/hadoop-3.1.3/share/hadoop/yarn/*&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>4）配置mapred-site.xml文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找到mapred-site.xml文件</span></span><br><span class="line">[atguigu@hadoop102 hadoop]$ sudo vim mapred-site.xml</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改配置如下</span></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定 MapReduce 程序运行在 Yarn 上 --&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line"> &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h5 id="④在集群上分发配置好的Hadoop配置文件"><a href="#④在集群上分发配置好的Hadoop配置文件" class="headerlink" title="④在集群上分发配置好的Hadoop配置文件"></a>④在集群上分发配置好的Hadoop配置文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 etc]$ xsync hadoop/</span><br><span class="line">============ hadoop102 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line"></span><br><span class="line">sent 887 bytes  received 18 bytes  603.33 bytes/sec</span><br><span class="line">total size is 107,531  speedup is 118.82</span><br><span class="line">============ hadoop103 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line">hadoop/</span><br><span class="line">hadoop/core-site.xml</span><br><span class="line">hadoop/hdfs-site.xml</span><br><span class="line">hadoop/mapred-site.xml</span><br><span class="line">hadoop/workers</span><br><span class="line">hadoop/yarn-site.xml</span><br><span class="line"></span><br><span class="line">sent 3,310 bytes  received 164 bytes  6,948.00 bytes/sec</span><br><span class="line">total size is 107,531  speedup is 30.95</span><br><span class="line">============ hadoop104 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line">hadoop/</span><br><span class="line">hadoop/capacity-scheduler.xml</span><br><span class="line">hadoop/configuration.xsl</span><br><span class="line">hadoop/container-executor.cfg</span><br><span class="line">hadoop/core-site.xml</span><br><span class="line">hadoop/hadoop-env.cmd</span><br><span class="line">hadoop/hadoop-env.sh</span><br><span class="line">hadoop/hadoop-metrics2.properties</span><br><span class="line">hadoop/hadoop-policy.xml</span><br><span class="line">hadoop/hadoop-user-functions.sh.example</span><br><span class="line">hadoop/hdfs-site.xml</span><br><span class="line">hadoop/httpfs-env.sh</span><br><span class="line">hadoop/httpfs-log4j.properties</span><br><span class="line">hadoop/httpfs-signature.secret</span><br><span class="line">hadoop/httpfs-site.xml</span><br><span class="line">hadoop/kms-acls.xml</span><br><span class="line">hadoop/kms-env.sh</span><br><span class="line">hadoop/kms-log4j.properties</span><br><span class="line">hadoop/kms-site.xml</span><br><span class="line">hadoop/log4j.properties</span><br><span class="line">hadoop/mapred-env.cmd</span><br><span class="line">hadoop/mapred-env.sh</span><br><span class="line">hadoop/mapred-queues.xml.template</span><br><span class="line">hadoop/mapred-site.xml</span><br><span class="line">hadoop/ssl-client.xml.example</span><br><span class="line">hadoop/ssl-server.xml.example</span><br><span class="line">hadoop/user_ec_policies.xml.template</span><br><span class="line">hadoop/workers</span><br><span class="line">hadoop/yarn-env.cmd</span><br><span class="line">hadoop/yarn-env.sh</span><br><span class="line">hadoop/yarn-site.xml</span><br><span class="line">hadoop/yarnservice-log4j.properties</span><br><span class="line">hadoop/shellprofile.d/</span><br><span class="line">hadoop/shellprofile.d/example.sh</span><br><span class="line"></span><br><span class="line">sent 5,002 bytes  received 1,638 bytes  4,426.67 bytes/sec</span><br><span class="line">total size is 107,531  speedup is 16.19</span><br></pre></td></tr></table></figure>

<h5 id="⑤配置workers文件"><a href="#⑤配置workers文件" class="headerlink" title="⑤配置workers文件"></a>⑤配置workers文件</h5><p>主节点服务器NameNode和ResourceManager的角色已经在配置文件中进行了配置，下面配置从节点服务器的角色。配置文件workers主要用于配置Hadoop分布式集群中各个从节点服务器的角色。对workers文件进行配置，将3台服务器全部指定为从节点服务器，启动DataNode和NodeManager进程。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找到workers文件</span></span><br><span class="line">[atguigu@hadoop102 hadoop]$ sudo vim workers</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">增加以下内容</span></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">分发给103和104</span></span><br><span class="line">[atguigu@hadoop102 hadoop]$ xsync workers </span><br><span class="line">============ hadoop102 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line"></span><br><span class="line">sent 56 bytes  received 12 bytes  45.33 bytes/sec</span><br><span class="line">total size is 30  speedup is 0.44</span><br><span class="line">============ hadoop103 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line">workers</span><br><span class="line"></span><br><span class="line">sent 133 bytes  received 41 bytes  348.00 bytes/sec</span><br><span class="line">total size is 30  speedup is 0.17</span><br><span class="line">============ hadoop104 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line">workers</span><br><span class="line"></span><br><span class="line">sent 133 bytes  received 41 bytes  116.00 bytes/sec</span><br><span class="line">total size is 30  speedup is 0.17</span><br></pre></td></tr></table></figure>

<h5 id="⑥启动集群"><a href="#⑥启动集群" class="headerlink" title="⑥启动集群"></a>⑥启动集群</h5><h6 id="1）在hadoop102节点格式化NameNode"><a href="#1）在hadoop102节点格式化NameNode" class="headerlink" title="1）在hadoop102节点格式化NameNode"></a>1）在hadoop102节点格式化NameNode</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">调整为root用户，否则权限不够</span></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ su root</span><br><span class="line">密码：</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hdfs namenode -format</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可以看到data文件和logs文件</span></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ ll</span><br><span class="line">总用量 176</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     183 9月  12 2019 bin</span><br><span class="line">drwxr-xr-x. 3 root root     17 9月  14 22:01 data  # 新增的文件</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     106 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh     288 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh  147145 9月   4 2019 LICENSE.txt</span><br><span class="line">drwxr-xr-x. 2 root root     37 9月  14 22:01 logs  # 新增的文件</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh   21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh    1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh    4096 9月  12 2019 sbin</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh      31 9月  12 2019 share</span><br><span class="line">drwxr-xr-x. 2 root root     22 8月   7 10:36 wcinput</span><br><span class="line">drwxr-xr-x. 2 root root     88 8月   7 10:45 wcoutput</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进一步查看data-dfs-name-current-VERSION</span></span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ cd data/</span><br><span class="line">[atguigu@hadoop102 data]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 3 root root 18 9月  14 22:01 dfs</span><br><span class="line">[atguigu@hadoop102 data]$ cd dfs/</span><br><span class="line">[atguigu@hadoop102 dfs]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 3 root root 21 9月  14 22:01 name</span><br><span class="line">[atguigu@hadoop102 dfs]$ cd name/</span><br><span class="line">[atguigu@hadoop102 name]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 2 root root 112 9月  14 22:01 current</span><br><span class="line">[atguigu@hadoop102 name]$ cd current/</span><br><span class="line">[atguigu@hadoop102 current]$ ll</span><br><span class="line">总用量 16</span><br><span class="line">-rw-r--r--. 1 root root 391 9月  14 22:01 fsimage_0000000000000000000</span><br><span class="line">-rw-r--r--. 1 root root  62 9月  14 22:01 fsimage_0000000000000000000.md5</span><br><span class="line">-rw-r--r--. 1 root root   2 9月  14 22:01 seen_txid</span><br><span class="line">-rw-r--r--. 1 root root 220 9月  14 22:01 VERSION</span><br><span class="line">[atguigu@hadoop102 current]$ vim VERSION </span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-14_22-08-06.png" alt="Snipaste_2023-09-14_22-08-06" style="zoom:50%;">

<p>可以查看到服务器的namespaceID</p>
<h6 id="2）启动HDFS"><a href="#2）启动HDFS" class="headerlink" title="2）启动HDFS"></a>2）启动HDFS</h6><p>在格式化NameNode后，执行start-dfs.sh命令启动HDFS，即可同时启动所有的DataNode和SecondaryNameNode</p>
<p>之前要修改如下文件，否则启动失败（在102，103，104上都修改）：</p>
<img src="Snipaste_2023-09-14_22-35-15.png" alt="Snipaste_2023-09-14_22-35-15" style="zoom:43%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">换成root用户</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# sbin/start-dfs.sh</span><br><span class="line">WARNING: HADOOP_SECURE_DN_USER has been replaced by HDFS_DATANODE_SECURE_USER. Using value of HADOOP_SECURE_DN_USER.</span><br><span class="line">Starting namenodes on [hadoop102]</span><br><span class="line">上一次登录：四 9月 14 22:30:10 CST 2023pts/0 上</span><br><span class="line">Starting datanodes</span><br><span class="line">上一次登录：四 9月 14 22:32:59 CST 2023pts/0 上</span><br><span class="line">Starting secondary namenodes [hadoop104]</span><br><span class="line">上一次登录：四 9月 14 22:33:01 CST 2023pts/0 上</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">13140 DataNode</span><br><span class="line">13476 Jps</span><br><span class="line">12959 NameNode</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">8524 DataNode</span><br><span class="line">9052 Jps</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">8118 SecondaryNameNode</span><br><span class="line">8921 Jps</span><br><span class="line">8013 DataNode</span><br></pre></td></tr></table></figure>

<p>可以看到所有服务器的HDFS都成功启动：</p>
<img src="Snipaste_2023-09-14_22-36-43.png" alt="Snipaste_2023-09-14_22-36-43" style="zoom:43%;">

<p>（亲尝：此时关机后HDFS会关闭，再次 [root用户下]cd &#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;———-&gt;sbin&#x2F;start-dfs.sh———-&gt;xcall.sh jps 即可启动三台服务器的HDFS并查看启动情况，YARN同理）</p>
<h6 id="3）启动YARN"><a href="#3）启动YARN" class="headerlink" title="3）启动YARN"></a>3）启动YARN</h6><p>通过执行start-yarn.sh命令启动YARN，可以同时启动ResourceManager和所有NodeManager，需要注意，因为NameNode和ResourceManager不在同一个服务器节点上，所以<strong>必须在hadoop103上启动YARN（即在ResourceManager所在的节点上）</strong>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 hadoop-3.1.3]# sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">之后我们在102上查看所有节点服务器的进程情况，可以发现与最初规划的表格内容一致</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">2900 NameNode</span><br><span class="line">3086 DataNode</span><br><span class="line">4158 Jps</span><br><span class="line">3999 NodeManager</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">2723 DataNode</span><br><span class="line">4595 NodeManager</span><br><span class="line">4822 Jps</span><br><span class="line">3325 ResourceManager</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">2849 SecondaryNameNode</span><br><span class="line">2723 DataNode</span><br><span class="line">3619 NodeManager</span><br><span class="line">3783 Jps</span><br></pre></td></tr></table></figure>

<h6 id="4）NameNode的Web端"><a href="#4）NameNode的Web端" class="headerlink" title="4）NameNode的Web端"></a>4）NameNode的Web端</h6><p>输入”<a target="_blank" rel="noopener" href="http://hadoop102:9870/">http://hadoop102:9870</a>“</p>
<img src="Snipaste_2023-09-15_11-10-08.png" alt="Snipaste_2023-09-15_11-10-08" style="zoom: 33%;">

<h6 id="5）YARN的Web端"><a href="#5）YARN的Web端" class="headerlink" title="5）YARN的Web端"></a>5）YARN的Web端</h6><p>输入”<a target="_blank" rel="noopener" href="http://hadoop103:8088/">http://hadoop103:8088</a>“</p>
<img src="Snipaste_2023-09-15_11-13-41.png" alt="Snipaste_2023-09-15_11-13-41" style="zoom: 33%;">

<h5 id="⑦集群基本测试"><a href="#⑦集群基本测试" class="headerlink" title="⑦集群基本测试"></a>⑦集群基本测试</h5><p>测试Hadoop的基本功能：数据存储和数据计算，测试主要围绕文件上传、文件下载和简单计算三方面展开</p>
<h6 id="1）文件上传测试"><a href="#1）文件上传测试" class="headerlink" title="1）文件上传测试"></a>1）文件上传测试</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">小文件上传</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在hadoop集群中的根目录下创建一个文件夹input，这里涉及到了Hadoop的shell命令，后续介绍</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -mkdir /input</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可以看到在Web端监视到了我们在集群中创建的文件夹input</span></span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-15_11-23-16.png" alt="Snipaste_2023-09-15_11-23-16" style="zoom:43%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">再将服务器本地的word.txt文件上传至集群的input文件夹中</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -put wcinput/word.txt /input</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在Web端根目录下的input文件夹下我们看到了文件word.txt</span></span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-15_11-28-33.png" alt="Snipaste_2023-09-15_11-28-33" style="zoom:33%;">

<p>点击即可查看文件内容，也可以下载</p>
<img src="Snipaste_2023-09-15_11-30-37.png" alt="Snipaste_2023-09-15_11-30-37" style="zoom:33%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">大文件上传测试，上传JDK安装包到Hadoop的根目录下</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -put wcinput/word.txt /input</span><br></pre></td></tr></table></figure>

<p>可以查看到</p>
<img src="Snipaste_2023-09-15_11-34-32.png" alt="Snipaste_2023-09-15_11-34-32" style="zoom: 50%;">

<hr>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">上传文件的存储位置</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行以下命令，进入HDFS文件的存储路径，查看路径下的文件列表</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">其中blk_************文件就是存储于Hadoop中的数据块文件（文件在HDFS中是分块存储的）</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# cd /opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1662165003-192.168.255.102-1694700063570/current/finalized/subdir0/subdir0/</span><br><span class="line">[root@hadoop102 subdir0]# ll</span><br><span class="line">总用量 191944</span><br><span class="line">-rw-r--r--. 1 root root        97 9月  15 11:27 blk_1073741825</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  15 11:27 blk_1073741825_1001.meta</span><br><span class="line">-rw-r--r--. 1 root root 134217728 9月  15 11:33 blk_1073741826</span><br><span class="line">-rw-r--r--. 1 root root   1048583 9月  15 11:33 blk_1073741826_1002.meta</span><br><span class="line">-rw-r--r--. 1 root root  60795424 9月  15 11:33 blk_1073741827</span><br><span class="line">-rw-r--r--. 1 root root    474975 9月  15 11:33 blk_1073741827_1003.meta</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用<span class="built_in">cat</span>命令可以查看HDFS在磁盘中存储的文件内容，较小的数据块blk_1073741825即之前上传的word.txt文件</span></span><br><span class="line">[root@hadoop102 subdir0]# cat blk_1073741825</span><br><span class="line">hadoop HDFS</span><br><span class="line">hadoop mapreduce</span><br><span class="line">hadoop yarn</span><br><span class="line">spark core</span><br><span class="line">spark MLlib</span><br><span class="line">spark RDD</span><br><span class="line">flink core</span><br><span class="line">flink alink</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">我们还可以看到两块较大的数据块blk_1073741826和blk_1073741827，即之前上传的JDK安装包，由于JDK安装包体积较大，因此HDFS将其切分成了两个数据块进行存储（何种标准切分，后续介绍）</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将数据块blk_1073741826和blk_1073741827重新拼接成一个压缩包，并将拼接的压缩包解压缩，即可得到原来的JDK安装包</span></span><br><span class="line">[root@hadoop102 subdir0]# cat blk_1073741826&gt;&gt;tmp.tar.gz</span><br><span class="line">[root@hadoop102 subdir0]# cat blk_1073741827&gt;&gt;tmp.tar.gz</span><br><span class="line">[root@hadoop102 subdir0]# ll</span><br><span class="line">总用量 585160</span><br><span class="line">-rw-r--r--. 1 root root        97 9月  15 11:27 blk_1073741825</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  15 11:27 blk_1073741825_1001.meta</span><br><span class="line">-rw-r--r--. 1 root root 134217728 9月  15 11:33 blk_1073741826</span><br><span class="line">-rw-r--r--. 1 root root   1048583 9月  15 11:33 blk_1073741826_1002.meta</span><br><span class="line">-rw-r--r--. 1 root root  60795424 9月  15 11:33 blk_1073741827</span><br><span class="line">-rw-r--r--. 1 root root    474975 9月  15 11:33 blk_1073741827_1003.meta</span><br><span class="line">-rw-r--r--. 1 root root 195013152 9月  15 17:25 tmp.tar.gz  # 可以看到tmp.tar.gz</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将tmp.tar.gz解压缩</span></span><br><span class="line">[root@hadoop102 subdir0]# tar -zxvf tmp.tar.gz</span><br><span class="line">......</span><br><span class="line">[root@hadoop102 subdir0]# ll</span><br><span class="line">总用量 585160</span><br><span class="line">-rw-r--r--. 1 root root        97 9月  15 11:27 blk_1073741825</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  15 11:27 blk_1073741825_1001.meta</span><br><span class="line">-rw-r--r--. 1 root root 134217728 9月  15 11:33 blk_1073741826</span><br><span class="line">-rw-r--r--. 1 root root   1048583 9月  15 11:33 blk_1073741826_1002.meta</span><br><span class="line">-rw-r--r--. 1 root root  60795424 9月  15 11:33 blk_1073741827</span><br><span class="line">-rw-r--r--. 1 root root    474975 9月  15 11:33 blk_1073741827_1003.meta</span><br><span class="line">drwxr-xr-x. 7   10  143       245 4月   2 2019 jdk1.8.0_212 # 解压出一个jdk</span><br><span class="line">-rw-r--r--. 1 root root 195013152 9月  15 17:25 tmp.tar.gz</span><br></pre></td></tr></table></figure>

<h6 id="2）文件下载测试"><a href="#2）文件下载测试" class="headerlink" title="2）文件下载测试"></a>2）文件下载测试</h6><p>可以直接在Web端操作下载</p>
<h6 id="3）简单计算测试"><a href="#3）简单计算测试" class="headerlink" title="3）简单计算测试"></a>3）简单计算测试</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">运行官方提供的示例程序jar包中的WordCount程序</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure>

<p>从YARN的Web端可以看到任务执行成功</p>
<p><img src="Snipaste_2023-09-15_18-29-51.png" alt="Snipaste_2023-09-15_18-29-51"></p>
<p>从NameNode的Web端可以看到output文件夹</p>
<img src="Snipaste_2023-09-15_18-31-15.png" alt="Snipaste_2023-09-15_18-31-15" style="zoom:33%;">

<p>点击output文件夹可以看到结果：</p>
<img src="Snipaste_2023-09-15_18-34-16.png" alt="Snipaste_2023-09-15_18-34-16" style="zoom:33%;">

<p>当然，点击文件后面的垃圾桶按钮可以将其删除，已经设置好了root用户权限</p>
<h4 id="3-2-5-NameNode格式化问题（集群挂了如何解决）"><a href="#3-2-5-NameNode格式化问题（集群挂了如何解决）" class="headerlink" title="3.2.5 NameNode格式化问题（集群挂了如何解决）"></a>3.2.5 NameNode格式化问题（集群挂了如何解决）</h4><p>参考书，这里面不做演示</p>
<h4 id="3-2-6-配置历史服务器"><a href="#3-2-6-配置历史服务器" class="headerlink" title="3.2.6 配置历史服务器"></a>3.2.6 配置历史服务器</h4><p>在Hadoop集群上执行完一个计算任务后，所有的运行信息和相关日志都会被清除，用户无法回溯查看历史任务的运行情况。为此，我们需要配置历史服务器。配置历史服务器可以在任意一个节点服务器上，我们选择102。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤1：打开配置文件mapred-site.xml,添加以下配置</span></span><br><span class="line">[root@hadoop102 hadoop]# vim mapred-site.xml</span><br><span class="line">&lt;!-- 历史服务器端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;hadoop102:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 历史服务器 web 端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;hadoop102:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤2：分发配置文件至其他节点服务器中</span></span><br><span class="line">[root@hadoop102 hadoop]# xsync mapred-site.xml</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤3：在102节点上启动历史服务器</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# bin/mapred --daemon start historyserver</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤4：查看102节点的历史服务器是否启动</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# jps</span><br><span class="line">9681 NameNode</span><br><span class="line">13649 Jps</span><br><span class="line">9867 DataNode</span><br><span class="line">10445 NodeManager</span><br><span class="line">13534 JobHistoryServer</span><br></pre></td></tr></table></figure>

<p>最后，登录历史服务器Web端（”<a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory%22%EF%BC%89">http://hadoop102:19888/jobhistory&quot;）</a></p>
<p><img src="Snipaste_2023-09-15_19-35-15.png" alt="Snipaste_2023-09-15_19-35-15"></p>
<p><img src="Snipaste_2023-09-15_19-37-55.png" alt="Snipaste_2023-09-15_19-37-55"></p>
<p>参数：</p>
<p><img src="Snipaste_2023-09-15_19-38-39.png" alt="Snipaste_2023-09-15_19-38-39"></p>
<h4 id="3-2-7-配置日志聚集功能"><a href="#3-2-7-配置日志聚集功能" class="headerlink" title="3.2.7 配置日志聚集功能"></a>3.2.7 配置日志聚集功能</h4><p>在程序运行完毕后，运行程序的节点服务器上会产生一些本地的日志文件。为了方便查看程序运行情况，进行程序调试，可以在程序运行完毕后，将程序运行的日志信息上传至HDFS中。</p>
<img src="Snipaste_2023-09-15_19-42-31.png" alt="Snipaste_2023-09-15_19-42-31" style="zoom:50%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤1：配置yarn-site.xml</span></span><br><span class="line">[root@hadoop102 hadoop]# vim yarn-site.xml</span><br><span class="line">&lt;!-- 开启日志聚集功能 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置日志聚集服务器地址 --&gt;</span><br><span class="line">&lt;property&gt; </span><br><span class="line"> &lt;name&gt;yarn.log.server.url&lt;/name&gt; </span><br><span class="line"> &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置日志保留时间为 7 天 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤2：分发配置文件</span></span><br><span class="line">[root@hadoop102 hadoop]# xsync yarn-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤3：关闭NodeManager，ResourceManager和HistoryServer进程</span></span><br><span class="line">[root@hadoop103 hadoop-3.1.3]# sbin/stop-yarn.sh              # 注意是103</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# mapred --daemon stop historyserver </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤4：启动NodeManager，ResourceManager和HistoryServer进程</span></span><br><span class="line">[root@hadoop103 hadoop-3.1.3]# sbin/start-yarn.sh            # 注意是103</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# mapred --daemon start historyserver</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">成功</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">14448 JobHistoryServer</span><br><span class="line">9681 NameNode</span><br><span class="line">14529 Jps</span><br><span class="line">9867 DataNode</span><br><span class="line">14255 NodeManager</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">7265 DataNode</span><br><span class="line">10483 Jps</span><br><span class="line">9910 ResourceManager</span><br><span class="line">10251 NodeManager</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">6887 DataNode</span><br><span class="line">7003 SecondaryNameNode</span><br><span class="line">9438 Jps</span><br><span class="line">9279 NodeManager</span><br></pre></td></tr></table></figure>

<p>那么后续执行的程序就会有logs记录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重新运行WordCount程序，将输出文件夹改为/output2</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcountt /input /output2</span><br></pre></td></tr></table></figure>

<p>查看日志：</p>
<p>登录历史服务器Web端（”<a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory%22%EF%BC%89%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%E4%B8%A4%E6%9D%A1%E4%BB%BB%E5%8A%A1%E5%88%97%E8%A1%A8%EF%BC%8C%E7%82%B9%E5%87%BB%E7%AC%AC%E4%B8%80%E8%A1%8C%E7%9A%84Job">http://hadoop102:19888/jobhistory&quot;）可以看到两条任务列表，点击第一行的Job</a> ID</p>
<p><img src="Snipaste_2023-09-15_20-13-29.png" alt="Snipaste_2023-09-15_20-13-29"></p>
<p>再点击logs超链接，可以查看历史任务的运行日志</p>
<p><img src="Snipaste_2023-09-15_20-14-55.png" alt="Snipaste_2023-09-15_20-14-55"></p>
<h4 id="3-2-8-集群启动-x2F-停止方式总结"><a href="#3-2-8-集群启动-x2F-停止方式总结" class="headerlink" title="3.2.8 集群启动&#x2F;停止方式总结"></a>3.2.8 集群启动&#x2F;停止方式总结</h4><p>1）各个模块分开启动&#x2F;停止（配置ssh是前提）【常用】</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">整体启动/停止HDFS</span></span><br><span class="line">start-dfs.sh/stop-dfs.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">整体启动/停止YARN</span></span><br><span class="line">start-yarn.sh/stop-yarn.sh</span><br></pre></td></tr></table></figure>

<p>2）各个服务组件逐一启动&#x2F;停止</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">分别启动/停止HDFS组件</span></span><br><span class="line">hdfs --daemon start/stop namenode/datanode/secondarynamenode</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动/停止YARN</span></span><br><span class="line">yarn --daemon start/stop resourcemanager/nodemanager</span><br></pre></td></tr></table></figure>

<h4 id="3-2-9-Hadoop集群启停脚本"><a href="#3-2-9-Hadoop集群启停脚本" class="headerlink" title="3.2.9 Hadoop集群启停脚本"></a>3.2.9 Hadoop集群启停脚本</h4><p>想要完整地启动Hadoop集群，需要执行多个启动命令，分别启动HDFS，YARN和HistoryServer等相关进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# sbin/start-dfs.sh</span><br><span class="line">[root@hadoop103 hadoop-3.1.3]# sbin/start-yarn.sh</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# bin/mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>

<p>现在我们编写一个脚本，用于一次性执行以上命令，从而快速启动和关闭Hadoop集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤1：在root的bin目录下创建脚本文件myhadoop.sh</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# cd /bin</span><br><span class="line">[root@hadoop102 bin]# vim myhadoop.sh</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line"> 	echo &quot;No Args Input...&quot;</span><br><span class="line"> 	exit ;</span><br><span class="line">fi</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line"> 		echo &quot; =================== 启动 hadoop 集群 ===================&quot;</span><br><span class="line"> 		echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line"> 		ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/start-dfs.sh&quot;</span><br><span class="line"> 		echo &quot; --------------- 启动 yarn ---------------&quot;</span><br><span class="line"> 		ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/start-yarn.sh&quot;</span><br><span class="line"> 		echo &quot; --------------- 启动 historyserver ---------------&quot;</span><br><span class="line"> 		ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line"> 		echo &quot; =================== 关闭 hadoop 集群 ===================&quot;</span><br><span class="line"> 		echo &quot; --------------- 关闭 historyserver ---------------&quot;</span><br><span class="line"> 		ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver&quot;</span><br><span class="line"> 		echo &quot; --------------- 关闭 yarn ---------------&quot;</span><br><span class="line"> 		ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh&quot;</span><br><span class="line"> 		echo &quot; --------------- 关闭 hdfs ---------------&quot;</span><br><span class="line"> 		ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line"> 	echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">步骤2：赋予脚本执行权限</span></span><br><span class="line">[root@hadoop102 bin]# chmod 777 myhadoop.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用myhadoop.sh脚本，关闭hadoop集群</span></span><br><span class="line">[root@hadoop102 bin]# myhadoop.sh stop</span><br><span class="line">[root@hadoop102 bin]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">15833 Jps</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">11509 Jps</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">10509 Jps</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用myhadoop.sh脚本，开启hadoop集群</span></span><br><span class="line">[root@hadoop102 bin]# myhadoop.sh start</span><br><span class="line">[root@hadoop102 bin]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">16051 NameNode</span><br><span class="line">16564 NodeManager</span><br><span class="line">16231 DataNode</span><br><span class="line">16823 Jps</span><br><span class="line">16750 JobHistoryServer</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">12144 NodeManager</span><br><span class="line">11810 ResourceManager</span><br><span class="line">12338 Jps</span><br><span class="line">11591 DataNode</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">10708 SecondaryNameNode</span><br><span class="line">10939 Jps</span><br><span class="line">10798 NodeManager</span><br><span class="line">10591 DataNode</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">分发脚本，保证脚本在所有节点服务器都可以使用</span></span><br><span class="line">[root@hadoop102 bin]# xsync myhadoop.sh</span><br></pre></td></tr></table></figure>

<p>这样每次开机之后只需要执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# myhadoop.sh start</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看集群运行情况</span></span><br><span class="line">[root@hadoop102 bin]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">2881 NameNode</span><br><span class="line">3666 Jps</span><br><span class="line">3065 DataNode</span><br><span class="line">3593 JobHistoryServer</span><br><span class="line">3404 NodeManager</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">2752 DataNode</span><br><span class="line">2978 ResourceManager</span><br><span class="line">3508 Jps</span><br><span class="line">3135 NodeManager</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">2976 NodeManager</span><br><span class="line">2883 SecondaryNameNode</span><br><span class="line">2761 DataNode</span><br><span class="line">3116 Jps</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">这样就可以方便快捷地启动集群啦</span></span><br></pre></td></tr></table></figure>

<h4 id="3-2-10-常用端口号和常用配置文件（两道面试题）"><a href="#3-2-10-常用端口号和常用配置文件（两道面试题）" class="headerlink" title="3.2.10 常用端口号和常用配置文件（两道面试题）"></a>3.2.10 常用端口号和常用配置文件（两道面试题）</h4><img src="Snipaste_2023-09-15_21-10-19.png" alt="Snipaste_2023-09-15_21-10-19" style="zoom:50%;">

<p>3.x : core-site.xml，hdfs-site.xml，yarn-site.xml，mapred-site.xml，workers</p>
<p>2.x：core-site.xml，hdfs-site.xml，yarn-site.xml，mapred-site.xml，slaves</p>
<h4 id="3-2-11-关于HDFS文件分块存储的概述"><a href="#3-2-11-关于HDFS文件分块存储的概述" class="headerlink" title="3.2.11 关于HDFS文件分块存储的概述"></a>3.2.11 关于HDFS文件分块存储的概述</h4><p>首先来看一下小文件word.txt，它的Block Size为128MB，实际大小为97B，所以没有分块</p>
<p><img src="Snipaste_2023-09-17_14-28-08.png" alt="Snipaste_2023-09-17_14-28-08"></p>
<img src="Snipaste_2023-09-17_14-30-31.png" alt="Snipaste_2023-09-17_14-30-31" style="zoom: 50%;">

<p>再看一下大文件，它的Block Size为128MB，实际大小185.98MB，分成了两块</p>
<p><img src="Snipaste_2023-09-17_14-39-46.png" alt="Snipaste_2023-09-17_14-39-46"></p>
<img src="Snipaste_2023-09-17_14-41-29.png" alt="Snipaste_2023-09-17_14-41-29" style="zoom: 50%;">

<h2 id="第四章-分布式文件系统HDFS"><a href="#第四章-分布式文件系统HDFS" class="headerlink" title="第四章 分布式文件系统HDFS"></a>第四章 分布式文件系统HDFS</h2><p>HDFS是Hadoop提供的分布式文件系统，使用HDFS可以在廉价的硬件设备上构建一套稳健的、可扩展的文件存储系统。</p>
<h3 id="4-1-HDFS概述"><a href="#4-1-HDFS概述" class="headerlink" title="4.1 HDFS概述"></a>4.1 HDFS概述</h3><h4 id="4-1-1-HGFS背景及意义"><a href="#4-1-1-HGFS背景及意义" class="headerlink" title="4.1.1 HGFS背景及意义"></a>4.1.1 HGFS背景及意义</h4><p>HDFS（Hadoop Distributed File System，Hadoop分布式文件系统）主要用于存储文件，通过<strong>目录树</strong>定位文件，HDFS的底层是<strong>分布式</strong>的，由多台服务器联合起来对外提供文件存储服务。</p>
<p>HDFS通常在<strong>一次写入，多次读取</strong>的场景使用。数据长期存储于HDFS中，用于进行分析计算。一个文件经过创建，写入和关闭之后就不需要改变了。对海量数据集进行分析计算的特别之处在于，每次计算的着眼点都是数据集的<strong>整体</strong>，而不是某条数据，因此读取数据集中第一条记录的延迟并不重要，重要的是获取整体数据集的延迟。</p>
<p><strong>HDFS的优点</strong>：</p>
<ul>
<li><strong>高容错性</strong>。HDFS以多副本的形式提供容错性。数据文件在被上传后，可以自动存储为多个副本，在其中一个副本丢失后，可以通过其他副本自动恢复</li>
<li><strong>适合处理大数据</strong>。<ul>
<li>数据规模：能够处理GB、TB、PB级别的数据</li>
<li>文件规模：能够处理百万规模以上的文件数量</li>
</ul>
</li>
<li><strong>Hadoop可以构建在廉价机器上</strong>。通过副本机制，提高可靠性</li>
</ul>
<p><strong>HDFS的缺点</strong>：</p>
<ul>
<li><strong>不适合低时间延迟的数据访问</strong>。毫秒级别的数据存储做不到。HDFS是为高数据量、高吞吐的数据应用程序设计的，而这些应用必然会以提高时间延迟为代价。</li>
<li><strong>无法高效存储大量小文件</strong>。<ul>
<li>HDFS会将文件的元数据（文件目录和块信息）存储在NameNode中，所以其文件存储量是受限于NameNode的内存量的。如果集群中存储了大量的小文件，则会对HDFS的存储能力造成极大损害。</li>
<li>小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。</li>
</ul>
</li>
<li><strong>文件不支持并发写入操作，不允许多个线程同时写文件</strong>。文件只能同时由一个写入者写入，不允许多个线程同时写文件。对于一个文件的写入操作，要以“仅添加”的模式在文件末尾写入数据，不支持在文件的任意位置进行修改。</li>
</ul>
<h4 id="4-1-2-HDFS的基本架构"><a href="#4-1-2-HDFS的基本架构" class="headerlink" title="4.1.2 HDFS的基本架构"></a>4.1.2 HDFS的基本架构</h4><p><img src="Snipaste_2023-09-17_15-51-45.png" alt="Snipaste_2023-09-17_15-51-45"></p>
<p><img src="Snipaste_2023-09-17_15-52-06.png" alt="Snipaste_2023-09-17_15-52-06"></p>
<h4 id="4-1-3-HDFS文件块大小（面试重点）"><a href="#4-1-3-HDFS文件块大小（面试重点）" class="headerlink" title="4.1.3 HDFS文件块大小（面试重点）"></a>4.1.3 HDFS文件块大小（面试重点）</h4><p><img src="Snipaste_2023-09-17_16-46-42.png" alt="Snipaste_2023-09-17_16-46-42"></p>
<p><img src="Snipaste_2023-09-17_16-47-10.png" alt="Snipaste_2023-09-17_16-47-10"></p>
<p><strong>另一种答案</strong>：</p>
<ul>
<li>如果数据块太小，那么一个大文件会被切分成<strong>过多个</strong>数据块，从而增加整个大文件的寻址时间，也会生成过多的<strong>元数据信息</strong>，对NameNode造成更大的存储负担。</li>
<li>如果数据块太大，那么虽然可以降低整个文件的寻址时间占比，但是磁盘传输数据的时间占比会大幅提高。</li>
</ul>
<h3 id="4-2-HDFS的shell操作（开发重点）"><a href="#4-2-HDFS的shell操作（开发重点）" class="headerlink" title="4.2 HDFS的shell操作（开发重点）"></a>4.2 HDFS的shell操作（开发重点）</h3><p>Hadoop为内部的文件系统提供了多种访问接口，包括Web端页面访问接口，shell命令行访问接口，JavaAPI访问接口等等。</p>
<p>基本语法:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs 具体命令  OR  hdfs dfs 具体命令</span><br></pre></td></tr></table></figure>

<h4 id="4-2-1-命令大全"><a href="#4-2-1-命令大全" class="headerlink" title="4.2.1 命令大全"></a>4.2.1 命令大全</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs </span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-checksum &lt;src&gt; ...]</span><br><span class="line">	[-chgrp [-R] GROUP PATH...]</span><br><span class="line">	[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">	[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">	[-copyFromLocal [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] [-e] &lt;path&gt; ...]</span><br><span class="line">	[-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">	[-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">	[-du [-s] [-h] [-v] [-x] &lt;path&gt; ...]</span><br><span class="line">	[-expunge]</span><br><span class="line">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">	[-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">	[-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-head &lt;file&gt;]</span><br><span class="line">	[-help [cmd ...]]</span><br><span class="line">	[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [&lt;path&gt; ...]]</span><br><span class="line">	[-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">	[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]</span><br><span class="line">	[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">	[-stat [format] &lt;path&gt; ...]</span><br><span class="line">	[-tail [-f] [-s &lt;sleep interval&gt;] &lt;file&gt;]</span><br><span class="line">	[-test -[defsz] &lt;path&gt;]</span><br><span class="line">	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-touch [-a] [-m] [-t TIMESTAMP ] [-c] &lt;path&gt; ...]</span><br><span class="line">	[-touchz &lt;path&gt; ...]</span><br><span class="line">	[-truncate [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">	[-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">Generic options supported are:</span><br><span class="line">-conf &lt;configuration file&gt;        specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;               define a value for a given property</span><br><span class="line">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides &#x27;fs.defaultFS&#x27; property from configurations.</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager</span><br><span class="line">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath</span><br><span class="line">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span><br><span class="line"></span><br><span class="line">The general command line syntax is:</span><br><span class="line">command [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>

<h4 id="4-2-2-命令行命令实操"><a href="#4-2-2-命令行命令实操" class="headerlink" title="4.2.2 命令行命令实操"></a>4.2.2 命令行命令实操</h4><h5 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1.准备工作"></a>1.准备工作</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（1）启动Hadoop集群</span></span><br><span class="line">[root@hadoop102 bin]# myhadoop.sh start</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看集群运行情况</span></span><br><span class="line">[root@hadoop102 bin]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">2881 NameNode</span><br><span class="line">3666 Jps</span><br><span class="line">3065 DataNode</span><br><span class="line">3593 JobHistoryServer</span><br><span class="line">3404 NodeManager</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">2752 DataNode</span><br><span class="line">2978 ResourceManager</span><br><span class="line">3508 Jps</span><br><span class="line">3135 NodeManager</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">2976 NodeManager</span><br><span class="line">2883 SecondaryNameNode</span><br><span class="line">2761 DataNode</span><br><span class="line">3116 Jps</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（2）使用-<span class="built_in">help</span>命令可以输出所查询命令的具体参数，比如<span class="built_in">rm</span>命令</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -help rm</span><br><span class="line">-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ... :</span><br><span class="line">  Delete all files that match the specified file pattern. Equivalent to the Unix</span><br><span class="line">  command &quot;rm &lt;src&gt;&quot;</span><br><span class="line">                                                                                 </span><br><span class="line">  -f          If the file does not exist, do not display a diagnostic message or </span><br><span class="line">              modify the exit status to reflect an error.                        </span><br><span class="line">  -[rR]       Recursively deletes directories.                                   </span><br><span class="line">  -skipTrash  option bypasses trash, if enabled, and immediately deletes &lt;src&gt;.  </span><br><span class="line">  -safely     option requires safety confirmation, if enabled, requires          </span><br><span class="line">              confirmation before deleting large directory with more than        </span><br><span class="line">              &lt;hadoop.shell.delete.limit.num.files&gt; files. Delay is expected when</span><br><span class="line">              walking over large directory recursively to count the number of    </span><br><span class="line">              files to be deleted before the confirmation.</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（3）在HDFS的根目录下创建测试文件夹/sanguo</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -mkdir /sanguo</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-17_20-14-14.png" alt="Snipaste_2023-09-17_20-14-14" style="zoom: 33%;">

<h5 id="2-上传"><a href="#2-上传" class="headerlink" title="2. 上传"></a>2. 上传</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">(1)-moveFromLocal命令：主要用于将文件从本地文件系统剪切到HDFS中</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建文件shuguo.txt，并且编写该文件中的内容（所有命令默认在/opt/module/hadoop3.1.3下执行）</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# vim shuguo.txt</span><br><span class="line">shuguo</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# ll</span><br><span class="line">总用量 184</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     183 9月  12 2019 bin</span><br><span class="line">drwxr-xr-x. 4 root root     37 9月  15 11:02 data</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     106 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh     288 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh  147145 9月   4 2019 LICENSE.txt</span><br><span class="line">drwxrwxrwx. 3 root root   4096 9月  17 14:13 logs</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh   21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh    1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh    4096 9月  14 22:32 sbin</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh      31 9月  12 2019 share</span><br><span class="line">-rw-r--r--. 1 root root      7 9月  17 20:22 shuguo.txt  # 文件在这里</span><br><span class="line">drwxr-xr-x. 2 root root     22 8月   7 10:36 wcinput</span><br><span class="line">drwxr-xr-x. 2 root root     88 8月   7 10:45 wcoutput</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行-moveFromLocal命令，将shuguo.txt文件上传至/sanguo路径下</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -moveFromLocal ./shuguo.txt  /sanguo</span><br><span class="line">2023-09-17 20:26:42,789 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看文件夹中已经没有shuguo.txt了，证明被剪切走了</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# ll</span><br><span class="line">总用量 180</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     183 9月  12 2019 bin</span><br><span class="line">drwxr-xr-x. 4 root root     37 9月  15 11:02 data</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     106 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh     288 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh  147145 9月   4 2019 LICENSE.txt</span><br><span class="line">drwxrwxrwx. 3 root root   4096 9月  17 14:13 logs</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh   21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh    1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh    4096 9月  14 22:32 sbin</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh      31 9月  12 2019 share</span><br><span class="line">drwxr-xr-x. 2 root root     22 8月   7 10:36 wcinput</span><br><span class="line">drwxr-xr-x. 2 root root     88 8月   7 10:45 wcoutput</span><br></pre></td></tr></table></figure>

<p>在Web端可以看到已经成功剪切到了HDFS文件中：</p>
<img src="Snipaste_2023-09-17_20-28-33.png" alt="Snipaste_2023-09-17_20-28-33" style="zoom:33%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（2）-copyFromLocal命令：主要用于从本地文件系统中上传（拷贝）文件到HDFS中</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# vim weiguo.txt</span><br><span class="line">weiguo</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -copyFromLocal weiguo.txt /sanguo</span><br><span class="line">2023-09-17 20:34:23,142 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-17_20-35-08.png" alt="Snipaste_2023-09-17_20-35-08" style="zoom:33%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（3）-put：等同于copyFromLocal,生产环境更常用</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# vim wuguo.txt</span><br><span class="line">wuguo</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -put ./wuguo.txt /sanguo</span><br><span class="line">2023-09-17 20:43:57,251 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-17_20-44-17.png" alt="Snipaste_2023-09-17_20-44-17" style="zoom:33%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（4）-appendToFile命令：主要用于将一个文件追加到已经存在的文件的末尾</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# vim liubei.txt</span><br><span class="line">liubei</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -appendToFile liubei.txt /sanguo/shuguo.txt</span><br><span class="line">2023-09-17 21:06:40,345 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-17_21-07-12.png" alt="Snipaste_2023-09-17_21-07-12" style="zoom:33%;">

<h5 id="3-下载"><a href="#3-下载" class="headerlink" title="3. 下载"></a>3. 下载</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（1）-copyToLocal命令：主要用于将文件从HDFS中下载（拷贝）到本地文件系统中</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -copyToLocal /sanguo/shuguo.txt ./</span><br><span class="line">2023-09-17 21:10:29,498 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# ll</span><br><span class="line">总用量 196</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     183 9月  12 2019 bin</span><br><span class="line">drwxr-xr-x. 4 root root     37 9月  15 11:02 data</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     106 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh     288 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh  147145 9月   4 2019 LICENSE.txt</span><br><span class="line">-rw-r--r--. 1 root root      7 9月  17 21:04 liubei.txt</span><br><span class="line">drwxrwxrwx. 3 root root   4096 9月  17 14:13 logs</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh   21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh    1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh    4096 9月  14 22:32 sbin</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh      31 9月  12 2019 share</span><br><span class="line">-rw-r--r--. 1 root root     14 9月  17 21:10 shuguo.txt # 文件在这里</span><br><span class="line">drwxr-xr-x. 2 root root     22 8月   7 10:36 wcinput</span><br><span class="line">drwxr-xr-x. 2 root root     88 8月   7 10:45 wcoutput</span><br><span class="line">-rw-r--r--. 1 root root      7 9月  17 20:31 weiguo.txt</span><br><span class="line">-rw-r--r--. 1 root root      6 9月  17 20:42 wuguo.txt</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（2）-get命令：与-copyToLocal功能相同，在生产环境中，-get命令更常用</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -get /sanguo/shuguo.txt ./shuguo2.txt</span><br><span class="line">2023-09-17 21:14:44,011 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# ll</span><br><span class="line">总用量 200</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     183 9月  12 2019 bin</span><br><span class="line">drwxr-xr-x. 4 root root     37 9月  15 11:02 data</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     106 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh     288 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh  147145 9月   4 2019 LICENSE.txt</span><br><span class="line">-rw-r--r--. 1 root root      7 9月  17 21:04 liubei.txt</span><br><span class="line">drwxrwxrwx. 3 root root   4096 9月  17 14:13 logs</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh   21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh    1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh    4096 9月  14 22:32 sbin</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh      31 9月  12 2019 share</span><br><span class="line">-rw-r--r--. 1 root root     14 9月  17 21:14 shuguo2.txt  # 文件在这里</span><br><span class="line">-rw-r--r--. 1 root root     14 9月  17 21:10 shuguo.txt</span><br><span class="line">drwxr-xr-x. 2 root root     22 8月   7 10:36 wcinput</span><br><span class="line">drwxr-xr-x. 2 root root     88 8月   7 10:45 wcoutput</span><br><span class="line">-rw-r--r--. 1 root root      7 9月  17 20:31 weiguo.txt</span><br><span class="line">-rw-r--r--. 1 root root      6 9月  17 20:42 wuguo.txt</span><br></pre></td></tr></table></figure>

<h5 id="4-HDFS直接操作"><a href="#4-HDFS直接操作" class="headerlink" title="4. HDFS直接操作"></a>4. HDFS直接操作</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（1）-<span class="built_in">ls</span>命令：主要用于显示目录信息</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -ls /</span><br><span class="line">Found 5 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2023-09-15 11:27 /input</span><br><span class="line">-rw-r--r--   3 root supergroup  195013152 2023-09-15 11:33 /jdk-8u212-linux-x64.tar.gz</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2023-09-15 18:21 /output</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2023-09-17 20:43 /sanguo</span><br><span class="line">drwx------   - root supergroup          0 2023-09-15 20:07 /tmp</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -ls /sanguo</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   3 root supergroup         14 2023-09-17 21:06 /sanguo/shuguo.txt</span><br><span class="line">-rw-r--r--   3 root supergroup          7 2023-09-17 20:34 /sanguo/weiguo.txt</span><br><span class="line">-rw-r--r--   3 root supergroup          6 2023-09-17 20:43 /sanguo/wuguo.txt</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（2）-<span class="built_in">cat</span>命令：主要用于显示文件内容</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -cat /sanguo/shuguo.txt</span><br><span class="line">2023-09-17 21:22:32,080 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">shuguo</span><br><span class="line">liubei</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（3）-<span class="built_in">chgrp</span>,-<span class="built_in">chmod</span>,-<span class="built_in">chown</span>命令：与Linux命令类似，不再演示</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（4）-<span class="built_in">mkdir</span>命令：主要用于创建路径</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -mkdir /jinguo</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-17_21-29-03.png" alt="Snipaste_2023-09-17_21-29-03" style="zoom:33%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（5）-<span class="built_in">cp</span>命令：主要用于将文件从HDFS中的一个路径下复制到HDFS中的另一个路径</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如，我们将shuguo.txt从/sanguo目录下拷贝到/jinguo目录下</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -cp /sanguo/shuguo.txt /jinguo</span><br><span class="line">2023-09-17 21:31:32,336 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">2023-09-17 21:31:32,418 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-17_21-32-51.png" alt="Snipaste_2023-09-17_21-32-51" style="zoom:33%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（6）-<span class="built_in">mv</span>命令：主要用于在HDFS中移动（剪切）文件，可以移动多个文件，当移动多个文件时，目标路径必须是文件夹。使用该命令也可以给文件重命名。</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -mv /sanguo/weiguo.txt /jinguo</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -mv /sanguo/wuguo.txt /jinguo</span><br></pre></td></tr></table></figure>

<p>可以看到&#x2F;sanguo文件夹中没有了weiguo.txt和wuguo.txt，而&#x2F;jinguo文件夹中出现了weiguo.txt和wuguo.txt</p>
<img src="Snipaste_2023-09-17_21-36-47.png" alt="Snipaste_2023-09-17_21-36-47" style="zoom: 33%;">

<img src="Snipaste_2023-09-17_21-38-11.png" alt="Snipaste_2023-09-17_21-38-11" style="zoom:33%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（7）-<span class="built_in">tail</span>命令：主要用于显示一个文件夹末尾1KB的数据</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -tail /jinguo/shuguo.txt</span><br><span class="line">2023-09-17 21:41:22,124 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">shuguo</span><br><span class="line">liubei</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（8）-<span class="built_in">rm</span>命令：主要用于删除文件或文件夹</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -rm /sanguo/shuguo.txt</span><br><span class="line">Deleted /sanguo/shuguo.txt</span><br></pre></td></tr></table></figure>

<p>此时的&#x2F;sanguo文件夹就变成了空文件夹</p>
<img src="Snipaste_2023-09-17_21-44-09.png" alt="Snipaste_2023-09-17_21-44-09" style="zoom:33%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（9）-<span class="built_in">rm</span> -r命令：主要用于递归删除目录及其中的内容</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -rm -r /sanguo</span><br><span class="line">Deleted /sanguo</span><br></pre></td></tr></table></figure>

<p>可以看到&#x2F;sanguo文件夹没有了</p>
<img src="Snipaste_2023-09-17_21-48-18.png" alt="Snipaste_2023-09-17_21-48-18" style="zoom:33%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（10）-<span class="built_in">du</span>命令：统计文件夹的大小。如果加上-s参数，则表示只统计文件夹的大小：如果不加-s参数，则表示统计文件夹中文件的大小。</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -du -s -h /jinguo</span><br><span class="line">27  81  /jinguo</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -du -h /jinguo</span><br><span class="line">14  42  /jinguo/shuguo.txt</span><br><span class="line">7   21  /jinguo/weiguo.txt</span><br><span class="line">6   18  /jinguo/wuguo.txt</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">说明：文件夹或文件前面的两个数字分别代表文件夹或文件的字节数和文件夹或文件的3个副本的总字节数</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">（11）-setrep：主要用于设置HDFS中文件的副本数量</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop fs -setrep 10 /jinguo/shuguo.txt</span><br><span class="line">Replication 10 set: /jinguo/shuguo.txt</span><br></pre></td></tr></table></figure>

<p>可以看到，&#x2F;jinguo&#x2F;shuguo.txt的副本变成了10个</p>
<img src="Snipaste_2023-09-17_22-13-23.png" alt="Snipaste_2023-09-17_22-13-23" style="zoom:33%;">

<p><strong>通过该命令设置的副本数量只能记录在NameNode的元数据中，是否真的有那么多副本，取决于DataNode的数量，因为目前只有3台节点服务器，所以最多存在3个副本，只有当节点服务器增加到10台的时候，真正的副本数量才能到达10个。</strong></p>
<h3 id="4-3-HDFS的API操作"><a href="#4-3-HDFS的API操作" class="headerlink" title="4.3 HDFS的API操作"></a>4.3 HDFS的API操作</h3><h4 id="4-3-1-客户端环境准备"><a href="#4-3-1-客户端环境准备" class="headerlink" title="4.3.1 客户端环境准备"></a>4.3.1 客户端环境准备</h4><p>（1）复制目录地址D:\hadoop_windows\hadoop-3.1.0</p>
<p>（2）将复制内容粘贴在：此电脑-属性-高级系统设置-高级-环境变量-新建</p>
<img src="Snipaste_2023-09-18_12-44-58.png" alt="Snipaste_2023-09-18_12-44-58" style="zoom: 50%;">

<img src="Snipaste_2023-09-18_12-45-42.png" alt="Snipaste_2023-09-18_12-45-42" style="zoom:50%;">

<p>再添加到Path目录：</p>
<img src="Snipaste_2023-09-18_12-47-31.png" alt="Snipaste_2023-09-18_12-47-31" style="zoom:50%;">

<p>（3）在hadoop-3.1.0目录下的bin文件中，双击winutils.exe文件，如果一闪而过，说明正常</p>
<hr>
<p>maven核心程序地址（中军大帐）：D:\software\apache-maven\apache-maven-3.9.4</p>
<p>maven本地仓库地址（兵营）：D:\maven-repo</p>
<p>maven工作空间（战场）：D:\maven-workspace\spaceVideo</p>
<p>（4）在IDEA中创建一个Maven工程HdfsClientDemo，在该工程的pom.xml文件中添加以下依赖，其中，hadoop-client依赖主要用于执行Hadoop的相关操作，junit依赖主要用于进行方法测试，slf4j-log4j12依赖主要用于打印日志。</p>
<img src="Snipaste_2023-09-18_20-39-28.png" alt="Snipaste_2023-09-18_20-39-28" style="zoom:50%;">

<img src="Snipaste_2023-09-18_20-47-23.png" alt="Snipaste_2023-09-18_20-47-23" style="zoom:50%;">

<img src="Snipaste_2023-09-18_20-49-38.png" alt="Snipaste_2023-09-18_20-49-38" style="zoom:50%;">

<p>如果新建的工程没有resources目录，那么就new-directory-resources</p>
<p>添加依赖：</p>
<img src="Snipaste_2023-09-18_20-52-56.png" alt="Snipaste_2023-09-18_20-52-56" style="zoom:50%;">

<p>在HdfsClientDemo工程中的src&#x2F;main&#x2F;resources目录下新建一个文件log4j.properties，在该文件中添加以下内容，可以在控制台中打印日志：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, stdout </span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender </span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n </span><br><span class="line">log4j.appender.logfile=org.apache.log4j.FileAppender </span><br><span class="line">log4j.appender.logfile.File=target/spring.log </span><br><span class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout </span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure>

<p>创建com.atguigu.hdfs包，在包中创建HdfsClient类</p>
<img src="Snipaste_2023-09-18_20-59-31.png" alt="Snipaste_2023-09-18_20-59-31" style="zoom:50%;">

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName: HdfsClient</span></span><br><span class="line"><span class="comment"> * Package: com.atguigu.hdfs</span></span><br><span class="line"><span class="comment"> * Description:</span></span><br><span class="line"><span class="comment"> * 编写HDFS API的具体思路如下：</span></span><br><span class="line"><span class="comment"> * ①创建一个Hadoop的配置类Configuration，在该配置类中封装有Hadoop的所有默认配置，</span></span><br><span class="line"><span class="comment"> *   若用户需要自定义配置，则可以通过该配置类进行修改</span></span><br><span class="line"><span class="comment"> * ②创建FileSystem对象，FileSystem是Hadoop提供的文件系统的客户端接口</span></span><br><span class="line"><span class="comment"> * ③通过FileSystem对象调用具体方法，对文件系统进行操作。</span></span><br><span class="line"><span class="comment"> * ④关闭客户端</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> 宇涵 王</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Create</span> 2023/9/18 0018 20:59</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HdfsClient</span> &#123;</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//0.连接集群的地址</span></span><br><span class="line">        <span class="type">URI</span> <span class="variable">uri</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:8020&quot;</span>);</span><br><span class="line">        <span class="comment">// 1.获取文件系统</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="comment">//获取到了客户端对象</span></span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(uri,configuration,<span class="string">&quot;root&quot;</span>);</span><br><span class="line">        <span class="comment">//2.创建目录</span></span><br><span class="line">        fs.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/xiyou/huaguoshan/&quot;</span>));</span><br><span class="line">        <span class="comment">//3.关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Web端查看到相应目录，说明程序执行成功：</p>
<img src="Snipaste_2023-09-18_21-20-46.png" alt="Snipaste_2023-09-18_21-20-46" style="zoom: 33%;">

<img src="Snipaste_2023-09-18_21-21-09.png" alt="Snipaste_2023-09-18_21-21-09" style="zoom:33%;">

<p>我们把代码进行初始化和关闭资源封装，@Test处为业务逻辑代码，@Before的代码在test之前执行，@After的代码在test之后执行</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HdfsClient</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FileSystem fs;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 对HDFS的初始化方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> URISyntaxException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//0.连接集群的地址</span></span><br><span class="line">        <span class="type">URI</span> <span class="variable">uri</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:8020&quot;</span>);</span><br><span class="line">        <span class="comment">// 1.获取文件系统</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="comment">//获取到了客户端对象</span></span><br><span class="line">        fs = FileSystem.get(uri,configuration,<span class="string">&quot;root&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 关闭资源方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">//3.关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//2.创建目录</span></span><br><span class="line">        fs.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/xiyou/huaguoshan1/&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<img src="Snipaste_2023-09-18_21-30-37.png" alt="Snipaste_2023-09-18_21-30-37" style="zoom: 33%;">

<h4 id="4-3-2-HDFS文件上传案例"><a href="#4-3-2-HDFS文件上传案例" class="headerlink" title="4.3.2 HDFS文件上传案例"></a>4.3.2 HDFS文件上传案例</h4><p>在D盘中新建一个文件sunwukong.txt，内容为sunwukong</p>
<p>编写程序（只显示业务逻辑代码）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//上传</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testPut</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="comment">//参数解读：参数一：表示删除原数据；参数二：是否允许覆盖；参数三：原数据路径；参数四：目的地路径</span></span><br><span class="line">    fs.copyFromLocalFile(<span class="literal">false</span>,<span class="literal">false</span>,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\sunwukong.txt&quot;</span>),<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/xiyou/huaguoshan&quot;</span>));</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行成功：</p>
<img src="Snipaste_2023-09-18_21-46-11.png" alt="Snipaste_2023-09-18_21-46-11" style="zoom: 33%;">

<p>新建hdfs-site.xml文件到resources目录，编辑如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>重新执行java代码可以看到，副本数变为1：</p>
<img src="Snipaste_2023-09-18_21-55-29.png" alt="Snipaste_2023-09-18_21-55-29" style="zoom:33%;">

<p>我们再将获取文件系统时的代码添加一行：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">configuration.set(<span class="string">&quot;dfs.replication&quot;</span>,<span class="string">&quot;2&quot;</span>);<span class="comment">//表示2个副本</span></span><br></pre></td></tr></table></figure>

<p>重新执行java代码可以看到，副本数变为2：</p>
<img src="Snipaste_2023-09-18_21-59-07.png" alt="Snipaste_2023-09-18_21-59-07" style="zoom: 33%;">

<p>综上所述，Hadoop服务器可以在不同的位置进行参数配置，具有不同的优先级，优先级由高到低分别如下：</p>
<p><strong>客户端代码中设置的值 &gt; ClassPath中的用户自定义配置文件（resources目录下的配置文件） &gt; 服务器中的自定义配置文件(xxx.site.xml) &gt; 服务器中的默认配置文件(xxx-default.xml)</strong></p>
<h4 id="4-3-3-HDFS文件下载案例"><a href="#4-3-3-HDFS文件下载案例" class="headerlink" title="4.3.3 HDFS文件下载案例"></a>4.3.3 HDFS文件下载案例</h4><p>目的：将&#x2F;xiyou&#x2F;huaguoshan&#x2F;sunwukong.txt文件下载至本地文件系统中。下载功能调用FileSystem对象的copyToLocalFile()方法，此方法有如下4个参数：</p>
<ul>
<li>delSrc：boolean类型，用于设置是否将原文件删除</li>
<li>src：Path类型，要下载的文件路径</li>
<li>dst：Path类型，存储下载文件的目标路径</li>
<li>useRawLocalFileSystem：boolean类型，用于设置是否启用文件校验功能（true是不校验，false是校验）</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//下载</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testGet</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    fs.copyToLocalFile(<span class="literal">false</span>,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/xiyou/huaguoshan/sunwukong.txt&quot;</span>),<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\&quot;</span>),<span class="literal">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-3-4-HDFS文件重命名与移动案例"><a href="#4-3-4-HDFS文件重命名与移动案例" class="headerlink" title="4.3.4 HDFS文件重命名与移动案例"></a>4.3.4 HDFS文件重命名与移动案例</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//文件重命名</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testRename</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="comment">//参数解读：参数一：原文件路径，参数二：目标文件路径</span></span><br><span class="line">    fs.rename(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/xiyou/huaguoshan/sunwukong.txt&quot;</span>),<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/xiyou/huaguoshan/sunwukong222.txt&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-19_11-06-51.png" alt="Snipaste_2023-09-19_11-06-51" style="zoom: 33%;">

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//文件移动并更名</span></span><br><span class="line"><span class="comment">//将/jinguo/shuguo.txt移动到根目录下，并修改文件名称为shuhan.txt</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testMove</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    fs.rename(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/jinguo/shuguo.txt&quot;</span>),<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/shuhan.txt&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-19_11-29-38.png" alt="Snipaste_2023-09-19_11-29-38" style="zoom:33%;">

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//目录更名</span></span><br><span class="line"><span class="comment">//将/jinguo目录更名为/Wudai</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testRenameCat</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    fs.rename(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/jinguo&quot;</span>),<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/Wudai&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-19_11-33-29.png" alt="Snipaste_2023-09-19_11-33-29" style="zoom:33%;">

<h4 id="4-3-5-HDFS文件删除案例"><a href="#4-3-5-HDFS文件删除案例" class="headerlink" title="4.3.5 HDFS文件删除案例"></a>4.3.5 HDFS文件删除案例</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//文件删除</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testRm</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="comment">//参数解读：第一个参数是需要删除的文件路径，第二个参数是boolean类型的参数，用于设置是否对</span></span><br><span class="line">    <span class="comment">//文件夹进行递归删除操作，当传入的文件路径为非空文件夹时，boolean类型的参数必须为true，才会执行</span></span><br><span class="line">    <span class="comment">//删除操作，否则会报错</span></span><br><span class="line">    fs.delete(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/xiyou&quot;</span>),<span class="literal">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>&#x2F;xiyou删除成功：</p>
<img src="Snipaste_2023-09-19_11-20-38.png" alt="Snipaste_2023-09-19_11-20-38" style="zoom: 33%;">

<h4 id="4-3-6-HDFS文件详情查看案例"><a href="#4-3-6-HDFS文件详情查看案例" class="headerlink" title="4.3.6 HDFS文件详情查看案例"></a>4.3.6 HDFS文件详情查看案例</h4><p>通过调用FileSystem对象的listFiles()方法实现查看文件名称、权限、长度、块信息等，返回值是一个封装了路径下所有文件状态的迭代器。迭代器内封装的对象是FileStatus类的子类LocatedFileStatus，通过它可以获取文件的所有信息。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取文件详细信息</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testFileDetail</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="comment">//获取所有文件信息（ctrl+alt+v快速生成返回值）</span></span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/&quot;</span>), <span class="literal">true</span>);</span><br><span class="line">    <span class="comment">//遍历文件</span></span><br><span class="line">    <span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">        <span class="type">LocatedFileStatus</span> <span class="variable">fileStatus</span> <span class="operator">=</span> listFiles.next();</span><br><span class="line">        System.out.println(<span class="string">&quot;==========&quot;</span> + fileStatus.getPath() + <span class="string">&quot;============&quot;</span>);</span><br><span class="line">        System.out.println(fileStatus.getPermission());</span><br><span class="line">        System.out.println(fileStatus.getOwner());</span><br><span class="line">        System.out.println(fileStatus.getGroup());</span><br><span class="line">        System.out.println(fileStatus.getLen());</span><br><span class="line">        System.out.println(fileStatus.getModificationTime());</span><br><span class="line">        System.out.println(fileStatus.getReplication());</span><br><span class="line">        System.out.println(fileStatus.getBlockSize());</span><br><span class="line">        System.out.println(fileStatus.getPath().getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取块信息</span></span><br><span class="line">        BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">        System.out.println(Arrays.toString(blockLocations));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">==========hdfs://hadoop102:8020/Wudai/weiguo.txt============</span><br><span class="line">rw-r--r--</span><br><span class="line">root</span><br><span class="line">supergroup</span><br><span class="line">7</span><br><span class="line">1694954063249</span><br><span class="line">3</span><br><span class="line">134217728</span><br><span class="line">weiguo.txt</span><br><span class="line">[0,7,hadoop104,hadoop103,hadoop102]</span><br><span class="line">==========hdfs://hadoop102:8020/Wudai/wuguo.txt============</span><br><span class="line">rw-r--r--</span><br><span class="line">root</span><br><span class="line">supergroup</span><br><span class="line">6</span><br><span class="line">1694954637356</span><br><span class="line">3</span><br><span class="line">134217728</span><br><span class="line">wuguo.txt</span><br><span class="line">[0,6,hadoop104,hadoop103,hadoop102]</span><br><span class="line">==========hdfs://hadoop102:8020/input/word.txt============</span><br><span class="line">rw-r--r--</span><br><span class="line">root</span><br><span class="line">supergroup</span><br><span class="line">97</span><br><span class="line">1694748469255</span><br><span class="line">3</span><br><span class="line">134217728</span><br><span class="line">word.txt</span><br><span class="line">[0,97,hadoop104,hadoop103,hadoop102]</span><br><span class="line">......</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<h4 id="4-3-7-HDFS文件和文件夹判断案例"><a href="#4-3-7-HDFS文件和文件夹判断案例" class="headerlink" title="4.3.7 HDFS文件和文件夹判断案例"></a>4.3.7 HDFS文件和文件夹判断案例</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//判断是文件还是文件夹</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="comment">//获取FileStatus集合</span></span><br><span class="line">    FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/&quot;</span>));</span><br><span class="line">    <span class="comment">//增强for循环</span></span><br><span class="line">    <span class="keyword">for</span> (FileStatus status :listStatus)&#123;</span><br><span class="line">        <span class="keyword">if</span> (status.isFile())&#123; <span class="comment">//如果是文件</span></span><br><span class="line">            System.out.println(<span class="string">&quot;文件: &quot;</span> + status.getPath().getName());</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;               <span class="comment">//如果是目录</span></span><br><span class="line">            System.out.println(<span class="string">&quot;目录: &quot;</span> + status.getPath().getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">目录: Wudai</span><br><span class="line">目录: input</span><br><span class="line">文件: jdk-8u212-linux-x64.tar.gz</span><br><span class="line">目录: output</span><br><span class="line">文件: shuhan.txt</span><br><span class="line">目录: tmp</span><br></pre></td></tr></table></figure>

<h3 id="4-4-HDFS的读-x2F-写流程（面试重点）"><a href="#4-4-HDFS的读-x2F-写流程（面试重点）" class="headerlink" title="4.4 HDFS的读&#x2F;写流程（面试重点）"></a>4.4 HDFS的读&#x2F;写流程（面试重点）</h3><h4 id="4-4-1-HDFS中数据块大小（之前说过了）"><a href="#4-4-1-HDFS中数据块大小（之前说过了）" class="headerlink" title="4.4.1 HDFS中数据块大小（之前说过了）"></a>4.4.1 HDFS中数据块大小（之前说过了）</h4><h4 id="4-4-2-写数据流程"><a href="#4-4-2-写数据流程" class="headerlink" title="4.4.2 写数据流程"></a>4.4.2 写数据流程</h4><h5 id="1-剖析写数据流程"><a href="#1-剖析写数据流程" class="headerlink" title="1. 剖析写数据流程"></a>1. 剖析写数据流程</h5><h5 id="2-副本放置策略与机架感知"><a href="#2-副本放置策略与机架感知" class="headerlink" title="2. 副本放置策略与机架感知"></a>2. 副本放置策略与机架感知</h5><h5 id="3-网络拓扑距离与PipeLine的形成"><a href="#3-网络拓扑距离与PipeLine的形成" class="headerlink" title="3. 网络拓扑距离与PipeLine的形成"></a>3. 网络拓扑距离与PipeLine的形成</h5><h4 id="4-4-3-读数据流程"><a href="#4-4-3-读数据流程" class="headerlink" title="4.4.3 读数据流程"></a>4.4.3 读数据流程</h4><h3 id="4-5-HDFS的工作流程"><a href="#4-5-HDFS的工作流程" class="headerlink" title="4.5 HDFS的工作流程"></a>4.5 HDFS的工作流程</h3><h4 id="4-5-1-NameNode和SecondaryNameNode的工作机制"><a href="#4-5-1-NameNode和SecondaryNameNode的工作机制" class="headerlink" title="4.5.1 NameNode和SecondaryNameNode的工作机制"></a>4.5.1 NameNode和SecondaryNameNode的工作机制</h4><h4 id="4-5-2-EditLog和FsImage文件解析"><a href="#4-5-2-EditLog和FsImage文件解析" class="headerlink" title="4.5.2 EditLog和FsImage文件解析"></a>4.5.2 EditLog和FsImage文件解析</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/module/hadoop-3.1.3/data/dfs/name/current/</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 current]# ll</span><br><span class="line">总用量 10380</span><br><span class="line">-rw-r--r--. 1 root root 1048576 9月  14 22:33 edits_0000000000000000001-0000000000000000001</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  15 11:21 edits_0000000000000000002-0000000000000000003</span><br><span class="line">-rw-r--r--. 1 root root    1364 9月  15 12:21 edits_0000000000000000004-0000000000000000021</span><br><span class="line">-rw-r--r--. 1 root root     154 9月  15 17:43 edits_0000000000000000022-0000000000000000025</span><br><span class="line">-rw-r--r--. 1 root root 1048576 9月  15 17:51 edits_0000000000000000026-0000000000000000060</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  15 18:05 edits_0000000000000000061-0000000000000000062</span><br><span class="line">-rw-r--r--. 1 root root 1048576 9月  15 18:06 edits_0000000000000000063-0000000000000000092</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  15 18:14 edits_0000000000000000093-0000000000000000094</span><br><span class="line">-rw-r--r--. 1 root root   14029 9月  15 19:14 edits_0000000000000000095-0000000000000000210</span><br><span class="line">-rw-r--r--. 1 root root   15050 9月  15 20:14 edits_0000000000000000211-0000000000000000328</span><br><span class="line">-rw-r--r--. 1 root root 1048576 9月  15 20:14 edits_0000000000000000329-0000000000000000329</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  15 20:58 edits_0000000000000000330-0000000000000000331</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  15 21:58 edits_0000000000000000332-0000000000000000333</span><br><span class="line">-rw-r--r--. 1 root root 1048576 9月  15 21:58 edits_0000000000000000334-0000000000000000334</span><br><span class="line">-rw-r--r--. 1 root root 1048576 9月  17 14:00 edits_0000000000000000335-0000000000000000336</span><br><span class="line">-rw-r--r--. 1 root root     195 9月  17 14:37 edits_0000000000000000337-0000000000000000341</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  17 15:37 edits_0000000000000000342-0000000000000000343</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  17 16:37 edits_0000000000000000344-0000000000000000345</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  17 17:37 edits_0000000000000000346-0000000000000000347</span><br><span class="line">-rw-r--r--. 1 root root    1169 9月  17 20:37 edits_0000000000000000348-0000000000000000362</span><br><span class="line">-rw-r--r--. 1 root root    1717 9月  17 21:37 edits_0000000000000000363-0000000000000000384</span><br><span class="line">-rw-r--r--. 1 root root 1048576 9月  17 22:12 edits_0000000000000000385-0000000000000000388</span><br><span class="line">-rw-r--r--. 1 root root 1048576 9月  18 12:35 edits_0000000000000000389-0000000000000000389</span><br><span class="line">-rw-r--r--. 1 root root     281 9月  18 21:34 edits_0000000000000000390-0000000000000000394</span><br><span class="line">-rw-r--r--. 1 root root 1048576 9月  18 21:58 edits_0000000000000000395-0000000000000000412</span><br><span class="line">-rw-r--r--. 1 root root     476 9月  19 11:33 edits_0000000000000000413-0000000000000000420</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  19 12:33 edits_0000000000000000421-0000000000000000422</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  19 16:30 edits_0000000000000000423-0000000000000000424</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  19 17:30 edits_0000000000000000425-0000000000000000426</span><br><span class="line">-rw-r--r--. 1 root root      42 9月  19 18:41 edits_0000000000000000427-0000000000000000428</span><br><span class="line">-rw-r--r--. 1 root root 1048576 9月  19 18:41 edits_inprogress_0000000000000000429</span><br><span class="line">-rw-r--r--. 1 root root    4149 9月  19 17:30 fsimage_0000000000000000426</span><br><span class="line">-rw-r--r--. 1 root root      62 9月  19 17:30 fsimage_0000000000000000426.md5</span><br><span class="line">-rw-r--r--. 1 root root    4149 9月  19 18:41 fsimage_0000000000000000428</span><br><span class="line">-rw-r--r--. 1 root root      62 9月  19 18:41 fsimage_0000000000000000428.md5</span><br><span class="line">-rw-r--r--. 1 root root       4 9月  19 18:41 seen_txid</span><br><span class="line">-rw-r--r--. 1 root root     220 9月  19 10:34 VERSION</span><br></pre></td></tr></table></figure>



<p>EditLog和FsImage文件都是二进制文件，所以不能直接查看</p>
<h5 id="1-使用oiv命令查看FsImage文件"><a href="#1-使用oiv命令查看FsImage文件" class="headerlink" title="1. 使用oiv命令查看FsImage文件"></a>1. 使用oiv命令查看FsImage文件</h5><p>基本语法：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs oiv -p 文件类型  -i  镜像文件 -o  转换后文件输出路径</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将fsimage_0000000000000000428文件转换为fsimage.xml文件</span></span><br><span class="line">[root@hadoop102 current]# hdfs oiv -p XML -i fsimage_0000000000000000428 -o /opt/software/fsimage.xml</span><br><span class="line">2023-09-19 20:11:26,790 INFO offlineImageViewer.FSImageHandler: Loading 4 strings</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入/opt/software目录</span></span><br><span class="line">[root@hadoop102 current]# cd /opt/software/</span><br><span class="line">[root@hadoop102 software]# ll</span><br><span class="line">总用量 520620</span><br><span class="line">-rw-r--r--. 1 root root     19367 9月  19 20:11 fsimage.xml</span><br><span class="line">-rw-r--r--. 1 root root 338075860 8月   5 22:31 hadoop-3.1.3.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root 195013152 8月   5 22:11 jdk-8u212-linux-x64.tar.gz</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将fsimage.xml文件下载到本机（windows环境）</span></span><br><span class="line">[root@hadoop102 software]# sz fsimage.xml</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-09-19_20-13-52.png" alt="Snipaste_2023-09-19_20-13-52" style="zoom: 33%;">

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>16388<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">type</span>&gt;</span>FILE<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>jdk-8u212-linux-x64.tar.gz<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">replication</span>&gt;</span>3<span class="tag">&lt;/<span class="name">replication</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1694748821284<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">atime</span>&gt;</span>1694931554367<span class="tag">&lt;/<span class="name">atime</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">preferredBlockSize</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">preferredBlockSize</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">permission</span>&gt;</span>root:supergroup:0644<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">blocks</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">block</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>1073741826<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">genstamp</span>&gt;</span>1002<span class="tag">&lt;/<span class="name">genstamp</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">numBytes</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">numBytes</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">block</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>1073741827<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">genstamp</span>&gt;</span>1003<span class="tag">&lt;/<span class="name">genstamp</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">numBytes</span>&gt;</span>60795424<span class="tag">&lt;/<span class="name">numBytes</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">blocks</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">storagePolicyId</span>&gt;</span>0<span class="tag">&lt;/<span class="name">storagePolicyId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">......</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>可以看到FsImage文件是由多个inode信息构成的.</p>
<p>FsImage文件中并没有记录数据块对应的DataNode信息，这是因为在启动集群后，NameNode会要求DataNode上报数据块信息，并且在间隔一段时间后再次上报。</p>
<h5 id="2-使用oev命令查看EditLog文件"><a href="#2-使用oev命令查看EditLog文件" class="headerlink" title="2. 使用oev命令查看EditLog文件"></a>2. 使用oev命令查看EditLog文件</h5><p>基本语法：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs oev -o  文件类型  -i  编辑日志  -o  转换后文件输出路径</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将edits_inprogress_0000000000000000433文件转换为edits.xml文件</span></span><br><span class="line">[root@hadoop102 current]# hdfs oev -p XML -i edits_inprogress_0000000000000000433 -o /opt/software/edits.xml</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入/opt/software目录</span></span><br><span class="line">[root@hadoop102 current]# cd /opt/software/</span><br><span class="line">[root@hadoop102 software]# ll</span><br><span class="line">总用量 520624</span><br><span class="line">-rw-r--r--. 1 root root       221 9月  19 22:17 edits.xml</span><br><span class="line">-rw-r--r--. 1 root root     19367 9月  19 20:11 fsimage.xml</span><br><span class="line">-rw-r--r--. 1 root root 338075860 8月   5 22:31 hadoop-3.1.3.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root 195013152 8月   5 22:11 jdk-8u212-linux-x64.tar.gz</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将edits.xml文件下载到本机（windows环境）</span></span><br><span class="line">[root@hadoop102 software]# sz edits.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span> standalone=<span class="string">&quot;yes&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">EDITS</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">EDITS_VERSION</span>&gt;</span>-64<span class="tag">&lt;/<span class="name">EDITS_VERSION</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_START_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>433<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">EDITS</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="4-5-3-检查点时间设置"><a href="#4-5-3-检查点时间设置" class="headerlink" title="4.5.3 检查点时间设置"></a>4.5.3 检查点时间设置</h4><h4 id="4-5-4-DataNode的工作机制"><a href="#4-5-4-DataNode的工作机制" class="headerlink" title="4.5.4 DataNode的工作机制"></a>4.5.4 DataNode的工作机制</h4><p>HDFS中的文件实际是存储于DataNode中的，数据块存储于以<strong>blk</strong>为前缀的文件中，文件名中包含该文件存储的数据块的原始字节数，每个数据块文件都有一个名字相同但是带有.meta后缀的<strong>元数据文件</strong>，在元数据文件中存储了数据块的长度、校验和、时间戳等信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1662165003-192.168.255.102-1694700063570/current/finalized/subdir0/subdir0/</span><br><span class="line">[root@hadoop102 subdir0]# ll</span><br><span class="line">总用量 384612</span><br><span class="line">-rw-r--r--. 1 root root        97 9月  15 11:27 blk_1073741825</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  15 11:27 blk_1073741825_1001.meta</span><br><span class="line">-rw-r--r--. 1 root root 134217728 9月  15 11:33 blk_1073741826</span><br><span class="line">-rw-r--r--. 1 root root   1048583 9月  15 11:33 blk_1073741826_1002.meta</span><br><span class="line">-rw-r--r--. 1 root root  60795424 9月  15 11:33 blk_1073741827</span><br><span class="line">-rw-r--r--. 1 root root    474975 9月  15 11:33 blk_1073741827_1003.meta</span><br><span class="line">-rw-r--r--. 1 root root    316382 9月  15 17:51 blk_1073741828</span><br><span class="line">-rw-r--r--. 1 root root      2479 9月  15 17:51 blk_1073741828_1004.meta</span><br><span class="line">-rw-r--r--. 1 root root       108 9月  15 17:51 blk_1073741829</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  15 17:51 blk_1073741829_1005.meta</span><br><span class="line">-rw-r--r--. 1 root root        43 9月  15 17:51 blk_1073741830</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  15 17:51 blk_1073741830_1006.meta</span><br><span class="line">-rw-r--r--. 1 root root    184966 9月  15 17:51 blk_1073741831</span><br><span class="line">-rw-r--r--. 1 root root      1455 9月  15 17:51 blk_1073741831_1007.meta</span><br><span class="line">-rw-r--r--. 1 root root    316382 9月  15 18:06 blk_1073741832</span><br><span class="line">-rw-r--r--. 1 root root      2479 9月  15 18:06 blk_1073741832_1008.meta</span><br><span class="line">-rw-r--r--. 1 root root       108 9月  15 18:06 blk_1073741833</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  15 18:06 blk_1073741833_1009.meta</span><br><span class="line">-rw-r--r--. 1 root root        43 9月  15 18:06 blk_1073741834</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  15 18:06 blk_1073741834_1010.meta</span><br><span class="line">-rw-r--r--. 1 root root    185463 9月  15 18:06 blk_1073741835</span><br><span class="line">-rw-r--r--. 1 root root      1459 9月  15 18:06 blk_1073741835_1011.meta</span><br><span class="line">-rw-r--r--. 1 root root    316382 9月  15 18:14 blk_1073741836</span><br><span class="line">-rw-r--r--. 1 root root      2479 9月  15 18:14 blk_1073741836_1012.meta</span><br><span class="line">-rw-r--r--. 1 root root       108 9月  15 18:14 blk_1073741837</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  15 18:14 blk_1073741837_1013.meta</span><br><span class="line">-rw-r--r--. 1 root root        43 9月  15 18:14 blk_1073741838</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  15 18:14 blk_1073741838_1014.meta</span><br><span class="line">-rw-r--r--. 1 root root    185433 9月  15 18:14 blk_1073741839</span><br><span class="line">-rw-r--r--. 1 root root      1459 9月  15 18:14 blk_1073741839_1015.meta</span><br><span class="line">-rw-r--r--. 1 root root        80 9月  15 18:21 blk_1073741846</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  15 18:21 blk_1073741846_1022.meta</span><br><span class="line">-rw-r--r--. 1 root root     22342 9月  15 18:21 blk_1073741848</span><br><span class="line">-rw-r--r--. 1 root root       183 9月  15 18:21 blk_1073741848_1024.meta</span><br><span class="line">-rw-r--r--. 1 root root    215649 9月  15 18:21 blk_1073741849</span><br><span class="line">-rw-r--r--. 1 root root      1695 9月  15 18:21 blk_1073741849_1025.meta</span><br><span class="line">-rw-r--r--. 1 root root     22342 9月  15 20:07 blk_1073741858</span><br><span class="line">-rw-r--r--. 1 root root       183 9月  15 20:07 blk_1073741858_1034.meta</span><br><span class="line">-rw-r--r--. 1 root root    215826 9月  15 20:07 blk_1073741859</span><br><span class="line">-rw-r--r--. 1 root root      1695 9月  15 20:07 blk_1073741859_1035.meta</span><br><span class="line">-rw-r--r--. 1 root root     34635 9月  15 20:07 blk_1073741860</span><br><span class="line">-rw-r--r--. 1 root root       279 9月  15 20:07 blk_1073741860_1036.meta</span><br><span class="line">-rw-r--r--. 1 root root     99463 9月  15 20:07 blk_1073741861</span><br><span class="line">-rw-r--r--. 1 root root       787 9月  15 20:07 blk_1073741861_1037.meta</span><br><span class="line">-rw-r--r--. 1 root root         7 9月  17 20:34 blk_1073741863</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  17 20:34 blk_1073741863_1039.meta</span><br><span class="line">-rw-r--r--. 1 root root         6 9月  17 20:43 blk_1073741864</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  17 20:43 blk_1073741864_1040.meta</span><br><span class="line">-rw-r--r--. 1 root root        14 9月  17 21:31 blk_1073741865</span><br><span class="line">-rw-r--r--. 1 root root        11 9月  17 21:31 blk_1073741865_1042.meta</span><br><span class="line">drwxr-xr-x. 7   10  143       245 4月   2 2019 jdk1.8.0_212</span><br><span class="line">-rw-r--r--. 1 root root 195013152 9月  15 17:25 tmp.tar.gz</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-09-22_18-24-09.png" alt="Snipaste_2023-09-22_18-24-09"></p>
<p>DataNode在启动后回想NameNode注册，在注册通过后，每隔一段时间都会向NameNode上报所有的数据块信息，时间间隔默认是6小时。</p>
<p>DataNode每隔一段时间都会扫描本节点中的数据块信息列表，时间间隔默认是6小时。</p>
<p>DataNode每隔3秒都会向NameNode发送心跳信息，心跳返回结果中包含NameNode对该DataNode的命令，如复制数据块、删除数据块等。如果DataNode进程挂掉，或者发生网络故障，导致DataNode无法与NameNode正常通信，那么NameNode不会立即将该节点判定为死亡，它会在一段时间（称为超时时长，默认配置为10分钟30秒）后将该节点判定为死亡。</p>
<p>点击HDFS的Web页面，可以查看DataNode的运行情况：</p>
<p><img src="Snipaste_2023-09-22_20-58-59.png" alt="Snipaste_2023-09-22_20-58-59"></p>
<h4 id="4-5-5-数据完整性"><a href="#4-5-5-数据完整性" class="headerlink" title="4.5.5 数据完整性"></a>4.5.5 数据完整性</h4><h2 id="第五章-分布式计算MapReduce"><a href="#第五章-分布式计算MapReduce" class="headerlink" title="第五章 分布式计算MapReduce"></a>第五章 分布式计算MapReduce</h2><h3 id="5-1-MapReduce概述"><a href="#5-1-MapReduce概述" class="headerlink" title="5.1 MapReduce概述"></a>5.1 MapReduce概述</h3><h4 id="5-1-1-MapReduce定义"><a href="#5-1-1-MapReduce定义" class="headerlink" title="5.1.1 MapReduce定义"></a>5.1.1 MapReduce定义</h4><p>MapReduce是一个分布式计算程序的编程框架。MapReduce的核心功能是将<strong>用户编写的业务逻辑代码</strong>和<strong>自带的默认组件</strong>整合成一个完整的分布式计算程序并且将其<strong>并发运行</strong>在一个Hadoop集群上。</p>
<h5 id="1-优点"><a href="#1-优点" class="headerlink" title="1. 优点"></a>1. 优点</h5><ul>
<li>易于编程</li>
<li>良好的扩展性：当计算资源不能满足海量数据的计算需求时，MapReduce可以简单地通过增加节点数量增强计算能力</li>
<li>高容错性：一个节点宕机，它可以将该节点上的计算任务转移到另一个节点运行，不至于导致这个任务运行失败</li>
<li>适合PB级以上数据的离线处理：MapReduce可以实现上千台服务器集群并发工作，提供数据处理能力。</li>
</ul>
<h5 id="2-缺点"><a href="#2-缺点" class="headerlink" title="2. 缺点"></a>2. 缺点</h5><ul>
<li>不擅长实时计算</li>
<li>不擅长流式计算：流式计算的输入数据是动态的，而MapReduce的输入是静态的，不能动态变化</li>
<li>不擅长DAG（有向无环图）计算：多个应用程序之间存在依赖关系，后一个应用程序是前一个应用程序的输出数据，</li>
</ul>
<h4 id="5-1-2-MapReduce核心思想"><a href="#5-1-2-MapReduce核心思想" class="headerlink" title="5.1.2 MapReduce核心思想"></a>5.1.2 MapReduce核心思想</h4><p><img src="Snipaste_2023-09-25_09-09-40.png" alt="Snipaste_2023-09-25_09-09-40"></p>
<p>在运行一个完整的MapReduce程序时，在集群中会发现以下3类实例进程：</p>
<ul>
<li>MRAppMaster：负责整个程序的过程调度及状态协调</li>
<li>MapTask：负责map阶段的整个数据处理流程</li>
<li>ReduceTask：负责reduce阶段的整个数据处理流程</li>
</ul>
<h3 id="5-2-MapReduce编程入门"><a href="#5-2-MapReduce编程入门" class="headerlink" title="5.2 MapReduce编程入门"></a>5.2 MapReduce编程入门</h3><h4 id="5-2-1-官方示例程序WorldCount源码"><a href="#5-2-1-官方示例程序WorldCount源码" class="headerlink" title="5.2.1 官方示例程序WorldCount源码"></a>5.2.1 官方示例程序WorldCount源码</h4><p>将hadoop-mapreduce-examples-3.1.3.jar下载到本地</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/</span><br><span class="line">[root@hadoop102 mapreduce]# ll</span><br><span class="line">总用量 5576</span><br><span class="line">-rw-r--r--. 1 wyh wyh  612175 9月  12 2019 hadoop-mapreduce-client-app-3.1.3.jar</span><br><span class="line">-rw-r--r--. 1 wyh wyh  804003 9月  12 2019 hadoop-mapreduce-client-common-3.1.3.jar</span><br><span class="line">-rw-r--r--. 1 wyh wyh 1655414 9月  12 2019 hadoop-mapreduce-client-core-3.1.3.jar</span><br><span class="line">-rw-r--r--. 1 wyh wyh  215372 9月  12 2019 hadoop-mapreduce-client-hs-3.1.3.jar</span><br><span class="line">-rw-r--r--. 1 wyh wyh   45334 9月  12 2019 hadoop-mapreduce-client-hs-plugins-3.1.3.jar</span><br><span class="line">-rw-r--r--. 1 wyh wyh   85396 9月  12 2019 hadoop-mapreduce-client-jobclient-3.1.3.jar</span><br><span class="line">-rw-r--r--. 1 wyh wyh 1659884 9月  12 2019 hadoop-mapreduce-client-jobclient-3.1.3-tests.jar</span><br><span class="line">-rw-r--r--. 1 wyh wyh  126143 9月  12 2019 hadoop-mapreduce-client-nativetask-3.1.3.jar</span><br><span class="line">-rw-r--r--. 1 wyh wyh   97155 9月  12 2019 hadoop-mapreduce-client-shuffle-3.1.3.jar</span><br><span class="line">-rw-r--r--. 1 wyh wyh   57652 9月  12 2019 hadoop-mapreduce-client-uploader-3.1.3.jar</span><br><span class="line">-rw-r--r--. 1 wyh wyh  316382 9月  12 2019 hadoop-mapreduce-examples-3.1.3.jar</span><br><span class="line">drwxr-xr-x. 2 wyh wyh    4096 9月  12 2019 jdiff</span><br><span class="line">drwxr-xr-x. 2 wyh wyh      57 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 2 wyh wyh      30 9月  12 2019 lib-examples</span><br><span class="line">drwxr-xr-x. 2 wyh wyh    4096 9月  12 2019 sources</span><br><span class="line">[root@hadoop102 mapreduce]# sz hadoop-mapreduce-examples-3.1.3.jar </span><br></pre></td></tr></table></figure>

<p>使用反编译工具，可以看到WordCount的源码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.hadoop.examples;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.PrintStream;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCount</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span></span><br><span class="line">    <span class="keyword">throws</span> Exception</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    String[] otherArgs = <span class="keyword">new</span> <span class="title class_">GenericOptionsParser</span>(conf, args).getRemainingArgs();</span><br><span class="line">    <span class="keyword">if</span> (otherArgs.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">      System.err.println(<span class="string">&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;</span>);</span><br><span class="line">      System.exit(<span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;word count&quot;</span>);</span><br><span class="line">    job.setJarByClass(WordCount.class);</span><br><span class="line">    job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">    job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">    job.setReducerClass(IntSumReducer.class);</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; otherArgs.length - <span class="number">1</span>; i++) &#123;</span><br><span class="line">      FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[i]));</span><br><span class="line">    &#125;</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[(otherArgs.length - <span class="number">1</span>)]));</span><br><span class="line"></span><br><span class="line">    System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">IntSumReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt;</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span></span><br><span class="line">      <span class="keyword">throws</span> IOException, InterruptedException</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">        sum += val.get();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="built_in">this</span>.result.set(sum);</span><br><span class="line">      context.write(key, <span class="built_in">this</span>.result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt;</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="type">StringTokenizer</span> <span class="variable">itr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString());</span><br><span class="line">      <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">        <span class="built_in">this</span>.word.set(itr.nextToken());</span><br><span class="line">        context.write(<span class="built_in">this</span>.word, one);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="补充：常用数据序列类型"><a href="#补充：常用数据序列类型" class="headerlink" title="补充：常用数据序列类型"></a>补充：常用数据序列类型</h4><img src="Snipaste_2023-09-25_09-30-13.png" alt="Snipaste_2023-09-25_09-30-13" style="zoom:50%;">

<h4 id="5-2-2-编程规范"><a href="#5-2-2-编程规范" class="headerlink" title="5.2.2 编程规范"></a>5.2.2 编程规范</h4><p>用户编写的MapReduce程序主要分为三个部分：Mapper、Reducer和Driver</p>
<ol>
<li>Mapper组件</li>
</ol>
<ul>
<li>用户自定义类会继承Mapper类，并给出4个泛型，即map方法的输入键、输入值、输出键和输出值的数据情况。</li>
<li>Mapper组件的输入数据是键&#x2F;值对的形式，输入键是一个长整型的偏移量，输入值是一行文本；输出数据也是键&#x2F;值对的形式，输出键和输出值的数据类型是由map()方法的处理逻辑决定。</li>
<li>Mapper组件的业务逻辑写在map()方法中，map()方法的实现逻辑是由用户自行编写。</li>
<li>map()方法对每个输入数据键&#x2F;值对都只调用一次。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt;</span><br><span class="line">    <span class="comment">//用户自定义类会继承Mapper类，并给出4个泛型，</span></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException<span class="comment">//即map方法的输入键、输入值、输出键和输出值的数据情况。</span></span><br><span class="line">    &#123;</span><br><span class="line">      <span class="type">StringTokenizer</span> <span class="variable">itr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString());</span><br><span class="line">      <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">        <span class="built_in">this</span>.word.set(itr.nextToken());</span><br><span class="line">        context.write(<span class="built_in">this</span>.word, one);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>Reducer组件</li>
</ol>
<ul>
<li>用户自定义类会继承Reducer类，并且给出4个泛型，即reduce()方法的输入键、输入值、输出键和输出值的数据类型。</li>
<li>Reducer组件的输入数据是键&#x2F;值对形式，其中，输入键和输入值的数据类型与Mapper的输出键和输出值的数据类型必须匹配。</li>
<li>Reducer组件的业务逻辑写在reduce()方法中，实现逻辑由用户自定义。</li>
<li>ReduceTask进程对每组相同的键&#x2F;值对都只调用一次reduce()方法。有多少个key就调用多少个reduce()方法。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">IntSumReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt;</span><br><span class="line">    <span class="comment">//用户自定义类会继承Reducer类，并且给出4个泛型，</span></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span><span class="comment">//即reduce()方法的输入键、输入值、输出键和输出值的数据类型</span></span><br><span class="line">      <span class="keyword">throws</span> IOException, InterruptedException</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">        sum += val.get();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="built_in">this</span>.result.set(sum);</span><br><span class="line">      context.write(key, <span class="built_in">this</span>.result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>Driver驱动器</li>
</ol>
<ul>
<li>主要负责运行整个MapReduce程序，指定MapReduce程序的执行规范，以及控制整个MapReduce程序的运行。在这部分中，我们首先构建一个Job对象，通过Job对象指定输入数据和输出数据的路径，指定MapReduce程序的Mapper组件和Reduce组件，指定输出键和输出值的数据类型。相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象。</li>
</ul>
<h4 id="5-2-3-WordCount案例实操"><a href="#5-2-3-WordCount案例实操" class="headerlink" title="5.2.3 WordCount案例实操"></a>5.2.3 WordCount案例实操</h4><h5 id="1-需求说明"><a href="#1-需求说明" class="headerlink" title="1. 需求说明"></a>1. 需求说明</h5><p>给hello.txt文件统计词频</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">atguigu atguigu</span><br><span class="line">ss ss</span><br><span class="line">cls cls</span><br><span class="line">jiao</span><br><span class="line">banzhang</span><br><span class="line">xue</span><br><span class="line">hadoop</span><br></pre></td></tr></table></figure>

<p>期望得到：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">atguigu 2</span><br><span class="line">banzhang 1</span><br><span class="line">cls 2</span><br><span class="line">hadoop 1</span><br><span class="line">jiao 1</span><br><span class="line">ss 2</span><br><span class="line">xue 1</span><br></pre></td></tr></table></figure>

<h5 id="2-分析"><a href="#2-分析" class="headerlink" title="2. 分析"></a>2. 分析</h5><p><img src="Snipaste_2023-09-25_14-39-40.png" alt="Snipaste_2023-09-25_14-39-40"></p>
<h5 id="3-环境准备"><a href="#3-环境准备" class="headerlink" title="3. 环境准备"></a>3. 环境准备</h5><p>使用IDEA新建一个Maven工程，命名为“MapReduceDemo”，在pom.xml文件中添加以下依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.30<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>在项目的 src&#x2F;main&#x2F;resources 目录下，新建一个文件，命名为“log4j.properties”，在文件中填入：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, stdout </span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender </span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n </span><br><span class="line">log4j.appender.logfile=org.apache.log4j.FileAppender </span><br><span class="line">log4j.appender.logfile.File=target/spring.log </span><br><span class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout </span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure>

<p>编写程序：三个类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//编写Mapper类</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * KEYIN, map阶段输入的key的类型：LongWritable类型</span></span><br><span class="line"><span class="comment"> * VALUEIN, map阶段输入value类型：Text类型</span></span><br><span class="line"><span class="comment"> * KEYOUT, map阶段输出的key类型：Text：类型</span></span><br><span class="line"><span class="comment"> * VALUEOUT,map阶段输出的value类型：IntWritable类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text,Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">Text</span> <span class="variable">k</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//LongWritable key 是map输入的key;Text value是map输入的value;Context context充当联络员的角色</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.获取一行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.对这行数据进行切割</span></span><br><span class="line">        String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.循环写出（增强for循环）</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words)&#123;</span><br><span class="line">            k.set(word);<span class="comment">//Text类中的set方法通常用于设置Text对象的值。它接受一个字节数组（byte array）作为参数，并将这个数组的内容复制到Text对象中</span></span><br><span class="line">            context.write(k,v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//经历了多次map操作结果为：</span></span><br><span class="line">    <span class="comment">//(atguigu,1)</span></span><br><span class="line">    <span class="comment">//(atguigu,1)</span></span><br><span class="line">    <span class="comment">//(ss,1)</span></span><br><span class="line">    <span class="comment">//(ss,1)</span></span><br><span class="line">    <span class="comment">//(cls,1)</span></span><br><span class="line">    <span class="comment">//(cls,1)</span></span><br><span class="line">    <span class="comment">//(jiao,1)</span></span><br><span class="line">    <span class="comment">//(banzhang,1)</span></span><br><span class="line">    <span class="comment">//(xue,1)</span></span><br><span class="line">    <span class="comment">//(hadoop,1)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//编写Reduce类</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * KEYIN, reduce阶段输入的key的类型：Text类型</span></span><br><span class="line"><span class="comment"> * VALUEIN, reduce阶段输入value类型：IntWritable类型</span></span><br><span class="line"><span class="comment"> * KEYOUT, reduce阶段输出的key类型：Text类型</span></span><br><span class="line"><span class="comment"> * VALUEOUT,reduce阶段输出的value类型：IntWritable类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line">    <span class="type">int</span> sum;</span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//1.累加求和</span></span><br><span class="line">        sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable count : values)&#123;</span><br><span class="line">            sum += count.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.输出</span></span><br><span class="line">        v.set(sum);</span><br><span class="line">        context.write(key,v);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//reduce()方法每次获取的都是相同输入键的一组键/值对</span></span><br><span class="line">    <span class="comment">// key    values</span></span><br><span class="line">    <span class="comment">//(atguigu,(1,1))----&gt;执行一次reduce()方法---&gt;输出(atguigu,2)</span></span><br><span class="line">    <span class="comment">//(ss,(1,1))----&gt;执行一次reduce()方法---&gt;输出(ss,2)</span></span><br><span class="line">    <span class="comment">//(cls,(1,1))----&gt;执行一次reduce()方法---&gt;输出(cls,2)</span></span><br><span class="line">    <span class="comment">//(jiao,1)----&gt;执行一次reduce()方法---&gt;输出(jiao,1)</span></span><br><span class="line">    <span class="comment">//(banzhang,1)----&gt;执行一次reduce()方法---&gt;输出(banzhang,1)</span></span><br><span class="line">    <span class="comment">//(xue,1)----&gt;执行一次reduce()方法---&gt;输出(xue,1)</span></span><br><span class="line">    <span class="comment">//(hadoop,1)----&gt;执行一次reduce()方法---&gt;输出(hadoop,1)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//编写Driver驱动器</span></span><br><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName: WordCountDriver</span></span><br><span class="line"><span class="comment"> * Package: com.atguigu.mapreduce.wordcount</span></span><br><span class="line"><span class="comment"> * Description:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> 宇涵 王</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Create</span> 2023/9/25 0025 15:01</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="comment">//1.获取配置信息以及获取 job 对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.关联本 Driver 程序的 jar</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.关联 Mapper 和 Reducer 的 jar</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.设置 Mapper 输出的 kv 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5.设置最终输出 kv 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6.设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\11_input\\inputword&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\output&quot;</span>));<span class="comment">//注意：这个output目录不能提前存在</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//7.提交 job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);<span class="comment">//如果成功返回0，失败返回1</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="4-本地测试"><a href="#4-本地测试" class="headerlink" title="4. 本地测试"></a>4. 本地测试</h5><p>运行Driver类的main()方法，在输出目录output中查看如下输出结果：</p>
<p><img src="Snipaste_2023-09-26_12-05-19.png" alt="Snipaste_2023-09-26_12-05-19"></p>
<p>可以发现有四个文件，其中真正的数据结果文件是part-r-00000，文件内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">atguigu	2</span><br><span class="line">banzhang	1</span><br><span class="line">cls	2</span><br><span class="line">hadoop	1</span><br><span class="line">jiao	1</span><br><span class="line">ss	2</span><br><span class="line">xue	1</span><br></pre></td></tr></table></figure>

<p>已经对原文件进行了词频统计</p>
<h5 id="5-集群测试"><a href="#5-集群测试" class="headerlink" title="5. 集群测试"></a>5. 集群测试</h5><p>在pom.xml文件中添加打包插件依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>因为程序最重要被提交到集群上运行，所以输入路径和输出路径不是在代码中设置的，而是通过参数传入的，将输入路径和输出路径的设置代码修改如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br></pre></td></tr></table></figure>

<p>将程序打包成jar包，操作如下：</p>
<p><img src="Snipaste_2023-09-26_12-51-41.png" alt="Snipaste_2023-09-26_12-51-41"></p>
<p>将此jar包复制到桌面并重命名为WC.jar，并将该jar包复制到&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;路径下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# ll</span><br><span class="line">总用量 212</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     183 9月  12 2019 bin</span><br><span class="line">drwxr-xr-x. 4 root root     37 9月  15 11:02 data</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 wyh  wyh     106 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh      20 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh     288 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh  147145 9月   4 2019 LICENSE.txt</span><br><span class="line">-rw-r--r--. 1 root root      7 9月  17 21:04 liubei.txt</span><br><span class="line">drwxrwxrwx. 3 root root   4096 9月  26 11:41 logs</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh   21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 wyh  wyh    1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 wyh  wyh    4096 9月  14 22:32 sbin</span><br><span class="line">drwxr-xr-x. 4 wyh  wyh      31 9月  12 2019 share</span><br><span class="line">-rw-r--r--. 1 root root     14 9月  17 21:14 shuguo2.txt</span><br><span class="line">-rw-r--r--. 1 root root     14 9月  17 21:10 shuguo.txt</span><br><span class="line">drwxr-xr-x. 2 root root     22 8月   7 10:36 wcinput</span><br><span class="line">-rw-r--r--. 1 root root   9910 9月  26 12:50 WC.jar   # jar包在这里</span><br><span class="line">drwxr-xr-x. 2 root root     88 8月   7 10:45 wcoutput</span><br><span class="line">-rw-r--r--. 1 root root      7 9月  17 20:31 weiguo.txt</span><br><span class="line">-rw-r--r--. 1 root root      6 9月  17 20:42 wuguo.txt</span><br></pre></td></tr></table></figure>

<p>启动hadoop集群；</p>
<p>执行WC.jar</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop jar WC.jar com.atguigu.mapreduce.wordcount2.WordCountDriver /input /output2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">其中，com.atguigu.mapreduce.wordcount2.WordCountDriver为全类名。HDFS的/input目录下有一个word.txt文件，要将结果输出在output2目录下（要求output2目录之前不存在）</span></span><br></pre></td></tr></table></figure>

<p>程序执行成功：</p>
<img src="Snipaste_2023-09-26_13-05-13.png" alt="Snipaste_2023-09-26_13-05-13" style="zoom: 33%;">

<p><img src="Snipaste_2023-09-26_13-05-45.png" alt="Snipaste_2023-09-26_13-05-45"></p>
<h3 id="5-3-Hadoop的序列化"><a href="#5-3-Hadoop的序列化" class="headerlink" title="5.3 Hadoop的序列化"></a>5.3 Hadoop的序列化</h3><h4 id="5-3-1-序列化的概念"><a href="#5-3-1-序列化的概念" class="headerlink" title="5.3.1 序列化的概念"></a>5.3.1 序列化的概念</h4><p>序列化是指将<strong>内存</strong>中的对象转换成字节序列（或其他数据传输协议），以便将其存储于<strong>磁盘</strong>中（持久化存储）或进行网络传输的过程。</p>
<p>反序列化是指将收到的字节序列（或其他数据传输协议）或<strong>磁盘</strong>中的持久化数据转换成<strong>内存</strong>中对象的过程。</p>
<p>通过序列化可以将对象在进程之间进行<strong>通信</strong>，以及使对象<strong>持久化存储</strong>。</p>
<p>Java具有一套序列化机制（Writable），但是Java的序列化机制是一个重量级的序列化框架（Serializable）。一个对象在被序列化后，会附带很多额外信息，不便于在网络中高效传输。所以，Hadoop开发了一套自己的序列化机制。</p>
<p><strong>Hadoop序列化机制的特点：</strong></p>
<ul>
<li>紧凑：紧凑的格式有助于高效使用存储空间，充分利用网络带宽</li>
<li>快速：序列化和反序列化的性能开销很小，可以实现进程之间的快速通信</li>
<li>互操作：统一的序列化框架可以支持多语言和服务器的交互</li>
</ul>
<h4 id="5-3-2-Writable接口"><a href="#5-3-2-Writable接口" class="headerlink" title="5.3.2 Writable接口"></a>5.3.2 Writable接口</h4><p>Hadoop使用的序列化接口是Writable，在Writable接口中定义了两个方法，分别是<strong>write()方法和readFields()<strong>方法。write()方法主要用于将类中的信息</strong>写入</strong>DataOutput二进制流，readFields()方法主要用于从DataInput二进制流中<strong>读取</strong>信息。</p>
<ul>
<li>write()方法：序列化，内存——–&gt;磁盘</li>
<li>readFields()方法：反序列化，磁盘——–&gt;内存</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@InterfaceAudience</span>.Public</span><br><span class="line"><span class="meta">@InterfaceStability</span>.Stable</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">Writable</span> &#123;</span><br><span class="line">  <span class="comment">/** </span></span><br><span class="line"><span class="comment">   * Serialize the fields of this object to &lt;code&gt;out&lt;/code&gt;.</span></span><br><span class="line"><span class="comment">   * </span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> out &lt;code&gt;DataOuput&lt;/code&gt; to serialize this object into.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** </span></span><br><span class="line"><span class="comment">   * Deserialize the fields of this object from &lt;code&gt;in&lt;/code&gt;.  </span></span><br><span class="line"><span class="comment">   * </span></span><br><span class="line"><span class="comment">   * &lt;p&gt;For efficiency, implementations should attempt to re-use storage in the </span></span><br><span class="line"><span class="comment">   * existing object where possible.&lt;/p&gt;</span></span><br><span class="line"><span class="comment">   * </span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> in &lt;code&gt;DataInput&lt;/code&gt; to deseriablize this object from.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Hadoop为java中的基本数据类型提供了Writable接口的实现类，Writable接口的所有实现类都提供了get()方法和set()方法，分别用于获取和存储所封装的值。</p>
<p><img src="Snipaste_2023-09-25_09-30-13.png" alt="Snipaste_2023-09-25_09-30-13"></p>
<p><strong>用户不仅可以使用Hadoop提供的序列化数据类型，还可以自定义Writable接口的实现类，用于在Hadoop环境中实现序列化。</strong></p>
<h4 id="5-3-3-序列化案例实操"><a href="#5-3-3-序列化案例实操" class="headerlink" title="5.3.3 序列化案例实操"></a>5.3.3 序列化案例实操</h4><h5 id="（1）需求分析"><a href="#（1）需求分析" class="headerlink" title="（1）需求分析"></a>（1）需求分析</h5><p>有一个数据文件，存储了大量的手记号码、IP地址、浏览页面、上行流量、下行流量等信息，要求统计每个手记号码耗费的总上行流量、总下行流量、总流量</p>
<img src="Snipaste_2023-09-26_15-20-12.png" alt="Snipaste_2023-09-26_15-20-12" style="zoom:33%;">

<p>map输出kv和reduce输入kv：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  K(Text)          V(bean对象)</span><br><span class="line">(手记号码，(上行流量，下行流量，总流量))</span><br></pre></td></tr></table></figure>

<p>reduce输出kv：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  K(Text)          V(bean对象)</span><br><span class="line">(手记号码，(上行流量，下行流量，总流量))</span><br></pre></td></tr></table></figure>

<h5 id="（2）实现思路分析"><a href="#（2）实现思路分析" class="headerlink" title="（2）实现思路分析"></a>（2）实现思路分析</h5><p>在map阶段对接收的每行数据都进行切分，抽取关键字段（手记号码、上行流量、下行流量、总流量），将其封装成FlowBean对象，并且将手机号码字段设置为key，然后将数据输出。在reduce阶段，每次接收到同一个手机号码的一组FlowBean对象，都对这组FlowBean对象进行汇总，计算得到总流量，将手机号码字段设置为key，然后将数据输出。</p>
<img src="webwxgetmsgi666666mg.jpg" alt="webwxgetmsgi666666mg" style="zoom:50%;">

<h5 id="（3）编写MapReduce程序"><a href="#（3）编写MapReduce程序" class="headerlink" title="（3）编写MapReduce程序"></a>（3）编写MapReduce程序</h5><p>①编写FlowBean类，使其继承Writable接口：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName: FlowBean</span></span><br><span class="line"><span class="comment"> * Package: com.atguigu.mapreduce.writable</span></span><br><span class="line"><span class="comment"> * Description:</span></span><br><span class="line"><span class="comment"> *  创建一个封装流量信息的JavaBean，要继承Writable接口，其中有三个重要属性，上行流量，</span></span><br><span class="line"><span class="comment"> *  下行流量，总流量</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> 宇涵 王</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Create</span> 2023/9/26 0026 15:37</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123; <span class="comment">//1.继承Writable接口</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;<span class="comment">//上行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;<span class="comment">//下行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;<span class="comment">//总流量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.提供无参构造器</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.get,set方法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getUpFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setUpFlow</span><span class="params">(<span class="type">long</span> upFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getDownFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setDownFlow</span><span class="params">(<span class="type">long</span> downFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">(<span class="type">long</span> sumFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span>&#123; <span class="comment">//相当于方法重载</span></span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;<span class="comment">//4.重写序列化方法</span></span><br><span class="line">        out.writeLong(upFlow);</span><br><span class="line">        out.writeLong(downFlow);</span><br><span class="line">        out.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">//4.重写反序列化方法，注意反序列化的顺序和序列化的顺序完全一致，并且读取方法的调用方法于参数的类型也要一一对应</span></span><br><span class="line">        <span class="built_in">this</span>.upFlow = in.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = in.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//5.重写toString方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>②编写Mapper接口：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName: FlowMapper</span></span><br><span class="line"><span class="comment"> * Package: com.atguigu.mapreduce.writable</span></span><br><span class="line"><span class="comment"> * Description:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> 宇涵 王</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Create</span> 2023/9/26 0026 16:00</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, FlowBean&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line">    <span class="comment">//重写map方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//1.获取一行数据，将其转换成字符串</span></span><br><span class="line">        <span class="comment">//1	  13736230513	192.196.100.1	www.atguigu.com	  2481	 24681	 200</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.切割数据</span></span><br><span class="line">        <span class="comment">//[1,13736230513,192.196.100.1,www.atguigu.com,2481,24681,200]</span></span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.抓取我们想要的数据</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> split[<span class="number">1</span>];<span class="comment">//手记号码</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">up</span> <span class="operator">=</span> split[split.length - <span class="number">3</span>];<span class="comment">//上行流量</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">down</span> <span class="operator">=</span> split[split.length - <span class="number">2</span>];<span class="comment">//下行流量</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//4.封装</span></span><br><span class="line">        outK.set(phone);</span><br><span class="line">        outV.setUpFlow(Long.parseLong(up));</span><br><span class="line">        outV.setDownFlow(Long.parseLong(down));</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//5.写出outK,outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>③编写Reducer组件，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName: FlowReducer</span></span><br><span class="line"><span class="comment"> * Package: com.atguigu.mapreduce.writable</span></span><br><span class="line"><span class="comment"> * Description:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> 宇涵 王</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Create</span> 2023/9/26 0026 21:12</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, FlowBean, Text, FlowBean&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Reducer&lt;Text, FlowBean, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">totalUp</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">long</span> <span class="variable">totalDown</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//1.遍历values，将其中的上行流量，下行流量分别累加</span></span><br><span class="line">        <span class="keyword">for</span> (FlowBean flowBean : values)&#123;</span><br><span class="line">            totalUp += flowBean.getUpFlow();</span><br><span class="line">            totalDown += flowBean.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//2.封装outV</span></span><br><span class="line">        outV.setUpFlow(totalUp);</span><br><span class="line">        outV.setDownFlow(totalDown);</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//3.写出key和outV</span></span><br><span class="line">        context.write(key,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>④编写Driver驱动类，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName: FlowDriver</span></span><br><span class="line"><span class="comment"> * Package: com.atguigu.mapreduce.writable</span></span><br><span class="line"><span class="comment"> * Description:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> 宇涵 王</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Create</span> 2023/9/26 0026 21:23</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Version</span> 1.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.获取Job对象job</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.关联当前Driver类</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.关联Mapper组件和Reducer组件</span></span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.设置map端输出KV的数据类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//5.设置程序最终输出KV的数据类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//6.设置程序的输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\11_input\\inputflow&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\output3&quot;</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//7.提交job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>⑤运行Driver类中的main()方法，查看输出文件中的内容。由此可见，JavaBean对象通过实现Writable接口，可以实现进程之间的通信。</p>
<img src="Snipaste_2023-09-26_21-41-49.png" alt="Snipaste_2023-09-26_21-41-49" style="zoom:33%;">

<h3 id="5-4-MapReduce框架原理之InputFormat数据输入"><a href="#5-4-MapReduce框架原理之InputFormat数据输入" class="headerlink" title="5.4 MapReduce框架原理之InputFormat数据输入"></a>5.4 MapReduce框架原理之InputFormat数据输入</h3><p>MapReduce计算框架将数据的计算过程分为<strong>数据输入阶段</strong>、<strong>map阶段</strong>、<strong>reduce阶段</strong>和<strong>数据输出阶段</strong>。</p>
<p>在数据输入阶段，我们需要考虑<strong>如何划分数据，可以在提高任务并发度的同时提高集群性能</strong>。</p>
<img src="Snipaste_2023-09-27_12-20-44.png" alt="Snipaste_2023-09-27_12-20-44" style="zoom:50%;">

<h4 id="5-4-1-切片与MapTask并行度决定机制"><a href="#5-4-1-切片与MapTask并行度决定机制" class="headerlink" title="5.4.1 切片与MapTask并行度决定机制"></a>5.4.1 切片与MapTask并行度决定机制</h4><p>MapTask的并行度决定map阶段的任务处理并发度，进而影响整个Job的处理速度。</p>
<ul>
<li>数据块：Block是HDFS<strong>物理上</strong>把数据分成若干个数据块，<strong>数据块是HDFS的数据存储单位</strong>。</li>
<li>数据切片：指在<strong>逻辑上</strong>把输入数据进行切分，并不会在物理层面上将输入数据切分成片。<strong>数据切片是MapReduce程序计算输入数据的单位</strong>，一个数据切片可以对应启动一个MapTask。</li>
</ul>
<p>假设一个数据集由两个文件构成，这两个文件大小分别为300MB和100MB。假设将数据切片大小设置为128MB，那么会将这两个文件切分成4片，切片信息如下：</p>
<ul>
<li>split1：0~128MB</li>
<li>split2：129~256MB</li>
<li>split3：257~300MB</li>
<li>split4：1~100MB</li>
</ul>
<p>数据切片大小与数据块大小正好吻合，每个数据块都是一个数据切片：</p>
<img src="webwxgetmsgimg (1).jpg" alt="webwxgetmsgimg (1)" style="zoom: 33%;">

<hr>
<p>假设将数据切片大小设置为100MB，那么：</p>
<ul>
<li>split1：0~100MB</li>
<li>split2：101~200MB</li>
<li>split3：201~300MB</li>
<li>split4：0~100MB</li>
</ul>
<p>这时候数据切片大小与数据块大小不吻合：</p>
<img src="webwxgetmsgimg (2).jpg" alt="webwxgetmsgimg (2)" style="zoom:33%;">

<p>总结：</p>
<ol>
<li>一个Job的Map阶段并行度是由客户端在提交Job时的<strong>切片数</strong>决定的</li>
<li>默认情况下，数据切片大小与数据块大小相同，或者是数据块大小的倍数，即128MB或128MB的倍数（原因：与数据块大小相同的数据切片，可以保证一个MapTask处理的任务正好是一个数据块，Hadoop可以将该MapTask发送给数据块所在的节点服务器，实现计算的<strong>数据本地化</strong>，提高计算性能，节省带宽资源）</li>
<li>每一个切片分配一个MapTask并行实例处理。</li>
<li>切片时不考虑数据整体，只会针对数据集中的每个文件单独进行切片。</li>
</ol>
<h4 id="5-4-2-Job提交流程源码和FileInputFormat切片源码详解"><a href="#5-4-2-Job提交流程源码和FileInputFormat切片源码详解" class="headerlink" title="5.4.2 Job提交流程源码和FileInputFormat切片源码详解"></a>5.4.2 Job提交流程源码和FileInputFormat切片源码详解</h4><h5 id="1-Job提交流程源码详解"><a href="#1-Job提交流程源码详解" class="headerlink" title="1. Job提交流程源码详解"></a>1. Job提交流程源码详解</h5><p>在编写WordCount程序的代码时，在Driver类中，最后会通过Job对象job调用waitForCompletion()方法提交任务</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br></pre></td></tr></table></figure>

<p>去看waitForCompletion()方法源码，可以发现，最终调用submit()方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">waitForCompletion</span><span class="params">(<span class="type">boolean</span> verbose</span></span><br><span class="line"><span class="params">                                   )</span> <span class="keyword">throws</span> IOException, InterruptedException,</span><br><span class="line">                                            ClassNotFoundException &#123;</span><br><span class="line">    <span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">      submit();</span><br><span class="line">    &#125;</span><br><span class="line">                                                </span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在submit()方法中，首先通过调用connect()方法建立连接，在建立连接时，首先通过现有配置文件获取Cluster对象，同时判断是本地运行环境还是YARN集群运行环境：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 建立连接</span></span><br><span class="line">connect();</span><br><span class="line">    <span class="comment">// 1）创建提交 Job 的代理</span></span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Cluster</span>(getConfiguration());</span><br><span class="line">        <span class="comment">// （1）判断是本地运行环境还是 yarn 集群运行环境</span></span><br><span class="line">        initialize(jobTrackAddr, conf);</span><br></pre></td></tr></table></figure>

<p>在建立连接后，创建JobSubmiter的实例化对象submitter，submitter对象最终调用submitJobInternal()方法完成任务</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2 提交 job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="built_in">this</span>, cluster)</span><br></pre></td></tr></table></figure>

<p>进一步追溯submitJobInternal()方法，该方法中关键步骤源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1）创建给集群提交数据的 Stag 路径</span></span><br><span class="line"><span class="type">Path</span> <span class="variable">jobStagingArea</span> <span class="operator">=</span> JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"><span class="comment">// 2）获取 jobid ，并创建 Job 路径</span></span><br><span class="line"><span class="type">JobID</span> <span class="variable">jobId</span> <span class="operator">=</span> submitClient.getNewJobID();</span><br><span class="line"><span class="comment">// 3）拷贝 jar 包到集群</span></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"><span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">input.getSplits(job);<span class="comment">//使用InputFormat类的getSplits()方法对文件进行切分</span></span><br><span class="line"><span class="comment">// 5）向 Stag 路径写 XML 配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">conf.writeXml(out);</span><br><span class="line"><span class="comment">// 6）提交 Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(),job.getCredentials());</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-09-27_14-07-39.png" alt="Snipaste_2023-09-27_14-07-39"></p>
<h5 id="2-FileInputFormat切片源码详解"><a href="#2-FileInputFormat切片源码详解" class="headerlink" title="2. FileInputFormat切片源码详解"></a>2. FileInputFormat切片源码详解</h5><p>分析FileInputFormat类的源码。FileInputFormat类是所有使用文件作为其数据源的InputFormat实现类的基类，该类主要实现了两个功能，一个是指定作业的<strong>输入文件位置</strong>，另一个是将<strong>输入文件切分成数据分片</strong>的代码实现。</p>
<p>在提交任务时，使用以下代码指定输入文件路径</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br></pre></td></tr></table></figure>

<p>查看FileInputFormat类源码，搜索getSplits()方法，对文件的分片逻辑主要体现在这个方法的实现中：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title function_">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="type">StopWatch</span> <span class="variable">sw</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StopWatch</span>().start();</span><br><span class="line">    <span class="type">long</span> <span class="variable">minSize</span> <span class="operator">=</span> Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">    <span class="type">long</span> <span class="variable">maxSize</span> <span class="operator">=</span> getMaxSplitSize(job);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// generate splits</span></span><br><span class="line">    List&lt;InputSplit&gt; splits = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;InputSplit&gt;();</span><br><span class="line">    List&lt;FileStatus&gt; files = listStatus(job);<span class="comment">//（1）程序先找到数据存储的目录</span></span><br><span class="line"></span><br><span class="line">    <span class="type">boolean</span> <span class="variable">ignoreDirs</span> <span class="operator">=</span> !getInputDirRecursive(job)</span><br><span class="line">      &amp;&amp; job.getConfiguration().getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, <span class="literal">false</span>);</span><br><span class="line">    <span class="keyword">for</span> (FileStatus file: files) &#123;<span class="comment">//（2）开始遍历处理（规划切片）数据存储目录下的每个文件</span></span><br><span class="line">      <span class="keyword">if</span> (ignoreDirs &amp;&amp; file.isDirectory()) &#123;</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> file.getPath();</span><br><span class="line">      <span class="type">long</span> <span class="variable">length</span> <span class="operator">=</span> file.getLen();<span class="comment">//（3）-①遍历第一个文件，获取文件大小</span></span><br><span class="line">      <span class="keyword">if</span> (length != <span class="number">0</span>) &#123;</span><br><span class="line">        BlockLocation[] blkLocations;</span><br><span class="line">        <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span><br><span class="line">          blkLocations = ((LocatedFileStatus) file).getBlockLocations();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> path.getFileSystem(job.getConfiguration());</span><br><span class="line">          blkLocations = fs.getFileBlockLocations(file, <span class="number">0</span>, length);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (isSplitable(job, path)) &#123;</span><br><span class="line">          <span class="type">long</span> <span class="variable">blockSize</span> <span class="operator">=</span> file.getBlockSize();</span><br><span class="line">          <span class="type">long</span> <span class="variable">splitSize</span> <span class="operator">=</span> computeSplitSize(blockSize, minSize, maxSize);<span class="comment">//（3）-②计算数据切片大小，计算公式看下面</span></span><br><span class="line"></span><br><span class="line">          <span class="type">long</span> <span class="variable">bytesRemaining</span> <span class="operator">=</span> length;</span><br><span class="line">          <span class="keyword">while</span> (((<span class="type">double</span>) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;<span class="comment">//(3)-③在每次进行切片操作时，都要判断剩余部分是否大于数据切片大小的1.1倍，如果剩余部分不大于数据切片大小的1.1倍，则将剩余部分划分为一个数据切片</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">blkIndex</span> <span class="operator">=</span> getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span><br><span class="line">                        blkLocations[blkIndex].getHosts(),</span><br><span class="line">                        blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">            bytesRemaining -= splitSize;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">blkIndex</span> <span class="operator">=</span> getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,</span><br><span class="line">                       blkLocations[blkIndex].getHosts(),</span><br><span class="line">                       blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// not splitable</span></span><br><span class="line">          <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">            <span class="comment">// Log only if the file is big enough to be splitted</span></span><br><span class="line">            <span class="keyword">if</span> (length &gt; Math.min(file.getBlockSize(), minSize)) &#123;</span><br><span class="line">              LOG.debug(<span class="string">&quot;File is not splittable so no parallelization &quot;</span></span><br><span class="line">                  + <span class="string">&quot;is possible: &quot;</span> + file.getPath());</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          splits.add(makeSplit(path, <span class="number">0</span>, length, blkLocations[<span class="number">0</span>].getHosts(),</span><br><span class="line">                      blkLocations[<span class="number">0</span>].getCachedHosts()));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">        <span class="comment">//Create empty hosts array for zero length files</span></span><br><span class="line">        splits.add(makeSplit(path, <span class="number">0</span>, length, <span class="keyword">new</span> <span class="title class_">String</span>[<span class="number">0</span>]));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Save the number of input files for metrics/loadgen</span></span><br><span class="line">    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());</span><br><span class="line">    sw.stop();</span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">&quot;Total # of splits generated by getSplits: &quot;</span> + splits.size()</span><br><span class="line">          + <span class="string">&quot;, TimeTaken: &quot;</span> + sw.now(TimeUnit.MILLISECONDS));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> splits;<span class="comment">//List&lt;InputSplit&gt; splits = new ArrayList&lt;InputSplit&gt;();将数据切片信息写入一个数据切片规划列表InputSplit，InputSplit中只记录数据切片的元数据信息，如起始位置、长度、所在节点列表等。</span></span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//（4）在切片操作完成后，将数据切片规划文件提交到YARN中，YARN中的MRAppMaster进程可以根据数据切片规划文件计算需要开启的MapTask数量（几个数据切片就开启几个MapTask实例）</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//计算数据切片大小的重要公式：</span></span><br><span class="line"><span class="keyword">protected</span> <span class="type">long</span> <span class="title function_">computeSplitSize</span><span class="params">(<span class="type">long</span> blockSize, <span class="type">long</span> minSize, <span class="type">long</span> maxSize)</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> Math.max(minSize, Math.min(maxSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>属性</th>
<th>数据类型</th>
<th>默认值（单位：字节）</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.input.fileinputformat.split.minsize</td>
<td>int</td>
<td>1</td>
<td>一个文件分片中最小的有效字节数</td>
</tr>
<tr>
<td>mapreduce.input.fileinputformat.split.maxsize</td>
<td>long</td>
<td>long.MAX_VALUE，即9223372036854775807</td>
<td>一个文件分片中最大的有效字节数</td>
</tr>
<tr>
<td>dfs.blocksize</td>
<td>long</td>
<td>134217728，即128MB</td>
<td>HDFS中数据块的大小</td>
</tr>
</tbody></table>
<p>在默认情况下，minsize&lt;blocksize&lt;maxsize，所以blocksize属性值表示默认的数据切片大小。在理论上，我们可以通过调整这3个关键属性的值调整数据切片的大小，但是与数据块大小相同的数据切片大小是最合理的。</p>
<p>切片大小的相关设置如下：</p>
<ul>
<li>maxsize（数据切片的最大值）：如果该属性的值比blocksize属性的值小，则会让数据切片变小，相当于配置这个属性的值</li>
<li>minsize（数据切片的最小值）：如果该属性的值比blocksize属性的值大，则会让数据切片大小变得比blocksize属性的值还大</li>
</ul>
<img src="Snipaste_2023-09-27_15-33-02.png" alt="Snipaste_2023-09-27_15-33-02" style="zoom:50%;">

<h4 id="5-4-3-FileInputFormat切片机制总结"><a href="#5-4-3-FileInputFormat切片机制总结" class="headerlink" title="5.4.3 FileInputFormat切片机制总结"></a>5.4.3 FileInputFormat切片机制总结</h4><h5 id="1-切片机制"><a href="#1-切片机制" class="headerlink" title="1. 切片机制"></a>1. 切片机制</h5><img src="Snipaste_2023-09-27_15-35-37.png" alt="Snipaste_2023-09-27_15-35-37" style="zoom: 50%;">

<h5 id="2-案例分析"><a href="#2-案例分析" class="headerlink" title="2. 案例分析"></a>2. 案例分析</h5><img src="Snipaste_2023-09-27_15-36-35.png" alt="Snipaste_2023-09-27_15-36-35" style="zoom:50%;">

<h5 id="3-参数设置"><a href="#3-参数设置" class="headerlink" title="3. 参数设置"></a>3. 参数设置</h5><p>略</p>
<h4 id="5-4-4-TextInputFormat"><a href="#5-4-4-TextInputFormat" class="headerlink" title="5.4.4 TextInputFormat"></a>5.4.4 TextInputFormat</h4><h5 id="1-FileInputFormat接口的实现类"><a href="#1-FileInputFormat接口的实现类" class="headerlink" title="1.FileInputFormat接口的实现类"></a>1.FileInputFormat接口的实现类</h5><p>在运行 MapReduce 程序时，输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等。那么，针对不同的数据类型，MapReduce 是如何读取这些数据的呢？</p>
<p>FileInputFormat 常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat 和自定义 InputFormat 等。</p>
<h5 id="2-TextInputFormat"><a href="#2-TextInputFormat" class="headerlink" title="2. TextInputFormat"></a>2. TextInputFormat</h5><p>TextInputFormat是进行普通文件输入的默认FileInputFormat接口实现类，可以<strong>按行</strong>读取每条记录，读取的键是<strong>该行在整个文件中的起始字节偏移量</strong>，数据类型为LongWritable类型；读取的值是<strong>该行的内容</strong>，不包括任何行终止符（换行符和回车符），数据类型为Text类型。</p>
<p>以下是一个示例，比如，一个分片包含了如下 4 条文本记录。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br></pre></td></tr></table></figure>

<p>每条记录表示为以下键&#x2F;值对：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(0,Rich learning form)</span><br><span class="line">(20,Intelligent learning engine)</span><br><span class="line">(49,Learning more convenient)</span><br><span class="line">(74,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure>

<h4 id="5-4-5-CombineTextInputFormat切片机制"><a href="#5-4-5-CombineTextInputFormat切片机制" class="headerlink" title="5.4.5 CombineTextInputFormat切片机制"></a>5.4.5 CombineTextInputFormat切片机制</h4><p> 框架默认的 TextInputFormat 切片机制是对任务按文件规划切片（一个文件，一个切片），不管文件多小，都会是一个单独的切片，都会交给一个 MapTask，这样如果有<strong>大量小文件</strong>，就会产生大量的MapTask，处理效率极其低下。</p>
<p>CombineTextInputFormat 用于<strong>小文件过多的场景</strong>，它可以将<strong>多个小文件</strong>从逻辑上规划到<strong>一个切片</strong>中，这样，多个小文件就可以交给一个 MapTask 处理。</p>
<p>通过以下代码设置数据切片的最大值：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);<span class="comment">// 4m</span></span><br></pre></td></tr></table></figure>

<p>CombineTextInputFormat切片机制的具体切片过程分为两部分，分别为虚拟存储过程和切片过程：</p>
<h5 id="1-虚拟存储过程"><a href="#1-虚拟存储过程" class="headerlink" title="1. 虚拟存储过程"></a>1. 虚拟存储过程</h5><p>将输入目录所有文件的大小 filesize 依次与设置的 setMaxInputSplitSize 值进行比较：</p>
<p>filesize &lt;&#x3D;  setMaxInputSplitSize，在逻辑上将其划分为一个虚拟存储块；</p>
<p>filesize &gt; 2*setMaxInputSplitSize，以setMaxInputSplitSize值划分第一个虚拟存储块；</p>
<p>setMaxInputSplitSize &lt; filesize &lt;&#x3D; 2*setMaxInputSplitSize，将文件均分为两个虚拟存储块；</p>
<h5 id="2-切片过程"><a href="#2-切片过程" class="headerlink" title="2. 切片过程"></a>2. 切片过程</h5><p>判断虚拟存储块的大小和setMaxInputSplitSize的大小：</p>
<p>虚拟存储块大小 &gt;&#x3D; setMaxInputSplitSize，将其单独划分为一个数据切片；</p>
<p>虚拟存储块大小 &lt; setMaxInputSplitSize，将与其下一个虚拟存储块进行合并，共同划分为一个数据切片；</p>
<p><img src="%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6(2).jpg" alt="未命名文件(2)"></p>
<h4 id="5-4-6-CombineTextInputFormat案例教程"><a href="#5-4-6-CombineTextInputFormat案例教程" class="headerlink" title="5.4.6 CombineTextInputFormat案例教程"></a>5.4.6 CombineTextInputFormat案例教程</h4><p>下面使用CombineTextInputFormat作为MapReduce的InputFormat，测试其对小文件的处理效果。准备4个小文件作为输入文件，期望将4个小文件合并成一个数据切片进行统一处理。4个输入文件的大小如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a.txt  1.7MB</span><br><span class="line">b.txt  5.1MB</span><br><span class="line">c.txt  3.4MB</span><br><span class="line">d.txt  6.8MB</span><br></pre></td></tr></table></figure>

<p>不进行任何处理，使用原来的WordCount程序，直接将这4个文件输入</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\11_input\\inputcombinetextinputformat&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\outputCombine1&quot;</span>));</span><br></pre></td></tr></table></figure>

<p>观察控制台中打印的日志，可以发现产生了4个数据切片（4个文件就有4个切片）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2023-09-27 17:19:42,685 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:4</span><br></pre></td></tr></table></figure>

<p>在 WordcountDriver 中增加如下代码，运行程序</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果不设置 InputFormat，它默认用的是 TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置 4m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br></pre></td></tr></table></figure>

<p>可以看到，运行结果为3个切片</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2023-09-27 17:25:28,331 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:3</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果不设置 InputFormat，它默认用的是 TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置 20m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">20971520</span>);</span><br></pre></td></tr></table></figure>

<p>可以看到，运行结果为1个切片</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2023-09-27 17:27:48,003 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:1</span><br></pre></td></tr></table></figure>

<h3 id="5-5-MapReduce框架原理之shuffle机制"><a href="#5-5-MapReduce框架原理之shuffle机制" class="headerlink" title="5.5 MapReduce框架原理之shuffle机制"></a>5.5 MapReduce框架原理之shuffle机制</h3><p>MapReduce执行排序操作并将map阶段输出的数据传递给reduce阶段的过程称为<strong>shuffle</strong>，shuffle阶段是MapReduce程序的核心阶段。</p>
<h4 id="5-5-1-shuffle机制"><a href="#5-5-1-shuffle机制" class="headerlink" title="5.5.1 shuffle机制"></a>5.5.1 shuffle机制</h4><p><img src="Snipaste_2023-09-28_13-48-05.png" alt="Snipaste_2023-09-28_13-48-05"></p>
<h4 id="5-5-2-分区"><a href="#5-5-2-分区" class="headerlink" title="5.5.2 分区"></a>5.5.2 分区</h4><h5 id="1-问题引出"><a href="#1-问题引出" class="headerlink" title="1. 问题引出"></a>1. 问题引出</h5><p>要求将统计结果<strong>按照条件</strong>输出到<strong>不同的分区（Partition）</strong>中，比如将统计结果按照手记归属地输出到不同分区中。</p>
<h5 id="2-默认分区器HashPartitioner"><a href="#2-默认分区器HashPartitioner" class="headerlink" title="2. 默认分区器HashPartitioner"></a>2. 默认分区器HashPartitioner</h5><p>默认分区器是根据key的hashCode对ReduceTask的数量取模得到的。用户没法控制哪个key存储于哪个分区中，源码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HashPartitioner</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;K, V&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(K key, V value, <span class="type">int</span> numReduceTasks)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>一个小实验：我们在最初的WordCount程序中的WordCountDriver类中添加如下代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">2</span>);<span class="comment">//将ReduceTask个数设置为2，则最后会产生两个结果文件</span></span><br></pre></td></tr></table></figure>

<p>运行程序，可以发现最后是两个文件：</p>
<p><img src="Snipaste_2023-09-28_14-04-43.png" alt="Snipaste_2023-09-28_14-04-43"></p>
<img src="Snipaste_2023-09-28_14-06-33.png" alt="Snipaste_2023-09-28_14-06-33" style="zoom:50%;">

<p>这种情况下分区标准只和hashcoed值有关，如果想自定义分区方式，只能自定义分区器。</p>
<h5 id="3-自定义分区器"><a href="#3-自定义分区器" class="headerlink" title="3. 自定义分区器"></a>3. 自定义分区器</h5><p>①自定义继承Partitioner接口的类，需要指定两个泛型，分别是该自定义分区器要应用的MapReduce程序中的Mapper输出键和输出值的数据类型；并且需要重写getPartition()，在该方法中编写代码逻辑，实现通过不同的键返回分区值，分区值是int类型的数据。例如，如果希望最终得到5个分区，则返回0~4的整数分区值。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text, FlowBean&gt; &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text key, FlowBean value, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line">        <span class="comment">// 控制分区代码逻辑</span></span><br><span class="line">        … …</span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>②在Job驱动中，设置自定义Partitioner</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br></pre></td></tr></table></figure>

<p>③自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask，如果自定义分区器中的逻辑指定返回的分区值为0~4，则需要将ReduceTask的数量设置为5个</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure>

<h5 id="4-分区总结"><a href="#4-分区总结" class="headerlink" title="4. 分区总结"></a>4. 分区总结</h5><ul>
<li>如果ReduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；</li>
<li>如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception报错；</li>
<li>如 果ReduceTask的数量&#x3D;1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000；</li>
<li>分区号必须从零开始，逐一累加。</li>
</ul>
<p>总结：尽量使ReduceTask的数量与getPartition()方法的返回值保持一致</p>
<h4 id="5-5-3-分区案例实操"><a href="#5-5-3-分区案例实操" class="headerlink" title="5.5.3 分区案例实操"></a>5.5.3 分区案例实操</h4><h5 id="1-需求分析"><a href="#1-需求分析" class="headerlink" title="1. 需求分析"></a>1. 需求分析</h5><p>在统计每个手机号码耗费的总上行流量、总下行流量、总流量的基础上，使用自定义分区器，将统计结果按照手机归属地的省份输出到不同的文件中。</p>
<p>①输入数据：</p>
<img src="Snipaste_2023-09-26_15-20-12.png" alt="Snipaste_2023-09-26_15-20-12" style="zoom:33%;">

<p>②期望效果：</p>
<p>把手机号码开头为136、137、138、139的数据分别存储于4个独立文件中，将其他数据存储于1个文件中。</p>
<img src="Snipaste_2023-09-28_14-50-52.png" alt="Snipaste_2023-09-28_14-50-52" style="zoom:33%;">

<h5 id="2-需求实现"><a href="#2-需求实现" class="headerlink" title="2. 需求实现"></a>2. 需求实现</h5><p>增加自定义分区器ProvincePartitioner，编写分区逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text,FlowBean&gt; &#123;<span class="comment">//泛型要和Map的输出键值保持一致</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line">        <span class="comment">//Text是手机号，获取手机号的前三位prePhone</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="type">String</span> <span class="variable">prePhone</span> <span class="operator">=</span> phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定义一个分区号变量partition,根据prePhone的值设置分区号</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">&quot;136&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;137&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;138&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;139&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//最后返回分区号partition</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在驱动类FlowDriver，配置自定义分区器，并设置ReduceTask的数量：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//指定自定义分区类</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//同时指定相应数量的ReduceTask</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure>

<p>设置完毕后，重写运行程序，查看运行过程中ReduceTask的数量及输出的文件数量，输出文件如下：</p>
<img src="Snipaste_2023-09-28_15-12-49.png" alt="Snipaste_2023-09-28_15-12-49" style="zoom:50%;">

<p><img src="Snipaste_2023-09-28_15-14-17.png" alt="Snipaste_2023-09-28_15-14-17"></p>
<p>可以发现，成功分区</p>
<p>如果将ReduceTask的数量设置为1个，重新运行程序，再次查看输出文件夹，发现只输出了一个结果文件，实际上分区失效</p>
<img src="Snipaste_2023-09-28_15-21-02.png" alt="Snipaste_2023-09-28_15-21-02" style="zoom:50%;">

<p>将ReduceTask的数量设置为6个，重新运行程序，再次查看输出文件夹，共输出了6个结果文件，但其中一个是空文件</p>
<img src="Snipaste_2023-09-28_15-23-33.png" alt="Snipaste_2023-09-28_15-23-33" style="zoom:50%;">

<h4 id="5-5-4-WritableComparable排序"><a href="#5-5-4-WritableComparable排序" class="headerlink" title="5.5.4 WritableComparable排序"></a>5.5.4 WritableComparable排序</h4><p>MapTask和ReduceTask均会对数据<strong>按照key</strong>进行排序。该操作属于Hadoop的默认行为。<strong>任何应用程序中的数据均会被排序，而不管逻辑上是否需要</strong>。</p>
<p>默认排序是按照<strong>字典顺序排序</strong>，且实现该排序的方法是<strong>快速排序</strong>。</p>
<p>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，<strong>ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序</strong>。</p>
<p>以上过程中提到的排序都是针对<strong>数据的键</strong>进行的，在不进行配置的情况下，必须使用Hadoop提供的序列化类，如Text类，IntWritable类等。若用户使用自定义JavaBean作为键，则需要使其继承（实现）<strong>WritableComparable接口</strong>，重写该接口中的**compareTo()**方法，定义比较逻辑。</p>
<hr>
<p>排序分类：</p>
<ul>
<li>部分排序（生产环境中常用）：MapReduce根据输入记录的键对数据集排序。<strong>保证输出的每个文件内部有序</strong>。</li>
<li>全排序（生产环境下慎用）：最终输出结果只有一个文件，并且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。</li>
<li>二次排序（自定义排序）：在自定义排序过程中，如果compareTo()方法中的判断条件为两个即为二次排序。</li>
</ul>
<h4 id="5-5-5-WritableComparable排序案例实操（全排序）"><a href="#5-5-5-WritableComparable排序案例实操（全排序）" class="headerlink" title="5.5.5 WritableComparable排序案例实操（全排序）"></a>5.5.5 WritableComparable排序案例实操（全排序）</h4><h5 id="1-需求分析-1"><a href="#1-需求分析-1" class="headerlink" title="1. 需求分析"></a>1. 需求分析</h5><p>针对以下输出结果进行倒序排列</p>
<img src="Snipaste_2023-09-28_20-22-50.png" alt="Snipaste_2023-09-28_20-22-50" style="zoom:33%;">

<p>期望输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">13509468723 7335 110349 117684</span><br><span class="line">13736230513 2481 24681 27162</span><br><span class="line">13956435636 132 1512 1644</span><br><span class="line">13846544121 264 0 264</span><br><span class="line">。。。 。。。</span><br></pre></td></tr></table></figure>

<h5 id="2-实现思路分析"><a href="#2-实现思路分析" class="headerlink" title="2. 实现思路分析"></a>2. 实现思路分析</h5><p>将FlowBean对象作为map阶段的输出键，然后使FlowBean继承WritableComparable接口，在重写compareTo()方法时，编写按照总流量倒序排列的代码逻辑。</p>
<p>在map()方法中，将FlowBean对象作为键，将手机号码作为值输出，即可按照FlowBean对象中的总流量进行排序。</p>
<p>在reduce()方法中，输入值是以迭代器的形式给出的，所以我们遍历这个迭代器，与输入键重新组合并交换位置输出，使最后输出的数据依然是手机号码在前、流量在后，与期望输出数据相同。</p>
<h5 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3. 代码实现"></a>3. 代码实现</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使FlowBean继承WritableComparable接口，重写compareTo()方法</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span>&lt;FlowBean&gt; &#123; <span class="comment">//1.继承WritableComparable接口</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;<span class="comment">//上行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;<span class="comment">//下行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;<span class="comment">//总流量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.提供无参构造器</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.get,set方法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getUpFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setUpFlow</span><span class="params">(<span class="type">long</span> upFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getDownFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setDownFlow</span><span class="params">(<span class="type">long</span> downFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">(<span class="type">long</span> sumFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span>&#123; <span class="comment">//相当于方法重载</span></span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;<span class="comment">//4.重写序列化方法</span></span><br><span class="line">        out.writeLong(upFlow);</span><br><span class="line">        out.writeLong(downFlow);</span><br><span class="line">        out.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">//4.重写反序列化方法，注意反序列化的顺序和序列化的顺序完全一致，并且读取方法的调用方法于参数的类型也要一一对应</span></span><br><span class="line">        <span class="built_in">this</span>.upFlow = in.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = in.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//5.重写toString方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写compareTo方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line">        <span class="comment">//按照总流量进行比较，并进行倒序排序</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; o.sumFlow)&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; o.sumFlow)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//编写Mapper类，在map()方法中将FlowBean作为键、将手机号码作为值输出</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, FlowBean,Text&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">FlowBean</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, FlowBean, Text&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//1.获取一行数据</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.按照“\t”符号切割数据</span></span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.封装outK,outV</span></span><br><span class="line">        outK.setUpFlow(Long.parseLong(split[<span class="number">1</span>]));</span><br><span class="line">        outK.setDownFlow(Long.parseLong(split[<span class="number">2</span>]));</span><br><span class="line">        outK.setSumFlow();</span><br><span class="line">        outV.set(split[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.写出outK,outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//编写Reducer类，输入数据的泛型需要与Mapper类输出数据的泛型保持一致，输出键的泛型为FlowBean，输出值的泛型为Text</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;FlowBean, Text, Text, FlowBean&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Reducer&lt;FlowBean, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//遍历values迭代器</span></span><br><span class="line">        <span class="keyword">for</span> (Text value : values)&#123;</span><br><span class="line">            <span class="comment">//调换KV位置，反向写出</span></span><br><span class="line">            context.write(value,key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//编写Driver类，进行必要设置</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.获取Job对象job</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.关联当前Driver类</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.关联Mapper组件和Reducer组件</span></span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.设置map端输出KV的数据类型</span></span><br><span class="line">        job.setMapOutputKeyClass(FlowBean.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//5.设置程序最终输出KV的数据类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//6.设置程序的输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\output3&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\output555&quot;</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//7.提交job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行程序，查看输出结果，可以看到按照总流量的倒序排列了：</p>
<img src="Snipaste_2023-09-28_21-10-40.png" alt="Snipaste_2023-09-28_21-10-40" style="zoom:33%;">

<p>修改compareTo()方法，使得总流量相同时候，按照上行流量进行倒序排序</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//重写compareTo方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line">    <span class="comment">//按照总流量进行比较，并进行倒序排序</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; o.sumFlow)&#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; o.sumFlow)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">//如果总流量相同，按照上行流量倒序排序</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.upFlow &gt; o.upFlow)&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.upFlow &lt; o.upFlow)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行程序，查看输出结果：</p>
<img src="Snipaste_2023-09-28_21-24-14.png" alt="Snipaste_2023-09-28_21-24-14" style="zoom:33%;">

<h4 id="5-5-6-WritableComparable排序案例实操（区内排序）"><a href="#5-5-6-WritableComparable排序案例实操（区内排序）" class="headerlink" title="5.5.6 WritableComparable排序案例实操（区内排序）"></a>5.5.6 WritableComparable排序案例实操（区内排序）</h4><h5 id="1-需求分析-2"><a href="#1-需求分析-2" class="headerlink" title="1. 需求分析"></a>1. 需求分析</h5><p>要求每个省份（136，137，138，139……）手机号输出的文件中按照总流量内部排序</p>
<h5 id="2-实现思路分析-1"><a href="#2-实现思路分析-1" class="headerlink" title="2. 实现思路分析"></a>2. 实现思路分析</h5><p>增加自定义分区器，按照省份手机号码设置分区</p>
<img src="Snipaste_2023-09-28_21-29-13.png" alt="Snipaste_2023-09-28_21-29-13" style="zoom:50%;">

<h5 id="3-案例实操"><a href="#3-案例实操" class="headerlink" title="3. 案例实操"></a>3. 案例实操</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//自定义分区器ProvincePartitioner2，指定泛型需要与Mapper类输出数据的泛型保持一致</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProvincePartitioner2</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;FlowBean, Text&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(FlowBean flowBean, Text text, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line">        <span class="comment">//获取手机号码前三位</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="type">String</span> <span class="variable">prePhone</span> <span class="operator">=</span> phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定义一个分区号变量partition，根据prePhone的值设置分区号</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">&quot;136&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;137&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;138&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;139&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//最后返回分区号partition</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//编写驱动类FlowDriver，配置自定义分区器ProvincePartitioner2</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.获取Job对象job</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.关联当前Driver类</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.关联Mapper组件和Reducer组件</span></span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.设置map端输出KV的数据类型</span></span><br><span class="line">        job.setMapOutputKeyClass(FlowBean.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//5.设置程序最终输出KV的数据类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置自定义分区器</span></span><br><span class="line">        job.setPartitionerClass(ProvincePartitioner2.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置对应的ReduceTask的数量</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">5</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//6.设置程序的输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\output3&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\output5555&quot;</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//7.提交job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>重新运行程序，查看输出结果：生成了5个结果文件，分别查看5个结果文件内容，都是按照总流量降序排列的</p>
<img src="Snipaste_2023-09-28_21-50-07.png" alt="Snipaste_2023-09-28_21-50-07" style="zoom:50%;">

<img src="Snipaste_2023-09-28_21-52-00.png" alt="Snipaste_2023-09-28_21-52-00" style="zoom:50%;">

<img src="Snipaste_2023-09-28_21-52-27.png" alt="Snipaste_2023-09-28_21-52-27" style="zoom:50%;">

<img src="Snipaste_2023-09-28_21-52-51.png" alt="Snipaste_2023-09-28_21-52-51" style="zoom:50%;">

<img src="Snipaste_2023-09-28_21-53-12.png" alt="Snipaste_2023-09-28_21-53-12" style="zoom:50%;">

<img src="Snipaste_2023-09-28_21-53-36.png" alt="Snipaste_2023-09-28_21-53-36" style="zoom:50%;">

<h4 id="5-5-7-Combiner合并"><a href="#5-5-7-Combiner合并" class="headerlink" title="5.5.7 Combiner合并"></a>5.5.7 Combiner合并</h4><ol>
<li>Combiner是MR程序中Mappe和Reducer之外的一种组件</li>
<li>Combiner组件的父类就是Reducer</li>
<li>Combiner和Reducer的区别在于运行的位置，<strong>Combiner是在每一个MapTask所在的节点运行，Reducer是接收全局所有Mapper的输出结果</strong></li>
<li>Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量</li>
<li><strong>Combiner能够应用的前提是不影响最终的业务逻辑</strong>，而且Combiner的输出kv应该跟Reducer的输入kv类型要对应起来。</li>
</ol>
<h4 id="5-5-8-Combiner合并案例实操"><a href="#5-5-8-Combiner合并案例实操" class="headerlink" title="5.5.8 Combiner合并案例实操"></a>5.5.8 Combiner合并案例实操</h4><h5 id="1-需求分析-3"><a href="#1-需求分析-3" class="headerlink" title="1. 需求分析"></a>1. 需求分析</h5><p>对输入文件进行词频统计，在统计过程中，使用Combiner组件对每个MapTask的输出数据进行局部汇总，从而减少网络传输数据量</p>
<h5 id="2-实现思路分析-2"><a href="#2-实现思路分析-2" class="headerlink" title="2. 实现思路分析"></a>2. 实现思路分析</h5><p><img src="Snipaste_2023-10-01_12-57-54.png" alt="Snipaste_2023-10-01_12-57-54"></p>
<h5 id="3-案例实操——方案1"><a href="#3-案例实操——方案1" class="headerlink" title="3. 案例实操——方案1"></a>3. 案例实操——方案1</h5><p>增加一个WordCountCombiner类，使其继承Reducer类，重写reduce()方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable,Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//封装outV</span></span><br><span class="line">        outV.set(sum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出key和outV</span></span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在WordCountDriver驱动类中配置Combiner组件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordCountCombiner.class);</span><br></pre></td></tr></table></figure>

<h5 id="4-案例实操——方案2"><a href="#4-案例实操——方案2" class="headerlink" title="4. 案例实操——方案2"></a>4. 案例实操——方案2</h5><p>在WordCountDriver驱动类中指定WordCountReducer类作为Combiner组件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordCountReducer.class);</span><br></pre></td></tr></table></figure>

<p>运行程序，观察控制台中打印的日志：</p>
<img src="Snipaste_2023-10-01_14-02-50.png" alt="Snipaste_2023-10-01_14-02-50" style="zoom:50%;">

<p>实际上，在以后，如果Combiner不影响最终的业务逻辑就可以用它，并且都用方案2</p>
<h3 id="5-6-MapReduce框架原理之OutputFormat数据输出"><a href="#5-6-MapReduce框架原理之OutputFormat数据输出" class="headerlink" title="5.6 MapReduce框架原理之OutputFormat数据输出"></a>5.6 MapReduce框架原理之OutputFormat数据输出</h3><h4 id="5-6-1-OutputFormat接口的实现类"><a href="#5-6-1-OutputFormat接口的实现类" class="headerlink" title="5.6.1 OutputFormat接口的实现类"></a>5.6.1 OutputFormat接口的实现类</h4><p>OutputFormat接口是MapReduce输出类的基类，几种该接口的常见实现类：</p>
<ol>
<li>TextOutputFormat类</li>
<li>SequenceFileOutputFormat类</li>
<li>自定义OutputFormat类（注意：之前没有讲自定义InputFormat类）</li>
</ol>
<p>当用户需要将数据输出至MySQL、HBase等存储框架中时，Hadoop就不能够提供对应的OutputFormat类了，需要用户自定义OutputFormat类，步骤如下（感觉挺麻烦）：</p>
<ul>
<li>自定义一个类，使其继承RecordWriter类，在该类中创建文件的输出流及文件的输出方式。</li>
<li>自定义一个类，使其继承FileOutputFormat类，重写getRecordWriter()方法，在getRecordWriter()方法中创建自定义的RecordWriter类并返回。</li>
</ul>
<h4 id="5-6-2-自定义OutputFormat类的案例实操"><a href="#5-6-2-自定义OutputFormat类的案例实操" class="headerlink" title="5.6.2 自定义OutputFormat类的案例实操"></a>5.6.2 自定义OutputFormat类的案例实操</h4><h5 id="1-需求分析-4"><a href="#1-需求分析-4" class="headerlink" title="1. 需求分析"></a>1. 需求分析</h5><p>过滤输入的日志信息，将包含atguigu字段的网址数据输出到…:&#x2F;atguigu.log文件，将不包含atguigu字段的网址数据输出到…:&#x2F;other.log文件中。</p>
<p><img src="Snipaste_2023-10-01_20-29-39.png" alt="Snipaste_2023-10-01_20-29-39"></p>
<h5 id="2-案例实操"><a href="#2-案例实操" class="headerlink" title="2. 案例实操"></a>2. 案例实操</h5><p>（1）编写LogMapper类，在map()方法中不对原数据进行任何处理，直接将一行数据写出</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//不进行任何处理，直接写出一行Log数据</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）编写LogReducer类，在reduce()方法中不进行任何特殊处理，只将数据迭代写出即可。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, NullWritable, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Reducer&lt;Text, NullWritable, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//防止有相同的数据，迭代写出</span></span><br><span class="line">        <span class="keyword">for</span> (NullWritable value : values) &#123;</span><br><span class="line">            context.write(key, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）自定义一个LogOutputFormat类，使其继承FileOutputFormat类，重写getRecordWrite()方法。getRecordWrite()方法主要用于创建一个RecordWriter对象并返回。RecordWriter类需要用户自定义</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogOutputFormat</span> <span class="keyword">extends</span> <span class="title class_">FileOutputFormat</span>&lt;Text, NullWritable&gt; &#123;<span class="comment">//它的泛型和Reduce输出的泛型一致</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title function_">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//创建一个自定义的RecordWriter对象并返回</span></span><br><span class="line">        <span class="type">LogRecordWriter</span> <span class="variable">lrw</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LogRecordWriter</span>(job);</span><br><span class="line">        <span class="keyword">return</span>  lrw;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（4）编写LogRecordWriter类，使其继承RecordWriter类，在构造方法中创建文件输出流，在write()方法中编写对文件内容进行判断并分流写出的逻辑代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogRecordWriter</span> <span class="keyword">extends</span> <span class="title class_">RecordWriter</span>&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FSDataOutputStream atguiguOut;</span><br><span class="line">    <span class="keyword">private</span> FSDataOutputStream otherOut;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">LogRecordWriter</span><span class="params">(TaskAttemptContext job)</span> &#123;</span><br><span class="line">        <span class="comment">//创建两条流</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//获取文件系统对象</span></span><br><span class="line">            <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(job.getConfiguration());</span><br><span class="line">            <span class="comment">//用文件系统对象创建两个输出流，对应不同的目录</span></span><br><span class="line">            atguiguOut = fs.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\liu\\atguigu.log&quot;</span>));<span class="comment">//升级为全局变量：ctrl+alt+f</span></span><br><span class="line">            otherOut = fs.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\liu\\other.log&quot;</span>));<span class="comment">//升级为全局变量：ctrl+alt+f</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(Text key, NullWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">log</span> <span class="operator">=</span> key.toString();</span><br><span class="line">        <span class="comment">//具体写：根据一行log数据是否包含atguigu字段，判断两条输出流输出的内容</span></span><br><span class="line">        <span class="keyword">if</span> (log.contains(<span class="string">&quot;atguigu&quot;</span>))&#123;</span><br><span class="line">            atguiguOut.writeBytes(log + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            otherOut.writeBytes(log + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//关闭流</span></span><br><span class="line">        IOUtils.closeStream(atguiguOut);</span><br><span class="line">        IOUtils.closeStream(otherOut);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（5）编写LogDriver类，将OutputFormat类设置为自定义的LogOutputFormat类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(LogDriver.class);</span><br><span class="line">        job.setMapperClass(LogMapper.class);</span><br><span class="line">        job.setReducerClass(LogReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        <span class="comment">//设置自定义的 outputformat</span></span><br><span class="line">        job.setOutputFormatClass(LogOutputFormat.class);</span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\11_input\\inputoutputformat&quot;</span>));</span><br><span class="line">        <span class="comment">// 虽 然 我 们 自 定 义 了 outputformat ， 但 是 因 为 我 们 的 outputformat 继承自</span></span><br><span class="line"><span class="comment">//        fileoutputformat</span></span><br><span class="line">        <span class="comment">//而 fileoutputformat 要输出一个_SUCCESS 文件，所以在这还得指定一个输出目录</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\output8&quot;</span>));</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（6）运行程序，观察输出结果，发现生成了两个结果文件，分别为atguigu.log文件和other.log文件</p>
<img src="Snipaste_2023-10-03_13-45-51.png" alt="Snipaste_2023-10-03_13-45-51" style="zoom: 50%;">

<img src="Snipaste_2023-10-03_13-46-25.png" alt="Snipaste_2023-10-03_13-46-25" style="zoom:50%;">

<p>而D:\尚硅谷大数据学习资料（无视频）\Hadoop 3.x\资料\资料\output8中只有一个运行成功的标记文件：</p>
<img src="Snipaste_2023-10-03_13-48-51.png" alt="Snipaste_2023-10-03_13-48-51" style="zoom:50%;">

<h3 id="5-7-MapReduce工作流程（面试重点）"><a href="#5-7-MapReduce工作流程（面试重点）" class="headerlink" title="5.7 MapReduce工作流程（面试重点）"></a>5.7 MapReduce工作流程（面试重点）</h3><h4 id="5-7-1-MapTask工作机制"><a href="#5-7-1-MapTask工作机制" class="headerlink" title="5.7.1 MapTask工作机制"></a>5.7.1 MapTask工作机制</h4><p><img src="Snipaste_2023-10-03_14-11-03.png" alt="Snipaste_2023-10-03_14-11-03"></p>
<p>（1）<strong>Read阶段</strong>：MapTask 通过 InputFormat 获得的 RecordReader，从输入 InputSplit 中解析出一个个 key&#x2F;value。</p>
<p>（2）<strong>Map阶段</strong>：该节点主要是将解析出的 key&#x2F;value 交给用户编写 map()函数处理，并产生一系列新的 key&#x2F;value。</p>
<p>（3）<strong>Collect 收集阶段</strong>：在用户编写 map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的 key&#x2F;value 分区（调用Partitioner），并写入一个环形内存缓冲区中。</p>
<p>（4）<strong>Spill 阶段：即“溢写”</strong>，当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>溢写阶段详情：</p>
<p>​		步骤 1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照 key 有序。</p>
<p>​		步骤 2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件 output&#x2F;spillN.out（N 表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p>
<p>​		步骤 3：将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过 1MB，则将内存索引写到文件 output&#x2F;spillN.out.index 中。</p>
<p>（5）<strong>Merge阶段</strong>：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件output&#x2F;file.out 中，同时生成相应的索引文件 output&#x2F;file.out.index。</p>
<p>在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并 mapreduce.task.io.sort.factor（默认 10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p>
<p>让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
<h4 id="5-7-2-ReduceTask工作机制"><a href="#5-7-2-ReduceTask工作机制" class="headerlink" title="5.7.2 ReduceTask工作机制"></a>5.7.2 ReduceTask工作机制</h4><p><img src="image-20231003141616898.png" alt="image-20231003141616898"></p>
<p>（1）<strong>Copy 阶段</strong>：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<p>（2）<strong>Sort 阶段</strong>：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照 MapReduce 语义，用户编写 reduce()函数输入数据是按 key 进行聚集的一组数据。为了将 key 相同的数据聚在一起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。</p>
<p>（3）<strong>Reduce 阶段</strong>：reduce()函数将计算结果写到 HDFS 上。</p>
<h4 id="5-7-3-ReduceTask并行度决定机制"><a href="#5-7-3-ReduceTask并行度决定机制" class="headerlink" title="5.7.3 ReduceTask并行度决定机制"></a>5.7.3 ReduceTask并行度决定机制</h4><p><strong>MapTask并行度由切片个数决定，切片个数由输入文件和切片规则决定</strong></p>
<p>思考：ReduceTask并行度由谁决定？</p>
<p>（1）设置ReduceTask并行度（个数）</p>
<p>ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认值是 1，手动设置为 4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>

<p>（2）实验：测试ReduceTask多少合适</p>
<p>实验环境：一个NameNode，16个DataNode，CPU：8GHz，内存2G</p>
<img src="Snipaste_2023-10-03_14-26-16.png" alt="Snipaste_2023-10-03_14-26-16" style="zoom:50%;">

<p>（3）<strong>一些注意事项</strong></p>
<ul>
<li>ReduceTask&#x3D;0，表示没有Reduce阶段，输出文件个数和Map个数一致。</li>
<li>ReduceTask默认值就是1，所以输出文件个数为一个。</li>
<li>如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜</li>
<li>ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask。</li>
<li>具体多少个ReduceTask，需要根据集群性能而定。</li>
<li>如果分区数不是1，但是ReduceTask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。</li>
</ul>
<h4 id="5-7-4-MapTask-amp-ReduceTask源码解析"><a href="#5-7-4-MapTask-amp-ReduceTask源码解析" class="headerlink" title="5.7.4 MapTask &amp; ReduceTask源码解析"></a>5.7.4 MapTask &amp; ReduceTask源码解析</h4><p>以分区案例代码为例进行源码分析：</p>
<img src="Snipaste_2023-09-28_14-50-52.png" alt="Snipaste_2023-09-28_14-50-52" style="zoom:50%;">

<img src="Snipaste_2023-10-03_14-46-27.png" alt="Snipaste_2023-10-03_14-46-27" style="zoom:50%;">

<p>FlowBean类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123; <span class="comment">//1.继承Writable接口</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;<span class="comment">//上行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;<span class="comment">//下行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;<span class="comment">//总流量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.提供无参构造器</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.get,set方法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getUpFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setUpFlow</span><span class="params">(<span class="type">long</span> upFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getDownFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setDownFlow</span><span class="params">(<span class="type">long</span> downFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">(<span class="type">long</span> sumFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span>&#123; <span class="comment">//相当于方法重载</span></span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;<span class="comment">//4.重写序列化方法</span></span><br><span class="line">        out.writeLong(upFlow);</span><br><span class="line">        out.writeLong(downFlow);</span><br><span class="line">        out.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">//4.重写反序列化方法，注意反序列化的顺序和序列化的顺序完全一致，并且读取方法的调用方法于参数的类型也要一一对应</span></span><br><span class="line">        <span class="built_in">this</span>.upFlow = in.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = in.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//5.重写toString方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FlowMapper类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, FlowBean&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line">    <span class="comment">//重写map方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//1.获取一行数据，将其转换成字符串</span></span><br><span class="line">        <span class="comment">//1	  13736230513	192.196.100.1	www.atguigu.com	  2481	 24681	 200</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.切割数据</span></span><br><span class="line">        <span class="comment">//[1,13736230513,192.196.100.1,www.atguigu.com,2481,24681,200]</span></span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.抓取我们想要的数据</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> split[<span class="number">1</span>];<span class="comment">//手记号码</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">up</span> <span class="operator">=</span> split[split.length - <span class="number">3</span>];<span class="comment">//上行流量</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">down</span> <span class="operator">=</span> split[split.length - <span class="number">2</span>];<span class="comment">//下行流量</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.封装</span></span><br><span class="line">        outK.set(phone);</span><br><span class="line">        outV.setUpFlow(Long.parseLong(up));</span><br><span class="line">        outV.setDownFlow(Long.parseLong(down));</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5.写出outK,outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FlowReducer类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, FlowBean, Text, FlowBean&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Reducer&lt;Text, FlowBean, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">totalUp</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">long</span> <span class="variable">totalDown</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.遍历values，将其中的上行流量，下行流量分别累加</span></span><br><span class="line">        <span class="keyword">for</span> (FlowBean flowBean : values)&#123;</span><br><span class="line">            totalUp += flowBean.getUpFlow();</span><br><span class="line">            totalDown += flowBean.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.封装outV</span></span><br><span class="line">        outV.setUpFlow(totalUp);</span><br><span class="line">        outV.setDownFlow(totalDown);</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.写出key和outV</span></span><br><span class="line">        context.write(key,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ProvincePartitioner类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text,FlowBean&gt; &#123;<span class="comment">//泛型要和Map的输出键值保持一致</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line">        <span class="comment">//Text是手机号，获取手机号的前三位prePhone</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="type">String</span> <span class="variable">prePhone</span> <span class="operator">=</span> phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定义一个分区号变量partition,根据prePhone的值设置分区号</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">&quot;136&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;137&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;138&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;139&quot;</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//最后返回分区号partition</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FlowDriver类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.获取Job对象job</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.关联当前Driver类</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.关联Mapper组件和Reducer组件</span></span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.设置map端输出KV的数据类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//5.设置程序最终输出KV的数据类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定自定义分区类</span></span><br><span class="line">        job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//同时指定相应数量的ReduceTask</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">5</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//6.设置程序的输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\11_input\\inputflow&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\output5&quot;</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//7.提交job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在FlowMapper中的context.write(outK, outV);打上断点：</p>
<img src="Snipaste_2023-10-03_14-51-30.png" alt="Snipaste_2023-10-03_14-51-30" style="zoom:50%;">

<p>在FlowReducer中的context.write(key,outV);打上断点：</p>
<img src="Snipaste_2023-10-03_14-53-30.png" alt="Snipaste_2023-10-03_14-53-30" style="zoom:50%;">

<p>检查输出路径存在后，进行debug，运行到FlowMapper类中的context.write方法，强制进入：</p>
<img src="Snipaste_2023-10-03_14-57-26.png" alt="Snipaste_2023-10-03_14-57-26" style="zoom:50%;">

<p>再次强行进入mapContext.write()方法：</p>
<p><img src="Snipaste_2023-10-04_13-24-24.png" alt="Snipaste_2023-10-04_13-24-24"></p>
<p>再次强行进入output.write()方法：</p>
<p><img src="Snipaste_2023-10-04_13-25-04.png" alt="Snipaste_2023-10-04_13-25-04"></p>
<p>进入getPartition()方法，即为我们自定义的方法：</p>
<p><img src="Snipaste_2023-10-04_13-26-53.png" alt="Snipaste_2023-10-04_13-26-53"></p>
<p>跳出该方法继续向下执行，进入collect环形缓冲区：</p>
<p><img src="Snipaste_2023-10-04_13-28-34.png" alt="Snipaste_2023-10-04_13-28-34"></p>
<p>继续向下走：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// serialize key bytes into buffer</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">keystart</span> <span class="operator">=</span> bufindex;<span class="comment">//把环形缓冲区的指针给了keystart</span></span><br><span class="line">    keySerializer.serialize(key);<span class="comment">//对keystart进行序列化</span></span><br><span class="line">    <span class="keyword">if</span> (bufindex &lt; keystart) &#123;</span><br><span class="line">        <span class="comment">// wrapped the key; must make contiguous</span></span><br><span class="line">        bb.shiftBufferedKey();</span><br><span class="line">        keystart = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// serialize value bytes into buffer</span></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span> <span class="variable">valstart</span> <span class="operator">=</span> bufindex;</span><br><span class="line">    valSerializer.serialize(value);<span class="comment">//对valuestart进行序列化</span></span><br><span class="line">    <span class="comment">// It&#x27;s possible for records to have zero length, i.e. the serializer</span></span><br><span class="line">    <span class="comment">// will perform no writes. To ensure that the boundary conditions are</span></span><br><span class="line">    <span class="comment">// checked and that the kvindex invariant is maintained, perform a</span></span><br><span class="line">    <span class="comment">// zero-length write into the buffer. The logic monitoring this could be</span></span><br><span class="line">    <span class="comment">// moved into collect, but this is cleaner and inexpensive. For now, it</span></span><br><span class="line">    <span class="comment">// is acceptable.</span></span><br><span class="line">    bb.write(b0, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// write accounting info,以下是写入Mate信息</span></span><br><span class="line">    kvmeta.put(kvindex + PARTITION, partition);</span><br><span class="line">    kvmeta.put(kvindex + KEYSTART, keystart);</span><br><span class="line">    kvmeta.put(kvindex + VALSTART, valstart);</span><br><span class="line">    kvmeta.put(kvindex + VALLEN, distanceTo(valstart, valend));</span><br><span class="line">    <span class="comment">// advance kvindex</span></span><br><span class="line">    kvindex = (kvindex - NMETA + kvmeta.capacity()) % kvmeta.capacity();</span><br><span class="line">&#125; <span class="keyword">catch</span> (MapBufferTooSmallException e) &#123;</span><br><span class="line">    LOG.info(<span class="string">&quot;Record too large for in-memory buffer: &quot;</span> + e.getMessage());</span><br><span class="line">    spillSingleRecord(key, value, partition);</span><br><span class="line">    mapOutputRecordCounter.increment(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>然后依次出write()方法，output.wirte()，mapContext,write()，直至出contxet.write()方法：</p>
<p><img src="Snipaste_2023-10-04_13-40-33.png" alt="Snipaste_2023-10-04_13-40-33"></p>
<p>至此，一行内容写完了。以此类推，一行一行写入，直到写入最后一行“13568436656”，进入方法查看：</p>
<p><img src="Snipaste_2023-10-04_13-45-57.png" alt="Snipaste_2023-10-04_13-45-57"></p>
<p><img src="Snipaste_2023-10-04_13-46-13.png" alt="Snipaste_2023-10-04_13-46-13"></p>
<p><img src="Snipaste_2023-10-04_13-46-38.png" alt="Snipaste_2023-10-04_13-46-38"></p>
<p><img src="Snipaste_2023-10-04_13-47-52.png" alt="Snipaste_2023-10-04_13-47-52"></p>
<p>之后一步步在退出方法：</p>
<p><img src="Snipaste_2023-10-04_13-49-09.png" alt="Snipaste_2023-10-04_13-49-09"></p>
<p>可以看到，最后一步退出循环，进入到cleanup(context)中：</p>
<p><img src="Snipaste_2023-10-04_13-49-34.png" alt="Snipaste_2023-10-04_13-49-34"></p>
<p>cleanup什么都不干，跳出run()方法，继续执行MapTask类：</p>
<img src="Snipaste_2023-10-04_13-51-35.png" alt="Snipaste_2023-10-04_13-51-35" style="zoom:50%;">

<p>强行进入output.close(mapperContext):</p>
<img src="Snipaste_2023-10-04_13-54-02.png" alt="Snipaste_2023-10-04_13-54-02" style="zoom:50%;">

<p>强行进入collector.flush()，在其中的源码中可以发现sortAndSpill()方法：</p>
<img src="Snipaste_2023-10-04_13-55-36.png" alt="Snipaste_2023-10-04_13-55-36" style="zoom:50%;">

<p>进入发现：</p>
<p><img src="Snipaste_2023-10-04_13-57-14.png" alt="Snipaste_2023-10-04_13-57-14"></p>
<p>再次进入：</p>
<p><img src="Snipaste_2023-10-04_13-58-21.png" alt="Snipaste_2023-10-04_13-58-21"></p>
<p>其中sortInternal()就是快排的逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">sortInternal</span><span class="params">(<span class="keyword">final</span> IndexedSortable s, <span class="type">int</span> p, <span class="type">int</span> r,</span></span><br><span class="line"><span class="params">      <span class="keyword">final</span> Progressable rep, <span class="type">int</span> depth)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="literal">null</span> != rep) &#123;</span><br><span class="line">      rep.progress();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (r-p &lt; <span class="number">13</span>) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> p; i &lt; r; ++i) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> i; j &gt; p &amp;&amp; s.compare(j-<span class="number">1</span>, j) &gt; <span class="number">0</span>; --j) &#123;</span><br><span class="line">          s.swap(j, j-<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (--depth &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// give up</span></span><br><span class="line">      alt.sort(s, p, r, rep);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// select, move pivot into first position</span></span><br><span class="line">    fix(s, (p+r) &gt;&gt;&gt; <span class="number">1</span>, p);</span><br><span class="line">    fix(s, (p+r) &gt;&gt;&gt; <span class="number">1</span>, r - <span class="number">1</span>);</span><br><span class="line">    fix(s, p, r-<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Divide</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> p;</span><br><span class="line">    <span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> r;</span><br><span class="line">    <span class="type">int</span> <span class="variable">ll</span> <span class="operator">=</span> p;</span><br><span class="line">    <span class="type">int</span> <span class="variable">rr</span> <span class="operator">=</span> r;</span><br><span class="line">    <span class="type">int</span> cr;</span><br><span class="line">    <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">      <span class="keyword">while</span> (++i &lt; j) &#123;</span><br><span class="line">        <span class="keyword">if</span> ((cr = s.compare(i, p)) &gt; <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">0</span> == cr &amp;&amp; ++ll != i) &#123;</span><br><span class="line">          s.swap(ll, i);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">while</span> (--j &gt; i) &#123;</span><br><span class="line">        <span class="keyword">if</span> ((cr = s.compare(p, j)) &gt; <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">0</span> == cr &amp;&amp; --rr != j) &#123;</span><br><span class="line">          s.swap(rr, j);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (i &lt; j) s.swap(i, j);</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    j = i;</span><br><span class="line">    <span class="comment">// swap pivot- and all eq values- into position</span></span><br><span class="line">    <span class="keyword">while</span> (ll &gt;= p) &#123;</span><br><span class="line">      s.swap(ll--, --i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (rr &lt; r) &#123;</span><br><span class="line">      s.swap(rr++, j++);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Conquer</span></span><br><span class="line">    <span class="comment">// Recurse on smaller interval first to keep stack shallow</span></span><br><span class="line">    <span class="keyword">assert</span> i != j;</span><br><span class="line">    <span class="keyword">if</span> (i - p &lt; r - j) &#123;</span><br><span class="line">      sortInternal(s, p, i, rep, depth);</span><br><span class="line">      p = j;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      sortInternal(s, j, r, rep, depth);</span><br><span class="line">      r = i;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>至此排序完成，回到sorter.sort()后的代码：</p>
<p><img src="Snipaste_2023-10-04_13-59-56.png" alt="Snipaste_2023-10-04_13-59-56"></p>
<p>开始循环遍历每一个分区：</p>
<img src="Snipaste_2023-10-04_14-02-49.png" alt="Snipaste_2023-10-04_14-02-49" style="zoom:50%;">

<p>当0号分区写成功后:</p>
<p><img src="Snipaste_2023-10-04_14-04-42.png" alt="Snipaste_2023-10-04_14-04-42"></p>
<p>在D:\tmp\hadoop-WangYuhan\mapred\local\localRunner\WangYuhan\jobcache\job_local1485257078_0001\attempt_local1485257078_0001_m_000000_0\output文件夹中的可以看到0号分区的溢写情况：</p>
<img src="Snipaste_2023-10-04_14-09-40.png" alt="Snipaste_2023-10-04_14-09-40" style="zoom:50%;">

<p>剩下的略吧，源码不是很重要，个人觉得对于数仓开发来说是浪费时间</p>
<h3 id="5-8-Join"><a href="#5-8-Join" class="headerlink" title="5.8 Join"></a>5.8 Join</h3><p>如果连接操作发生在map阶段，则称之为Map Join；如果连接操作发生在reduce阶段，则称之为Reduce Join。</p>
<h4 id="5-8-1-Reduce-Join"><a href="#5-8-1-Reduce-Join" class="headerlink" title="5.8.1 Reduce Join"></a>5.8.1 Reduce Join</h4><p>该操作的比较常见的的连接操作，并且对数据集没有特定要求。</p>
<p>map阶段的主要工作：首先为来自不同表或文件的key&#x2F;value对打标签，用于区别不同来源的记录；然后<strong>将连接字段作为key</strong>，<strong>将其他部分和新加的标志作为value</strong>,最后进行输出。</p>
<p>reduce阶段的主要工作：在shuffle过程中的reduce端，将连接字段作为key的分组已经完成，在reduce方法中将每个分组中来自不同文件的记录（在map阶段已经打标签）分开并进行合并。</p>
<p><img src="Snipaste_2023-10-04_14-46-29.png" alt="Snipaste_2023-10-04_14-46-29"></p>
<p>代码实现：</p>
<p>（1）创建订单数据表和商品信息表合并后的TableBean类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123;<span class="comment">//实现Writable，从而完成序列化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id;     <span class="comment">//订单编号</span></span><br><span class="line">    <span class="keyword">private</span> String pid;    <span class="comment">//商品编号</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> amount;    <span class="comment">//数量</span></span><br><span class="line">    <span class="keyword">private</span> String pname;  <span class="comment">//商品名称</span></span><br><span class="line">    <span class="keyword">private</span> String flag;   <span class="comment">//标志字段，用于判断是订单数据表，还是商品数据表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">TableBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getId</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setId</span><span class="params">(String id)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getPid</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> pid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setPid</span><span class="params">(String pid)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.pid = pid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getAmount</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setAmount</span><span class="params">(<span class="type">int</span> amount)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.amount = amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getPname</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> pname;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setPname</span><span class="params">(String pname)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.pname = pname;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getFlag</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> flag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setFlag</span><span class="params">(String flag)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.flag = flag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span>  id + <span class="string">&quot;\t&quot;</span> + pname + <span class="string">&quot;\t&quot;</span> + amount ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        out.writeUTF(id);</span><br><span class="line">        out.writeUTF(pid);</span><br><span class="line">        out.writeInt(amount);</span><br><span class="line">        out.writeUTF(pname);</span><br><span class="line">        out.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = in.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.pid = in.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.amount = in.readInt();</span><br><span class="line">        <span class="built_in">this</span>.pname = in.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.flag = in.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）编写TableMapper类，需要重写setup()方法。（重写该方法的目的在于通过Context对象获取文件名称）。重写map()方法，针对不同的文件，对数据进行适当的切分操作，并且将切分结果写入TableBean对象作为value、将商品编号作为key发送出去</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, TableBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String filename;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">TableBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, TableBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//初始化</span></span><br><span class="line">        <span class="comment">//获取对应文件名称</span></span><br><span class="line">        <span class="type">InputSplit</span> <span class="variable">split</span> <span class="operator">=</span> context.getInputSplit();</span><br><span class="line">        <span class="type">FileSplit</span> <span class="variable">fileSplit</span> <span class="operator">=</span> (FileSplit) split;</span><br><span class="line">        filename = fileSplit.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, TableBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//获取一行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//判断是哪个文件，然后针对不同的文件进行不同的操作</span></span><br><span class="line">        <span class="keyword">if</span> (filename.contains(<span class="string">&quot;order&quot;</span>))&#123;  <span class="comment">//对订单数据表的处理</span></span><br><span class="line">            String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            <span class="comment">//封装outK</span></span><br><span class="line">            outK.set(split[<span class="number">1</span>]);</span><br><span class="line">            <span class="comment">//封装outV</span></span><br><span class="line">            outV.setId(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setPid(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setAmount(Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">            outV.setPname(<span class="string">&quot;&quot;</span>);</span><br><span class="line">            outV.setFlag(<span class="string">&quot;order&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            <span class="comment">//封装outK</span></span><br><span class="line">            outK.set(split[<span class="number">0</span>]);</span><br><span class="line">            <span class="comment">//封装outV</span></span><br><span class="line">            outV.setId(<span class="string">&quot;&quot;</span>);</span><br><span class="line">            outV.setPid(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setAmount(<span class="number">0</span>);</span><br><span class="line">            outV.setPname(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setFlag(<span class="string">&quot;pd&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出outK,outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）编写TableReducer类，重写reduce()方法，在reduce()方法中，先对本组数据按照来源进行分类，分成订单信息和商品信息，再将商品信息中的商品名称写入订单信息</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, TableBean, TableBean, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Reducer&lt;Text, TableBean, TableBean, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"><span class="comment">//        01 1001 1 order</span></span><br><span class="line"><span class="comment">//        01 1004 4 order</span></span><br><span class="line"><span class="comment">//        01 小米 pd</span></span><br><span class="line"></span><br><span class="line">        ArrayList&lt;TableBean&gt; orderBeans = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="type">TableBean</span> <span class="variable">pdBeans</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//循环遍历</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//判断数据来自哪个表</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="string">&quot;order&quot;</span>.equals(value.getFlag()))&#123;  <span class="comment">//订单数据表</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">//创建一个临时的TableBean对象，用于接收value</span></span><br><span class="line">                <span class="type">TableBean</span> <span class="variable">tmpOrderBean</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(tmpOrderBean, value);<span class="comment">//将value的值赋值给tmpOrderBean</span></span><br><span class="line">                &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//将临时的TableBean对象添加到集合orderBeans中</span></span><br><span class="line">                orderBeans.add(tmpOrderBean);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;    <span class="comment">//商品信息表</span></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(pdBeans, value);<span class="comment">//将value的值赋值给pdBean</span></span><br><span class="line">                &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//以上，至此完成了将&quot;01 1001 1 order&quot;和&quot;01 1004 4 order&quot;赋值到集合orderBeans中，将</span></span><br><span class="line">        <span class="comment">//&quot;01 小米 pd&quot;赋值给了pdBean对象</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历集合orderBeans，将每个orderBeans对象的商品编号替换为商品名称，然后写出</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean orderBean : orderBeans) &#123;</span><br><span class="line">            orderBean.setPname(pdBeans.getPname());</span><br><span class="line"></span><br><span class="line">            <span class="comment">//写出修改后的orderBeans对象</span></span><br><span class="line">            context.write(orderBean, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（4）编写TableDriver类，进行必要的配置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line">        job.setJarByClass(TableDriver.class);</span><br><span class="line">        job.setMapperClass(TableMapper.class);</span><br><span class="line">        job.setReducerClass(TableReducer.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(TableBean.class);</span><br><span class="line">        job.setOutputKeyClass(TableBean.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\11_input\\inputtable&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\output9&quot;</span>));</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行程序，查看输出结果：</p>
<img src="Snipaste_2023-10-04_16-24-57.png" alt="Snipaste_2023-10-04_16-24-57" style="zoom:50%;">

<p>缺点：这种方式中，合并的操作是在 Reduce 阶段完成，Reduce 端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生<strong>数据倾斜</strong>。（如果某个商品编号的订单数据远远超过其余商品编号的订单数据，那么处理该商品编号的ReduceTask的处理时间会大大延长）</p>
<p>解决方案：Map 端实现数据合并。</p>
<h4 id="5-8-3-Map-Join"><a href="#5-8-3-Map-Join" class="headerlink" title="5.8.3 Map Join"></a>5.8.3 Map Join</h4><ol>
<li>适用场景</li>
</ol>
<p>Map Join适用于一个表很小（缓存到内存中，以至于可以分发至集群中的每个节点上），另一张表很大的情景。</p>
<ol start="2">
<li>优点</li>
</ol>
<p>再map端缓存多个表，提前处理业务逻辑，从而增加map端业务，减轻reduce端的数据处理压力，可以尽可能地<strong>减少数据倾斜</strong></p>
<ol start="3">
<li>具体方法</li>
</ol>
<p>（1）在Mapper组件的setup()方法中，将文件读取到缓存集合中（<strong>先</strong>）；在map()方法中，实现数据连接操作（<strong>后</strong>）</p>
<p>（2）在Driver类驱动类中加载缓存</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//缓存普通文件到 Task 运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///e:/cache/pd.txt&quot;</span>));</span><br><span class="line"><span class="comment">//如果是集群运行,需要设置 HDFS 路径</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:8020/cache/pd.txt&quot;</span>));</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-10-04_16-44-41.png" alt="Snipaste_2023-10-04_16-44-41" style="zoom:50%;">

<ol start="4">
<li>代码实现</li>
</ol>
<p>（1）在MapJoinDriver驱动类中添加缓存文件。因为所有的数据连接操作都在Mapper组件中完成，不需要Reducer组件，所以将ReduceTask的数量设置为0个。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MapJoinDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException, URISyntaxException &#123;</span><br><span class="line">        <span class="comment">// 1 获取 job 信息</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line">        <span class="comment">// 2 设置加载 jar 包路径</span></span><br><span class="line">        job.setJarByClass(MapJoinDriver.class);</span><br><span class="line">        <span class="comment">// 3 关联 mapper</span></span><br><span class="line">        job.setMapperClass(MapJoinMapper.class);</span><br><span class="line">        <span class="comment">// 4 设置 Map 输出 KV 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        <span class="comment">// 5 设置最终输出 KV 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        <span class="comment">// 加载缓存数据</span></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\11_input\\tablecache\\pd.txt&quot;</span>));</span><br><span class="line">        <span class="comment">// Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 6 设置输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\11_input\\inputtable2&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\output99&quot;</span>));</span><br><span class="line">        <span class="comment">// 7 提交</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）在MapJoinMapper类中的setup()方法中读取缓存文件，将缓存文件中的数据写入HashMap，在map()方法中编写数据连接的逻辑代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MapJoinMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String ,String&gt; pdMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在任务开始前，将商品信息表pd.txt中的数据缓存进pdMap</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过缓存文件得到商品信息表pd.txt中的数据</span></span><br><span class="line">        URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">        <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(cacheFiles[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取文件系统对象并开流</span></span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(context.getConfiguration());</span><br><span class="line">        <span class="type">FSDataInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> fs.open(path);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将字节流转换为字符串流，方便按行读取</span></span><br><span class="line">        <span class="type">BufferedReader</span> <span class="variable">reader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(fis, <span class="string">&quot;UTF-8&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//按行读取，按行处理</span></span><br><span class="line">        String line;</span><br><span class="line">        <span class="keyword">while</span> (StringUtils.isNotEmpty(line = reader.readLine()))&#123;</span><br><span class="line">            <span class="comment">//切割一行</span></span><br><span class="line">            <span class="comment">//01  小米</span></span><br><span class="line">            String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            pdMap.put(fields[<span class="number">0</span>], fields[<span class="number">1</span>]);<span class="comment">//put方法相当于在一个HashMap中添加值，fields[0]为key,fields[1]为value</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭流</span></span><br><span class="line">        IOUtils.closeStreams(reader);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//处理order.txt</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">        String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取pid</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">pid</span> <span class="operator">=</span> fields[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">//根据订单数据表order.txt中每行数据的pid（商品编号），从pdMap中获取pname（商品名称）</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">pname</span> <span class="operator">=</span> pdMap.get(pid);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将订单数据表order.txt中每行数据的pid替换为pname</span></span><br><span class="line">        outK.set(fields[<span class="number">0</span>] + <span class="string">&quot;\t&quot;</span> + pname + <span class="string">&quot;\t&quot;</span> + fields[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出</span></span><br><span class="line">        context.write(outK, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行程序，查看输出结果：</p>
<img src="Snipaste_2023-10-04_21-16-30.png" alt="Snipaste_2023-10-04_21-16-30" style="zoom:50%;">

<h3 id="5-9-数据清洗"><a href="#5-9-数据清洗" class="headerlink" title="5.9 数据清洗"></a>5.9 数据清洗</h3><p>ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL 一词较常用在数据仓库，但其对象并不限于数据仓库</p>
<p>在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。<strong>清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序</strong>。</p>
<ol>
<li>需求分析：过滤掉日志中字段数量不超过11个的数据</li>
</ol>
<p>原数据共14619行</p>
<ol start="2">
<li>代码实现</li>
</ol>
<p>（1）编写WebLogMapper类，在map()方法中实现对日志数据的过滤：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WebLogMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.获取一行数据</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.解析日志</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> parseLog(line, context);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.日志不合法，退出</span></span><br><span class="line">        <span class="keyword">if</span> (!result)&#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.日志合法，直接写出</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//封装解析日志的方法</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">parseLog</span><span class="params">(String line, Context context)</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.截取</span></span><br><span class="line">        String[] fields = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.日志长度大于11为合法</span></span><br><span class="line">        <span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）编写WebLogDriver类，进行必要的配置，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WebLogDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">        args = <span class="keyword">new</span> <span class="title class_">String</span>[] &#123; <span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\11_input\\inputlog&quot;</span>,</span><br><span class="line">                <span class="string">&quot;D:\\尚硅谷大数据学习资料（无视频）\\Hadoop 3.x\\资料\\资料\\output222&quot;</span> &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取 job 信息</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 加载 jar 包</span></span><br><span class="line">        job.setJarByClass(LogDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关联 map</span></span><br><span class="line">        job.setMapperClass(WebLogMapper.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 reducetask 个数为 0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 提交</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）运行程序，查看输出结果：</p>
<p><img src="Snipaste_2023-10-04_21-49-35.png" alt="Snipaste_2023-10-04_21-49-35"></p>
<h3 id="5-10-Hadoop中的数据压缩"><a href="#5-10-Hadoop中的数据压缩" class="headerlink" title="5.10 Hadoop中的数据压缩"></a>5.10 Hadoop中的数据压缩</h3><h4 id="5-10-1-数据压缩概述"><a href="#5-10-1-数据压缩概述" class="headerlink" title="5.10.1 数据压缩概述"></a>5.10.1 数据压缩概述</h4><ol>
<li>压缩的好处和坏处</li>
</ol>
<ul>
<li>优点：以减少磁盘 IO、减少磁盘存储空间。</li>
<li>缺点：增加 CPU 开销</li>
</ul>
<ol start="2">
<li>压缩yuanze</li>
</ol>
<ul>
<li>运算密集型的 Job，少用压缩</li>
<li>IO 密集型的 Job，多用压缩</li>
</ul>
<h5 id="MapReduce支持的压缩编码"><a href="#MapReduce支持的压缩编码" class="headerlink" title="MapReduce支持的压缩编码"></a>MapReduce支持的压缩编码</h5><table>
<thead>
<tr>
<th>压缩格式</th>
<th>Hadoop 自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切片</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要创建索引，还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>是，直接使用</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
</tbody></table>
<h5 id="压缩性能比较"><a href="#压缩性能比较" class="headerlink" title="压缩性能比较"></a>压缩性能比较</h5><img src="Snipaste_2023-10-05_14-00-19.png" alt="Snipaste_2023-10-05_14-00-19" style="zoom:50%;">

<h5 id="1-压缩格式的选择"><a href="#1-压缩格式的选择" class="headerlink" title="1. 压缩格式的选择"></a>1. 压缩格式的选择</h5><p>压缩方式选择时重点考虑：压缩&#x2F;解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片，在企业中LZO和Snappy用的较多。</p>
<p>①GZIP</p>
<p>优点：压缩率较高</p>
<p>缺点：不支持切片，压缩&#x2F;解压缩速度一般</p>
<p>②bzip2</p>
<p>优点：压缩率高，支持切片</p>
<p>缺点：压缩&#x2F;解压缩速度慢</p>
<p>③LZO</p>
<p>优点：压缩&#x2F;解压缩速度较快，支持切片</p>
<p>缺点：压缩率一般，支持切片需要额外创建索引</p>
<p>④Snappy</p>
<p>优点：压缩&#x2F;解压缩速度块</p>
<p>缺点：不支持切片，压缩率一般</p>
<h5 id="2-压缩位置的选择"><a href="#2-压缩位置的选择" class="headerlink" title="2. 压缩位置的选择"></a>2. 压缩位置的选择</h5><p>压缩可以在 MapReduce 作用的任意阶段启用。</p>
<img src="Snipaste_2023-10-05_14-06-57.png" alt="Snipaste_2023-10-05_14-06-57" style="zoom:50%;">

<h4 id="5-10-2-压缩参数配置"><a href="#5-10-2-压缩参数配置" class="headerlink" title="5.10.2 压缩参数配置"></a>5.10.2 压缩参数配置</h4><p>为了支持多种压缩&#x2F;解压缩算法，Hadoop 引入了编码&#x2F;解码器</p>
<img src="Snipaste_2023-10-05_15-05-32.png" alt="Snipaste_2023-10-05_15-05-32" style="zoom:50%;">

<p>要在 Hadoop 中启用压缩，可以配置如下参数</p>
<img src="Snipaste_2023-10-05_15-06-04.png" alt="Snipaste_2023-10-05_15-06-04" style="zoom:50%;">

<img src="Snipaste_2023-10-05_15-06-18.png" alt="Snipaste_2023-10-05_15-06-18" style="zoom:50%;">

<h4 id="5-10-3-压缩案例实操"><a href="#5-10-3-压缩案例实操" class="headerlink" title="5.10.3 压缩案例实操"></a>5.10.3 压缩案例实操</h4><h5 id="1-Mapper输出采用压缩"><a href="#1-Mapper输出采用压缩" class="headerlink" title="1. Mapper输出采用压缩"></a>1. Mapper输出采用压缩</h5><p>即使MapReduce的输入文件和输出文件都是未压缩的文件，也可以对<strong>MapTask的中间输出过程</strong>进行数据压缩，因为需要将其写入硬盘并通过网络将其传输到ReduceTask所在的节点中，<strong>对其进行压缩可以提高很多性能</strong>，这些工作只要设置两个属性即可，具体实现如下：</p>
<p>（1）以WordCount程序为基础，在Driver驱动类中启用Mapper输出压缩功能，并且设置压缩格式为bzip2，其余代码不变:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启 map 端输出压缩</span></span><br><span class="line">conf.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line"><span class="comment">// 设置 map 端输出压缩方式</span></span><br><span class="line">conf.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>,</span><br><span class="line">              BZip2Codec.class, CompressionCodec.class);</span><br></pre></td></tr></table></figure>

<p>（2）Mapper组件保持不变</p>
<p>（3）Reducer组件保持不变</p>
<p>（4）运行程序，发现对输出结果没有任何影响，数据压缩只发生在map阶段</p>
<img src="Snipaste_2023-10-05_15-21-45.png" alt="Snipaste_2023-10-05_15-21-45" style="zoom:50%;">

<h5 id="2-Reducer输出采用压缩"><a href="#2-Reducer输出采用压缩" class="headerlink" title="2. Reducer输出采用压缩"></a>2. Reducer输出采用压缩</h5><p>（1）以WordCount程序为例，在Driver驱动类中启用reducer输出压缩功能，并且设置压缩格式为bzip2，其余代码不变</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置 reduce 端输出压缩开启</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="literal">true</span>);</span><br><span class="line"><span class="comment">// 设置压缩的方式</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span><br><span class="line"><span class="comment">// FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); </span></span><br><span class="line"><span class="comment">// FileOutputFormat.setOutputCompressorClass(job,DefaultCodec.class);</span></span><br></pre></td></tr></table></figure>

<p>（2）Mapper组件和Reducer组件均保持不变，适用bzip2压缩格式的输出结果如下（其余同理）：</p>
<img src="Snipaste_2023-10-05_15-34-32.png" alt="Snipaste_2023-10-05_15-34-32" style="zoom:50%;">

<h3 id="5-11-MapReduce开发总结"><a href="#5-11-MapReduce开发总结" class="headerlink" title="5.11 MapReduce开发总结"></a>5.11 MapReduce开发总结</h3><h4 id="1-输入数据接口：InputFormat"><a href="#1-输入数据接口：InputFormat" class="headerlink" title="1. 输入数据接口：InputFormat"></a>1. 输入数据接口：<strong>InputFormat</strong></h4><p>（1）默认使用的实现类是：TextInputFormat</p>
<p>（2）TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为 value 返回。</p>
<p>（3）CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</p>
<p>（4）InputFormat用的比较少了，所以没介绍自定义InputFormat</p>
<h4 id="2-逻辑处理接口：Mapper"><a href="#2-逻辑处理接口：Mapper" class="headerlink" title="2. 逻辑处理接口：Mapper"></a>2. 逻辑处理接口：<strong>Mapper</strong></h4><p>用户根据业务需求实现其中三个方法：</p>
<p>setup() ：初始化</p>
<p>map() ：用户的业务逻辑</p>
<p>cleanup () ：关闭资源</p>
<h4 id="3-Partitioner-分区"><a href="#3-Partitioner-分区" class="headerlink" title="3. Partitioner 分区"></a>3. <strong>Partitioner</strong> <strong>分区</strong></h4><p>（1）有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces</p>
<p>（2）如果业务上有特别的需求，可以自定义分区。</p>
<h4 id="4-Comparable-排序"><a href="#4-Comparable-排序" class="headerlink" title="4. Comparable 排序"></a>4. <strong>Comparable</strong> <strong>排序</strong></h4><p>（1）当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接口，重写其中的 compareTo()方法。</p>
<p>（2）部分排序：对最终输出的每一个文件进行内部排序。</p>
<p>（3）全排序：对所有数据进行排序，通常只有一个 Reduce。</p>
<p>（4）二次排序：排序的条件有两个。</p>
<h4 id="5-Combiner-合并"><a href="#5-Combiner-合并" class="headerlink" title="5. Combiner 合并"></a>5. <strong>Combiner</strong> <strong>合并</strong></h4><p>Combiner 合并可以提高程序执行效率，减少 IO 传输。在map阶段提前聚合。但是使用时必须不能影响原有的业务处理结果（求和没问题，求平均值有问题）。解决数据倾斜的一个方法。</p>
<h4 id="6-逻辑处理接口：Reducer"><a href="#6-逻辑处理接口：Reducer" class="headerlink" title="6. 逻辑处理接口：Reducer"></a>6. 逻辑处理接口：<strong>Reducer</strong></h4><p>用户根据业务需求实现其中三个方法：</p>
<p>setup() ：初始化</p>
<p>reduce() ：用户的业务逻辑</p>
<p>cleanup () ：关闭资源</p>
<h4 id="7-输出数据接口：OutputFormat"><a href="#7-输出数据接口：OutputFormat" class="headerlink" title="7. 输出数据接口：OutputFormat"></a>7. 输出数据接口：<strong>OutputFormat</strong></h4><p>（1）默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。</p>
<p>（2）用户还可以自定义 OutputFormat。</p>
<h2 id="第六章-资源调度器YARN"><a href="#第六章-资源调度器YARN" class="headerlink" title="第六章 资源调度器YARN"></a>第六章 资源调度器YARN</h2><h3 id="6-1-YARN概述"><a href="#6-1-YARN概述" class="headerlink" title="6.1 YARN概述"></a>6.1 YARN概述</h3><p>YARN是一个<strong>资源调度平台</strong>，负责为运算程序提供<strong>服务器运算资源</strong>，相当于一个<strong>分布式的操作系统</strong>，而MapReduce等运算程序相当于运行在操作系统上的应用程序。</p>
<h4 id="6-1-1-基本架构"><a href="#6-1-1-基本架构" class="headerlink" title="6.1.1 基本架构"></a>6.1.1 基本架构</h4><p>YARN采用了常见的Master-Slaver架构，其中，资源管理器<strong>ResourceManager</strong>担任Master角色，负责<strong>整个框架的资源统一管理和调度</strong>；<strong>NodeManager</strong>担任Slave角色，负责<strong>任务的执行及当前节点的资源管理</strong>。</p>
<p><img src="Snipaste_2023-10-05_16-12-45.png" alt="Snipaste_2023-10-05_16-12-45"></p>
<p>YARN主要由<strong>ResourceManager</strong>，<strong>NodeManager</strong>，<strong>ApplicationMaster</strong>和<strong>Container</strong>等组件组成。</p>
<h5 id="1-资源管理器ResourceManager"><a href="#1-资源管理器ResourceManager" class="headerlink" title="1. 资源管理器ResourceManager"></a>1. 资源管理器ResourceManager</h5><p>ResourceManager（RM）是一个全局资源管理器，负责管理整个集群的资源，主要作用：</p>
<ul>
<li>处理客户端（Client）请求</li>
<li>监控NodeManager</li>
<li>启动或监控ApplicationMaster</li>
<li>分配与调度资源</li>
</ul>
<h5 id="2-节点管理器NodeManager"><a href="#2-节点管理器NodeManager" class="headerlink" title="2. 节点管理器NodeManager"></a>2. 节点管理器NodeManager</h5><p>NodeManager（NM）是每个节点上的资源和任务的管理器，主要作用：</p>
<ul>
<li>管理单个节点上的资源和运行任务</li>
<li>处理来自ResourceManager的命令</li>
<li>定时汇报本节点的资源使用情况及各个Container的运行状态</li>
<li>处理来自ApplicationMaster的命令</li>
</ul>
<h5 id="3-ApplicationMaster"><a href="#3-ApplicationMaster" class="headerlink" title="3. ApplicationMaster"></a>3. ApplicationMaster</h5><p>用户提交的每个应用程序中均包含一个ApplicationMaster（AM），其主要作用如下：</p>
<ul>
<li>为应用程序申请资源并分配给内部的任务</li>
<li>与NodeMaster通信，以便启动或停止任务</li>
<li>监控任务的运行状态，并且在任务运行失败时重新申请资源，以便重启任务</li>
</ul>
<h5 id="4-容器（Container）"><a href="#4-容器（Container）" class="headerlink" title="4. 容器（Container）"></a>4. 容器（Container）</h5><p>Container（容器）是YARN中的资源抽象，它封装了某个节点服务器的多维度资源，如内存、CPU、磁盘、网络等。</p>
<h4 id="6-1-2-工作机制（重点面试题）"><a href="#6-1-2-工作机制（重点面试题）" class="headerlink" title="6.1.2 工作机制（重点面试题）"></a>6.1.2 工作机制（重点面试题）</h4><p><img src="Snipaste_2023-10-05_21-23-16.png" alt="Snipaste_2023-10-05_21-23-16"></p>
<p>（0）客户端通过调用job.waitForCompletion()方法，将MapReduce任务提交给整个集群，并且在客户端创建一个<strong>YARNRunner对象</strong></p>
<p>（1）YARNRunner对象向资源管理器<strong>ResourceManager</strong>申请一个<strong>Application</strong></p>
<p>（2）<strong>ResourceManager</strong>将该<strong>Application</strong>的资源提交路径返回给YARNRunner对象</p>
<p>（3）YARNRunner对象根据<strong>ResourceManager</strong>给出的资源提交路径，将该程序所需的资源（如文件分片信息、运行参数信息、运行jar包等）提交给HDFS</p>
<blockquote>
<p>0~3为准备阶段</p>
</blockquote>
<p>（4）在程序将资源提交完毕后，客户端向<strong>ResourceManager正式申请运行ApplicationMaster</strong></p>
<p>（5）<strong>ResourceManager</strong>将客户端的请求初始化成一个Task，并且为该Task调度分配资源</p>
<p>（6）集群中的一个<strong>空闲NodeManager</strong>领取Task</p>
<p>（7）该<strong>NodeManager</strong>会创建一个<strong>Container</strong>，并且在该<strong>Container</strong>中启动Task的<strong>ApplicationMaster</strong>，ApplicationMaster首先向ResourceManager注册，以便用户通过ResourceManager查看任务的运行状态</p>
<p>（8）ApplicationMaster从HDFS中将程序运行资源复制到本地节点中</p>
<blockquote>
<p>4~8为客户端向ResourceManager申请ApplicationMaster，ResourceManager在某个NodeManager创建ApplicationMaster</p>
</blockquote>
<p>（9）<strong>ApplicationMaster</strong>根据复制的程序运行资源决定需要几个<strong>MapTask</strong>，并且向<strong>ResourceManager</strong>申请运行MapTask所需的资源（此处假设需要运行2个MapTask）</p>
<p>（10）<strong>ResourceManager</strong>将MapTask任务分配给另外2个<strong>NodeManager</strong>，这两个NodeManager分别领取MapTask任务并创建<strong>Container</strong></p>
<p>（11）<strong>ApplicationMaster</strong>向2个接收到任务的<strong>NodeManager</strong>发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据进行计算，最终生成对应<strong>ReduceTask</strong>数量的分区文件。MapTask在运行过程中，不断向ApplicationMaster汇报各任务的<strong>运行状态和进度</strong>，以便让ApplicationMaster掌握各任务的运行状态和进展，并且在任务失败时尝试重启。</p>
<blockquote>
<p>9~11为ApplicationMaster向ResourceManager申请MapTask的资源，ResourceManager在某几个NodeManager上分配MapTask任务</p>
</blockquote>
<p>（12）<strong>ApplicationMaster</strong>在等待<strong>MapTask</strong>进行到一定程度（完成MapTask数量占总MapTask数量的5%以上）后，会向<strong>ResourceManager</strong>申请运行<strong>ReduceTask</strong>所需的资源。</p>
<p>（13）<strong>ResourceManager</strong>为<strong>ReduceTask</strong>分配<strong>NodeManager</strong>，NodeManager领取任务并创建2个Container。ApplicationMaster向这2个Container发送任务启动脚本，启动ReduceTask。ReduceTask向MapTask获取相应分区的数据，完成ReduceTask的计算任务，将数据输出至指定路径下。</p>
<blockquote>
<p>12~13为ApplicationMaster向ResourceManager申请ReduceTask的资源，ResourceManager在某几个NodeManager上分配ReduceTask任务</p>
</blockquote>
<p>（14）在所有程序运行完毕后，ApplicationMaster会向ResourceManager申请注销并关闭自己。</p>
<p>综上所述：</p>
<p>整个流程大体可以分为两个阶段：</p>
<ul>
<li>客户端向ResourceManager请求启动ApplicationMaster，ResourceManager为任务分配第一个Container，并且要求在这个Container中启动ApplicationMaster。</li>
<li>由ApplicationMaster获取任务运行资源，并且开始申请资源，监控任务运行全过程，直到任务完成。</li>
</ul>
<p>HDFS、YARN、MapReduce之间的关系：</p>
<p><img src="Snipaste_2023-10-06_12-05-22.png" alt="Snipaste_2023-10-06_12-05-22"></p>
<p>在客户端向ResourceManager提交任务后，ResourceManager为MapReduce程序分配资源，并且在任意节点上启动ApplicationMaster，ApplicationMaster为各任务申请资源。其中，MapTask的计算资源会被优先分配到数据块所在的节点中。在计算完成后，将输出数据上传至HDFS集群中。在这个过程中，HDFS集群主要负责存储输入数据和输出数据。</p>
<p>NameNode主要负责处理存储数据的读&#x2F;写请求、管理元数据信息等工作。</p>
<p>DataNode主要负责文件的具体存储工作。</p>
<p>SecondaryNameNode主要负责与NameNode协调完成定期合并EditLog与FsImage文件的工作。</p>
<h3 id="6-2-YARN的资源调度器和调度算法"><a href="#6-2-YARN的资源调度器和调度算法" class="headerlink" title="6.2 YARN的资源调度器和调度算法"></a>6.2 YARN的资源调度器和调度算法</h3><p><strong>资源调度器</strong>是YARN的核心组件之一，负责整个集群的资源调度工作，主要解决如何根据管理人员的既定策略分配资源的问题。</p>
<p>目前，YARN的资源调度器主要有三种：</p>
<ul>
<li>FIFO调度器</li>
<li>容量调度器（Apache Hadoop3.1.3默认使用）</li>
<li>公平调度器（CDH框架默认使用）</li>
</ul>
<p>查看默认配置文件yarn-default.xml文件：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="6-2-1-FIFO调度器"><a href="#6-2-1-FIFO调度器" class="headerlink" title="6.2.1 FIFO调度器"></a>6.2.1 FIFO调度器</h4><p>在Hadoop的最初版本中，使用的是简单的FIFO（先进先出）调度器，即单队列调度器，它将任务按照其到达的时间进行排序，<strong>队列中先到达的任务先获得资源</strong></p>
<img src="Snipaste_2023-10-06_12-43-37.png" alt="Snipaste_2023-10-06_12-43-37" style="zoom:50%;">

<p>优点：简单易懂</p>
<p>缺点：不支持多队列，会造成任务堵塞。如果job1占用了集群的所有资源且任务运行需要很长时间，那么队列后方的任务将无法获取任何计算资源，在生产环境中很少使用FIFO调度器。</p>
<h4 id="6-2-2-容量调度器"><a href="#6-2-2-容量调度器" class="headerlink" title="6.2.2 容量调度器"></a>6.2.2 容量调度器</h4><p>Yahoo开发</p>
<p><img src="Snipaste_2023-10-06_12-55-03.png" alt="Snipaste_2023-10-06_12-55-03"></p>
<p><img src="Snipaste_2023-10-06_12-58-40.png" alt="Snipaste_2023-10-06_12-58-40"></p>
<p>简而言之，就是先找资源占用率最低的那个队列先执行，选好队列后，按照优先级或提交时间优先顺序执行哪个任务，选好任务后，再按照容器优先级（本地性原则）优先分配资源。</p>
<h4 id="6-2-3-公平调度器"><a href="#6-2-3-公平调度器" class="headerlink" title="6.2.3 公平调度器"></a>6.2.3 公平调度器</h4><p>Facebook开发</p>
<p><img src="Snipaste_2023-10-06_13-25-36.png" alt="Snipaste_2023-10-06_13-25-36"></p>
<p>3）公平调度器的特点</p>
<p><img src="Snipaste_2023-10-06_13-26-09.png" alt="Snipaste_2023-10-06_13-26-09"></p>
<p>4）公平调度器的资源调度策略</p>
<p><img src="Snipaste_2023-10-06_13-26-45.png" alt="Snipaste_2023-10-06_13-26-45"></p>
<p><img src="Snipaste_2023-10-06_13-30-29.png" alt="Snipaste_2023-10-06_13-30-29"></p>
<p><img src="Snipaste_2023-10-06_13-35-55.png" alt="Snipaste_2023-10-06_13-35-55"></p>
<p><img src="Snipaste_2023-10-06_13-36-38.png" alt="Snipaste_2023-10-06_13-36-38"></p>
<h3 id="6-3-YARN实操"><a href="#6-3-YARN实操" class="headerlink" title="6.3 YARN实操"></a>6.3 YARN实操</h3><h4 id="6-3-1-常用的命令行命令"><a href="#6-3-1-常用的命令行命令" class="headerlink" title="6.3.1 常用的命令行命令"></a>6.3.1 常用的命令行命令</h4><p>对于YARN的状态，除了可以在hadoop103：8088页面查看，还可以通过命令行操作查看</p>
<p>执行WordCount官方示例程序，然后使用YARN的命令查看任务执行状况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/module/hadoop-3.1.3/</span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output22</span><br></pre></td></tr></table></figure>

<p>在页面端可以查看到：</p>
<p><img src="Snipaste_2023-10-06_13-55-55.png" alt="Snipaste_2023-10-06_13-55-55"></p>
<h5 id="1-查看任务命令yarn-application"><a href="#1-查看任务命令yarn-application" class="headerlink" title="1. 查看任务命令yarn application"></a>1. 查看任务命令yarn application</h5><p>1）yarn application -list命令主要用于列出所有的Application</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn application -list</span><br><span class="line">2023-10-06 13:57:43,405 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.255.103:8032</span><br><span class="line">Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):0</span><br><span class="line">                Application-Id	    Application-Name	    Application-Type	      User	     Queue               State	       Final-State	       Progress	                       Tracking-URL</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>2）yarn application -list -appStates命令主要用于根据Application的状态筛选（所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行以下命令，筛选所有处于FINISHED状态的Application</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn application -list -appStates FINISHED</span><br><span class="line">2023-10-06 14:01:50,118 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.255.103:8032</span><br><span class="line">Total number of applications (application-types: [], states: [FINISHED] and tags: []):1</span><br><span class="line">                Application-Id	    Application-Name	    Application-Type	      User	     Queue               State	       Final-State	       Progress	                       Tracking-URL</span><br><span class="line">application_1696571130879_0001	          word count	           MAPREDUCE	      root	   default            FINISHED	         SUCCEEDED	           100%	http://hadoop102:19888/jobhistory/job/job_1696571130879_0001</span><br></pre></td></tr></table></figure>

<p>3）yarn application -kill <ApplicationId>用于杀死Application</ApplicationId></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn application -kill application_1696571130879_0001</span><br><span class="line">2023-10-06 14:05:58,053 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.255.103:8032</span><br><span class="line">Application application_1696571130879_0001 has already finished </span><br></pre></td></tr></table></figure>

<h5 id="2-查看日志命令yarn-logs"><a href="#2-查看日志命令yarn-logs" class="headerlink" title="2. 查看日志命令yarn logs"></a>2. 查看日志命令yarn logs</h5><p>1）yarn logs -applicationId <ApplicationId>用于查询指定ApplicationId的Application日志</ApplicationId></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn logs -applicationId application_1696571130879_0001</span><br></pre></td></tr></table></figure>

<p>2）查询 Container 日志：yarn logs -applicationId <ApplicationId> -containerId <ContainerId></ContainerId></ApplicationId></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn logs -applicationId application_1696571130879_0001 -containerId container_1696571130879_0001_01_000001</span><br></pre></td></tr></table></figure>

<h5 id="3-查看尝试运行的任务命令yarn-applicationattempt"><a href="#3-查看尝试运行的任务命令yarn-applicationattempt" class="headerlink" title="3. 查看尝试运行的任务命令yarn applicationattempt"></a>3. 查看尝试运行的任务命令yarn applicationattempt</h5><p>1）yarn applicationattempt -list <ApplicationId>列出所有 Application 尝试的列表</ApplicationId></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn applicationattempt -list application_1696571130879_0001</span><br><span class="line">2023-10-06 14:14:44,321 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.255.103:8032</span><br><span class="line">Total number of application attempts :1</span><br><span class="line">         ApplicationAttempt-Id	               State	                    AM-Container-Id	                       Tracking-URL</span><br><span class="line">appattempt_1696571130879_0001_000001	            FINISHED	container_1696571130879_0001_01_000001	http://hadoop103:8088/proxy/application_1696571130879_0001/</span><br></pre></td></tr></table></figure>

<p>2）打印 ApplicationAttemp 状态：yarn applicationattempt -status <ApplicationAttemptId></ApplicationAttemptId></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn applicationattempt -status appattempt_1696571130879_0001_000001</span><br><span class="line">2023-10-06 14:19:20,970 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.255.103:8032</span><br><span class="line">Application Attempt Report : </span><br><span class="line">	ApplicationAttempt-Id : appattempt_1696571130879_0001_000001</span><br><span class="line">	State : FINISHED</span><br><span class="line">	AMContainer : container_1696571130879_0001_01_000001</span><br><span class="line">	Tracking-URL : http://hadoop103:8088/proxy/application_1696571130879_0001/</span><br><span class="line">	RPC Port : 44652</span><br><span class="line">	AM Host : hadoop104</span><br><span class="line">	Diagnostics : </span><br></pre></td></tr></table></figure>

<h5 id="4-查看容器命令yarn-container"><a href="#4-查看容器命令yarn-container" class="headerlink" title="4. 查看容器命令yarn container"></a>4. 查看容器命令yarn container</h5><p>1）列出所有 Container：yarn container -list <ApplicationAttemptId></ApplicationAttemptId></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn container -list appattempt_1696571130879_0001_000001</span><br><span class="line">2023-10-06 14:20:56,801 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.255.103:8032</span><br><span class="line">Total number of containers :0</span><br><span class="line">                  Container-Id	          Start Time	         Finish Time	               State	                Host	   Node Http Address	                            LOG-URL</span><br></pre></td></tr></table></figure>

<p>2）打印 Container 状态：yarn container -status <ContainerId></ContainerId></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn container -status container_1696571130879_0001_01_000001</span><br><span class="line">2023-10-06 14:22:08,834 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.255.103:8032</span><br><span class="line">Container with id &#x27;container_1696571130879_0001_01_000001&#x27; doesn&#x27;t exist in RM or Timeline Server.</span><br></pre></td></tr></table></figure>

<p><strong>注意：只有在任务运行的过程中才能看到Container的状态</strong></p>
<h5 id="5-查看节点状态命令yarn-node"><a href="#5-查看节点状态命令yarn-node" class="headerlink" title="5. 查看节点状态命令yarn node"></a>5. 查看节点状态命令yarn node</h5><p>列出所有NodeManager：yarn node -list -all</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn node -list -all</span><br><span class="line">2023-10-06 14:24:49,863 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.255.103:8032</span><br><span class="line">Total Nodes:3</span><br><span class="line">         Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers</span><br><span class="line"> hadoop103:39784	        RUNNING	   hadoop103:8042	                           0</span><br><span class="line"> hadoop102:33563	        RUNNING	   hadoop102:8042	                           0</span><br><span class="line"> hadoop104:44199	        RUNNING	   hadoop104:8042	                           0</span><br></pre></td></tr></table></figure>

<h5 id="6-更新配置命令yarn-rmadmin"><a href="#6-更新配置命令yarn-rmadmin" class="headerlink" title="6. 更新配置命令yarn rmadmin"></a>6. 更新配置命令yarn rmadmin</h5><p>重载队列配置：yarn rmadmin -refreshQueues</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]#  yarn rmadmin -refreshQueues</span><br><span class="line">2023-10-06 14:26:55,694 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.255.103:8033</span><br></pre></td></tr></table></figure>

<h5 id="7-查看队列命令yarn-queue"><a href="#7-查看队列命令yarn-queue" class="headerlink" title="7. 查看队列命令yarn queue"></a>7. 查看队列命令yarn queue</h5><p>yarn queue -status <QueueName>用于打印指定名称的队列的状态信息</QueueName></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn queue -status default</span><br><span class="line">2023-10-06 14:28:30,807 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.255.103:8032</span><br><span class="line">Queue Information : </span><br><span class="line">Queue Name : default</span><br><span class="line">	State : RUNNING</span><br><span class="line">	Capacity : 100.0%</span><br><span class="line">	Current Capacity : .0%</span><br><span class="line">	Maximum Capacity : 100.0%</span><br><span class="line">	Default Node Label expression : &lt;DEFAULT_PARTITION&gt;</span><br><span class="line">	Accessible Node Labels : *</span><br><span class="line">	Preemption : disabled</span><br><span class="line">	Intra-queue Preemption : disabled</span><br></pre></td></tr></table></figure>

<h4 id="6-3-2-核心参数（重要）"><a href="#6-3-2-核心参数（重要）" class="headerlink" title="6.3.2 核心参数（重要）"></a>6.3.2 核心参数（重要）</h4><p><img src="Snipaste_2023-10-06_14-33-37.png" alt="Snipaste_2023-10-06_14-33-37"></p>
<h4 id="6-3-3-核心参数配置案例"><a href="#6-3-3-核心参数配置案例" class="headerlink" title="6.3.3 核心参数配置案例"></a>6.3.3 核心参数配置案例</h4><p>本节主要学习这些参数是什么意思，如何调的，基本上在企业中是调好了不轻易更改了，因为不同的服务器节点内存和CPU信息不同，所以本节的数据仅供参考，如果配置完后发现某个队列的任务跑不起来，及时返回到快照yarn1即可。</p>
<p>首先给hadoop102,hadoop103,hadoop104分别设置快照</p>
<img src="Snipaste_2023-10-06_15-12-00.png" alt="Snipaste_2023-10-06_15-12-00" style="zoom: 50%;">

<img src="Snipaste_2023-10-06_15-13-15.png" alt="Snipaste_2023-10-06_15-13-15" style="zoom:50%;">

<img src="Snipaste_2023-10-06_15-13-44.png" alt="Snipaste_2023-10-06_15-13-44" style="zoom:50%;">

<h5 id="1-需求分析-5"><a href="#1-需求分析-5" class="headerlink" title="1. 需求分析"></a>1. 需求分析</h5><p>1）需求：从 1G 数据中，统计每个单词出现次数。服务器 3 台，每台配置 4G 内存，4 核CPU，4 线程。</p>
<p>2）需求分析：</p>
<p>1G &#x2F; 128m &#x3D; 8 个 MapTask；1 个 ReduceTask；1 个 mrAppMaster</p>
<p>平均每个节点运行 10 个 &#x2F; 3 台 ≈ 3 个任务（4 ，3 ，3）</p>
<h5 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2. 代码实现"></a>2. 代码实现</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改yarn-site.xml配置参数</span></span><br><span class="line">[root@hadoop102 hadoop-3.1.3]# cd etc/hadoop/</span><br><span class="line">[root@hadoop102 hadoop]# vim yarn-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 选择调度器，默认容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capaci</span><br><span class="line">ty.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- ResourceManager 处理调度器请求的线程数量,默认 50；如果提交的任务数大于 50，可以</span></span><br><span class="line"><span class="comment">增加该值，但是不能超过 3 台 * 4 线程 = 12 线程（去除其他应用程序实际不能超过 8） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of threads to handle scheduler </span><br><span class="line">interface.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.client.thread-count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 是否让 yarn 自动检测硬件进行配置，默认是 false，如果该节点有很多其他应用程序，建议</span></span><br><span class="line"><span class="comment">手动配置。如果该节点没有其他应用程序，可以采用自动 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Enable auto-detection of node capabilities such as</span><br><span class="line">    memory and CPU.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.detect-hardware-capabilities<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 是否将虚拟核数当作 CPU 核数，默认是 false，采用物理 CPU 核数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">    hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">    when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.count-logical-processors-as-cores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 虚拟核数和物理核数乘数，默认是 1.0 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Multiplier to determine how to convert phyiscal cores to</span><br><span class="line">    vcores. This value is used if yarn.nodemanager.resource.cpu-vcores</span><br><span class="line">    is set to -1(which implies auto-calculate vcores) and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is set to true. </span><br><span class="line">    The number of vcores will be calculated as number of CPUs * multiplier.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.pcores-vcores-multiplier<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- NodeManager 使用内存数，默认 8G，修改为 4G 内存 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Amount of physical memory, in MB, that can be allocated </span><br><span class="line">    for containers. If set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">    automatically calculated(in case of Windows and Linux).</span><br><span class="line">    In other cases, the default is 8192MB.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nodemanager 的 CPU 核数，不按照硬件环境自动设定时默认是 8 个，修改为 4 个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">    for containers. This is used by the RM scheduler when allocating</span><br><span class="line">    resources for containers. This is not used to limit the number of</span><br><span class="line">    CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">    automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">    In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最小内存，默认 1G --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at theRM in MBs. Memory requests lower than this will be set to the value of </span><br><span class="line">    this property. Additionally, a node manager that is configured to have </span><br><span class="line">    less memory than this value will be shut down by the resource manager.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最大内存，默认 8G --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the </span><br><span class="line">    RM in MBs. Memory requests higher than this will throw an</span><br><span class="line">    InvalidResourceRequestException.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>8192<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 容器最小 CPU 核数，默认 1 个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the </span><br><span class="line">    RM in terms of virtual CPU cores. Requests lower than this will be set to </span><br><span class="line">    the value of this property. Additionally, a node manager that is configured </span><br><span class="line">    to have fewer virtual cores than this value will be shut down by the </span><br><span class="line">    resource manager.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 容器最大 CPU 核数，默认 4 个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the </span><br><span class="line">    RM in terms of virtual CPU cores. Requests higher than this will throw an</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存检查，默认打开，修改为关闭 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存设置比例,默认 2.1 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when</span><br><span class="line">    setting memory limits for containers. Container allocations are</span><br><span class="line">    expressed in terms of physical memory, and virtual memory usage is </span><br><span class="line">    allowed to exceed this allocation by this ratio.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">分发配置文件</span></span><br><span class="line">[root@hadoop102 hadoop]# xsync yarn-site.xml </span><br><span class="line">============ hadoop102 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line"></span><br><span class="line">sent 62 bytes  received 12 bytes  148.00 bytes/sec</span><br><span class="line">total size is 7,887  speedup is 106.58</span><br><span class="line">============ hadoop103 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line">yarn-site.xml</span><br><span class="line"></span><br><span class="line">sent 5,908 bytes  received 59 bytes  11,934.00 bytes/sec</span><br><span class="line">total size is 7,887  speedup is 1.32</span><br><span class="line">============ hadoop104 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line">yarn-site.xml</span><br><span class="line"></span><br><span class="line">sent 5,908 bytes  received 59 bytes  3,978.00 bytes/sec</span><br><span class="line">total size is 7,887  speedup is 1.32</span><br></pre></td></tr></table></figure>

<p><strong>注意：如果集群中每台节点服务器的硬件资源都不一致，那么对每个NodeManager单独进行配置</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重新启动集群</span></span><br><span class="line">[root@hadoop102 hadoop]# myhadoop.sh start</span><br><span class="line"> =================== 启动 hadoop 集群 ===================</span><br><span class="line"> --------------- 启动 hdfs ---------------</span><br><span class="line">WARNING: HADOOP_SECURE_DN_USER has been replaced by HDFS_DATANODE_SECURE_USER. Using value of HADOOP_SECURE_DN_USER.</span><br><span class="line">Starting namenodes on [hadoop102]</span><br><span class="line">hadoop102: namenode is running as process 2962.  Stop it first.</span><br><span class="line">上一次登录：五 10月  6 13:48:04 CST 2023</span><br><span class="line">Starting datanodes</span><br><span class="line">hadoop102: datanode is running as process 3969.  Stop it first.</span><br><span class="line">hadoop104: datanode is running as process 2700.  Stop it first.</span><br><span class="line">hadoop103: datanode is running as process 2768.  Stop it first.</span><br><span class="line">上一次登录：五 10月  6 15:41:06 CST 2023</span><br><span class="line">Starting secondary namenodes [hadoop104]</span><br><span class="line">hadoop104: secondarynamenode is running as process 2825.  Stop it first.</span><br><span class="line">上一次登录：五 10月  6 15:41:06 CST 2023</span><br><span class="line"> --------------- 启动 yarn ---------------</span><br><span class="line">Starting resourcemanager</span><br><span class="line">resourcemanager is running as process 2988.  Stop it first.</span><br><span class="line">上一次登录：五 10月  6 13:48:06 CST 2023</span><br><span class="line">Starting nodemanagers</span><br><span class="line">hadoop103: nodemanager is running as process 3745.  Stop it first.</span><br><span class="line">hadoop102: nodemanager is running as process 3402.  Stop it first.</span><br><span class="line">hadoop104: nodemanager is running as process 2921.  Stop it first.</span><br><span class="line">上一次登录：五 10月  6 15:41:09 CST 2023</span><br><span class="line"> --------------- 启动 historyserver ---------------</span><br><span class="line">historyserver is running as process 4361.  Stop it first.</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看各节点集群启动状态</span></span><br><span class="line">[root@hadoop102 hadoop]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">3969 DataNode</span><br><span class="line">2962 NameNode</span><br><span class="line">7206 Jps</span><br><span class="line">4361 JobHistoryServer</span><br><span class="line">3402 NodeManager</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">2768 DataNode</span><br><span class="line">3745 NodeManager</span><br><span class="line">5498 Jps</span><br><span class="line">2988 ResourceManager</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">5264 Jps</span><br><span class="line">2825 SecondaryNameNode</span><br><span class="line">2921 NodeManager</span><br><span class="line">2700 DataNode</span><br></pre></td></tr></table></figure>

<p>执行任务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output222</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-10-06_15-53-21.png" alt="Snipaste_2023-10-06_15-53-21"></p>
<h4 id="6-3-4-容量调度器配置案例"><a href="#6-3-4-容量调度器配置案例" class="headerlink" title="6.3.4 容量调度器配置案例"></a>6.3.4 容量调度器配置案例</h4><p>在使用容量调度器时候，常见的两个问题：</p>
<p><strong>问题一：容量调度器默认只有1个default队列，不能满足实际需求，那么在生产环境中以什么标准划分队列？</strong></p>
<ul>
<li>按照<strong>计算框架</strong>：按照不同的计算框架（Hive，Spark，Flink等）划分队列。（企业中用的不多）</li>
<li>按照<strong>业务模块</strong>：按照不同的业务模块（登陆注册、购物车业务、下单业务、业务部门ABC）划分队列。（企业中用的多）</li>
</ul>
<p><strong>问题二：创建多队列有什么好处？</strong></p>
<ul>
<li>可以避免因为某个用户的代码失误，在代码中使用递归死循环等不当逻辑将所有资源耗尽。</li>
<li>实现不同业务的任务降级使用，在特殊时期，可以保证重要任务的队列资源充足。</li>
</ul>
<h5 id="1-需求分析-6"><a href="#1-需求分析-6" class="headerlink" title="1. 需求分析"></a>1. 需求分析</h5><p>需求 1：default 队列占总内存的 40%，最大资源容量占总资源 60%，hive 队列占总内存的 60%，最大资源容量占总资源 80%。</p>
<p>需求 2：配置队列优先级</p>
<h5 id="2-配置多队列的容器调度器"><a href="#2-配置多队列的容器调度器" class="headerlink" title="2. 配置多队列的容器调度器"></a>2. 配置多队列的容器调度器</h5><p>（1）capacity-scheduler.xml文件中的配置如下，在根队列下配置2个队列，分别为default队列和hive队列</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定多队列，增加 hive 队列 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        The queues at the this level (root is the root queue).</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）配置default队列的资源额定容量和最大资源占有量，代码如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 降低 default 队列资源额定容量为 40%，默认 100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 降低 default 队列资源最大容量为 60%，默认 100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（3）配置hive队列的资源额定容量和最大资源占有量，代码如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定 hive 队列的资源额定容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 用户最多可以使用队列资源的倍数，默认为1，确保无论集群有多空闲，单个用户都不会占有超过队列配置的资源 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定 hive 队列的资源最大容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>80<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（4）对hive队列进行必要的配置，代码如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 启动 hive 队列 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 哪些用户有权向队列提交作业 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_submit_applications<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 哪些用户有权操作队列，管理员权限（查看/杀死） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_administer_queue<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 哪些用户有权配置提交任务优先级 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_application_max_priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 任务的超时时间设置：yarn application -appId appId -updateLifetime Timeout</span></span><br><span class="line"><span class="comment">参考资料： https://blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 如果 application 指定了超时时间，则提交到该队列的 application 能够指定的最大超时</span></span><br><span class="line"><span class="comment">时间不能超过该值。默认值为-1代表无上限</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 如果 application 没指定超时时间，则用 default-application-lifetime 作为默认</span></span><br><span class="line"><span class="comment">值 ，默认值为-1代表无上限--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.default-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（5）分发配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# xsync capacity-scheduler.xml</span><br></pre></td></tr></table></figure>

<p>（6）执行以下命令刷新队列</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# yarn rmadmin -refreshQueues</span><br><span class="line">2023-10-06 19:15:29,957 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.255.103:8033</span><br></pre></td></tr></table></figure>

<p>（7）观察YARN的Web端，可以看到两个队列的配置情况：</p>
<p><img src="Snipaste_2023-10-06_19-28-39.png" alt="Snipaste_2023-10-06_19-28-39"></p>
<h5 id="3-向hive队列提交任务"><a href="#3-向hive队列提交任务" class="headerlink" title="3. 向hive队列提交任务"></a>3. 向hive队列提交任务</h5><p>（1）使用hadoop jar命令运行WordCount程序，通过<strong>mapreduce.job.queuename</strong>参数向hive队列提交任务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output3</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-10-06_20-17-09.png" alt="Snipaste_2023-10-06_20-17-09"></p>
<p>（2）在代码中对队列进行配置</p>
<p>默认的任务提交都是提交到 default 队列的。如果希望向其他队列提交任务，需要在Driver 中声明：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WcDrvier</span> &#123;</span><br><span class="line">     <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, </span><br><span class="line">        ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">         <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">         conf.set(<span class="string">&quot;mapreduce.job.queuename&quot;</span>,<span class="string">&quot;hive&quot;</span>);</span><br><span class="line">         <span class="comment">//1. 获取一个 Job 实例</span></span><br><span class="line">         <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line">         。。。 。。。</span><br><span class="line">         <span class="comment">//6. 提交 Job</span></span><br><span class="line">         <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">         System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="4-任务优先级"><a href="#4-任务优先级" class="headerlink" title="4. 任务优先级"></a>4. 任务优先级</h5><p>容量调度器，支持任务优先级的配置，在资源紧张时，优先级高的任务将优先获取资源。默认情况，Yarn 将所有任务的优先级限制为 0，若想使用任务的优先级功能，须开放该限制。</p>
<p>（1）修改yarn-site.xml文件，添加以下参数，将任务的最高优先级设置为5</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.cluster.max-application-priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）分发配置文件，并重启YARN</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# xsync yarn-site.xml</span><br><span class="line">[root@hadoop103 hadoop-3.1.3]# sbin/stop-yarn.sh</span><br><span class="line">[root@hadoop103 hadoop-3.1.3]# sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>（3）模拟资源紧张环境，可连续提交以下任务，直到新提交的任务申请不到资源为止。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 5 2000000</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-10-06_20-52-10.png" alt="Snipaste_2023-10-06_20-52-10"></p>
<p>（4）再次重新提交优先级高的任务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -D mapreduce.job.priority=5 5 2000000</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-10-06_20-53-49.png" alt="Snipaste_2023-10-06_20-53-49"></p>
<p>（5）也可以通过以下命令修改正在执行的任务的优先级。yarn application -appID <ApplicationID> -updatePriority 优先级</ApplicationID></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ yarn application -appID application_1611133087930_0009 -updatePriority 5</span><br></pre></td></tr></table></figure>

<h4 id="6-3-5-公平调度器配置案例"><a href="#6-3-5-公平调度器配置案例" class="headerlink" title="6.3.5 公平调度器配置案例"></a>6.3.5 公平调度器配置案例</h4><h5 id="1-需求分析-7"><a href="#1-需求分析-7" class="headerlink" title="1. 需求分析"></a>1. 需求分析</h5><p>创建两个队列，分别是 test 和 atguigu（以用户所属组命名）。期望实现以下效果：若用户提交任务时指定队列，则任务提交到指定队列运行；若未指定队列，test 用户提交的任务到 root.group.test 队列运行，atguigu 提交的任务到 root.group.atguigu 队列运行（注：group 为用户所属组）。</p>
<p>公平调度器的配置涉及到两个文件，一个是 yarn-site.xml，另一个是公平调度器队列分配文件 fair-scheduler.xml（文件名可自定义）。</p>
<h5 id="2-配置多队列的公平调度器"><a href="#2-配置多队列的公平调度器" class="headerlink" title="2. 配置多队列的公平调度器"></a>2. 配置多队列的公平调度器</h5><p>（1）修改 yarn-site.xml 文件，加入以下参数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop102 hadoop-3.1.3]# cd etc/hadoop/</span><br><span class="line">[root@hadoop102 hadoop]# vim yarn-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">description</span>&gt;</span>配置使用公平调度器<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.fair.allocation.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/fair-scheduler.xml<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">description</span>&gt;</span>指明公平调度器队列分配配置文件<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.fair.preemption<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">description</span>&gt;</span>禁止队列间资源抢占<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）创建fair-scheduler.xml文件，并且在该文件中添加以下内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# vim fair-scheduler.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">allocations</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 单个队列中 Application Master 占用资源的最大比例,取值 0-1 ，企业一般配置 0.1 </span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">queueMaxAMShareDefault</span>&gt;</span>0.5<span class="tag">&lt;/<span class="name">queueMaxAMShareDefault</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 单个队列最大资源的默认值 test atguigu default --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">queueMaxResourcesDefault</span>&gt;</span>4096mb,4vcores<span class="tag">&lt;/<span class="name">queueMaxResourcesDefault</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 增加一个队列 test --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">queue</span> <span class="attr">name</span>=<span class="string">&quot;test&quot;</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 队列最小资源 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">minResources</span>&gt;</span>2048mb,2vcores<span class="tag">&lt;/<span class="name">minResources</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 队列最大资源 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">maxResources</span>&gt;</span>4096mb,4vcores<span class="tag">&lt;/<span class="name">maxResources</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 队列中最多同时运行的应用数，默认 50，根据线程数配置 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">maxRunningApps</span>&gt;</span>4<span class="tag">&lt;/<span class="name">maxRunningApps</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 队列中 Application Master 占用资源的最大比例 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">maxAMShare</span>&gt;</span>0.5<span class="tag">&lt;/<span class="name">maxAMShare</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 该队列资源权重,默认值为 1.0 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 队列内部的资源分配策略 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">schedulingPolicy</span>&gt;</span>fair<span class="tag">&lt;/<span class="name">schedulingPolicy</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">queue</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 增加一个队列 atguigu --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">queue</span> <span class="attr">name</span>=<span class="string">&quot;atguigu&quot;</span> <span class="attr">type</span>=<span class="string">&quot;parent&quot;</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 队列最小资源 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">minResources</span>&gt;</span>2048mb,2vcores<span class="tag">&lt;/<span class="name">minResources</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 队列最大资源 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">maxResources</span>&gt;</span>4096mb,4vcores<span class="tag">&lt;/<span class="name">maxResources</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 队列中最多同时运行的应用数，默认 50，根据线程数配置 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">maxRunningApps</span>&gt;</span>4<span class="tag">&lt;/<span class="name">maxRunningApps</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 队列中 Application Master 占用资源的最大比例 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">maxAMShare</span>&gt;</span>0.5<span class="tag">&lt;/<span class="name">maxAMShare</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 该队列资源权重,默认值为 1.0 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 队列内部的资源分配策略 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">schedulingPolicy</span>&gt;</span>fair<span class="tag">&lt;/<span class="name">schedulingPolicy</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">queue</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">queuePlacementPolicy</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; false 表示：如果指</span></span><br><span class="line"><span class="comment">定队列不存在,不允许自动创建--&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">&quot;specified&quot;</span> <span class="attr">create</span>=<span class="string">&quot;false&quot;</span>/&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 提交到 root.group.username 队列,若 root.group 不存在,不允许自动创建；若</span></span><br><span class="line"><span class="comment">root.group.user 不存在,允许自动创建 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">&quot;nestedUserQueue&quot;</span> <span class="attr">create</span>=<span class="string">&quot;true&quot;</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">&quot;primaryGroup&quot;</span> <span class="attr">create</span>=<span class="string">&quot;false&quot;</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">rule</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 最后一个规则必须为 reject 或者 default。Reject 表示拒绝创建提交失败，</span></span><br><span class="line"><span class="comment">default 表示把任务提交到 default 队列 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">&quot;reject&quot;</span> /&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">queuePlacementPolicy</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（3）分发配置文件并重启YARN</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# xsync yarn-site.xml</span><br><span class="line">[root@hadoop102 hadoop]# xsync fair-scheduler.xml</span><br><span class="line"></span><br><span class="line">[root@hadoop103 hadoop-3.1.3]# sbin/stop-yarn.sh</span><br><span class="line">[root@hadoop103 hadoop-3.1.3]# sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-10-06_21-21-40.png" alt="Snipaste_2023-10-06_21-21-40"></p>
<h5 id="3-测试提交任务"><a href="#3-测试提交任务" class="headerlink" title="3. 测试提交任务"></a>3. 测试提交任务</h5><p>（1）提交任务时指定队列test</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -Dmapreduce.job.queuename=root.test 1 1</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-10-06_21-25-54.png" alt="Snipaste_2023-10-06_21-25-54"></p>
<h4 id="6-3-6-Tool接口案例"><a href="#6-3-6-Tool接口案例" class="headerlink" title="6.3.6 Tool接口案例"></a>6.3.6 Tool接口案例</h4><p>使用-D参数动态修改程序时可能会出错，因为会错把该设置当作传入的数据输入路径（第一个参数），使用Tool接口才能安全实现动态修改参数的功能</p>
<p>（1）新建Maven工程YarnDemo，在pom.xml文件中添加以下依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）创建com.atguigu.yarn包</p>
<p>（3）创建WordCount类，使其实现Tool接口，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCount</span> <span class="keyword">implements</span> <span class="title class_">Tool</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Configuration conf;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//核心驱动（conf需要传入）</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span>: <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setConf</span><span class="params">(Configuration conf)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.conf = conf;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Configuration <span class="title function_">getConf</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> conf;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//mapper</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">        <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">            String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String word : words)&#123;</span><br><span class="line">                outK.set(word);</span><br><span class="line"></span><br><span class="line">                context.write(outK, outV);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//reducer</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (IntWritable value: values)&#123;</span><br><span class="line">                sum += value.get();</span><br><span class="line">            &#125;</span><br><span class="line">            outV.set(sum);</span><br><span class="line"></span><br><span class="line">            context.write(key, outV);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（4）创建WordCountDriver类，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Tool tool;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">//1,创建配置文件</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.判断是否有Tool接口</span></span><br><span class="line">        <span class="keyword">switch</span> (args[<span class="number">0</span>])&#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;wordcount&quot;</span>:</span><br><span class="line">                tool = <span class="keyword">new</span> <span class="title class_">WordCount</span>();</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;No such tool : &quot;</span> + args[<span class="number">0</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.使用ToolRunner执行程序</span></span><br><span class="line">        <span class="comment">//Arrays.copyOfRange将原数组中的元素放到新数组中</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">run</span> <span class="operator">=</span> ToolRunner.run(conf, tool, Arrays.copyOfRange(args, <span class="number">1</span>, args.length));</span><br><span class="line"></span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（5）将程序打包，将jar包重命名为YarnDemo.jar，最后将jar包上传至Hadoop的安装目录下</p>
<img src="Snipaste_2023-10-07_12-56-34.png" alt="Snipaste_2023-10-07_12-56-34" style="zoom:50%;">

<img src="Snipaste_2023-10-07_12-57-57.png" alt="Snipaste_2023-10-07_12-57-57" style="zoom:50%;">

<p>（6）向集群提交jar包，带-D参数的情况，此时不会报错了</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop-3.1.3]# yarn jar YarnDemo.jar com.atguigu.yarn.WordCountDriver wordcount -Dmapreduce.job.queuename=root.test /input /output4</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-10-07_13-08-37.png" alt="Snipaste_2023-10-07_13-08-37" style="zoom:50%;">

<img src="Snipaste_2023-10-07_13-09-10.png" alt="Snipaste_2023-10-07_13-09-10" style="zoom: 33%;">

<p>测试完YARN后恢复到快照前的状态。</p>
<h2 id="第七章-高可用HA"><a href="#第七章-高可用HA" class="headerlink" title="第七章 高可用HA"></a>第七章 高可用HA</h2><h3 id="7-1-ZooKeeper详解"><a href="#7-1-ZooKeeper详解" class="headerlink" title="7.1  ZooKeeper详解"></a>7.1  ZooKeeper详解</h3><p>见《zookeeper框架学习笔记》</p>
<h3 id="7-2-HA概述"><a href="#7-2-HA概述" class="headerlink" title="7.2  HA概述"></a>7.2  HA概述</h3><h4 id="7-2-1-什么是HA"><a href="#7-2-1-什么是HA" class="headerlink" title="7.2.1 什么是HA"></a>7.2.1 什么是HA</h4><p>所谓 HA（High Availablity），即高可用（7*24 小时不中断服务）。实现高可用最关键的策略是消除单点故障。HA 严格来说应该分成各个组件的 HA机制：**HDFS 的 HA **和 <strong>YARN 的 HA</strong>。</p>
<p>NameNode 主要在以下两个方面影响 HDFS 集群：</p>
<ul>
<li>NameNode 机器发生意外，如宕机，集群将无法使用，直到管理员重启</li>
<li>NameNode 机器需要升级，包括软件、硬件升级，此时集群也将无法使用</li>
</ul>
<p>HDFS HA 功能通过配置多个 NameNodes(Active&#x2F;Standby)实现在集群中对 NameNode 的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将 NameNode 很快的切换到另外一台机器。</p>
<h4 id="7-2-2-HDFS-HA的工作机制"><a href="#7-2-2-HDFS-HA的工作机制" class="headerlink" title="7.2.2 HDFS HA的工作机制"></a>7.2.2 HDFS HA的工作机制</h4><p>在配置多个NN实现HA前，HDFS通过SecondaryNameNode机制保障NN的正常运行及宕机重启。在配置了多个NN后，我们需要回答以下几个问题：</p>
<p><strong>（1）如何保证多个NN的数据一致性</strong></p>
<p>FsImage：其中一个NN负责生成快照文件FsImage，其他NN拉取同步；</p>
<p>Edits：通过引进新的集群角色日志节点JournalNode，保证多个NN中的编辑日志文件EditLog的数据一致性。</p>
<p>当FsImage文件与EditLog文件都能保持一致时，NN可以提供相同的元数据管理服务。在备用NN接管工作后，会加载所有现有FsImage文件和EditLog文件，实现状态同步。</p>
<p><strong>（2）如何使多个NN中的一个处于Active状态，其他处于Standby状态</strong></p>
<p>同时有两个NN处于Active状态对于Hadoop集群来说是致命的，这种现象称为<strong>脑裂</strong>，应该避免。</p>
<p>Hadoop提供了一种称为<strong>故障转移控制器</strong>的监控进程，它可以在每一个NN上启动，时刻监控NN的状态。</p>
<p><strong>ZKFC</strong>是一种常用的故障转移控制器，是基于ZooKeeper实现的，当ZKFC运行在处于Active状态的NN上时，会<strong>在发现其状态不正常</strong>时向ZooKeeper中写入数据。当ZKFC运行在处于Standby状态的NN上时，会从ZooKeeper中读取数据，从而感知处于Active状态的NN是否在正常工作，以便顺利完成故障转移工作。</p>
<p>备用NN在感知到处于Active状态的NN出现异常后，通过以下步骤实现故障自动转移：</p>
<p>①通过SSH远程杀死处于Active状态的NN进程</p>
<p>②撤销处于Active状态的NN访问共享存储目录的权限</p>
<p>③通过远程管理命令屏蔽相应的网络接口</p>
<p>④通过一个特定的供电单元对相应主机进行断点操作</p>
<p><strong>（3）HA架构中不包含SecondaryNameNode，那么定期合并FsImage文件和EditLog文件的工作由谁负责</strong></p>
<p>备用NN中包含SecondaryNameNode的角色，处于Standby状态的NN会定时为处于Active状态的NN合并FsImage文件和EditLog文件</p>
<h3 id="7-3-Hadoop-HA集群的搭建"><a href="#7-3-Hadoop-HA集群的搭建" class="headerlink" title="7.3 Hadoop HA集群的搭建"></a>7.3 Hadoop HA集群的搭建</h3><h4 id="7-3-1-HDFS-HA手动故障转移"><a href="#7-3-1-HDFS-HA手动故障转移" class="headerlink" title="7.3.1 HDFS HA手动故障转移"></a>7.3.1 HDFS HA手动故障转移</h4><img src="Snipaste_2023-11-06_19-13-03.png" alt="Snipaste_2023-11-06_19-13-03" style="zoom:50%;">

<p>高可用集群规划：</p>
<img src="Snipaste_2023-11-06_19-14-06.png" alt="Snipaste_2023-11-06_19-14-06" style="zoom:50%;">

<p>（1）在opt目录下创建一个ha文件夹，用于安装高可用Hadoop HA集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt</span><br><span class="line">[root@hadoop102 opt]# mkdir ha</span><br><span class="line">[root@hadoop102 opt]# chown atguigu:atguigu /opt/ha</span><br></pre></td></tr></table></figure>

<p>（2）将&#x2F;opt&#x2F;module&#x2F;目录下的hadoop-3.1.3复制到&#x2F;opt&#x2F;ha目录下，并且删除data和log目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# cp -r /opt/module/hadoop-3.1.3 /opt/ha/</span><br><span class="line">[root@hadoop102 opt]# rm -rf /opt/ha/hadoop-3.1.3/data/</span><br><span class="line">[root@hadoop102 opt]# rm -rf /opt/ha/hadoop-3.1.3/logs/</span><br></pre></td></tr></table></figure>

<p>（3）修改配置文件core-site.xml，代码如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 把多个 NameNode 的地址组装成一个集群 mycluster --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定 hadoop 运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/ha/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（4）修改配置文件hdfs-site.xml，将hadoop102、hadoop103和hadoop104节点服务器中NN分别命名为nn1、nn2和nn3</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- NameNode 数据存储目录 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- DataNode 数据存储目录 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- JournalNode 数据存储目录 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;hadoop.tmp.dir&#125;/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 完全分布式集群名称 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 集群中 NameNode 节点都有哪些 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2,nn3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- NameNode 的 RPC 通信地址 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- NameNode 的 http 通信地址 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 NameNode 元数据在 JournalNode 上的存放位置 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 访问代理类：client 用于确定哪个 NameNode 为 Active --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 使用隔离机制时需要 ssh 秘钥登录--&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/atguigu/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（5）修改完成配置之后，将配置好的hadoop-3.1.3分发到其他节点中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# xsync /opt/ha/hadoop-3.1.3</span><br></pre></td></tr></table></figure>

<p>（6）修改环境变量，将HADOOP_HOME环境变量放到HA目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">老的</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新的</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/ha/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<p>source一下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# source /etc/profile</span><br></pre></td></tr></table></figure>

<p>103和104都要做</p>
<p>（7）在各个JournalNode上执行以下命令，启动JournalNode服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# hdfs --daemon start journalnode</span><br><span class="line">[root@hadoop103 ha]# hdfs --daemon start journalnode</span><br><span class="line">[root@hadoop104 ~]# hdfs --daemon start journalnode</span><br></pre></td></tr></table></figure>

<p>（8）在hadoop102节点服务器上对NN进行格式化并启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# hdfs namenode -format</span><br><span class="line">[root@hadoop102 hadoop]# hdfs --daemon start namenode</span><br></pre></td></tr></table></figure>

<p>（9）在hadoop103和104节点服务器中同步hadoop102节点服务器中的NN的元数据信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 ha]# hdfs namenode -bootstrapStandby</span><br><span class="line">[root@hadoop104 ~]# hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure>

<p>（10）分别在hadoop103和104上启动NN</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 ha]# hdfs --daemon start namenode</span><br><span class="line">[root@hadoop104 ~]# hdfs --daemon start namenode</span><br></pre></td></tr></table></figure>

<p>（11）分别登录3台节点服务器中NN的web端页面，可以看到，3台节点服务器中的NameNode都处于Standby状态</p>
<img src="Snipaste_2023-11-06_20-05-13.png" alt="Snipaste_2023-11-06_20-05-13" style="zoom:33%;">

<img src="Snipaste_2023-11-06_20-05-23.png" alt="Snipaste_2023-11-06_20-05-23" style="zoom:33%;">

<img src="Snipaste_2023-11-06_20-05-36.png" alt="Snipaste_2023-11-06_20-05-36" style="zoom:33%;">

<p>（12）在所有节点服务器上启动DataNode</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]#  hdfs --daemon start datanode</span><br><span class="line">[root@hadoop103 ha]# hdfs --daemon start datanode</span><br><span class="line">[root@hadoop104 ~]# hdfs --daemon start datanode</span><br></pre></td></tr></table></figure>

<p>（13）将nn1的状态切换为Active</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# hdfs haadmin -transitionToActive nn1</span><br></pre></td></tr></table></figure>

<p>（14）查看nn1是否处于Active状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# hdfs haadmin -getServiceState nn1</span><br><span class="line">active</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">5584 JournalNode</span><br><span class="line">6643 Jps</span><br><span class="line">5820 NameNode</span><br><span class="line">6316 DataNode</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">5938 Jps</span><br><span class="line">5348 NameNode</span><br><span class="line">5017 JournalNode</span><br><span class="line">5738 DataNode</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">4486 JournalNode</span><br><span class="line">5191 DataNode</span><br><span class="line">4794 NameNode</span><br><span class="line">5391 Jps</span><br></pre></td></tr></table></figure>

<h4 id="7-3-2-HDFS-HA自动故障转移"><a href="#7-3-2-HDFS-HA自动故障转移" class="headerlink" title="7.3.2 HDFS HA自动故障转移"></a>7.3.2 HDFS HA自动故障转移</h4><p>自动故障转移为 HDFS 部署增加了两个新组件：ZooKeeper 和 ZKFailoverController（ZKFC）进程，其中，ZooKeeper会维护NN的状态数据，并且通知ZKFC这些数据的变化情况。</p>
<p><img src="Snipaste_2023-11-06_20-27-43.png" alt="Snipaste_2023-11-06_20-27-43"></p>
<p>集群规划：</p>
<img src="Snipaste_2023-11-06_20-35-08.png" alt="Snipaste_2023-11-06_20-35-08" style="zoom:50%;">

<p>具体步骤如下：</p>
<p>（1）修改配置文件</p>
<p>在 hdfs-site.xml 中增加，启用HDFS HA自动故障转移功能</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 启用 nn 故障自动转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>在 core-site.xml 文件中增加，指定ZooKeeper服务器的地址和端口号</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定 zkfc 要连接的 zkServer 地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>分发修改后的配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 etc]# xsync hadoop/</span><br></pre></td></tr></table></figure>

<p>（2）启动</p>
<p>关闭所有的HDFS服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# stop-dfs.sh</span><br><span class="line">[root@hadoop102 ~]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">9036 Jps</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">7348 Jps</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">6561 Jps</span><br></pre></td></tr></table></figure>

<p>启动ZooKeeper集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# zk.sh start</span><br></pre></td></tr></table></figure>

<p>启动zk集群后，初始化HA在ZooKeeper集群中的状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure>

<p>再次启动HDFS服务，代码如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# start-dfs.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">9105 QuorumPeerMain</span><br><span class="line">12035 DFSZKFailoverController</span><br><span class="line">11397 NameNode</span><br><span class="line">11814 JournalNode</span><br><span class="line">12150 Jps</span><br><span class="line">11547 DataNode</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">8352 DataNode</span><br><span class="line">8674 Jps</span><br><span class="line">8262 NameNode</span><br><span class="line">7415 QuorumPeerMain</span><br><span class="line">8456 JournalNode</span><br><span class="line">8586 DFSZKFailoverController</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">7570 DataNode</span><br><span class="line">7874 Jps</span><br><span class="line">6627 QuorumPeerMain</span><br><span class="line">7480 NameNode</span><br><span class="line">7674 JournalNode</span><br><span class="line">7805 DFSZKFailoverController</span><br></pre></td></tr></table></figure>

<p>**注意：目前为止集群启动脚本为：zk.sh start —–&gt; start-dfs.sh **</p>
<p>此时去查看3台节点服务器中NN的Web端页面，查看NN状态，可以看到，自动地102NN处于Active状态，103和104NN处于Standby状态。</p>
<img src="Snipaste_2023-11-06_21-26-10.png" alt="Snipaste_2023-11-06_21-26-10" style="zoom: 33%;">

<img src="Snipaste_2023-11-06_21-26-20.png" alt="Snipaste_2023-11-06_21-26-20" style="zoom:33%;">

<img src="Snipaste_2023-11-06_21-26-27.png" alt="Snipaste_2023-11-06_21-26-27" style="zoom:33%;">

<p>上传文件测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将README.md文件上传至根目录/</span></span><br><span class="line">[root@hadoop102 zookeeper-3.5.7]# hadoop fs -put README.md /</span><br><span class="line">2023-11-06 22:03:59,796 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br></pre></td></tr></table></figure>

<p>上传成功：</p>
<img src="Snipaste_2023-11-06_22-05-32.png" alt="Snipaste_2023-11-06_22-05-32" style="zoom:50%;">

<h4 id="7-3-3-YARN-HA"><a href="#7-3-3-YARN-HA" class="headerlink" title="7.3.3 YARN HA"></a>7.3.3 YARN HA</h4><p>YARN HA配置多个ResourceManager，其中一个ResourceManager处于Active状态，其他ResourceManager处于Standby状态，处于Active状态的ResourceManager将状态写入ZooKeeper，当其他备用ResourceManager切换状态时，可以直接从ZooKeeper中读取，从而继续进行任务和资源调度。</p>
<img src="Snipaste_2023-11-07_12-45-56.png" alt="Snipaste_2023-11-07_12-45-56" style="zoom:33%;">

<p>集群规划：</p>
<img src="Snipaste_2023-11-07_12-48-36.png" alt="Snipaste_2023-11-07_12-48-36" style="zoom:50%;">

<p>具体配置：</p>
<p>（1）修改配置文件yarn-site.xml，将Hadoop102、103、104节点服务器中的ResourceManager分别命名为rm1，rm2和rm3</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/ha/hadoop-3.1.3/etc/hadoop/</span><br><span class="line">[root@hadoop102 hadoop]# vim yarn-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!-- 启用 resourcemanager ha --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!-- 声明两台 resourcemanager 的地址 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-yarn1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!--指定 resourcemanager 的逻辑列表--&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2,rm3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- ========== rm1 的配置 ========== --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 rm1 的主机名 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 rm1 的 web 端地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 rm1 的内部通信地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 AM 向 rm1 申请资源的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定供 NM 连接的地址 --&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- ========== rm2 的配置 ========== --&gt;</span></span><br><span class="line">     <span class="comment">&lt;!-- 指定 rm2 的主机名 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- ========== rm3 的配置 ========== --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 rm1 的主机名 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 rm1 的 web 端地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 rm1 的内部通信地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 AM 向 rm1 申请资源的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定供 NM 连接的地址 --&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!-- 指定 zookeeper 集群的地址 --&gt;</span> </span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!-- 启用自动恢复 --&gt;</span> </span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!-- 指定 resourcemanager 的状态信息存储在 zookeeper 集群 --&gt;</span> </span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span>     </span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）将配置文件分发至其他节点服务器中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# xsync yarn-site.xml</span><br></pre></td></tr></table></figure>

<p>（3）启动hadoop集群（HDFS和YARN都启动）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# zk.sh start</span><br><span class="line">[root@hadoop102 hadoop]# myhadoop.sh start</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">5824 DataNode</span><br><span class="line">6337 DFSZKFailoverController</span><br><span class="line">6946 JobHistoryServer</span><br><span class="line">6579 NodeManager</span><br><span class="line">5430 QuorumPeerMain</span><br><span class="line">6089 JournalNode</span><br><span class="line">7050 Jps</span><br><span class="line">6459 ResourceManager</span><br><span class="line">5676 NameNode</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">6065 ResourceManager</span><br><span class="line">5460 JournalNode</span><br><span class="line">5604 DFSZKFailoverController</span><br><span class="line">5352 DataNode</span><br><span class="line">6233 NodeManager</span><br><span class="line">5147 QuorumPeerMain</span><br><span class="line">5259 NameNode</span><br><span class="line">6463 Jps</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">4450 DataNode</span><br><span class="line">4818 ResourceManager</span><br><span class="line">4357 NameNode</span><br><span class="line">4923 NodeManager</span><br><span class="line">4220 QuorumPeerMain</span><br><span class="line">5100 Jps</span><br><span class="line">4558 JournalNode</span><br><span class="line">4702 DFSZKFailoverController</span><br></pre></td></tr></table></figure>

<p>（4）查看rm1的服务状态，是Active状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# yarn rmadmin -getServiceState rm1</span><br><span class="line">active</span><br></pre></td></tr></table></figure>

<p>（5）在YARN的Web端页面查看hadoop102:8088，hadoop103:8088，hadoop104:8088的YARN的状态，需要注意的是，无论用户访问哪台节点服务器的8088端口，都会自动跳转到处于Active状态的节点服务器。</p>
<img src="Snipaste_2023-11-07_13-24-21.png" alt="Snipaste_2023-11-07_13-24-21" style="zoom: 33%;">

<h4 id="7-3-4-Hadoop-HA最终规划"><a href="#7-3-4-Hadoop-HA最终规划" class="headerlink" title="7.3.4 Hadoop HA最终规划"></a>7.3.4 Hadoop HA最终规划</h4><table>
<thead>
<tr>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>NameNode</td>
<td>NameNode</td>
<td>NameNode</td>
</tr>
<tr>
<td>JournalNode</td>
<td>JournalNode</td>
<td>JournalNode</td>
</tr>
<tr>
<td>DataNode</td>
<td>DataNode</td>
<td>DataNode</td>
</tr>
<tr>
<td>Zookeeper</td>
<td>Zookeeper</td>
<td>Zookeeper</td>
</tr>
<tr>
<td>ZKFC</td>
<td>ZKFC</td>
<td>ZKFC</td>
</tr>
<tr>
<td>ResourceManager</td>
<td>ResourceManager</td>
<td>ResourceManager</td>
</tr>
<tr>
<td>NodeManager</td>
<td>NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<h2 id="高可用和非高可用模式的切换"><a href="#高可用和非高可用模式的切换" class="headerlink" title="高可用和非高可用模式的切换"></a>高可用和非高可用模式的切换</h2><p>如果我想回到原来的非高可用hadoop怎么办？</p>
<p>（1）首先关闭掉所有后台进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">12044 Jps</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">10095 Jps</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">6898 Jps</span><br></pre></td></tr></table></figure>

<p>（2）修改环境变量（并source）在103和104上做同样的操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="built_in">export</span> HADOOP_HOME=/opt/ha/hadoop-3.1.3</span></span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br><span class="line"></span><br><span class="line">[root@hadoop102 ~]# source /etc/profile</span><br></pre></td></tr></table></figure>

<p>（3）启动集群即为非高可用hadoop集群（4-3-3模式）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# myhadoop.sh start</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">13105 Jps</span><br><span class="line">12498 DataNode</span><br><span class="line">12835 NodeManager</span><br><span class="line">12317 NameNode</span><br><span class="line">13023 JobHistoryServer</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">10192 DataNode</span><br><span class="line">10946 Jps</span><br><span class="line">10745 NodeManager</span><br><span class="line">10410 ResourceManager</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">6995 DataNode</span><br><span class="line">7205 NodeManager</span><br><span class="line">7353 Jps</span><br><span class="line">7114 SecondaryNameNode</span><br></pre></td></tr></table></figure>

<p>（4）修改环境变量（并source）在103和104上做同样的操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HADOOP_HOME</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-3.1.3</span></span><br><span class="line">export HADOOP_HOME=/opt/ha/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br><span class="line"></span><br><span class="line">[root@hadoop102 ~]# source /etc/profile</span><br></pre></td></tr></table></figure>

<p>（5）启动集群即为高可用hadoop集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先启动zk</span></span><br><span class="line">[root@hadoop102 ~]# zk.sh start</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">再启动高可用hadoop</span></span><br><span class="line">[root@hadoop102 ~]# myhadoop.sh start</span><br><span class="line">[root@hadoop102 ~]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">5824 DataNode</span><br><span class="line">6337 DFSZKFailoverController</span><br><span class="line">6946 JobHistoryServer</span><br><span class="line">6579 NodeManager</span><br><span class="line">5430 QuorumPeerMain</span><br><span class="line">6089 JournalNode</span><br><span class="line">7050 Jps</span><br><span class="line">6459 ResourceManager</span><br><span class="line">5676 NameNode</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">6065 ResourceManager</span><br><span class="line">5460 JournalNode</span><br><span class="line">5604 DFSZKFailoverController</span><br><span class="line">5352 DataNode</span><br><span class="line">6233 NodeManager</span><br><span class="line">5147 QuorumPeerMain</span><br><span class="line">5259 NameNode</span><br><span class="line">6463 Jps</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">4450 DataNode</span><br><span class="line">4818 ResourceManager</span><br><span class="line">4357 NameNode</span><br><span class="line">4923 NodeManager</span><br><span class="line">4220 QuorumPeerMain</span><br><span class="line">5100 Jps</span><br><span class="line">4558 JournalNode</span><br><span class="line">4702 DFSZKFailoverController</span><br></pre></td></tr></table></figure>

<h2 id="第八章-生产调优手册"><a href="#第八章-生产调优手册" class="headerlink" title="第八章 生产调优手册"></a>第八章 生产调优手册</h2>
      

      
        <div class="page-reward">
          <a href="javascript:;" class="page-reward-btn tooltip-top">
            <div class="tooltip tooltip-east">
            <span class="tooltip-item">
              赏
            </span>
            <span class="tooltip-content">
              <span class="tooltip-text">
                <span class="tooltip-inner">
                  <p class="reward-p"><i class="icon icon-quo-left"></i>谢谢您的支持<i class="icon icon-quo-right"></i></p>
                  <div class="reward-box">
                    
                    
                    <div class="reward-box-item">
                      <img class="reward-img" src="/img/wechatpay.jpg">
                      <span class="reward-type">微信</span>
                    </div>
                    
                  </div>
                </span>
              </span>
            </span>
          </div>
          </a>
        </div>
      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">Linux</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


      

      
        
<div class="share-btn share-icons tooltip-left">
  <div class="tooltip tooltip-east">
    <span class="tooltip-item">
      <a href="javascript:;" class="share-sns share-outer">
        <i class="icon icon-share"></i>
      </a>
    </span>
    <span class="tooltip-content">
      <div class="share-wrap">
        <div class="share-icons">
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="icon icon-weibo"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="icon icon-weixin"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="icon icon-qq"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="icon icon-douban"></i>
          </a>
          <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a>
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="icon icon-facebook"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="icon icon-twitter"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="icon icon-google"></i>
          </a>
        </div>
      </div>
    </span>
  </div>
</div>

<div class="page-modal wx-share js-wx-box">
    <a class="close js-modal-close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//pan.baidu.com/share/qrcode?url=http://example.com/2023/08/05/Hadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" alt="微信分享二维码">
    </div>
</div>

<div class="mask js-mask"></div>
      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
    <a href="/2023/10/07/Hive%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" id="article-nav-newer" class="article-nav-link-wrap">
      <i class="icon-circle-left"></i>
      <div class="article-nav-title">
        
          Hive框架学习笔记
        
      </div>
    </a>
  
  
    <a href="/2023/07/14/Python%E9%87%91%E8%9E%8D%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E5%88%86%E6%9E%90/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Python金融大数据挖掘与分析</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>


<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
        <div class="toc-container tooltip-left">
            <i class="icon-font icon-category"></i>
            <div class="tooltip tooltip-east">
                <span class="tooltip-item">
                </span>
                <span class="tooltip-content">
                    <div class="toc-article">
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">第一部分 大数据概论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.</span> <span class="toc-text">第一章 大数据概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%89%B9%E7%82%B9%EF%BC%884V%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">第二章 大数据特点（4V）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%83%A8%E9%97%A8%E5%86%85%E7%BB%84%E7%BB%87%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.</span> <span class="toc-text">第三章 大数据部门内组织结构</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86-Hadoop%E5%85%A5%E9%97%A8"><span class="toc-number">2.</span> <span class="toc-text">第二部分 Hadoop入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0"><span class="toc-number">2.1.</span> <span class="toc-text">第一章</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-Hadoop%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%EF%BC%88%E5%BC%80%E5%8F%91%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">第二章 Hadoop运行环境搭建（开发重点）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%A8%A1%E6%9D%BF%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.1 模板虚拟机环境准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%85%8B%E9%9A%86%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2 克隆虚拟机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%9C%A8hadoop102%E5%AE%89%E8%A3%85JDK"><span class="toc-number">2.2.3.</span> <span class="toc-text">2.3 在hadoop102安装JDK</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%9C%A8hadoop102%E5%AE%89%E8%A3%85Hadoop"><span class="toc-number">2.2.4.</span> <span class="toc-text">2.4 在hadoop102安装Hadoop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-Hadoop%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="toc-number">2.2.5.</span> <span class="toc-text">2.5 Hadoop目录结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-Hadoop%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="toc-number">2.3.</span> <span class="toc-text">第三章 Hadoop运行模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%AE%98%E6%96%B9WordCount%EF%BC%89"><span class="toc-number">2.3.1.</span> <span class="toc-text">3.1 本地运行模式（官方WordCount）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%BC%80%E5%8F%91%E9%87%8D%E7%82%B9%E3%80%82%E8%87%B3%E5%B0%91%E8%87%AA%E5%B7%B1%E5%AE%89%E8%A3%85%E4%B8%89%E9%81%8D%E4%BB%A5%E4%B8%8A%EF%BC%89"><span class="toc-number">2.3.2.</span> <span class="toc-text">3.2 完全分布式运行模式（开发重点。至少自己安装三遍以上）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%87%86%E5%A4%87"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">3.2.1 虚拟机准备</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-%E7%BC%96%E5%86%99%E4%B8%80%E4%BA%9BShell%E8%84%9A%E6%9C%AC"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">3.2.2 编写一些Shell脚本</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%EF%BC%89scp%EF%BC%88secure-copy%EF%BC%89%E5%AE%89%E5%85%A8%E6%8B%B7%E8%B4%9D"><span class="toc-number">2.3.2.2.1.</span> <span class="toc-text">1）scp（secure copy）安全拷贝</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89rsync%E8%BF%9C%E7%A8%8B%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7"><span class="toc-number">2.3.2.2.2.</span> <span class="toc-text">2）rsync远程同步工具</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%EF%BC%89xsync%E9%9B%86%E7%BE%A4%E5%88%86%E5%8F%91%E8%84%9A%E6%9C%AC"><span class="toc-number">2.3.2.2.3.</span> <span class="toc-text">3）xsync集群分发脚本</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4%EF%BC%89%E9%9B%86%E7%BE%A4%E5%91%BD%E4%BB%A4%E5%90%8C%E6%97%B6%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC"><span class="toc-number">2.3.2.2.4.</span> <span class="toc-text">4）集群命令同时执行脚本</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-SSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95"><span class="toc-number">2.3.2.3.</span> <span class="toc-text">3.2.3 SSH免密登录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-4-%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE"><span class="toc-number">2.3.2.4.</span> <span class="toc-text">3.2.4 集群配置</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A0%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E8%A7%84%E5%88%92"><span class="toc-number">2.3.2.4.1.</span> <span class="toc-text">①集群部署规划</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A1%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E"><span class="toc-number">2.3.2.4.2.</span> <span class="toc-text">②配置文件说明</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A2%E9%85%8D%E7%BD%AE%E9%9B%86%E7%BE%A4"><span class="toc-number">2.3.2.4.3.</span> <span class="toc-text">③配置集群</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A3%E5%9C%A8%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%88%86%E5%8F%91%E9%85%8D%E7%BD%AE%E5%A5%BD%E7%9A%84Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">2.3.2.4.4.</span> <span class="toc-text">④在集群上分发配置好的Hadoop配置文件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A4%E9%85%8D%E7%BD%AEworkers%E6%96%87%E4%BB%B6"><span class="toc-number">2.3.2.4.5.</span> <span class="toc-text">⑤配置workers文件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A5%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="toc-number">2.3.2.4.6.</span> <span class="toc-text">⑥启动集群</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1%EF%BC%89%E5%9C%A8hadoop102%E8%8A%82%E7%82%B9%E6%A0%BC%E5%BC%8F%E5%8C%96NameNode"><span class="toc-number">2.3.2.4.6.1.</span> <span class="toc-text">1）在hadoop102节点格式化NameNode</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2%EF%BC%89%E5%90%AF%E5%8A%A8HDFS"><span class="toc-number">2.3.2.4.6.2.</span> <span class="toc-text">2）启动HDFS</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3%EF%BC%89%E5%90%AF%E5%8A%A8YARN"><span class="toc-number">2.3.2.4.6.3.</span> <span class="toc-text">3）启动YARN</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#4%EF%BC%89NameNode%E7%9A%84Web%E7%AB%AF"><span class="toc-number">2.3.2.4.6.4.</span> <span class="toc-text">4）NameNode的Web端</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#5%EF%BC%89YARN%E7%9A%84Web%E7%AB%AF"><span class="toc-number">2.3.2.4.6.5.</span> <span class="toc-text">5）YARN的Web端</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A6%E9%9B%86%E7%BE%A4%E5%9F%BA%E6%9C%AC%E6%B5%8B%E8%AF%95"><span class="toc-number">2.3.2.4.7.</span> <span class="toc-text">⑦集群基本测试</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1%EF%BC%89%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E6%B5%8B%E8%AF%95"><span class="toc-number">2.3.2.4.7.1.</span> <span class="toc-text">1）文件上传测试</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2%EF%BC%89%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD%E6%B5%8B%E8%AF%95"><span class="toc-number">2.3.2.4.7.2.</span> <span class="toc-text">2）文件下载测试</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3%EF%BC%89%E7%AE%80%E5%8D%95%E8%AE%A1%E7%AE%97%E6%B5%8B%E8%AF%95"><span class="toc-number">2.3.2.4.7.3.</span> <span class="toc-text">3）简单计算测试</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-5-NameNode%E6%A0%BC%E5%BC%8F%E5%8C%96%E9%97%AE%E9%A2%98%EF%BC%88%E9%9B%86%E7%BE%A4%E6%8C%82%E4%BA%86%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%89"><span class="toc-number">2.3.2.5.</span> <span class="toc-text">3.2.5 NameNode格式化问题（集群挂了如何解决）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-6-%E9%85%8D%E7%BD%AE%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-number">2.3.2.6.</span> <span class="toc-text">3.2.6 配置历史服务器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-7-%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97%E8%81%9A%E9%9B%86%E5%8A%9F%E8%83%BD"><span class="toc-number">2.3.2.7.</span> <span class="toc-text">3.2.7 配置日志聚集功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-8-%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8-x2F-%E5%81%9C%E6%AD%A2%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93"><span class="toc-number">2.3.2.8.</span> <span class="toc-text">3.2.8 集群启动&#x2F;停止方式总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-9-Hadoop%E9%9B%86%E7%BE%A4%E5%90%AF%E5%81%9C%E8%84%9A%E6%9C%AC"><span class="toc-number">2.3.2.9.</span> <span class="toc-text">3.2.9 Hadoop集群启停脚本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-10-%E5%B8%B8%E7%94%A8%E7%AB%AF%E5%8F%A3%E5%8F%B7%E5%92%8C%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%88%E4%B8%A4%E9%81%93%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%89"><span class="toc-number">2.3.2.10.</span> <span class="toc-text">3.2.10 常用端口号和常用配置文件（两道面试题）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-11-%E5%85%B3%E4%BA%8EHDFS%E6%96%87%E4%BB%B6%E5%88%86%E5%9D%97%E5%AD%98%E5%82%A8%E7%9A%84%E6%A6%82%E8%BF%B0"><span class="toc-number">2.3.2.11.</span> <span class="toc-text">3.2.11 关于HDFS文件分块存储的概述</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FHDFS"><span class="toc-number">2.4.</span> <span class="toc-text">第四章 分布式文件系统HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-HDFS%E6%A6%82%E8%BF%B0"><span class="toc-number">2.4.1.</span> <span class="toc-text">4.1 HDFS概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-HGFS%E8%83%8C%E6%99%AF%E5%8F%8A%E6%84%8F%E4%B9%89"><span class="toc-number">2.4.1.1.</span> <span class="toc-text">4.1.1 HGFS背景及意义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-HDFS%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84"><span class="toc-number">2.4.1.2.</span> <span class="toc-text">4.1.2 HDFS的基本架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-3-HDFS%E6%96%87%E4%BB%B6%E5%9D%97%E5%A4%A7%E5%B0%8F%EF%BC%88%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">2.4.1.3.</span> <span class="toc-text">4.1.3 HDFS文件块大小（面试重点）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-HDFS%E7%9A%84shell%E6%93%8D%E4%BD%9C%EF%BC%88%E5%BC%80%E5%8F%91%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">2.4.2.</span> <span class="toc-text">4.2 HDFS的shell操作（开发重点）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8"><span class="toc-number">2.4.2.1.</span> <span class="toc-text">4.2.1 命令大全</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%91%BD%E4%BB%A4%E5%AE%9E%E6%93%8D"><span class="toc-number">2.4.2.2.</span> <span class="toc-text">4.2.2 命令行命令实操</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-number">2.4.2.2.1.</span> <span class="toc-text">1.准备工作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E4%B8%8A%E4%BC%A0"><span class="toc-number">2.4.2.2.2.</span> <span class="toc-text">2. 上传</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E4%B8%8B%E8%BD%BD"><span class="toc-number">2.4.2.2.3.</span> <span class="toc-text">3. 下载</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-HDFS%E7%9B%B4%E6%8E%A5%E6%93%8D%E4%BD%9C"><span class="toc-number">2.4.2.2.4.</span> <span class="toc-text">4. HDFS直接操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-HDFS%E7%9A%84API%E6%93%8D%E4%BD%9C"><span class="toc-number">2.4.3.</span> <span class="toc-text">4.3 HDFS的API操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">2.4.3.1.</span> <span class="toc-text">4.3.1 客户端环境准备</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-HDFS%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E6%A1%88%E4%BE%8B"><span class="toc-number">2.4.3.2.</span> <span class="toc-text">4.3.2 HDFS文件上传案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-3-HDFS%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD%E6%A1%88%E4%BE%8B"><span class="toc-number">2.4.3.3.</span> <span class="toc-text">4.3.3 HDFS文件下载案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-4-HDFS%E6%96%87%E4%BB%B6%E9%87%8D%E5%91%BD%E5%90%8D%E4%B8%8E%E7%A7%BB%E5%8A%A8%E6%A1%88%E4%BE%8B"><span class="toc-number">2.4.3.4.</span> <span class="toc-text">4.3.4 HDFS文件重命名与移动案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-5-HDFS%E6%96%87%E4%BB%B6%E5%88%A0%E9%99%A4%E6%A1%88%E4%BE%8B"><span class="toc-number">2.4.3.5.</span> <span class="toc-text">4.3.5 HDFS文件删除案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-6-HDFS%E6%96%87%E4%BB%B6%E8%AF%A6%E6%83%85%E6%9F%A5%E7%9C%8B%E6%A1%88%E4%BE%8B"><span class="toc-number">2.4.3.6.</span> <span class="toc-text">4.3.6 HDFS文件详情查看案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-7-HDFS%E6%96%87%E4%BB%B6%E5%92%8C%E6%96%87%E4%BB%B6%E5%A4%B9%E5%88%A4%E6%96%AD%E6%A1%88%E4%BE%8B"><span class="toc-number">2.4.3.7.</span> <span class="toc-text">4.3.7 HDFS文件和文件夹判断案例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-HDFS%E7%9A%84%E8%AF%BB-x2F-%E5%86%99%E6%B5%81%E7%A8%8B%EF%BC%88%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">2.4.4.</span> <span class="toc-text">4.4 HDFS的读&#x2F;写流程（面试重点）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1-HDFS%E4%B8%AD%E6%95%B0%E6%8D%AE%E5%9D%97%E5%A4%A7%E5%B0%8F%EF%BC%88%E4%B9%8B%E5%89%8D%E8%AF%B4%E8%BF%87%E4%BA%86%EF%BC%89"><span class="toc-number">2.4.4.1.</span> <span class="toc-text">4.4.1 HDFS中数据块大小（之前说过了）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-2-%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-number">2.4.4.2.</span> <span class="toc-text">4.4.2 写数据流程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E5%89%96%E6%9E%90%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-number">2.4.4.2.1.</span> <span class="toc-text">1. 剖析写数据流程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5%E4%B8%8E%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5"><span class="toc-number">2.4.4.2.2.</span> <span class="toc-text">2. 副本放置策略与机架感知</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91%E8%B7%9D%E7%A6%BB%E4%B8%8EPipeLine%E7%9A%84%E5%BD%A2%E6%88%90"><span class="toc-number">2.4.4.2.3.</span> <span class="toc-text">3. 网络拓扑距离与PipeLine的形成</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-3-%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-number">2.4.4.3.</span> <span class="toc-text">4.4.3 读数据流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-HDFS%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">2.4.5.</span> <span class="toc-text">4.5 HDFS的工作流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-1-NameNode%E5%92%8CSecondaryNameNode%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">2.4.5.1.</span> <span class="toc-text">4.5.1 NameNode和SecondaryNameNode的工作机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-2-EditLog%E5%92%8CFsImage%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90"><span class="toc-number">2.4.5.2.</span> <span class="toc-text">4.5.2 EditLog和FsImage文件解析</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E4%BD%BF%E7%94%A8oiv%E5%91%BD%E4%BB%A4%E6%9F%A5%E7%9C%8BFsImage%E6%96%87%E4%BB%B6"><span class="toc-number">2.4.5.2.1.</span> <span class="toc-text">1. 使用oiv命令查看FsImage文件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E4%BD%BF%E7%94%A8oev%E5%91%BD%E4%BB%A4%E6%9F%A5%E7%9C%8BEditLog%E6%96%87%E4%BB%B6"><span class="toc-number">2.4.5.2.2.</span> <span class="toc-text">2. 使用oev命令查看EditLog文件</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-3-%E6%A3%80%E6%9F%A5%E7%82%B9%E6%97%B6%E9%97%B4%E8%AE%BE%E7%BD%AE"><span class="toc-number">2.4.5.3.</span> <span class="toc-text">4.5.3 检查点时间设置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-4-DataNode%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">2.4.5.4.</span> <span class="toc-text">4.5.4 DataNode的工作机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-5-%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7"><span class="toc-number">2.4.5.5.</span> <span class="toc-text">4.5.5 数据完整性</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97MapReduce"><span class="toc-number">2.5.</span> <span class="toc-text">第五章 分布式计算MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-MapReduce%E6%A6%82%E8%BF%B0"><span class="toc-number">2.5.1.</span> <span class="toc-text">5.1 MapReduce概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-1-MapReduce%E5%AE%9A%E4%B9%89"><span class="toc-number">2.5.1.1.</span> <span class="toc-text">5.1.1 MapReduce定义</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E4%BC%98%E7%82%B9"><span class="toc-number">2.5.1.1.1.</span> <span class="toc-text">1. 优点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E7%BC%BA%E7%82%B9"><span class="toc-number">2.5.1.1.2.</span> <span class="toc-text">2. 缺点</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-2-MapReduce%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">2.5.1.2.</span> <span class="toc-text">5.1.2 MapReduce核心思想</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-MapReduce%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8"><span class="toc-number">2.5.2.</span> <span class="toc-text">5.2 MapReduce编程入门</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-1-%E5%AE%98%E6%96%B9%E7%A4%BA%E4%BE%8B%E7%A8%8B%E5%BA%8FWorldCount%E6%BA%90%E7%A0%81"><span class="toc-number">2.5.2.1.</span> <span class="toc-text">5.2.1 官方示例程序WorldCount源码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A1%A5%E5%85%85%EF%BC%9A%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E7%B1%BB%E5%9E%8B"><span class="toc-number">2.5.2.2.</span> <span class="toc-text">补充：常用数据序列类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-2-%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83"><span class="toc-number">2.5.2.3.</span> <span class="toc-text">5.2.2 编程规范</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-3-WordCount%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">2.5.2.4.</span> <span class="toc-text">5.2.3 WordCount案例实操</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E9%9C%80%E6%B1%82%E8%AF%B4%E6%98%8E"><span class="toc-number">2.5.2.4.1.</span> <span class="toc-text">1. 需求说明</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E5%88%86%E6%9E%90"><span class="toc-number">2.5.2.4.2.</span> <span class="toc-text">2. 分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">2.5.2.4.3.</span> <span class="toc-text">3. 环境准备</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E6%9C%AC%E5%9C%B0%E6%B5%8B%E8%AF%95"><span class="toc-number">2.5.2.4.4.</span> <span class="toc-text">4. 本地测试</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95"><span class="toc-number">2.5.2.4.5.</span> <span class="toc-text">5. 集群测试</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-Hadoop%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">2.5.3.</span> <span class="toc-text">5.3 Hadoop的序列化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-1-%E5%BA%8F%E5%88%97%E5%8C%96%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">2.5.3.1.</span> <span class="toc-text">5.3.1 序列化的概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-2-Writable%E6%8E%A5%E5%8F%A3"><span class="toc-number">2.5.3.2.</span> <span class="toc-text">5.3.2 Writable接口</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-3-%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">2.5.3.3.</span> <span class="toc-text">5.3.3 序列化案例实操</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90"><span class="toc-number">2.5.3.3.1.</span> <span class="toc-text">（1）需求分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90"><span class="toc-number">2.5.3.3.2.</span> <span class="toc-text">（2）实现思路分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E7%BC%96%E5%86%99MapReduce%E7%A8%8B%E5%BA%8F"><span class="toc-number">2.5.3.3.3.</span> <span class="toc-text">（3）编写MapReduce程序</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86%E4%B9%8BInputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5"><span class="toc-number">2.5.4.</span> <span class="toc-text">5.4 MapReduce框架原理之InputFormat数据输入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-1-%E5%88%87%E7%89%87%E4%B8%8EMapTask%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.4.1.</span> <span class="toc-text">5.4.1 切片与MapTask并行度决定机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-2-Job%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E5%92%8CFileInputFormat%E5%88%87%E7%89%87%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3"><span class="toc-number">2.5.4.2.</span> <span class="toc-text">5.4.2 Job提交流程源码和FileInputFormat切片源码详解</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-Job%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3"><span class="toc-number">2.5.4.2.1.</span> <span class="toc-text">1. Job提交流程源码详解</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-FileInputFormat%E5%88%87%E7%89%87%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3"><span class="toc-number">2.5.4.2.2.</span> <span class="toc-text">2. FileInputFormat切片源码详解</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-3-FileInputFormat%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93"><span class="toc-number">2.5.4.3.</span> <span class="toc-text">5.4.3 FileInputFormat切片机制总结</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.4.3.1.</span> <span class="toc-text">1. 切片机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90"><span class="toc-number">2.5.4.3.2.</span> <span class="toc-text">2. 案例分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">2.5.4.3.3.</span> <span class="toc-text">3. 参数设置</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-4-TextInputFormat"><span class="toc-number">2.5.4.4.</span> <span class="toc-text">5.4.4 TextInputFormat</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-FileInputFormat%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%AE%9E%E7%8E%B0%E7%B1%BB"><span class="toc-number">2.5.4.4.1.</span> <span class="toc-text">1.FileInputFormat接口的实现类</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-TextInputFormat"><span class="toc-number">2.5.4.4.2.</span> <span class="toc-text">2. TextInputFormat</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-5-CombineTextInputFormat%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.4.5.</span> <span class="toc-text">5.4.5 CombineTextInputFormat切片机制</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E8%99%9A%E6%8B%9F%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B"><span class="toc-number">2.5.4.5.1.</span> <span class="toc-text">1. 虚拟存储过程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E5%88%87%E7%89%87%E8%BF%87%E7%A8%8B"><span class="toc-number">2.5.4.5.2.</span> <span class="toc-text">2. 切片过程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-6-CombineTextInputFormat%E6%A1%88%E4%BE%8B%E6%95%99%E7%A8%8B"><span class="toc-number">2.5.4.6.</span> <span class="toc-text">5.4.6 CombineTextInputFormat案例教程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86%E4%B9%8Bshuffle%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.5.</span> <span class="toc-text">5.5 MapReduce框架原理之shuffle机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-1-shuffle%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.5.1.</span> <span class="toc-text">5.5.1 shuffle机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-2-%E5%88%86%E5%8C%BA"><span class="toc-number">2.5.5.2.</span> <span class="toc-text">5.5.2 分区</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E9%97%AE%E9%A2%98%E5%BC%95%E5%87%BA"><span class="toc-number">2.5.5.2.1.</span> <span class="toc-text">1. 问题引出</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E9%BB%98%E8%AE%A4%E5%88%86%E5%8C%BA%E5%99%A8HashPartitioner"><span class="toc-number">2.5.5.2.2.</span> <span class="toc-text">2. 默认分区器HashPartitioner</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA%E5%99%A8"><span class="toc-number">2.5.5.2.3.</span> <span class="toc-text">3. 自定义分区器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E5%88%86%E5%8C%BA%E6%80%BB%E7%BB%93"><span class="toc-number">2.5.5.2.4.</span> <span class="toc-text">4. 分区总结</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-3-%E5%88%86%E5%8C%BA%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">2.5.5.3.</span> <span class="toc-text">5.5.3 分区案例实操</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90"><span class="toc-number">2.5.5.3.1.</span> <span class="toc-text">1. 需求分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E9%9C%80%E6%B1%82%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.5.5.3.2.</span> <span class="toc-text">2. 需求实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-4-WritableComparable%E6%8E%92%E5%BA%8F"><span class="toc-number">2.5.5.4.</span> <span class="toc-text">5.5.4 WritableComparable排序</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-5-WritableComparable%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%EF%BC%88%E5%85%A8%E6%8E%92%E5%BA%8F%EF%BC%89"><span class="toc-number">2.5.5.5.</span> <span class="toc-text">5.5.5 WritableComparable排序案例实操（全排序）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90-1"><span class="toc-number">2.5.5.5.1.</span> <span class="toc-text">1. 需求分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90"><span class="toc-number">2.5.5.5.2.</span> <span class="toc-text">2. 实现思路分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.5.5.5.3.</span> <span class="toc-text">3. 代码实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-6-WritableComparable%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%EF%BC%88%E5%8C%BA%E5%86%85%E6%8E%92%E5%BA%8F%EF%BC%89"><span class="toc-number">2.5.5.6.</span> <span class="toc-text">5.5.6 WritableComparable排序案例实操（区内排序）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90-2"><span class="toc-number">2.5.5.6.1.</span> <span class="toc-text">1. 需求分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90-1"><span class="toc-number">2.5.5.6.2.</span> <span class="toc-text">2. 实现思路分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">2.5.5.6.3.</span> <span class="toc-text">3. 案例实操</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-7-Combiner%E5%90%88%E5%B9%B6"><span class="toc-number">2.5.5.7.</span> <span class="toc-text">5.5.7 Combiner合并</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-8-Combiner%E5%90%88%E5%B9%B6%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">2.5.5.8.</span> <span class="toc-text">5.5.8 Combiner合并案例实操</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90-3"><span class="toc-number">2.5.5.8.1.</span> <span class="toc-text">1. 需求分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90-2"><span class="toc-number">2.5.5.8.2.</span> <span class="toc-text">2. 实现思路分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E2%80%94%E2%80%94%E6%96%B9%E6%A1%881"><span class="toc-number">2.5.5.8.3.</span> <span class="toc-text">3. 案例实操——方案1</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E2%80%94%E2%80%94%E6%96%B9%E6%A1%882"><span class="toc-number">2.5.5.8.4.</span> <span class="toc-text">4. 案例实操——方案2</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-6-MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86%E4%B9%8BOutputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA"><span class="toc-number">2.5.6.</span> <span class="toc-text">5.6 MapReduce框架原理之OutputFormat数据输出</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-6-1-OutputFormat%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%AE%9E%E7%8E%B0%E7%B1%BB"><span class="toc-number">2.5.6.1.</span> <span class="toc-text">5.6.1 OutputFormat接口的实现类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-6-2-%E8%87%AA%E5%AE%9A%E4%B9%89OutputFormat%E7%B1%BB%E7%9A%84%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">2.5.6.2.</span> <span class="toc-text">5.6.2 自定义OutputFormat类的案例实操</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90-4"><span class="toc-number">2.5.6.2.1.</span> <span class="toc-text">1. 需求分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">2.5.6.2.2.</span> <span class="toc-text">2. 案例实操</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-7-MapReduce%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%EF%BC%88%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">2.5.7.</span> <span class="toc-text">5.7 MapReduce工作流程（面试重点）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-7-1-MapTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.7.1.</span> <span class="toc-text">5.7.1 MapTask工作机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-7-2-ReduceTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.7.2.</span> <span class="toc-text">5.7.2 ReduceTask工作机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-7-3-ReduceTask%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.7.3.</span> <span class="toc-text">5.7.3 ReduceTask并行度决定机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-7-4-MapTask-amp-ReduceTask%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="toc-number">2.5.7.4.</span> <span class="toc-text">5.7.4 MapTask &amp; ReduceTask源码解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-8-Join"><span class="toc-number">2.5.8.</span> <span class="toc-text">5.8 Join</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-8-1-Reduce-Join"><span class="toc-number">2.5.8.1.</span> <span class="toc-text">5.8.1 Reduce Join</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-8-3-Map-Join"><span class="toc-number">2.5.8.2.</span> <span class="toc-text">5.8.3 Map Join</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-9-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97"><span class="toc-number">2.5.9.</span> <span class="toc-text">5.9 数据清洗</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-10-Hadoop%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.5.10.</span> <span class="toc-text">5.10 Hadoop中的数据压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-10-1-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E6%A6%82%E8%BF%B0"><span class="toc-number">2.5.10.1.</span> <span class="toc-text">5.10.1 数据压缩概述</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E6%94%AF%E6%8C%81%E7%9A%84%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81"><span class="toc-number">2.5.10.1.1.</span> <span class="toc-text">MapReduce支持的压缩编码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83"><span class="toc-number">2.5.10.1.2.</span> <span class="toc-text">压缩性能比较</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">2.5.10.1.3.</span> <span class="toc-text">1. 压缩格式的选择</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E5%8E%8B%E7%BC%A9%E4%BD%8D%E7%BD%AE%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">2.5.10.1.4.</span> <span class="toc-text">2. 压缩位置的选择</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-10-2-%E5%8E%8B%E7%BC%A9%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE"><span class="toc-number">2.5.10.2.</span> <span class="toc-text">5.10.2 压缩参数配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-10-3-%E5%8E%8B%E7%BC%A9%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">2.5.10.3.</span> <span class="toc-text">5.10.3 压缩案例实操</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-Mapper%E8%BE%93%E5%87%BA%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.5.10.3.1.</span> <span class="toc-text">1. Mapper输出采用压缩</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-Reducer%E8%BE%93%E5%87%BA%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9"><span class="toc-number">2.5.10.3.2.</span> <span class="toc-text">2. Reducer输出采用压缩</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-11-MapReduce%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93"><span class="toc-number">2.5.11.</span> <span class="toc-text">5.11 MapReduce开发总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E6%8E%A5%E5%8F%A3%EF%BC%9AInputFormat"><span class="toc-number">2.5.11.1.</span> <span class="toc-text">1. 输入数据接口：InputFormat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%80%BB%E8%BE%91%E5%A4%84%E7%90%86%E6%8E%A5%E5%8F%A3%EF%BC%9AMapper"><span class="toc-number">2.5.11.2.</span> <span class="toc-text">2. 逻辑处理接口：Mapper</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Partitioner-%E5%88%86%E5%8C%BA"><span class="toc-number">2.5.11.3.</span> <span class="toc-text">3. Partitioner 分区</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Comparable-%E6%8E%92%E5%BA%8F"><span class="toc-number">2.5.11.4.</span> <span class="toc-text">4. Comparable 排序</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-Combiner-%E5%90%88%E5%B9%B6"><span class="toc-number">2.5.11.5.</span> <span class="toc-text">5. Combiner 合并</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E9%80%BB%E8%BE%91%E5%A4%84%E7%90%86%E6%8E%A5%E5%8F%A3%EF%BC%9AReducer"><span class="toc-number">2.5.11.6.</span> <span class="toc-text">6. 逻辑处理接口：Reducer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-%E8%BE%93%E5%87%BA%E6%95%B0%E6%8D%AE%E6%8E%A5%E5%8F%A3%EF%BC%9AOutputFormat"><span class="toc-number">2.5.11.7.</span> <span class="toc-text">7. 输出数据接口：OutputFormat</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8YARN"><span class="toc-number">2.6.</span> <span class="toc-text">第六章 资源调度器YARN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-YARN%E6%A6%82%E8%BF%B0"><span class="toc-number">2.6.1.</span> <span class="toc-text">6.1 YARN概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-1-%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84"><span class="toc-number">2.6.1.1.</span> <span class="toc-text">6.1.1 基本架构</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%99%A8ResourceManager"><span class="toc-number">2.6.1.1.1.</span> <span class="toc-text">1. 资源管理器ResourceManager</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E8%8A%82%E7%82%B9%E7%AE%A1%E7%90%86%E5%99%A8NodeManager"><span class="toc-number">2.6.1.1.2.</span> <span class="toc-text">2. 节点管理器NodeManager</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-ApplicationMaster"><span class="toc-number">2.6.1.1.3.</span> <span class="toc-text">3. ApplicationMaster</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E5%AE%B9%E5%99%A8%EF%BC%88Container%EF%BC%89"><span class="toc-number">2.6.1.1.4.</span> <span class="toc-text">4. 容器（Container）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-2-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%EF%BC%88%E9%87%8D%E7%82%B9%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%89"><span class="toc-number">2.6.1.2.</span> <span class="toc-text">6.1.2 工作机制（重点面试题）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-YARN%E7%9A%84%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8%E5%92%8C%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="toc-number">2.6.2.</span> <span class="toc-text">6.2 YARN的资源调度器和调度算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-1-FIFO%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">2.6.2.1.</span> <span class="toc-text">6.2.1 FIFO调度器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-2-%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">2.6.2.2.</span> <span class="toc-text">6.2.2 容量调度器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-3-%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">2.6.2.3.</span> <span class="toc-text">6.2.3 公平调度器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-YARN%E5%AE%9E%E6%93%8D"><span class="toc-number">2.6.3.</span> <span class="toc-text">6.3 YARN实操</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-1-%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%91%BD%E4%BB%A4"><span class="toc-number">2.6.3.1.</span> <span class="toc-text">6.3.1 常用的命令行命令</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E6%9F%A5%E7%9C%8B%E4%BB%BB%E5%8A%A1%E5%91%BD%E4%BB%A4yarn-application"><span class="toc-number">2.6.3.1.1.</span> <span class="toc-text">1. 查看任务命令yarn application</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E6%9F%A5%E7%9C%8B%E6%97%A5%E5%BF%97%E5%91%BD%E4%BB%A4yarn-logs"><span class="toc-number">2.6.3.1.2.</span> <span class="toc-text">2. 查看日志命令yarn logs</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E6%9F%A5%E7%9C%8B%E5%B0%9D%E8%AF%95%E8%BF%90%E8%A1%8C%E7%9A%84%E4%BB%BB%E5%8A%A1%E5%91%BD%E4%BB%A4yarn-applicationattempt"><span class="toc-number">2.6.3.1.3.</span> <span class="toc-text">3. 查看尝试运行的任务命令yarn applicationattempt</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E6%9F%A5%E7%9C%8B%E5%AE%B9%E5%99%A8%E5%91%BD%E4%BB%A4yarn-container"><span class="toc-number">2.6.3.1.4.</span> <span class="toc-text">4. 查看容器命令yarn container</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-%E6%9F%A5%E7%9C%8B%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81%E5%91%BD%E4%BB%A4yarn-node"><span class="toc-number">2.6.3.1.5.</span> <span class="toc-text">5. 查看节点状态命令yarn node</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#6-%E6%9B%B4%E6%96%B0%E9%85%8D%E7%BD%AE%E5%91%BD%E4%BB%A4yarn-rmadmin"><span class="toc-number">2.6.3.1.6.</span> <span class="toc-text">6. 更新配置命令yarn rmadmin</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#7-%E6%9F%A5%E7%9C%8B%E9%98%9F%E5%88%97%E5%91%BD%E4%BB%A4yarn-queue"><span class="toc-number">2.6.3.1.7.</span> <span class="toc-text">7. 查看队列命令yarn queue</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-2-%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89"><span class="toc-number">2.6.3.2.</span> <span class="toc-text">6.3.2 核心参数（重要）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-3-%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E6%A1%88%E4%BE%8B"><span class="toc-number">2.6.3.3.</span> <span class="toc-text">6.3.3 核心参数配置案例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90-5"><span class="toc-number">2.6.3.3.1.</span> <span class="toc-text">1. 需求分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.6.3.3.2.</span> <span class="toc-text">2. 代码实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-4-%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8%E9%85%8D%E7%BD%AE%E6%A1%88%E4%BE%8B"><span class="toc-number">2.6.3.4.</span> <span class="toc-text">6.3.4 容量调度器配置案例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90-6"><span class="toc-number">2.6.3.4.1.</span> <span class="toc-text">1. 需求分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E9%85%8D%E7%BD%AE%E5%A4%9A%E9%98%9F%E5%88%97%E7%9A%84%E5%AE%B9%E5%99%A8%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">2.6.3.4.2.</span> <span class="toc-text">2. 配置多队列的容器调度器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E5%90%91hive%E9%98%9F%E5%88%97%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1"><span class="toc-number">2.6.3.4.3.</span> <span class="toc-text">3. 向hive队列提交任务</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E4%BB%BB%E5%8A%A1%E4%BC%98%E5%85%88%E7%BA%A7"><span class="toc-number">2.6.3.4.4.</span> <span class="toc-text">4. 任务优先级</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-5-%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8%E9%85%8D%E7%BD%AE%E6%A1%88%E4%BE%8B"><span class="toc-number">2.6.3.5.</span> <span class="toc-text">6.3.5 公平调度器配置案例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90-7"><span class="toc-number">2.6.3.5.1.</span> <span class="toc-text">1. 需求分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E9%85%8D%E7%BD%AE%E5%A4%9A%E9%98%9F%E5%88%97%E7%9A%84%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">2.6.3.5.2.</span> <span class="toc-text">2. 配置多队列的公平调度器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E6%B5%8B%E8%AF%95%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1"><span class="toc-number">2.6.3.5.3.</span> <span class="toc-text">3. 测试提交任务</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-6-Tool%E6%8E%A5%E5%8F%A3%E6%A1%88%E4%BE%8B"><span class="toc-number">2.6.3.6.</span> <span class="toc-text">6.3.6 Tool接口案例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-%E9%AB%98%E5%8F%AF%E7%94%A8HA"><span class="toc-number">2.7.</span> <span class="toc-text">第七章 高可用HA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-ZooKeeper%E8%AF%A6%E8%A7%A3"><span class="toc-number">2.7.1.</span> <span class="toc-text">7.1  ZooKeeper详解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-HA%E6%A6%82%E8%BF%B0"><span class="toc-number">2.7.2.</span> <span class="toc-text">7.2  HA概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-1-%E4%BB%80%E4%B9%88%E6%98%AFHA"><span class="toc-number">2.7.2.1.</span> <span class="toc-text">7.2.1 什么是HA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-2-HDFS-HA%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">2.7.2.2.</span> <span class="toc-text">7.2.2 HDFS HA的工作机制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-Hadoop-HA%E9%9B%86%E7%BE%A4%E7%9A%84%E6%90%AD%E5%BB%BA"><span class="toc-number">2.7.3.</span> <span class="toc-text">7.3 Hadoop HA集群的搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-1-HDFS-HA%E6%89%8B%E5%8A%A8%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB"><span class="toc-number">2.7.3.1.</span> <span class="toc-text">7.3.1 HDFS HA手动故障转移</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-2-HDFS-HA%E8%87%AA%E5%8A%A8%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB"><span class="toc-number">2.7.3.2.</span> <span class="toc-text">7.3.2 HDFS HA自动故障转移</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-3-YARN-HA"><span class="toc-number">2.7.3.3.</span> <span class="toc-text">7.3.3 YARN HA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-4-Hadoop-HA%E6%9C%80%E7%BB%88%E8%A7%84%E5%88%92"><span class="toc-number">2.7.3.4.</span> <span class="toc-text">7.3.4 Hadoop HA最终规划</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E5%92%8C%E9%9D%9E%E9%AB%98%E5%8F%AF%E7%94%A8%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%88%87%E6%8D%A2"><span class="toc-number">2.8.</span> <span class="toc-text">高可用和非高可用模式的切换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-%E7%94%9F%E4%BA%A7%E8%B0%83%E4%BC%98%E6%89%8B%E5%86%8C"><span class="toc-number">2.9.</span> <span class="toc-text">第八章 生产调优手册</span></a></li></ol></li></ol>
                    </div>
                </span>
            </div>
        </div>
        
    </div>
</aside>



  
  
  

  

  

  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2023 John Doe
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">数据科学</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">因子投资</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">随笔</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">推荐系统</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">数据仓库</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">常用算法</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">聚宽</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">贝叶斯</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">NLP基础</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">考试</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">java</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Linux</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Java</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">项目</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">面试</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://www.csdn.net/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>CSDN</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.zhihu.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>知乎</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.huaweicloud.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>华为云</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.aliyun.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>阿里云</a>
            </li>
          
            <li class="search-li">
              <a href="https://leetcode.cn/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>力扣</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.joinquant.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>聚宽</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">王宇涵//本科：哈尔滨工程大学//研究生：大连理工大学//专业：计算机技术//方向：量化交易与深度学习//热爱大数据平台开发与数仓开发，会分享一些技术文章和读书笔记</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>