<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://example.com">
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div> 
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/123.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/categories">分类</a></li>
	        
			</ul>
		</nav>
		<nav>
			总文章数 35
		</nav>		
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/Realmakers" title="github"><i class="icon-github"></i></a>
		        
					<a class="qq" target="_blank" href="/3558084726" title="qq"><i class="icon-qq"></i></a>
		        
					<a class="mail" target="_blank" href="mailto: 17745182605@163.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>



    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/img/123.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/Realmakers" title="github"><i class="icon-github"></i></a>
			        
						<a class="qq" target="_blank" href="/3558084726" title="qq"><i class="icon-qq"></i></a>
			        
						<a class="mail" target="_blank" href="mailto: 17745182605@163.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 50%">
				
				
					<li style="width: 50%"><a href="/">主页</a></li>
		        
					<li style="width: 50%"><a href="/categories">分类</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            
  
    <article id="post-python大数据分析与机器学习商业案例实战-part5" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/02/03/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part5/">Python大数据分析与机器学习商业案例实战-part5</a>
    </h1>
  

        
        <a href="/2023/02/03/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part5/" class="archive-article-date">
  	<time datetime="2023-02-03T12:13:14.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-02-03</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="15-关联分析：Apriori算法"><a href="#15-关联分析：Apriori算法" class="headerlink" title="15 关联分析：Apriori算法"></a>15 关联分析：Apriori算法</h1><h2 id="15-1-关联分析的基础概念和Apriori算法"><a href="#15-1-关联分析的基础概念和Apriori算法" class="headerlink" title="15.1 关联分析的基础概念和Apriori算法"></a>15.1 关联分析的基础概念和Apriori算法</h2><p>关联分析是数据挖掘中一种简单而实用的技术，它通过深入分析数据集，寻找事物间的关联性，挖掘频繁出现的组合，并描述组合内对象同时出现的模式和规律，例如，对超时购物的数据进行关联分析，通过发现顾客所购买的不同商品之间的关系，分析顾客的购买习惯，设计商品的组合摆放位置，指定相应的营销策略，从而制造需求，提高销售量，创造额外收入。关联分析技术也可以应用于智能推荐系统，当我们挖掘出频繁出现的组合和强关联规则之后，就可以通过强关联规则为顾客推荐商品。该技术不仅在商品推荐领域应用广泛，在医疗、保险、电信和证券领域也同样大有可为。</p>
<h3 id="15-1-1-关联分析的基本概念"><a href="#15-1-1-关联分析的基本概念" class="headerlink" title="15.1.1 关联分析的基本概念"></a>15.1.1 关联分析的基本概念</h3><p><img src="%E4%B8%8B%E8%BD%BD.png" alt="下载"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(1).png" alt="下载 (1)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(2).png" alt="下载 (2)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(3).png" alt="下载 (3)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(4).png" alt="下载 (4)"></p>
<p>这个结果说明了购买商品A和商品B的人中有67%的人也购买了商品C，该概率还是很高的，因此，商场可以向购买了商品A和商品B的人推荐商品C，如可以将这3种商品放得较近</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(5).png" alt="下载 (5)"></p>
<h3 id="15-1-2-Apriori算法的数学演示"><a href="#15-1-2-Apriori算法的数学演示" class="headerlink" title="15.1.2 Apriori算法的数学演示"></a>15.1.2 Apriori算法的数学演示</h3><p>1.基本思路</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(6).png" alt="下载 (6)"></p>
<p>2.案例演示</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(7).png" alt="下载 (7)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(8).png" alt="下载 (8)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(9).png" alt="下载 (9)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(10).png" alt="下载 (10)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(11).png" alt="下载 (11)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(12).png" alt="下载 (12)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(13).png" alt="下载 (13)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(14).png" alt="下载 (14)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(15).png" alt="下载 (15)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(16).png" alt="下载 (16)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(17).png" alt="下载 (17)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(18).png" alt="下载 (18)"></p>
<h3 id="15-1-3-Apriori算法的代码实现"><a href="#15-1-3-Apriori算法的代码实现" class="headerlink" title="15.1.3 Apriori算法的代码实现"></a>15.1.3 Apriori算法的代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transactions = [[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>], [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>], [<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>], [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>], [<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>]]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> apyori <span class="keyword">import</span> apriori</span><br><span class="line">rules = apriori(transactions, min_support=<span class="number">0.4</span>, min_confidence=<span class="number">0.8</span>)</span><br><span class="line">results = <span class="built_in">list</span>(rules)</span><br></pre></td></tr></table></figure>

<p>第二行代码：min_support参数为最小支持度，这里设置为0.4，min_confidence参数为最小置信度，这里设置为0.8，将获取到的关联规则赋给变量rules，第三行代码用list()函数将获得的关联规则转换为列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">results</span><br></pre></td></tr></table></figure>




<pre><code>[RelationRecord(items=frozenset(&#123;&#39;B&#39;&#125;), support=1.0, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset(&#123;&#39;B&#39;&#125;), confidence=1.0, lift=1.0)]),
 RelationRecord(items=frozenset(&#123;&#39;C&#39;&#125;), support=0.8, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset(&#123;&#39;C&#39;&#125;), confidence=0.8, lift=1.0)]),
 RelationRecord(items=frozenset(&#123;&#39;A&#39;, &#39;B&#39;&#125;), support=0.6, ordered_statistics=[OrderedStatistic(items_base=frozenset(&#123;&#39;A&#39;&#125;), items_add=frozenset(&#123;&#39;B&#39;&#125;), confidence=1.0, lift=1.0)]),
 RelationRecord(items=frozenset(&#123;&#39;B&#39;, &#39;C&#39;&#125;), support=0.8, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset(&#123;&#39;B&#39;, &#39;C&#39;&#125;), confidence=0.8, lift=1.0), OrderedStatistic(items_base=frozenset(&#123;&#39;B&#39;&#125;), items_add=frozenset(&#123;&#39;C&#39;&#125;), confidence=0.8, lift=1.0), OrderedStatistic(items_base=frozenset(&#123;&#39;C&#39;&#125;), items_add=frozenset(&#123;&#39;B&#39;&#125;), confidence=1.0, lift=1.0)]),
 RelationRecord(items=frozenset(&#123;&#39;D&#39;, &#39;B&#39;&#125;), support=0.4, ordered_statistics=[OrderedStatistic(items_base=frozenset(&#123;&#39;D&#39;&#125;), items_add=frozenset(&#123;&#39;B&#39;&#125;), confidence=1.0, lift=1.0)]),
 RelationRecord(items=frozenset(&#123;&#39;D&#39;, &#39;C&#39;&#125;), support=0.4, ordered_statistics=[OrderedStatistic(items_base=frozenset(&#123;&#39;D&#39;&#125;), items_add=frozenset(&#123;&#39;C&#39;&#125;), confidence=1.0, lift=1.25)]),
 RelationRecord(items=frozenset(&#123;&#39;A&#39;, &#39;B&#39;, &#39;C&#39;&#125;), support=0.4, ordered_statistics=[OrderedStatistic(items_base=frozenset(&#123;&#39;A&#39;, &#39;C&#39;&#125;), items_add=frozenset(&#123;&#39;B&#39;&#125;), confidence=1.0, lift=1.0)]),
 RelationRecord(items=frozenset(&#123;&#39;D&#39;, &#39;B&#39;, &#39;C&#39;&#125;), support=0.4, ordered_statistics=[OrderedStatistic(items_base=frozenset(&#123;&#39;D&#39;&#125;), items_add=frozenset(&#123;&#39;B&#39;, &#39;C&#39;&#125;), confidence=1.0, lift=1.25), OrderedStatistic(items_base=frozenset(&#123;&#39;D&#39;, &#39;B&#39;&#125;), items_add=frozenset(&#123;&#39;C&#39;&#125;), confidence=1.0, lift=1.25), OrderedStatistic(items_base=frozenset(&#123;&#39;D&#39;, &#39;C&#39;&#125;), items_add=frozenset(&#123;&#39;B&#39;&#125;), confidence=1.0, lift=1.0)])]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">type</span>(results[<span class="number">0</span>].ordered_statistics)</span><br></pre></td></tr></table></figure>




<pre><code>list
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> results:  <span class="comment"># 遍历results中的每一个频繁项集</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> i.ordered_statistics:  <span class="comment"># 获取频繁项集中的关联规则</span></span><br><span class="line">        X = j.items_base  <span class="comment"># 关联规则的前件</span></span><br><span class="line">        Y = j.items_add  <span class="comment"># 关联规则的后件</span></span><br><span class="line">        x = <span class="string">&#x27;, &#x27;</span>.join([item <span class="keyword">for</span> item <span class="keyword">in</span> X])  <span class="comment"># 连接前件中的元素</span></span><br><span class="line">        y = <span class="string">&#x27;, &#x27;</span>.join([item <span class="keyword">for</span> item <span class="keyword">in</span> Y])  <span class="comment"># 连接后件中的元素</span></span><br><span class="line">        <span class="keyword">if</span> x != <span class="string">&#x27;&#x27;</span>:  <span class="comment"># 防止出现关联规则前件为空的情况</span></span><br><span class="line">            <span class="built_in">print</span>(x + <span class="string">&#x27; → &#x27;</span> + y)  <span class="comment"># 通过字符串拼接的方式更好呈现结果</span></span><br></pre></td></tr></table></figure>

<pre><code>A → B
B → C
C → B
D → B
D → C
A, C → B
D → B, C
D, B → C
D, C → B
</code></pre>
<p>可以看到apyori库获取了所有强关联规则</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过在内容后面加?可以查看官方介绍</span></span><br><span class="line"><span class="comment"># apriori?</span></span><br></pre></td></tr></table></figure>

<h2 id="15-2-案例实战：病症关联分析"><a href="#15-2-案例实战：病症关联分析" class="headerlink" title="15.2 案例实战：病症关联分析"></a>15.2 案例实战：病症关联分析</h2><h3 id="案例背景"><a href="#案例背景" class="headerlink" title="案例背景"></a>案例背景</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(19).png" alt="下载 (19)"></p>
<h3 id="15-2-2-数据读取与处理"><a href="#15-2-2-数据读取与处理" class="headerlink" title="15.2.2 数据读取与处理"></a>15.2.2 数据读取与处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;中医辨证.xlsx&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>病人编号</th>
      <th>病人症状</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>消化不良,便秘</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>心悸,失眠</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>腰疼,脱发,眼干</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>腹胀,便秘,哮喘,胸闷气短,消化不良</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>神经衰弱,失眠,月经不调</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简单演示下tolist()函数</span></span><br><span class="line">df[<span class="string">&#x27;病人症状&#x27;</span>].tolist()</span><br></pre></td></tr></table></figure>




<pre><code>[&#39;消化不良,便秘&#39;,
 &#39;心悸,失眠&#39;,
 &#39;腰疼,脱发,眼干&#39;,
 &#39;腹胀,便秘,哮喘,胸闷气短,消化不良&#39;,
 &#39;神经衰弱,失眠,月经不调&#39;,
 &#39;神经衰弱,消化不良,月经不调&#39;,
 &#39;失眠,眼干,月经不调&#39;,
 &#39;腹胀,便秘,哮喘,胸闷气短,消化不良&#39;,
 &#39;腰疼,脱发,眼干,心悸&#39;,
 &#39;神经衰弱,消化不良,月经不调&#39;,
 &#39;腰疼,眼干,月经不调&#39;,
 &#39;心悸,腹胀,便秘,消化不良&#39;,
 &#39;心悸,月经不调,消化不良&#39;,
 &#39;心悸,失眠,月经不调&#39;,
 &#39;心悸,神经衰弱,消化不良,便秘&#39;,
 &#39;失眠,月经不调,胸闷气短&#39;,
 &#39;心悸,失眠,脱发,眼干,月经不调&#39;,
 &#39;哮喘,胸闷气短&#39;,
 &#39;心悸,月经不调,消化不良&#39;,
 &#39;消化不良,月经不调&#39;,
 &#39;腹胀,便秘,消化不良&#39;,
 &#39;失眠,月经不调&#39;,
 &#39;腰疼,脱发,眼干,易怒&#39;,
 &#39;失眠,月经不调&#39;,
 &#39;哮喘,腰疼&#39;,
 &#39;心悸,腹胀&#39;,
 &#39;失眠,眼干,月经不调&#39;,
 &#39;失眠,眼干,月经不调&#39;,
 &#39;消化不良,便秘&#39;,
 &#39;失眠,月经不调&#39;,
 &#39;消化不良,便秘&#39;,
 &#39;心悸,神经衰弱,消化不良,便秘&#39;,
 &#39;哮喘,鼻炎,脱发&#39;,
 &#39;心悸,腹胀&#39;,
 &#39;心悸,失眠,月经不调&#39;,
 &#39;腰疼,眼干,便秘&#39;,
 &#39;心悸,失眠&#39;,
 &#39;心悸,神经衰弱,易怒,消化不良&#39;,
 &#39;神经衰弱,消化不良&#39;,
 &#39;心悸,失眠,月经不调&#39;,
 &#39;哮喘,胸闷气短&#39;,
 &#39;心悸,失眠,眼干,月经不调,消化不良&#39;,
 ……………………………………
 ……………………………………]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 转换为双重列表结构</span></span><br><span class="line">symptoms = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df[<span class="string">&#x27;病人症状&#x27;</span>].tolist():</span><br><span class="line">    symptoms.append(i.split(<span class="string">&#x27;,&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(symptoms)</span><br></pre></td></tr></table></figure>

<pre><code>[[&#39;消化不良&#39;, &#39;便秘&#39;], [&#39;心悸&#39;, &#39;失眠&#39;], [&#39;腰疼&#39;, &#39;脱发&#39;, &#39;眼干&#39;], [&#39;腹胀&#39;, &#39;便秘&#39;, &#39;哮喘&#39;, &#39;胸闷气短&#39;, &#39;消化不良&#39;], [&#39;神经衰弱&#39;, &#39;失眠&#39;, &#39;月经不调&#39;], [&#39;神经衰弱&#39;, &#39;消化不良&#39;, &#39;月经不调&#39;], [&#39;失眠&#39;, &#39;眼干&#39;, &#39;月经不调&#39;], [&#39;腹胀&#39;, &#39;便秘&#39;, &#39;哮喘&#39;, &#39;胸闷气短&#39;, &#39;消化不良&#39;], [&#39;腰疼&#39;, &#39;脱发&#39;, &#39;眼干&#39;, &#39;心悸&#39;], [&#39;神经衰弱&#39;, &#39;消化不良&#39;, &#39;月经不调&#39;], [&#39;腰疼&#39;, &#39;眼干&#39;, &#39;月经不调&#39;], [&#39;心悸&#39;, &#39;腹胀&#39;, &#39;便秘&#39;, &#39;消化不良&#39;], [&#39;心悸&#39;, &#39;月经不调&#39;, &#39;消化不良&#39;], [&#39;心悸&#39;, &#39;失眠&#39;, &#39;月经不调&#39;], [&#39;心悸&#39;, &#39;神经衰弱&#39;, &#39;消化不良&#39;, &#39;便秘&#39;], [&#39;失眠&#39;, &#39;月经不调&#39;, &#39;胸闷气短&#39;], [&#39;心悸&#39;, &#39;失眠&#39;, &#39;脱发&#39;, &#39;眼干&#39;, &#39;月经不调&#39;], [&#39;哮喘&#39;, &#39;胸闷气短&#39;], [&#39;心悸&#39;, &#39;月经不调&#39;, &#39;消化不良&#39;], [&#39;消化不良&#39;, &#39;月经不调&#39;], [&#39;腹胀&#39;, &#39;便秘&#39;, &#39;消化不良&#39;], [&#39;失眠&#39;, &#39;月经不调&#39;], [&#39;腰疼&#39;, &#39;脱发&#39;, &#39;眼干&#39;, &#39;易怒&#39;], [&#39;失眠&#39;, &#39;月经不调&#39;], [&#39;哮喘&#39;, &#39;腰疼&#39;], [&#39;心悸&#39;, &#39;腹胀&#39;], [&#39;失眠&#39;, &#39;眼干&#39;, &#39;月经不调&#39;], [&#39;失眠&#39;, &#39;眼干&#39;, &#39;月经不调&#39;], [&#39;消化不良&#39;, &#39;便秘&#39;], [&#39;失眠&#39;, &#39;月经不调&#39;], [&#39;消化不良&#39;, &#39;便秘&#39;], [&#39;心悸&#39;, &#39;神经衰弱&#39;, &#39;消化不良&#39;, &#39;便秘&#39;], [&#39;哮喘&#39;, &#39;鼻炎&#39;, &#39;脱发&#39;], [&#39;心悸&#39;, &#39;腹胀&#39;], [&#39;心悸&#39;, &#39;失眠&#39;, &#39;月经不调&#39;], [&#39;腰疼&#39;, &#39;眼干&#39;, &#39;便秘&#39;], [&#39;心悸&#39;, &#39;失眠&#39;], [&#39;心悸&#39;, &#39;神经衰弱&#39;, &#39;易怒&#39;, &#39;消化不良&#39;], [&#39;神经衰弱&#39;, &#39;消化不良&#39;], [&#39;心悸&#39;, &#39;失眠&#39;, &#39;月经不调&#39;],………………………………]]
</code></pre>
<p>第一行代码创建一个空列表symptoms（病症），用来存储之后提取的每一个患者的病症数据；第二行代码遍历每个患者的“患者病症”列，并用tolist()函数将该列的内容转换为一个列表，列表中的每个元素就是每个患者的所有病症，如第一个元素为“消化不良，便秘”，两个病症通过逗号连接；第三行代码先用split()函数按逗号对列表元素进行分割，将患者的一个个病症分割开来，并存储在一个个子列表中，再用append()函数将所有患者的病症子列表汇总到symptoms列表中</p>
<h3 id="15-2-3-关联规则分析"><a href="#15-2-3-关联规则分析" class="headerlink" title="15.2.3 关联规则分析"></a>15.2.3 关联规则分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> apyori <span class="keyword">import</span> apriori</span><br><span class="line">rules = apriori(symptoms, min_support=<span class="number">0.1</span>, min_confidence=<span class="number">0.7</span>)</span><br><span class="line">results = <span class="built_in">list</span>(rules)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> results:  <span class="comment"># 遍历results中的每一个频繁项集</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> i.ordered_statistics:  <span class="comment"># 获取频繁项集中的关联规则</span></span><br><span class="line">        X = j.items_base  <span class="comment"># 关联规则的前件</span></span><br><span class="line">        Y = j.items_add  <span class="comment"># 关联规则的后件</span></span><br><span class="line">        x = <span class="string">&#x27;, &#x27;</span>.join([item <span class="keyword">for</span> item <span class="keyword">in</span> X])  <span class="comment"># 连接前件中的元素</span></span><br><span class="line">        y = <span class="string">&#x27;, &#x27;</span>.join([item <span class="keyword">for</span> item <span class="keyword">in</span> Y])  <span class="comment"># 连接后件中的元素</span></span><br><span class="line">        <span class="keyword">if</span> x != <span class="string">&#x27;&#x27;</span>:  <span class="comment"># 防止出现关联规则前件为空的情况</span></span><br><span class="line">            <span class="built_in">print</span>(x + <span class="string">&#x27; → &#x27;</span> + y)  <span class="comment"># 通过字符串拼接的方式更好呈现结果</span></span><br></pre></td></tr></table></figure>

<pre><code>便秘 → 消化不良
失眠 → 月经不调
神经衰弱 → 消化不良
脱发 → 眼干
腰疼 → 眼干
心悸, 失眠 → 月经不调
心悸, 神经衰弱 → 消化不良
</code></pre>
<p>可以看到，在获得的关联规则中，的确有之前提到的同一脏器导致的病症关联规则，如便秘和消化不良（脾的关联病症）的关联规则，并且还有不同脏器之间相互影响导致的病症关联规则，如脱发（肾的关联病症）和眼干（肝的关联病症）的关联规则，其余的关联规则也说明了不同病症之间存在一些关联性</p>
<hr>
<p><strong>金融产品交叉销售案例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;金融产品购买数据.xlsx&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>用户编号</th>
      <th>购买产品</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>华小智2号产品,华小智4号产品,华小智5号产品,华小智6号产品</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>华大智1号产品,华大智2号产品,华大智5号产品,华大智6号产品</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>华小智9号产品,华小智10号产品,华小智12号产品</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>华大智1号产品,华大智5号产品</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>华大智5号产品,华大智6号产品</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简单演示下tolist()函数</span></span><br><span class="line">df[<span class="string">&#x27;购买产品&#x27;</span>].tolist()</span><br></pre></td></tr></table></figure>




<pre><code>[&#39;华小智2号产品,华小智4号产品,华小智5号产品,华小智6号产品&#39;,
 &#39;华大智1号产品,华大智2号产品,华大智5号产品,华大智6号产品&#39;,
 &#39;华小智9号产品,华小智10号产品,华小智12号产品&#39;,
 &#39;华大智1号产品,华大智5号产品&#39;,
 &#39;华大智5号产品,华大智6号产品&#39;,
 &#39;华中智2号产品&#39;,
 &#39;华大智2号产品,华大智3号产品,华大智6号产品&#39;,
 &#39;华小智7号产品,华小智8号产品,华小智11号产品&#39;,
 &#39;华大智1号产品,华大智3号产品,华大智4号产品,华大智5号产品&#39;,
 &#39;华中智2号产品,华中智5号产品&#39;,
 &#39;华大智3号产品,华大智4号产品,华大智5号产品,华大智6号产品&#39;,
 …………………………
 …………………………]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 转换为双重列表结构</span></span><br><span class="line">products = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df[<span class="string">&#x27;购买产品&#x27;</span>].tolist():</span><br><span class="line">    products.append(i.split(<span class="string">&#x27;,&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(products)</span><br></pre></td></tr></table></figure>

<pre><code>[[&#39;华小智2号产品&#39;, &#39;华小智4号产品&#39;, &#39;华小智5号产品&#39;, &#39;华小智6号产品&#39;], [&#39;华大智1号产品&#39;, &#39;华大智2号产品&#39;, &#39;华大智5号产品&#39;, &#39;华大智6号产品&#39;], [&#39;华小智9号产品&#39;, &#39;华小智10号产品&#39;, &#39;华小智12号产品&#39;], [&#39;华大智1号产品&#39;, &#39;华大智5号产品&#39;], [&#39;华大智5号产品&#39;, &#39;华大智6号产品&#39;], [&#39;华中智2号产品&#39;], [&#39;华大智2号产品&#39;, &#39;华大智3号产品&#39;, &#39;华大智6号产品&#39;], [&#39;华小智7号产品&#39;, &#39;华小智8号产品&#39;, &#39;华小智11号产品&#39;], [&#39;华大智1号产品&#39;, &#39;华大智3号产品&#39;, &#39;华大智4号产品&#39;, &#39;华大智5号产品&#39;], [&#39;华中智2号产品&#39;, &#39;华中智5号产品&#39;], [&#39;华大智3号产品&#39;, &#39;华大智4号产品&#39;, &#39;华大智5号产品&#39;, &#39;华大智6号产品&#39;], [&#39;华小智13号产品&#39;, &#39;华小智17号产品&#39;], [&#39;华中智3号产品&#39;, &#39;华中智4号产品&#39;, &#39;华中智5号产品&#39;], [&#39;华大智2号产品&#39;, &#39;华大智4号产品&#39;, &#39;华大智5号产品&#39;], [&#39;华小智2号产品&#39;, &#39;华小智3号产品&#39;, &#39;华小智6号产品&#39;], [&#39;华小智1号产品&#39;, &#39;华小智3号产品&#39;,……………………………………]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> apyori <span class="keyword">import</span> apriori</span><br><span class="line">rules = apriori(products, min_support=<span class="number">0.01</span>, min_confidence=<span class="number">0.5</span>)</span><br><span class="line">results = <span class="built_in">list</span>(rules)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> results:  <span class="comment"># 遍历results中的每一个频繁项集</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> i.ordered_statistics:  <span class="comment"># 获取频繁项集中的关联规则</span></span><br><span class="line">        X = j.items_base  <span class="comment"># 关联规则的前件</span></span><br><span class="line">        Y = j.items_add  <span class="comment"># 关联规则的后件</span></span><br><span class="line">        x = <span class="string">&#x27;, &#x27;</span>.join([item <span class="keyword">for</span> item <span class="keyword">in</span> X])  <span class="comment"># 连接前件中的元素</span></span><br><span class="line">        y = <span class="string">&#x27;, &#x27;</span>.join([item <span class="keyword">for</span> item <span class="keyword">in</span> Y])  <span class="comment"># 连接后件中的元素</span></span><br><span class="line">        <span class="keyword">if</span> x != <span class="string">&#x27;&#x27;</span>:  <span class="comment"># 防止出现关联规则前件为空的情况</span></span><br><span class="line">            <span class="built_in">print</span>(x + <span class="string">&#x27; → &#x27;</span> + y)  <span class="comment"># 通过字符串拼接的方式更好呈现结果</span></span><br></pre></td></tr></table></figure>

<pre><code>华中智2号产品 → 华中智1号产品
华中智1号产品 → 华中智3号产品
华中智3号产品 → 华中智1号产品
华中智4号产品 → 华中智1号产品
华中智1号产品 → 华中智6号产品
华中智6号产品 → 华中智1号产品
华中智2号产品 → 华中智3号产品
华中智2号产品 → 华中智6号产品
华中智4号产品 → 华中智3号产品
华中智5号产品 → 华中智3号产品
华中智3号产品 → 华中智6号产品
华中智6号产品 → 华中智3号产品
华中智4号产品 → 华中智5号产品
华中智4号产品 → 华中智6号产品
华大智1号产品 → 华大智2号产品
华大智1号产品 → 华大智3号产品
华大智1号产品 → 华大智4号产品
华大智4号产品 → 华大智1号产品
华大智5号产品 → 华大智1号产品
华大智1号产品 → 华大智6号产品
华大智6号产品 → 华大智1号产品
华大智4号产品 → 华大智2号产品
华大智2号产品 → 华大智6号产品
华大智4号产品 → 华大智3号产品
华大智5号产品 → 华大智3号产品
华大智3号产品 → 华大智6号产品
华大智6号产品 → 华大智3号产品
华大智4号产品 → 华大智5号产品
华大智5号产品 → 华大智4号产品
华大智4号产品 → 华大智6号产品
华大智5号产品 → 华大智6号产品
华小智10号产品 → 华小智11号产品
………………………………
………………………………
</code></pre>
<h1 id="16-深度学习之神经网络模型"><a href="#16-深度学习之神经网络模型" class="headerlink" title="16 深度学习之神经网络模型"></a>16 深度学习之神经网络模型</h1><h2 id="16-1-深度学习基础：神经网络模型"><a href="#16-1-深度学习基础：神经网络模型" class="headerlink" title="16.1 深度学习基础：神经网络模型"></a>16.1 深度学习基础：神经网络模型</h2><h3 id="16-1-1-神经网络模型的基本原理"><a href="#16-1-1-神经网络模型的基本原理" class="headerlink" title="16.1.1 神经网络模型的基本原理"></a>16.1.1 神经网络模型的基本原理</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(20).png" alt="下载 (20)"></p>
<p>1.单层神经网路模型</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(21).png" alt="下载 (21)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(22).png" alt="下载 (22)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(23).png" alt="下载 (23)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(24).png" alt="下载 (24)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(25).png" alt="下载 (25)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(26).png" alt="下载 (26)"></p>
<p>2.多层神经网络模型</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(27).png" alt="下载 (27)"></p>
<h3 id="16-1-2-神经网络模型的简单代码实现"><a href="#16-1-2-神经网络模型的简单代码实现" class="headerlink" title="16.1.2 神经网络模型的简单代码实现"></a>16.1.2 神经网络模型的简单代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">5</span>, <span class="number">1</span>], [<span class="number">6</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">2</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier  <span class="comment"># 引入MLP多层神经网络模型</span></span><br><span class="line">mlp =MLPClassifier()</span><br><span class="line">mlp.fit(X, y)</span><br></pre></td></tr></table></figure>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = mlp.predict(X)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p><strong>补充知识点 - 神经网络回归模型：MLPRegressor</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPRegressor</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line">y = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">model = MLPRegressor(random_state=<span class="number">123</span>)  <span class="comment"># 设置random_state随机状态参数，使得每次训练的模型都是一样的</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[2.85598566]
</code></pre>
<h2 id="16-2-案例实战：用户评论情感分析模型"><a href="#16-2-案例实战：用户评论情感分析模型" class="headerlink" title="16.2 案例实战：用户评论情感分析模型"></a>16.2 案例实战：用户评论情感分析模型</h2><h3 id="16-2-1-案例背景"><a href="#16-2-1-案例背景" class="headerlink" title="16.2.1 案例背景"></a>16.2.1 案例背景</h3><h3 id="16-2-2-数据读取、中文分词、文本向量化"><a href="#16-2-2-数据读取、中文分词、文本向量化" class="headerlink" title="16.2.2 数据读取、中文分词、文本向量化"></a>16.2.2 数据读取、中文分词、文本向量化</h3><p>1.读取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;产品评价.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>客户编号</th>
      <th>评论</th>
      <th>评价</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>是iPhone8 XR正品，按键屏幕反应蛮快的很灵活，屏幕6.0的不算很大，刚刚好，这款面容...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>外形外观：外光非常漂亮，黑色的非常大气。适合男士拥有。屏幕音效：刚开机就下载了一个QQ音乐试...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>从苹果4s，到6s，再到xr，就是喜欢苹果的手感和风格，视频流畅，图片清晰，纠结了好久买哪个...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>主要是手感，太沉了，比苹果6，沉一倍，厚太多了，看中双卡双待机，刚买回来用，待机时间还不错，...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>外形外观：红色超级好看，送妈妈的。屏幕音效：音效还可以，也什么特别的，屏幕看着也挺舒服。拍照...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>2.中文分词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过第2章讲的iloc获取数据表DataFrame第一行信息，0表示第一行</span></span><br><span class="line">df.iloc[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>




<pre><code>客户编号                                                    1
评论      是iPhone8 XR正品，按键屏幕反应蛮快的很灵活，屏幕6.0的不算很大，刚刚好，这款面容...
评价                                                      1
Name: 0, dtype: object
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了循序渐进，这里先演示第一条评论的分词效果</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">word = jieba.cut(df.iloc[<span class="number">0</span>][<span class="string">&#x27;评论&#x27;</span>]) <span class="comment"># 读取第一条评论</span></span><br><span class="line">result = <span class="string">&#x27; &#x27;</span>.join(word)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<pre><code>Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\王宇涵\AppData\Local\Temp\jieba.cache
Loading model cost 0.445 seconds.
Prefix dict has been built successfully.


是 iPhone8   XR 正品 ， 按键 屏幕 反应 蛮快 的 很 灵活 ， 屏幕 6.0 的 不算 很大 ， 刚刚 好 ， 这 款 面容 识别 开锁 比 指纹 方便 多 了 ， 内外 的 整体 看起来 很 美观 ， 整机 子 不算 是 很厚感 ， 像素 高 比较 清晰 ， 双卡 双待 ， 续航 强 ， 跟 8plus 差价 300 元 ， 还是 选 XR 款好 ， 性能 不错 ， 处理器 、 芯片 也 是 最新 一代
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 遍历整张表格，对所有评论进行分词</span></span><br><span class="line">words = []</span><br><span class="line"><span class="keyword">for</span> i, row <span class="keyword">in</span> df.iterrows(): <span class="comment"># df.iterrows()是pandas库遍历表格每一行的方法，i是每一行的行号，row是每一行的内容</span></span><br><span class="line">    word = jieba.cut(row[<span class="string">&#x27;评论&#x27;</span>]) <span class="comment"># cut()分词函数，row代表每一行的数据，row[&#x27;评论&#x27;]就代表该行中“评论”列的内容</span></span><br><span class="line">    result = <span class="string">&#x27; &#x27;</span>.join(word) </span><br><span class="line">    words.append(result) <span class="comment"># 将每一条评论的分词结果添加到words列表中</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words[<span class="number">0</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>




<pre><code>[&#39;是 iPhone8   XR 正品 ， 按键 屏幕 反应 蛮快 的 很 灵活 ， 屏幕 6.0 的 不算 很大 ， 刚刚 好 ， 这 款 面容 识别 开锁 比 指纹 方便 多 了 ， 内外 的 整体 看起来 很 美观 ， 整机 子 不算 是 很厚感 ， 像素 高 比较 清晰 ， 双卡 双待 ， 续航 强 ， 跟 8plus 差价 300 元 ， 还是 选 XR 款好 ， 性能 不错 ， 处理器 、 芯片 也 是 最新 一代&#39;,
 &#39;外形 外观 ： 外光 非常 漂亮 ， 黑色 的 非常 大气 。 适合 男士 拥有 。 屏幕 音效 ： 刚 开机 就 下载 了 一个 QQ 音乐 试 了 一下 。   音效 还是 非常 不错 的 。 拍照 效果 ： 拍照 很 清晰 ， 照亮 你 脸上 的 痘痘 。 运行 速度 ： 运行 速度 就 不用说 了 。   一个 字快 。 待机时间 ： 待机 很 不错 。 用 一段时间 再 来 评价 。 其他 特色 ： 个人感觉 比 Ｘ 好 。   可能 是因为 上手 的 手感 比较 好 吧 ， 总之 还是 值得 入手 的&#39;,
 &#39;从 苹果 4s ， 到 6s ， 再 到 xr ， 就是 喜欢 苹果 的 手感 和 风格 ， 视频 流畅 ， 图片 清晰 ， 纠结 了 好久 买 哪个 颜色 ， 白色 干净 ， 同事 买 的 黄色 ， 感觉 也 很 好看 ， 蓝色 ， 珊瑚 我 也 喜欢 ， 最终 还是 选择 比较 适合 女生 的 珊瑚 色 ， 实物 比 图片 更 漂亮 ， 超级 喜欢 ， 运行 速度 快 ， 全屏 显示 ， 体积小 了 ， 可 显示 区域 变得 了 ， 很棒 。&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果对上面过程如果熟悉后，也可以直接写成如下的合并代码形式</span></span><br><span class="line">words = []</span><br><span class="line"><span class="keyword">for</span> i, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    words.append(<span class="string">&#x27; &#x27;</span>.join(jieba.cut(row[<span class="string">&#x27;评论&#x27;</span>])))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # iterrows()函数相关知识点，不熟悉DataFrame数据表遍历的话，可以把下面的注释取消了，看看效果</span></span><br><span class="line"><span class="comment"># for i, row in df.iterrows():</span></span><br><span class="line"><span class="comment">#     print(i)</span></span><br><span class="line"><span class="comment">#     print(row)</span></span><br></pre></td></tr></table></figure>

<p>3.构造特征变量和目标变量</p>
<p>（1）特征变量提取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文本向量化CountVectorizer()函数的使用技巧：使用示例</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">test = [<span class="string">&#x27;手机 外观 漂亮&#x27;</span>, <span class="string">&#x27;手机 图片 清晰&#x27;</span>]</span><br><span class="line">vect = CountVectorizer()</span><br><span class="line">X = vect.fit_transform(test)</span><br><span class="line">X = X.toarray()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">words_bag = vect.vocabulary_</span><br><span class="line"><span class="built_in">print</span>(words_bag)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;手机&#39;: 2, &#39;外观&#39;: 1, &#39;漂亮&#39;: 4, &#39;图片&#39;: 0, &#39;清晰&#39;: 3&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实际应用</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">vect = CountVectorizer()</span><br><span class="line">X = vect.fit_transform(words)</span><br><span class="line">X = X.toarray()</span><br><span class="line"><span class="built_in">print</span>(X)</span><br></pre></td></tr></table></figure>

<pre><code>[[0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">words_bag = vect.vocabulary_</span><br><span class="line"><span class="built_in">print</span>(words_bag)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;iphone8&#39;: 194, &#39;xr&#39;: 264, &#39;正品&#39;: 2660, &#39;按键&#39;: 2221, &#39;屏幕&#39;: 1798, &#39;反应&#39;: 1210, &#39;蛮快&#39;: 3492, &#39;灵活&#39;: 2843, &#39;不算&#39;: 517, &#39;很大&#39;: 1967, &#39;刚刚&#39;: 1031, &#39;面容&#39;: 3979, &#39;识别&#39;: 3570, &#39;开锁&#39;: 1915, &#39;指纹&#39;: 2218, &#39;方便&#39;: 2362, &#39;内外&#39;: 941, &#39;整体&#39;: 2341, &#39;看起来&#39;: 3101, &#39;美观&#39;: 3345, &#39;整机&#39;: 2344, &#39;很厚感&#39;: 1959, &#39;像素&#39;: 862, &#39;比较&#39;: 2704, &#39;清晰&#39;: 2808, &#39;双卡&#39;: 1201, &#39;双待&#39;: 1203, &#39;续航&#39;: 3301, &#39;8plus&#39;: 143, &#39;差价&#39;: 1823, &#39;300&#39;: 50, &#39;还是&#39;: 3758, &#39;款好&#39;: 2655, &#39;性能&#39;: 2040, &#39;不错&#39;: 538, &#39;处理器&#39;: 1460, &#39;芯片&#39;: 3455, &#39;最新&#39;: 2506, &#39;一代&#39;: 290, &#39;外形&#39;: 1471, &#39;外观&#39;: 1473, &#39;外光&#39;: 1468, &#39;非常&#39;: 3972, &#39;漂亮&#39;: 2832, &#39;黑色&#39;: 4068, &#39;大气&#39;: 1521, &#39;适合&#39;: 3827, &#39;男士&#39;: 2997, &#39;拥有&#39;: 2207, &#39;音效&#39;: 3985, &#39;开机&#39;: 1910, &#39;下载&#39;: 445, &#39;一个&#39;: 280, &#39;qq&#39;: 234, &#39;音乐&#39;: 3983, &#39;一下&#39;: 276, &#39;拍照&#39;: 2203, &#39;效果&#39;: 2330, &#39;照亮&#39;: 2863, &#39;脸上&#39;: 3409, &#39;痘痘&#39;: 3019, &#39;运行&#39;: 3744, &#39;速度&#39;: 3854, &#39;不用说&#39;: 514, &#39;字快&#39;: 1666, &#39;待机时间&#39;: 1951, &#39;待机&#39;: 1950, &#39;一段时间&#39;: 354, &#39;评价&#39;: 3566, &#39;其他&#39;: 928, &#39;特色&#39;: 2908, &#39;个人感觉&#39;: 583, &#39;可能&#39;: 1271, &#39;是因为&#39;: 2449, &#39;上手&#39;: 420,…………………………&#125;
</code></pre>
<p>可以看到，所得到的词袋就是一个字典，其内容是对评论的分词结果进行去重，再对不同的词进行编号</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(words_bag) <span class="comment"># 查看词袋中一共有多少个词，产生了多少特征变量</span></span><br></pre></td></tr></table></figure>




<pre><code>4075
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)  <span class="comment"># 添加这行代码可以显示所有列，如果讲None改成500，则表示可最多显示500列</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_rows&#x27;</span>, <span class="literal">None</span>)  <span class="comment"># 添加这行代码可以显示所有行，如果讲None改成500，则表示可最多显示500行</span></span><br><span class="line">pd.DataFrame(X).head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
      <th>26</th>
      <th>27</th>
      <th>28</th>
      <th>29</th>
      <th>30</th>
      <th>31</th>
      <th>32</th>
      <th>33</th>
      <th>34</th>
      <th>35</th>
      <th>36</th>
      <th>37</th>
      <th>38</th>
      <th>39</th>
      <th>40</th>
      <th>41</th>
      <th>42</th>
      <th>43</th>
      <th>44</th>
      <th>45</th>
      <th>46</th>
      <th>47</th>
      <th>48</th>
      <th>49</th>
      <th>50</th>
      <th>51</th>
      <th>52</th>
      <th>53</th>
      <th>54</th>
      <th>55</th>
      <th>56</th>
      <th>57</th>
      <th>58</th>
      <th>59</th>
      <th>60</th>
      <th>61</th>
      <th>62</th>
      <th>63</th>
      <th>64</th>
      <th>65</th>
      <th>66</th>
      <th>67</th>
      <th>68</th>
      <th>69</th>
      <th>70</th>
      <th>71</th>
      <th>72</th>
      <th>73</th>
      <th>74</th>
      <th>75</th>
      <th>76</th>
      <th>77</th>
      <th>78</th>
      <th>79</th>
      <th>80</th>
      <th>81</th>
      <th>82</th>
      <th>83</th>
      <th>84</th>
      <th>85</th>
      <th>86</th>
      <th>87</th>
      <th>88</th>
      <th>89</th>
      <th>90</th>
      <th>91</th>
      <th>92</th>
      <th>93</th>
      <th>94</th>
      <th>95</th>
      <th>96</th>
      <th>97</th>
      <th>98</th>
      <th>99</th>
      <th>100</th>
      <th>101</th>
      <th>102</th>
      <th>103</th>
      <th>104</th>
      <th>105</th>
      <th>106</th>
      <th>107</th>
      <th>108</th>
      <th>109</th>
      <th>110</th>
      <th>111</th>
      <th>112</th>
      <th>113</th>
      <th>114</th>
      <th>115</th>
      <th>116</th>
      <th>117</th>
      <th>118</th>
      <th>119</th>
      <th>120</th>
      <th>121</th>
      <th>122</th>
      <th>123</th>
      <th>124</th>
      <th>125</th>
      <th>126</th>
      <th>127</th>
      <th>128</th>
      <th>129</th>
      <th>130</th>
      <th>131</th>
      <th>132</th>
      <th>133</th>
      <th>134</th>
      <th>135</th>
      <th>136</th>
      <th>137</th>
      <th>138</th>
      <th>139</th>
      <th>140</th>
      <th>141</th>
      <th>142</th>
      <th>143</th>
      <th>144</th>
      <th>145</th>
      <th>146</th>
      <th>147</th>
      <th>148</th>
      <th>149</th>
      <th>150</th>
      <th>151</th>
      <th>152</th>
      <th>153</th>
      <th>154</th>
      <th>155</th>
      <th>156</th>
      <th>157</th>
      <th>158</th>
      <th>159</th>
      <th>160</th>
      <th>161</th>
      <th>162</th>
      <th>163</th>
      <th>164</th>
      <th>165</th>
      <th>166</th>
      <th>167</th>
      <th>168</th>
      <th>169</th>
      <th>170</th>
      <th>171</th>
      <th>172</th>
      <th>173</th>
      <th>174</th>
      <th>175</th>
      <th>176</th>
      <th>177</th>
      <th>178</th>
      <th>179</th>
      <th>180</th>
      <th>181</th>
      <th>182</th>
      <th>183</th>
      <th>184</th>
      <th>185</th>
      <th>186</th>
      <th>187</th>
      <th>188</th>
      <th>189</th>
      <th>190</th>
      <th>191</th>
      <th>192</th>
      <th>193</th>
      <th>194</th>
      <th>195</th>
      <th>196</th>
      <th>197</th>
      <th>198</th>
      <th>199</th>
      <th>200</th>
      <th>201</th>
      <th>202</th>
      <th>203</th>
      <th>204</th>
      <th>205</th>
      <th>206</th>
      <th>207</th>
      <th>208</th>
      <th>209</th>
      <th>210</th>
      <th>211</th>
      <th>212</th>
      <th>213</th>
      <th>214</th>
      <th>215</th>
      <th>216</th>
      <th>217</th>
      <th>218</th>
      <th>219</th>
      <th>220</th>
      <th>221</th>
      <th>222</th>
      <th>223</th>
      <th>224</th>
      <th>225</th>
      <th>226</th>
      <th>227</th>
      <th>228</th>
      <th>229</th>
      <th>230</th>
      <th>231</th>
      <th>232</th>
      <th>233</th>
      <th>234</th>
      <th>235</th>
      <th>236</th>
      <th>237</th>
      <th>238</th>
      <th>239</th>
      <th>240</th>
      <th>241</th>
      <th>242</th>
      <th>243</th>
      <th>244</th>
      <th>245</th>
      <th>246</th>
      <th>247</th>
      <th>248</th>
      <th>249</th>
      <th>250</th>
      <th>251</th>
      <th>252</th>
      <th>253</th>
      <th>254</th>
      <th>255</th>
      <th>256</th>
      <th>257</th>
      <th>258</th>
      <th>259</th>
      <th>260</th>
      <th>261</th>
      <th>262</th>
      <th>263</th>
      <th>264</th>
      <th>265</th>
      <th>266</th>
      <th>267</th>
      <th>268</th>
      <th>269</th>
      <th>270</th>
      <th>271</th>
      <th>272</th>
      <th>273</th>
      <th>274</th>
      <th>275</th>
      <th>276</th>
      <th>277</th>
      <th>278</th>
      <th>279</th>
      <th>280</th>
      <th>281</th>
      <th>282</th>
      <th>283</th>
      <th>284</th>
      <th>285</th>
      <th>286</th>
      <th>287</th>
      <th>288</th>
      <th>289</th>
      <th>290</th>
      <th>291</th>
      <th>292</th>
      <th>293</th>
      <th>294</th>
      <th>295</th>
      <th>296</th>
      <th>297</th>
      <th>298</th>
      <th>299</th>
      <th>300</th>
      <th>301</th>
      <th>302</th>
      <th>303</th>
      <th>304</th>
      <th>305</th>
      <th>306</th>
      <th>307</th>
      <th>308</th>
      <th>309</th>
      <th>310</th>
      <th>311</th>
      <th>312</th>
      <th>313</th>
      <th>314</th>
      <th>315</th>
      <th>316</th>
      <th>317</th>
      <th>318</th>
      <th>319</th>
      <th>320</th>
      <th>321</th>
      <th>322</th>
      <th>323</th>
      <th>324</th>
      <th>325</th>
      <th>326</th>
      <th>327</th>
      <th>328</th>
      <th>329</th>
      <th>330</th>
      <th>331</th>
      <th>332</th>
      <th>333</th>
      <th>334</th>
      <th>335</th>
      <th>336</th>
      <th>337</th>
      <th>338</th>
      <th>339</th>
      <th>340</th>
      <th>341</th>
      <th>342</th>
      <th>343</th>
      <th>344</th>
      <th>345</th>
      <th>346</th>
      <th>347</th>
      <th>348</th>
      <th>349</th>
      <th>350</th>
      <th>351</th>
      <th>352</th>
      <th>353</th>
      <th>354</th>
      <th>355</th>
      <th>356</th>
      <th>357</th>
      <th>358</th>
      <th>359</th>
      <th>360</th>
      <th>361</th>
      <th>362</th>
      <th>363</th>
      <th>364</th>
      <th>365</th>
      <th>366</th>
      <th>367</th>
      <th>368</th>
      <th>369</th>
      <th>370</th>
      <th>371</th>
      <th>372</th>
      <th>373</th>
      <th>374</th>
      <th>375</th>
      <th>376</th>
      <th>377</th>
      <th>378</th>
      <th>379</th>
      <th>380</th>
      <th>381</th>
      <th>382</th>
      <th>383</th>
      <th>384</th>
      <th>385</th>
      <th>386</th>
      <th>387</th>
      <th>388</th>
      <th>389</th>
      <th>390</th>
      <th>391</th>
      <th>392</th>
      <th>393</th>
      <th>394</th>
      <th>395</th>
      <th>396</th>
      <th>397</th>
      <th>398</th>
      <th>399</th>
      <th>400</th>
      <th>401</th>
      <th>402</th>
      <th>403</th>
      <th>404</th>
      <th>405</th>
      <th>406</th>
      <th>407</th>
      <th>408</th>
      <th>409</th>
      <th>410</th>
      <th>411</th>
      <th>412</th>
      <th>413</th>
      <th>414</th>
      <th>415</th>
      <th>416</th>
      <th>417</th>
      <th>418</th>
      <th>419</th>
      <th>420</th>
      <th>421</th>
      <th>422</th>
      <th>423</th>
      <th>424</th>
      <th>425</th>
      <th>426</th>
      <th>427</th>
      <th>428</th>
      <th>429</th>
      <th>430</th>
      <th>431</th>
      <th>432</th>
      <th>433</th>
      <th>434</th>
      <th>435</th>
      <th>436</th>
      <th>437</th>
      <th>438</th>
      <th>439</th>
      <th>440</th>
      <th>441</th>
      <th>442</th>
      <th>443</th>
      <th>444</th>
      <th>445</th>
      <th>446</th>
      <th>447</th>
      <th>448</th>
      <th>449</th>
      <th>450</th>
      <th>451</th>
      <th>452</th>
      <th>453</th>
      <th>454</th>
      <th>455</th>
      <th>456</th>
      <th>457</th>
      <th>458</th>
      <th>459</th>
      <th>460</th>
      <th>461</th>
      <th>462</th>
      <th>463</th>
      <th>464</th>
      <th>465</th>
      <th>466</th>
      <th>467</th>
      <th>468</th>
      <th>469</th>
      <th>470</th>
      <th>471</th>
      <th>472</th>
      <th>473</th>
      <th>474</th>
      <th>475</th>
      <th>476</th>
      <th>477</th>
      <th>478</th>
      <th>479</th>
      <th>480</th>
      <th>481</th>
      <th>482</th>
      <th>483</th>
      <th>484</th>
      <th>485</th>
      <th>486</th>
      <th>487</th>
      <th>488</th>
      <th>489</th>
      <th>490</th>
      <th>491</th>
      <th>492</th>
      <th>493</th>
      <th>494</th>
      <th>495</th>
      <th>496</th>
      <th>497</th>
      <th>498</th>
      <th>499</th>
      <th>500</th>
      <th>501</th>
      <th>502</th>
      <th>503</th>
      <th>504</th>
      <th>505</th>
      <th>506</th>
      <th>507</th>
      <th>508</th>
      <th>509</th>
      <th>510</th>
      <th>511</th>
      <th>512</th>
      <th>513</th>
      <th>514</th>
      <th>515</th>
      <th>516</th>
      <th>517</th>
      <th>518</th>
      <th>519</th>
      <th>520</th>
      <th>521</th>
      <th>522</th>
      <th>523</th>
      <th>524</th>
      <th>525</th>
      <th>526</th>
      <th>527</th>
      <th>528</th>
      <th>529</th>
      <th>530</th>
      <th>531</th>
      <th>532</th>
      <th>533</th>
      <th>534</th>
      <th>535</th>
      <th>536</th>
      <th>537</th>
      <th>538</th>
      <th>539</th>
      <th>540</th>
      <th>541</th>
      <th>542</th>
      <th>543</th>
      <th>544</th>
      <th>545</th>
      <th>546</th>
      <th>547</th>
      <th>548</th>
      <th>549</th>
      <th>550</th>
      <th>551</th>
      <th>552</th>
      <th>553</th>
      <th>554</th>
      <th>555</th>
      <th>556</th>
      <th>557</th>
      <th>558</th>
      <th>559</th>
      <th>560</th>
      <th>561</th>
      <th>562</th>
      <th>563</th>
      <th>564</th>
      <th>565</th>
      <th>566</th>
      <th>567</th>
      <th>568</th>
      <th>569</th>
      <th>570</th>
      <th>571</th>
      <th>572</th>
      <th>573</th>
      <th>574</th>
      <th>575</th>
      <th>576</th>
      <th>577</th>
      <th>578</th>
      <th>579</th>
      <th>580</th>
      <th>581</th>
      <th>582</th>
      <th>583</th>
      <th>584</th>
      <th>585</th>
      <th>586</th>
      <th>587</th>
      <th>588</th>
      <th>589</th>
      <th>590</th>
      <th>591</th>
      <th>592</th>
      <th>593</th>
      <th>594</th>
      <th>595</th>
      <th>596</th>
      <th>597</th>
      <th>598</th>
      <th>599</th>
      <th>600</th>
      <th>601</th>
      <th>602</th>
      <th>603</th>
      <th>604</th>
      <th>605</th>
      <th>606</th>
      <th>607</th>
      <th>608</th>
      <th>609</th>
      <th>610</th>
      <th>611</th>
      <th>612</th>
      <th>613</th>
      <th>614</th>
      <th>615</th>
      <th>616</th>
      <th>617</th>
      <th>618</th>
      <th>619</th>
      <th>620</th>
      <th>621</th>
      <th>622</th>
      <th>623</th>
      <th>624</th>
      <th>625</th>
      <th>626</th>
      <th>627</th>
      <th>628</th>
      <th>629</th>
      <th>630</th>
      <th>631</th>
      <th>632</th>
      <th>633</th>
      <th>634</th>
      <th>635</th>
      <th>636</th>
      <th>637</th>
      <th>638</th>
      <th>639</th>
      <th>640</th>
      <th>641</th>
      <th>642</th>
      <th>643</th>
      <th>644</th>
      <th>645</th>
      <th>646</th>
      <th>647</th>
      <th>648</th>
      <th>649</th>
      <th>650</th>
      <th>651</th>
      <th>652</th>
      <th>653</th>
      <th>654</th>
      <th>655</th>
      <th>656</th>
      <th>657</th>
      <th>658</th>
      <th>659</th>
      <th>660</th>
      <th>661</th>
      <th>662</th>
      <th>663</th>
      <th>664</th>
      <th>665</th>
      <th>666</th>
      <th>667</th>
      <th>668</th>
      <th>669</th>
      <th>670</th>
      <th>671</th>
      <th>672</th>
      <th>673</th>
      <th>674</th>
      <th>675</th>
      <th>676</th>
      <th>677</th>
      <th>678</th>
      <th>679</th>
      <th>680</th>
      <th>681</th>
      <th>682</th>
      <th>683</th>
      <th>684</th>
      <th>685</th>
      <th>686</th>
      <th>687</th>
      <th>688</th>
      <th>689</th>
      <th>690</th>
      <th>691</th>
      <th>692</th>
      <th>693</th>
      <th>694</th>
      <th>695</th>
      <th>696</th>
      <th>697</th>
      <th>698</th>
      <th>699</th>
      <th>700</th>
      <th>701</th>
      <th>702</th>
      <th>703</th>
      <th>704</th>
      <th>705</th>
      <th>706</th>
      <th>707</th>
      <th>708</th>
      <th>709</th>
      <th>710</th>
      <th>711</th>
      <th>712</th>
      <th>713</th>
      <th>714</th>
      <th>715</th>
      <th>716</th>
      <th>717</th>
      <th>718</th>
      <th>719</th>
      <th>720</th>
      <th>721</th>
      <th>722</th>
      <th>723</th>
      <th>724</th>
      <th>725</th>
      <th>726</th>
      <th>727</th>
      <th>728</th>
      <th>729</th>
      <th>730</th>
      <th>731</th>
      <th>732</th>
      <th>733</th>
      <th>734</th>
      <th>735</th>
      <th>736</th>
      <th>737</th>
      <th>738</th>
      <th>739</th>
      <th>740</th>
      <th>741</th>
      <th>742</th>
      <th>743</th>
      <th>744</th>
      <th>745</th>
      <th>746</th>
      <th>747</th>
      <th>748</th>
      <th>749</th>
      <th>750</th>
      <th>751</th>
      <th>752</th>
      <th>753</th>
      <th>754</th>
      <th>755</th>
      <th>756</th>
      <th>757</th>
      <th>758</th>
      <th>759</th>
      <th>760</th>
      <th>761</th>
      <th>762</th>
      <th>763</th>
      <th>764</th>
      <th>765</th>
      <th>766</th>
      <th>767</th>
      <th>768</th>
      <th>769</th>
      <th>770</th>
      <th>771</th>
      <th>772</th>
      <th>773</th>
      <th>774</th>
      <th>775</th>
      <th>776</th>
      <th>777</th>
      <th>778</th>
      <th>779</th>
      <th>780</th>
      <th>781</th>
      <th>782</th>
      <th>783</th>
      <th>784</th>
      <th>785</th>
      <th>786</th>
      <th>787</th>
      <th>788</th>
      <th>789</th>
      <th>790</th>
      <th>791</th>
      <th>792</th>
      <th>793</th>
      <th>794</th>
      <th>795</th>
      <th>796</th>
      <th>797</th>
      <th>798</th>
      <th>799</th>
      <th>800</th>
      <th>801</th>
      <th>802</th>
      <th>803</th>
      <th>804</th>
      <th>805</th>
      <th>806</th>
      <th>807</th>
      <th>808</th>
      <th>809</th>
      <th>810</th>
      <th>811</th>
      <th>812</th>
      <th>813</th>
      <th>814</th>
      <th>815</th>
      <th>816</th>
      <th>817</th>
      <th>818</th>
      <th>819</th>
      <th>820</th>
      <th>821</th>
      <th>822</th>
      <th>823</th>
      <th>824</th>
      <th>825</th>
      <th>826</th>
      <th>827</th>
      <th>828</th>
      <th>829</th>
      <th>830</th>
      <th>831</th>
      <th>832</th>
      <th>833</th>
      <th>834</th>
      <th>835</th>
      <th>836</th>
      <th>837</th>
      <th>838</th>
      <th>839</th>
      <th>840</th>
      <th>841</th>
      <th>842</th>
      <th>843</th>
      <th>844</th>
      <th>845</th>
      <th>846</th>
      <th>847</th>
      <th>848</th>
      <th>849</th>
      <th>850</th>
      <th>851</th>
      <th>852</th>
      <th>853</th>
      <th>854</th>
      <th>855</th>
      <th>856</th>
      <th>857</th>
      <th>858</th>
      <th>859</th>
      <th>860</th>
      <th>861</th>
      <th>862</th>
      <th>863</th>
      <th>864</th>
      <th>865</th>
      <th>866</th>
      <th>867</th>
      <th>868</th>
      <th>869</th>
      <th>870</th>
      <th>871</th>
      <th>872</th>
      <th>873</th>
      <th>874</th>
      <th>875</th>
      <th>876</th>
      <th>877</th>
      <th>878</th>
      <th>879</th>
      <th>880</th>
      <th>881</th>
      <th>882</th>
      <th>883</th>
      <th>884</th>
      <th>885</th>
      <th>886</th>
      <th>887</th>
      <th>888</th>
      <th>889</th>
      <th>890</th>
      <th>891</th>
      <th>892</th>
      <th>893</th>
      <th>894</th>
      <th>895</th>
      <th>896</th>
      <th>897</th>
      <th>898</th>
      <th>899</th>
      <th>900</th>
      <th>901</th>
      <th>902</th>
      <th>903</th>
      <th>904</th>
      <th>905</th>
      <th>906</th>
      <th>907</th>
      <th>908</th>
      <th>909</th>
      <th>910</th>
      <th>911</th>
      <th>912</th>
      <th>913</th>
      <th>914</th>
      <th>915</th>
      <th>916</th>
      <th>917</th>
      <th>918</th>
      <th>919</th>
      <th>920</th>
      <th>921</th>
      <th>922</th>
      <th>923</th>
      <th>924</th>
      <th>925</th>
      <th>926</th>
      <th>927</th>
      <th>928</th>
      <th>929</th>
      <th>930</th>
      <th>931</th>
      <th>932</th>
      <th>933</th>
      <th>934</th>
      <th>935</th>
      <th>936</th>
      <th>937</th>
      <th>938</th>
      <th>939</th>
      <th>940</th>
      <th>941</th>
      <th>942</th>
      <th>943</th>
      <th>944</th>
      <th>945</th>
      <th>946</th>
      <th>947</th>
      <th>948</th>
      <th>949</th>
      <th>950</th>
      <th>951</th>
      <th>952</th>
      <th>953</th>
      <th>954</th>
      <th>955</th>
      <th>956</th>
      <th>957</th>
      <th>958</th>
      <th>959</th>
      <th>960</th>
      <th>961</th>
      <th>962</th>
      <th>963</th>
      <th>964</th>
      <th>965</th>
      <th>966</th>
      <th>967</th>
      <th>968</th>
      <th>969</th>
      <th>970</th>
      <th>971</th>
      <th>972</th>
      <th>973</th>
      <th>974</th>
      <th>975</th>
      <th>976</th>
      <th>977</th>
      <th>978</th>
      <th>979</th>
      <th>980</th>
      <th>981</th>
      <th>982</th>
      <th>983</th>
      <th>984</th>
      <th>985</th>
      <th>986</th>
      <th>987</th>
      <th>988</th>
      <th>989</th>
      <th>990</th>
      <th>991</th>
      <th>992</th>
      <th>993</th>
      <th>994</th>
      <th>995</th>
      <th>996</th>
      <th>997</th>
      <th>998</th>
      <th>999</th>
      <th>1000</th>
      <th>1001</th>
      <th>1002</th>
      <th>1003</th>
      <th>1004</th>
      <th>1005</th>
      <th>1006</th>
      <th>1007</th>
      <th>1008</th>
      <th>1009</th>
      <th>1010</th>
      <th>1011</th>
      <th>1012</th>
      <th>1013</th>
      <th>1014</th>
      <th>1015</th>
      <th>1016</th>
      <th>1017</th>
      <th>1018</th>
      <th>1019</th>
      <th>1020</th>
      <th>1021</th>
      <th>1022</th>
      <th>1023</th>
      <th>1024</th>
      <th>1025</th>
      <th>1026</th>
      <th>1027</th>
      <th>1028</th>
      <th>1029</th>
      <th>1030</th>
      <th>1031</th>
      <th>1032</th>
      <th>1033</th>
      <th>1034</th>
      <th>1035</th>
      <th>1036</th>
      <th>1037</th>
      <th>1038</th>
      <th>1039</th>
      <th>1040</th>
      <th>1041</th>
      <th>1042</th>
      <th>1043</th>
      <th>1044</th>
      <th>1045</th>
      <th>1046</th>
      <th>1047</th>
      <th>1048</th>
      <th>1049</th>
      <th>1050</th>
      <th>1051</th>
      <th>1052</th>
      <th>1053</th>
      <th>1054</th>
      <th>1055</th>
      <th>1056</th>
      <th>1057</th>
      <th>1058</th>
      <th>1059</th>
      <th>1060</th>
      <th>1061</th>
      <th>1062</th>
      <th>1063</th>
      <th>1064</th>
      <th>1065</th>
      <th>1066</th>
      <th>1067</th>
      <th>1068</th>
      <th>1069</th>
      <th>1070</th>
      <th>1071</th>
      <th>1072</th>
      <th>1073</th>
      <th>1074</th>
      <th>1075</th>
      <th>1076</th>
      <th>1077</th>
      <th>1078</th>
      <th>1079</th>
      <th>1080</th>
      <th>1081</th>
      <th>1082</th>
      <th>1083</th>
      <th>1084</th>
      <th>1085</th>
      <th>1086</th>
      <th>1087</th>
      <th>1088</th>
      <th>1089</th>
      <th>1090</th>
      <th>1091</th>
      <th>1092</th>
      <th>1093</th>
      <th>1094</th>
      <th>1095</th>
      <th>1096</th>
      <th>1097</th>
      <th>1098</th>
      <th>1099</th>
      <th>1100</th>
      <th>1101</th>
      <th>1102</th>
      <th>1103</th>
      <th>1104</th>
      <th>1105</th>
      <th>1106</th>
      <th>1107</th>
      <th>1108</th>
      <th>1109</th>
      <th>1110</th>
      <th>1111</th>
      <th>1112</th>
      <th>1113</th>
      <th>1114</th>
      <th>1115</th>
      <th>1116</th>
      <th>1117</th>
      <th>1118</th>
      <th>1119</th>
      <th>1120</th>
      <th>1121</th>
      <th>1122</th>
      <th>1123</th>
      <th>1124</th>
      <th>1125</th>
      <th>1126</th>
      <th>1127</th>
      <th>1128</th>
      <th>1129</th>
      <th>1130</th>
      <th>1131</th>
      <th>1132</th>
      <th>1133</th>
      <th>1134</th>
      <th>1135</th>
      <th>1136</th>
      <th>1137</th>
      <th>1138</th>
      <th>1139</th>
      <th>1140</th>
      <th>1141</th>
      <th>1142</th>
      <th>1143</th>
      <th>1144</th>
      <th>1145</th>
      <th>1146</th>
      <th>1147</th>
      <th>1148</th>
      <th>1149</th>
      <th>1150</th>
      <th>1151</th>
      <th>1152</th>
      <th>1153</th>
      <th>1154</th>
      <th>1155</th>
      <th>1156</th>
      <th>1157</th>
      <th>1158</th>
      <th>1159</th>
      <th>1160</th>
      <th>1161</th>
      <th>1162</th>
      <th>1163</th>
      <th>1164</th>
      <th>1165</th>
      <th>1166</th>
      <th>1167</th>
      <th>1168</th>
      <th>1169</th>
      <th>1170</th>
      <th>1171</th>
      <th>1172</th>
      <th>1173</th>
      <th>1174</th>
      <th>1175</th>
      <th>1176</th>
      <th>1177</th>
      <th>1178</th>
      <th>1179</th>
      <th>1180</th>
      <th>1181</th>
      <th>1182</th>
      <th>1183</th>
      <th>1184</th>
      <th>1185</th>
      <th>1186</th>
      <th>1187</th>
      <th>1188</th>
      <th>1189</th>
      <th>1190</th>
      <th>1191</th>
      <th>1192</th>
      <th>1193</th>
      <th>1194</th>
      <th>1195</th>
      <th>1196</th>
      <th>1197</th>
      <th>1198</th>
      <th>1199</th>
      <th>1200</th>
      <th>1201</th>
      <th>1202</th>
      <th>1203</th>
      <th>1204</th>
      <th>1205</th>
      <th>1206</th>
      <th>1207</th>
      <th>1208</th>
      <th>1209</th>
      <th>1210</th>
      <th>1211</th>
      <th>1212</th>
      <th>1213</th>
      <th>1214</th>
      <th>1215</th>
      <th>1216</th>
      <th>1217</th>
      <th>1218</th>
      <th>1219</th>
      <th>1220</th>
      <th>1221</th>
      <th>1222</th>
      <th>1223</th>
      <th>1224</th>
      <th>1225</th>
      <th>1226</th>
      <th>1227</th>
      <th>1228</th>
      <th>1229</th>
      <th>1230</th>
      <th>1231</th>
      <th>1232</th>
      <th>1233</th>
      <th>1234</th>
      <th>1235</th>
      <th>1236</th>
      <th>1237</th>
      <th>1238</th>
      <th>1239</th>
      <th>1240</th>
      <th>1241</th>
      <th>1242</th>
      <th>1243</th>
      <th>1244</th>
      <th>1245</th>
      <th>1246</th>
      <th>1247</th>
      <th>1248</th>
      <th>1249</th>
      <th>1250</th>
      <th>1251</th>
      <th>1252</th>
      <th>1253</th>
      <th>1254</th>
      <th>1255</th>
      <th>1256</th>
      <th>1257</th>
      <th>1258</th>
      <th>1259</th>
      <th>1260</th>
      <th>1261</th>
      <th>1262</th>
      <th>1263</th>
      <th>1264</th>
      <th>1265</th>
      <th>1266</th>
      <th>1267</th>
      <th>1268</th>
      <th>1269</th>
      <th>1270</th>
      <th>1271</th>
      <th>1272</th>
      <th>1273</th>
      <th>1274</th>
      <th>1275</th>
      <th>1276</th>
      <th>1277</th>
      <th>1278</th>
      <th>1279</th>
      <th>1280</th>
      <th>1281</th>
      <th>1282</th>
      <th>1283</th>
      <th>1284</th>
      <th>1285</th>
      <th>1286</th>
      <th>1287</th>
      <th>1288</th>
      <th>1289</th>
      <th>1290</th>
      <th>1291</th>
      <th>1292</th>
      <th>1293</th>
      <th>1294</th>
      <th>1295</th>
      <th>1296</th>
      <th>1297</th>
      <th>1298</th>
      <th>1299</th>
      <th>1300</th>
      <th>1301</th>
      <th>1302</th>
      <th>1303</th>
      <th>1304</th>
      <th>1305</th>
      <th>1306</th>
      <th>1307</th>
      <th>1308</th>
      <th>1309</th>
      <th>1310</th>
      <th>1311</th>
      <th>1312</th>
      <th>1313</th>
      <th>1314</th>
      <th>1315</th>
      <th>1316</th>
      <th>1317</th>
      <th>1318</th>
      <th>1319</th>
      <th>1320</th>
      <th>1321</th>
      <th>1322</th>
      <th>1323</th>
      <th>1324</th>
      <th>1325</th>
      <th>1326</th>
      <th>1327</th>
      <th>1328</th>
      <th>1329</th>
      <th>1330</th>
      <th>1331</th>
      <th>1332</th>
      <th>1333</th>
      <th>1334</th>
      <th>1335</th>
      <th>1336</th>
      <th>1337</th>
      <th>1338</th>
      <th>1339</th>
      <th>1340</th>
      <th>1341</th>
      <th>1342</th>
      <th>1343</th>
      <th>1344</th>
      <th>1345</th>
      <th>1346</th>
      <th>1347</th>
      <th>1348</th>
      <th>1349</th>
      <th>1350</th>
      <th>1351</th>
      <th>1352</th>
      <th>1353</th>
      <th>1354</th>
      <th>1355</th>
      <th>1356</th>
      <th>1357</th>
      <th>1358</th>
      <th>1359</th>
      <th>1360</th>
      <th>1361</th>
      <th>1362</th>
      <th>1363</th>
      <th>1364</th>
      <th>1365</th>
      <th>1366</th>
      <th>1367</th>
      <th>1368</th>
      <th>1369</th>
      <th>1370</th>
      <th>1371</th>
      <th>1372</th>
      <th>1373</th>
      <th>1374</th>
      <th>1375</th>
      <th>1376</th>
      <th>1377</th>
      <th>1378</th>
      <th>1379</th>
      <th>1380</th>
      <th>1381</th>
      <th>1382</th>
      <th>1383</th>
      <th>1384</th>
      <th>1385</th>
      <th>1386</th>
      <th>1387</th>
      <th>1388</th>
      <th>1389</th>
      <th>1390</th>
      <th>1391</th>
      <th>1392</th>
      <th>1393</th>
      <th>1394</th>
      <th>1395</th>
      <th>1396</th>
      <th>1397</th>
      <th>1398</th>
      <th>1399</th>
      <th>1400</th>
      <th>1401</th>
      <th>1402</th>
      <th>1403</th>
      <th>1404</th>
      <th>1405</th>
      <th>1406</th>
      <th>1407</th>
      <th>1408</th>
      <th>1409</th>
      <th>1410</th>
      <th>1411</th>
      <th>1412</th>
      <th>1413</th>
      <th>1414</th>
      <th>1415</th>
      <th>1416</th>
      <th>1417</th>
      <th>1418</th>
      <th>1419</th>
      <th>1420</th>
      <th>1421</th>
      <th>1422</th>
      <th>1423</th>
      <th>1424</th>
      <th>1425</th>
      <th>1426</th>
      <th>1427</th>
      <th>1428</th>
      <th>1429</th>
      <th>1430</th>
      <th>1431</th>
      <th>1432</th>
      <th>1433</th>
      <th>1434</th>
      <th>1435</th>
      <th>1436</th>
      <th>1437</th>
      <th>1438</th>
      <th>1439</th>
      <th>1440</th>
      <th>1441</th>
      <th>1442</th>
      <th>1443</th>
      <th>1444</th>
      <th>1445</th>
      <th>1446</th>
      <th>1447</th>
      <th>1448</th>
      <th>1449</th>
      <th>1450</th>
      <th>1451</th>
      <th>1452</th>
      <th>1453</th>
      <th>1454</th>
      <th>1455</th>
      <th>1456</th>
      <th>1457</th>
      <th>1458</th>
      <th>1459</th>
      <th>1460</th>
      <th>1461</th>
      <th>1462</th>
      <th>1463</th>
      <th>1464</th>
      <th>1465</th>
      <th>1466</th>
      <th>1467</th>
      <th>1468</th>
      <th>1469</th>
      <th>1470</th>
      <th>1471</th>
      <th>1472</th>
      <th>1473</th>
      <th>1474</th>
      <th>1475</th>
      <th>1476</th>
      <th>1477</th>
      <th>1478</th>
      <th>1479</th>
      <th>1480</th>
      <th>1481</th>
      <th>1482</th>
      <th>1483</th>
      <th>1484</th>
      <th>1485</th>
      <th>1486</th>
      <th>1487</th>
      <th>1488</th>
      <th>1489</th>
      <th>1490</th>
      <th>1491</th>
      <th>1492</th>
      <th>1493</th>
      <th>1494</th>
      <th>1495</th>
      <th>1496</th>
      <th>1497</th>
      <th>1498</th>
      <th>1499</th>
      <th>1500</th>
      <th>1501</th>
      <th>1502</th>
      <th>1503</th>
      <th>1504</th>
      <th>1505</th>
      <th>1506</th>
      <th>1507</th>
      <th>1508</th>
      <th>1509</th>
      <th>1510</th>
      <th>1511</th>
      <th>1512</th>
      <th>1513</th>
      <th>1514</th>
      <th>1515</th>
      <th>1516</th>
      <th>1517</th>
      <th>1518</th>
      <th>1519</th>
      <th>1520</th>
      <th>1521</th>
      <th>1522</th>
      <th>1523</th>
      <th>1524</th>
      <th>1525</th>
      <th>1526</th>
      <th>1527</th>
      <th>1528</th>
      <th>1529</th>
      <th>1530</th>
      <th>1531</th>
      <th>1532</th>
      <th>1533</th>
      <th>1534</th>
      <th>1535</th>
      <th>1536</th>
      <th>1537</th>
      <th>1538</th>
      <th>1539</th>
      <th>1540</th>
      <th>1541</th>
      <th>1542</th>
      <th>1543</th>
      <th>1544</th>
      <th>1545</th>
      <th>1546</th>
      <th>1547</th>
      <th>1548</th>
      <th>1549</th>
      <th>1550</th>
      <th>1551</th>
      <th>1552</th>
      <th>1553</th>
      <th>1554</th>
      <th>1555</th>
      <th>1556</th>
      <th>1557</th>
      <th>1558</th>
      <th>1559</th>
      <th>1560</th>
      <th>1561</th>
      <th>1562</th>
      <th>1563</th>
      <th>1564</th>
      <th>1565</th>
      <th>1566</th>
      <th>1567</th>
      <th>1568</th>
      <th>1569</th>
      <th>1570</th>
      <th>1571</th>
      <th>1572</th>
      <th>1573</th>
      <th>1574</th>
      <th>1575</th>
      <th>1576</th>
      <th>1577</th>
      <th>1578</th>
      <th>1579</th>
      <th>1580</th>
      <th>1581</th>
      <th>1582</th>
      <th>1583</th>
      <th>1584</th>
      <th>1585</th>
      <th>1586</th>
      <th>1587</th>
      <th>1588</th>
      <th>1589</th>
      <th>1590</th>
      <th>1591</th>
      <th>1592</th>
      <th>1593</th>
      <th>1594</th>
      <th>1595</th>
      <th>1596</th>
      <th>1597</th>
      <th>1598</th>
      <th>1599</th>
      <th>1600</th>
      <th>1601</th>
      <th>1602</th>
      <th>1603</th>
      <th>1604</th>
      <th>1605</th>
      <th>1606</th>
      <th>1607</th>
      <th>1608</th>
      <th>1609</th>
      <th>1610</th>
      <th>1611</th>
      <th>1612</th>
      <th>1613</th>
      <th>1614</th>
      <th>1615</th>
      <th>1616</th>
      <th>1617</th>
      <th>1618</th>
      <th>1619</th>
      <th>1620</th>
      <th>1621</th>
      <th>1622</th>
      <th>1623</th>
      <th>1624</th>
      <th>1625</th>
      <th>1626</th>
      <th>1627</th>
      <th>1628</th>
      <th>1629</th>
      <th>1630</th>
      <th>1631</th>
      <th>1632</th>
      <th>1633</th>
      <th>1634</th>
      <th>1635</th>
      <th>1636</th>
      <th>1637</th>
      <th>1638</th>
      <th>1639</th>
      <th>1640</th>
      <th>1641</th>
      <th>1642</th>
      <th>1643</th>
      <th>1644</th>
      <th>1645</th>
      <th>1646</th>
      <th>1647</th>
      <th>1648</th>
      <th>1649</th>
      <th>1650</th>
      <th>1651</th>
      <th>1652</th>
      <th>1653</th>
      <th>1654</th>
      <th>1655</th>
      <th>1656</th>
      <th>1657</th>
      <th>1658</th>
      <th>1659</th>
      <th>1660</th>
      <th>1661</th>
      <th>1662</th>
      <th>1663</th>
      <th>1664</th>
      <th>1665</th>
      <th>1666</th>
      <th>1667</th>
      <th>1668</th>
      <th>1669</th>
      <th>1670</th>
      <th>1671</th>
      <th>1672</th>
      <th>1673</th>
      <th>1674</th>
      <th>1675</th>
      <th>1676</th>
      <th>1677</th>
      <th>1678</th>
      <th>1679</th>
      <th>1680</th>
      <th>1681</th>
      <th>1682</th>
      <th>1683</th>
      <th>1684</th>
      <th>1685</th>
      <th>1686</th>
      <th>1687</th>
      <th>1688</th>
      <th>1689</th>
      <th>1690</th>
      <th>1691</th>
      <th>1692</th>
      <th>1693</th>
      <th>1694</th>
      <th>1695</th>
      <th>1696</th>
      <th>1697</th>
      <th>1698</th>
      <th>1699</th>
      <th>1700</th>
      <th>1701</th>
      <th>1702</th>
      <th>1703</th>
      <th>1704</th>
      <th>1705</th>
      <th>1706</th>
      <th>1707</th>
      <th>1708</th>
      <th>1709</th>
      <th>1710</th>
      <th>1711</th>
      <th>1712</th>
      <th>1713</th>
      <th>1714</th>
      <th>1715</th>
      <th>1716</th>
      <th>1717</th>
      <th>1718</th>
      <th>1719</th>
      <th>1720</th>
      <th>1721</th>
      <th>1722</th>
      <th>1723</th>
      <th>1724</th>
      <th>1725</th>
      <th>1726</th>
      <th>1727</th>
      <th>1728</th>
      <th>1729</th>
      <th>1730</th>
      <th>1731</th>
      <th>1732</th>
      <th>1733</th>
      <th>1734</th>
      <th>1735</th>
      <th>1736</th>
      <th>1737</th>
      <th>1738</th>
      <th>1739</th>
      <th>1740</th>
      <th>1741</th>
      <th>1742</th>
      <th>1743</th>
      <th>1744</th>
      <th>1745</th>
      <th>1746</th>
      <th>1747</th>
      <th>1748</th>
      <th>1749</th>
      <th>1750</th>
      <th>1751</th>
      <th>1752</th>
      <th>1753</th>
      <th>1754</th>
      <th>1755</th>
      <th>1756</th>
      <th>1757</th>
      <th>1758</th>
      <th>1759</th>
      <th>1760</th>
      <th>1761</th>
      <th>1762</th>
      <th>1763</th>
      <th>1764</th>
      <th>1765</th>
      <th>1766</th>
      <th>1767</th>
      <th>1768</th>
      <th>1769</th>
      <th>1770</th>
      <th>1771</th>
      <th>1772</th>
      <th>1773</th>
      <th>1774</th>
      <th>1775</th>
      <th>1776</th>
      <th>1777</th>
      <th>1778</th>
      <th>1779</th>
      <th>1780</th>
      <th>1781</th>
      <th>1782</th>
      <th>1783</th>
      <th>1784</th>
      <th>1785</th>
      <th>1786</th>
      <th>1787</th>
      <th>1788</th>
      <th>1789</th>
      <th>1790</th>
      <th>1791</th>
      <th>1792</th>
      <th>1793</th>
      <th>1794</th>
      <th>1795</th>
      <th>1796</th>
      <th>1797</th>
      <th>1798</th>
      <th>1799</th>
      <th>1800</th>
      <th>1801</th>
      <th>1802</th>
      <th>1803</th>
      <th>1804</th>
      <th>1805</th>
      <th>1806</th>
      <th>1807</th>
      <th>1808</th>
      <th>1809</th>
      <th>1810</th>
      <th>1811</th>
      <th>1812</th>
      <th>1813</th>
      <th>1814</th>
      <th>1815</th>
      <th>1816</th>
      <th>1817</th>
      <th>1818</th>
      <th>1819</th>
      <th>1820</th>
      <th>1821</th>
      <th>1822</th>
      <th>1823</th>
      <th>1824</th>
      <th>1825</th>
      <th>1826</th>
      <th>1827</th>
      <th>1828</th>
      <th>1829</th>
      <th>1830</th>
      <th>1831</th>
      <th>1832</th>
      <th>1833</th>
      <th>1834</th>
      <th>1835</th>
      <th>1836</th>
      <th>1837</th>
      <th>1838</th>
      <th>1839</th>
      <th>1840</th>
      <th>1841</th>
      <th>1842</th>
      <th>1843</th>
      <th>1844</th>
      <th>1845</th>
      <th>1846</th>
      <th>1847</th>
      <th>1848</th>
      <th>1849</th>
      <th>1850</th>
      <th>1851</th>
      <th>1852</th>
      <th>1853</th>
      <th>1854</th>
      <th>1855</th>
      <th>1856</th>
      <th>1857</th>
      <th>1858</th>
      <th>1859</th>
      <th>1860</th>
      <th>1861</th>
      <th>1862</th>
      <th>1863</th>
      <th>1864</th>
      <th>1865</th>
      <th>1866</th>
      <th>1867</th>
      <th>1868</th>
      <th>1869</th>
      <th>1870</th>
      <th>1871</th>
      <th>1872</th>
      <th>1873</th>
      <th>1874</th>
      <th>1875</th>
      <th>1876</th>
      <th>1877</th>
      <th>1878</th>
      <th>1879</th>
      <th>1880</th>
      <th>1881</th>
      <th>1882</th>
      <th>1883</th>
      <th>1884</th>
      <th>1885</th>
      <th>1886</th>
      <th>1887</th>
      <th>1888</th>
      <th>1889</th>
      <th>1890</th>
      <th>1891</th>
      <th>1892</th>
      <th>1893</th>
      <th>1894</th>
      <th>1895</th>
      <th>1896</th>
      <th>1897</th>
      <th>1898</th>
      <th>1899</th>
      <th>1900</th>
      <th>1901</th>
      <th>1902</th>
      <th>1903</th>
      <th>1904</th>
      <th>1905</th>
      <th>1906</th>
      <th>1907</th>
      <th>1908</th>
      <th>1909</th>
      <th>1910</th>
      <th>1911</th>
      <th>1912</th>
      <th>1913</th>
      <th>1914</th>
      <th>1915</th>
      <th>1916</th>
      <th>1917</th>
      <th>1918</th>
      <th>1919</th>
      <th>1920</th>
      <th>1921</th>
      <th>1922</th>
      <th>1923</th>
      <th>1924</th>
      <th>1925</th>
      <th>1926</th>
      <th>1927</th>
      <th>1928</th>
      <th>1929</th>
      <th>1930</th>
      <th>1931</th>
      <th>1932</th>
      <th>1933</th>
      <th>1934</th>
      <th>1935</th>
      <th>1936</th>
      <th>1937</th>
      <th>1938</th>
      <th>1939</th>
      <th>1940</th>
      <th>1941</th>
      <th>1942</th>
      <th>1943</th>
      <th>1944</th>
      <th>1945</th>
      <th>1946</th>
      <th>1947</th>
      <th>1948</th>
      <th>1949</th>
      <th>1950</th>
      <th>1951</th>
      <th>1952</th>
      <th>1953</th>
      <th>1954</th>
      <th>1955</th>
      <th>1956</th>
      <th>1957</th>
      <th>1958</th>
      <th>1959</th>
      <th>1960</th>
      <th>1961</th>
      <th>1962</th>
      <th>1963</th>
      <th>1964</th>
      <th>1965</th>
      <th>1966</th>
      <th>1967</th>
      <th>1968</th>
      <th>1969</th>
      <th>1970</th>
      <th>1971</th>
      <th>1972</th>
      <th>1973</th>
      <th>1974</th>
      <th>1975</th>
      <th>1976</th>
      <th>1977</th>
      <th>1978</th>
      <th>1979</th>
      <th>1980</th>
      <th>1981</th>
      <th>1982</th>
      <th>1983</th>
      <th>1984</th>
      <th>1985</th>
      <th>1986</th>
      <th>1987</th>
      <th>1988</th>
      <th>1989</th>
      <th>1990</th>
      <th>1991</th>
      <th>1992</th>
      <th>1993</th>
      <th>1994</th>
      <th>1995</th>
      <th>1996</th>
      <th>1997</th>
      <th>1998</th>
      <th>1999</th>
      <th>2000</th>
      <th>2001</th>
      <th>2002</th>
      <th>2003</th>
      <th>2004</th>
      <th>2005</th>
      <th>2006</th>
      <th>2007</th>
      <th>2008</th>
      <th>2009</th>
      <th>2010</th>
      <th>2011</th>
      <th>2012</th>
      <th>2013</th>
      <th>2014</th>
      <th>2015</th>
      <th>2016</th>
      <th>2017</th>
      <th>2018</th>
      <th>2019</th>
      <th>2020</th>
      <th>2021</th>
      <th>2022</th>
      <th>2023</th>
      <th>2024</th>
      <th>2025</th>
      <th>2026</th>
      <th>2027</th>
      <th>2028</th>
      <th>2029</th>
      <th>2030</th>
      <th>2031</th>
      <th>2032</th>
      <th>2033</th>
      <th>2034</th>
      <th>2035</th>
      <th>2036</th>
      <th>2037</th>
      <th>2038</th>
      <th>2039</th>
      <th>2040</th>
      <th>2041</th>
      <th>2042</th>
      <th>2043</th>
      <th>2044</th>
      <th>2045</th>
      <th>2046</th>
      <th>2047</th>
      <th>2048</th>
      <th>2049</th>
      <th>2050</th>
      <th>2051</th>
      <th>2052</th>
      <th>2053</th>
      <th>2054</th>
      <th>2055</th>
      <th>2056</th>
      <th>2057</th>
      <th>2058</th>
      <th>2059</th>
      <th>2060</th>
      <th>2061</th>
      <th>2062</th>
      <th>2063</th>
      <th>2064</th>
      <th>2065</th>
      <th>2066</th>
      <th>2067</th>
      <th>2068</th>
      <th>2069</th>
      <th>2070</th>
      <th>2071</th>
      <th>2072</th>
      <th>2073</th>
      <th>2074</th>
      <th>2075</th>
      <th>2076</th>
      <th>2077</th>
      <th>2078</th>
      <th>2079</th>
      <th>2080</th>
      <th>2081</th>
      <th>2082</th>
      <th>2083</th>
      <th>2084</th>
      <th>2085</th>
      <th>2086</th>
      <th>2087</th>
      <th>2088</th>
      <th>2089</th>
      <th>2090</th>
      <th>2091</th>
      <th>2092</th>
      <th>2093</th>
      <th>2094</th>
      <th>2095</th>
      <th>2096</th>
      <th>2097</th>
      <th>2098</th>
      <th>2099</th>
      <th>2100</th>
      <th>2101</th>
      <th>2102</th>
      <th>2103</th>
      <th>2104</th>
      <th>2105</th>
      <th>2106</th>
      <th>2107</th>
      <th>2108</th>
      <th>2109</th>
      <th>2110</th>
      <th>2111</th>
      <th>2112</th>
      <th>2113</th>
      <th>2114</th>
      <th>2115</th>
      <th>2116</th>
      <th>2117</th>
      <th>2118</th>
      <th>2119</th>
      <th>2120</th>
      <th>2121</th>
      <th>2122</th>
      <th>2123</th>
      <th>2124</th>
      <th>2125</th>
      <th>2126</th>
      <th>2127</th>
      <th>2128</th>
      <th>2129</th>
      <th>2130</th>
      <th>2131</th>
      <th>2132</th>
      <th>2133</th>
      <th>2134</th>
      <th>2135</th>
      <th>2136</th>
      <th>2137</th>
      <th>2138</th>
      <th>2139</th>
      <th>2140</th>
      <th>2141</th>
      <th>2142</th>
      <th>2143</th>
      <th>2144</th>
      <th>2145</th>
      <th>2146</th>
      <th>2147</th>
      <th>2148</th>
      <th>2149</th>
      <th>2150</th>
      <th>2151</th>
      <th>2152</th>
      <th>2153</th>
      <th>2154</th>
      <th>2155</th>
      <th>2156</th>
      <th>2157</th>
      <th>2158</th>
      <th>2159</th>
      <th>2160</th>
      <th>2161</th>
      <th>2162</th>
      <th>2163</th>
      <th>2164</th>
      <th>2165</th>
      <th>2166</th>
      <th>2167</th>
      <th>2168</th>
      <th>2169</th>
      <th>2170</th>
      <th>2171</th>
      <th>2172</th>
      <th>2173</th>
      <th>2174</th>
      <th>2175</th>
      <th>2176</th>
      <th>2177</th>
      <th>2178</th>
      <th>2179</th>
      <th>2180</th>
      <th>2181</th>
      <th>2182</th>
      <th>2183</th>
      <th>2184</th>
      <th>2185</th>
      <th>2186</th>
      <th>2187</th>
      <th>2188</th>
      <th>2189</th>
      <th>2190</th>
      <th>2191</th>
      <th>2192</th>
      <th>2193</th>
      <th>2194</th>
      <th>2195</th>
      <th>2196</th>
      <th>2197</th>
      <th>2198</th>
      <th>2199</th>
      <th>2200</th>
      <th>2201</th>
      <th>2202</th>
      <th>2203</th>
      <th>2204</th>
      <th>2205</th>
      <th>2206</th>
      <th>2207</th>
      <th>2208</th>
      <th>2209</th>
      <th>2210</th>
      <th>2211</th>
      <th>2212</th>
      <th>2213</th>
      <th>2214</th>
      <th>2215</th>
      <th>2216</th>
      <th>2217</th>
      <th>2218</th>
      <th>2219</th>
      <th>2220</th>
      <th>2221</th>
      <th>2222</th>
      <th>2223</th>
      <th>2224</th>
      <th>2225</th>
      <th>2226</th>
      <th>2227</th>
      <th>2228</th>
      <th>2229</th>
      <th>2230</th>
      <th>2231</th>
      <th>2232</th>
      <th>2233</th>
      <th>2234</th>
      <th>2235</th>
      <th>2236</th>
      <th>2237</th>
      <th>2238</th>
      <th>2239</th>
      <th>2240</th>
      <th>2241</th>
      <th>2242</th>
      <th>2243</th>
      <th>2244</th>
      <th>2245</th>
      <th>2246</th>
      <th>2247</th>
      <th>2248</th>
      <th>2249</th>
      <th>2250</th>
      <th>2251</th>
      <th>2252</th>
      <th>2253</th>
      <th>2254</th>
      <th>2255</th>
      <th>2256</th>
      <th>2257</th>
      <th>2258</th>
      <th>2259</th>
      <th>2260</th>
      <th>2261</th>
      <th>2262</th>
      <th>2263</th>
      <th>2264</th>
      <th>2265</th>
      <th>2266</th>
      <th>2267</th>
      <th>2268</th>
      <th>2269</th>
      <th>2270</th>
      <th>2271</th>
      <th>2272</th>
      <th>2273</th>
      <th>2274</th>
      <th>2275</th>
      <th>2276</th>
      <th>2277</th>
      <th>2278</th>
      <th>2279</th>
      <th>2280</th>
      <th>2281</th>
      <th>2282</th>
      <th>2283</th>
      <th>2284</th>
      <th>2285</th>
      <th>2286</th>
      <th>2287</th>
      <th>2288</th>
      <th>2289</th>
      <th>2290</th>
      <th>2291</th>
      <th>2292</th>
      <th>2293</th>
      <th>2294</th>
      <th>2295</th>
      <th>2296</th>
      <th>2297</th>
      <th>2298</th>
      <th>2299</th>
      <th>2300</th>
      <th>2301</th>
      <th>2302</th>
      <th>2303</th>
      <th>2304</th>
      <th>2305</th>
      <th>2306</th>
      <th>2307</th>
      <th>2308</th>
      <th>2309</th>
      <th>2310</th>
      <th>2311</th>
      <th>2312</th>
      <th>2313</th>
      <th>2314</th>
      <th>2315</th>
      <th>2316</th>
      <th>2317</th>
      <th>2318</th>
      <th>2319</th>
      <th>2320</th>
      <th>2321</th>
      <th>2322</th>
      <th>2323</th>
      <th>2324</th>
      <th>2325</th>
      <th>2326</th>
      <th>2327</th>
      <th>2328</th>
      <th>2329</th>
      <th>2330</th>
      <th>2331</th>
      <th>2332</th>
      <th>2333</th>
      <th>2334</th>
      <th>2335</th>
      <th>2336</th>
      <th>2337</th>
      <th>2338</th>
      <th>2339</th>
      <th>2340</th>
      <th>2341</th>
      <th>2342</th>
      <th>2343</th>
      <th>2344</th>
      <th>2345</th>
      <th>2346</th>
      <th>2347</th>
      <th>2348</th>
      <th>2349</th>
      <th>2350</th>
      <th>2351</th>
      <th>2352</th>
      <th>2353</th>
      <th>2354</th>
      <th>2355</th>
      <th>2356</th>
      <th>2357</th>
      <th>2358</th>
      <th>2359</th>
      <th>2360</th>
      <th>2361</th>
      <th>2362</th>
      <th>2363</th>
      <th>2364</th>
      <th>2365</th>
      <th>2366</th>
      <th>2367</th>
      <th>2368</th>
      <th>2369</th>
      <th>2370</th>
      <th>2371</th>
      <th>2372</th>
      <th>2373</th>
      <th>2374</th>
      <th>2375</th>
      <th>2376</th>
      <th>2377</th>
      <th>2378</th>
      <th>2379</th>
      <th>2380</th>
      <th>2381</th>
      <th>2382</th>
      <th>2383</th>
      <th>2384</th>
      <th>2385</th>
      <th>2386</th>
      <th>2387</th>
      <th>2388</th>
      <th>2389</th>
      <th>2390</th>
      <th>2391</th>
      <th>2392</th>
      <th>2393</th>
      <th>2394</th>
      <th>2395</th>
      <th>2396</th>
      <th>2397</th>
      <th>2398</th>
      <th>2399</th>
      <th>2400</th>
      <th>2401</th>
      <th>2402</th>
      <th>2403</th>
      <th>2404</th>
      <th>2405</th>
      <th>2406</th>
      <th>2407</th>
      <th>2408</th>
      <th>2409</th>
      <th>2410</th>
      <th>2411</th>
      <th>2412</th>
      <th>2413</th>
      <th>2414</th>
      <th>2415</th>
      <th>2416</th>
      <th>2417</th>
      <th>2418</th>
      <th>2419</th>
      <th>2420</th>
      <th>2421</th>
      <th>2422</th>
      <th>2423</th>
      <th>2424</th>
      <th>2425</th>
      <th>2426</th>
      <th>2427</th>
      <th>2428</th>
      <th>2429</th>
      <th>2430</th>
      <th>2431</th>
      <th>2432</th>
      <th>2433</th>
      <th>2434</th>
      <th>2435</th>
      <th>2436</th>
      <th>2437</th>
      <th>2438</th>
      <th>2439</th>
      <th>2440</th>
      <th>2441</th>
      <th>2442</th>
      <th>2443</th>
      <th>2444</th>
      <th>2445</th>
      <th>2446</th>
      <th>2447</th>
      <th>2448</th>
      <th>2449</th>
      <th>2450</th>
      <th>2451</th>
      <th>2452</th>
      <th>2453</th>
      <th>2454</th>
      <th>2455</th>
      <th>2456</th>
      <th>2457</th>
      <th>2458</th>
      <th>2459</th>
      <th>2460</th>
      <th>2461</th>
      <th>2462</th>
      <th>2463</th>
      <th>2464</th>
      <th>2465</th>
      <th>2466</th>
      <th>2467</th>
      <th>2468</th>
      <th>2469</th>
      <th>2470</th>
      <th>2471</th>
      <th>2472</th>
      <th>2473</th>
      <th>2474</th>
      <th>2475</th>
      <th>2476</th>
      <th>2477</th>
      <th>2478</th>
      <th>2479</th>
      <th>2480</th>
      <th>2481</th>
      <th>2482</th>
      <th>2483</th>
      <th>2484</th>
      <th>2485</th>
      <th>2486</th>
      <th>2487</th>
      <th>2488</th>
      <th>2489</th>
      <th>2490</th>
      <th>2491</th>
      <th>2492</th>
      <th>2493</th>
      <th>2494</th>
      <th>2495</th>
      <th>2496</th>
      <th>2497</th>
      <th>2498</th>
      <th>2499</th>
      <th>2500</th>
      <th>2501</th>
      <th>2502</th>
      <th>2503</th>
      <th>2504</th>
      <th>2505</th>
      <th>2506</th>
      <th>2507</th>
      <th>2508</th>
      <th>2509</th>
      <th>2510</th>
      <th>2511</th>
      <th>2512</th>
      <th>2513</th>
      <th>2514</th>
      <th>2515</th>
      <th>2516</th>
      <th>2517</th>
      <th>2518</th>
      <th>2519</th>
      <th>2520</th>
      <th>2521</th>
      <th>2522</th>
      <th>2523</th>
      <th>2524</th>
      <th>2525</th>
      <th>2526</th>
      <th>2527</th>
      <th>2528</th>
      <th>2529</th>
      <th>2530</th>
      <th>2531</th>
      <th>2532</th>
      <th>2533</th>
      <th>2534</th>
      <th>2535</th>
      <th>2536</th>
      <th>2537</th>
      <th>2538</th>
      <th>2539</th>
      <th>2540</th>
      <th>2541</th>
      <th>2542</th>
      <th>2543</th>
      <th>2544</th>
      <th>2545</th>
      <th>2546</th>
      <th>2547</th>
      <th>2548</th>
      <th>2549</th>
      <th>2550</th>
      <th>2551</th>
      <th>2552</th>
      <th>2553</th>
      <th>2554</th>
      <th>2555</th>
      <th>2556</th>
      <th>2557</th>
      <th>2558</th>
      <th>2559</th>
      <th>2560</th>
      <th>2561</th>
      <th>2562</th>
      <th>2563</th>
      <th>2564</th>
      <th>2565</th>
      <th>2566</th>
      <th>2567</th>
      <th>2568</th>
      <th>2569</th>
      <th>2570</th>
      <th>2571</th>
      <th>2572</th>
      <th>2573</th>
      <th>2574</th>
      <th>2575</th>
      <th>2576</th>
      <th>2577</th>
      <th>2578</th>
      <th>2579</th>
      <th>2580</th>
      <th>2581</th>
      <th>2582</th>
      <th>2583</th>
      <th>2584</th>
      <th>2585</th>
      <th>2586</th>
      <th>2587</th>
      <th>2588</th>
      <th>2589</th>
      <th>2590</th>
      <th>2591</th>
      <th>2592</th>
      <th>2593</th>
      <th>2594</th>
      <th>2595</th>
      <th>2596</th>
      <th>2597</th>
      <th>2598</th>
      <th>2599</th>
      <th>2600</th>
      <th>2601</th>
      <th>2602</th>
      <th>2603</th>
      <th>2604</th>
      <th>2605</th>
      <th>2606</th>
      <th>2607</th>
      <th>2608</th>
      <th>2609</th>
      <th>2610</th>
      <th>2611</th>
      <th>2612</th>
      <th>2613</th>
      <th>2614</th>
      <th>2615</th>
      <th>2616</th>
      <th>2617</th>
      <th>2618</th>
      <th>2619</th>
      <th>2620</th>
      <th>2621</th>
      <th>2622</th>
      <th>2623</th>
      <th>2624</th>
      <th>2625</th>
      <th>2626</th>
      <th>2627</th>
      <th>2628</th>
      <th>2629</th>
      <th>2630</th>
      <th>2631</th>
      <th>2632</th>
      <th>2633</th>
      <th>2634</th>
      <th>2635</th>
      <th>2636</th>
      <th>2637</th>
      <th>2638</th>
      <th>2639</th>
      <th>2640</th>
      <th>2641</th>
      <th>2642</th>
      <th>2643</th>
      <th>2644</th>
      <th>2645</th>
      <th>2646</th>
      <th>2647</th>
      <th>2648</th>
      <th>2649</th>
      <th>2650</th>
      <th>2651</th>
      <th>2652</th>
      <th>2653</th>
      <th>2654</th>
      <th>2655</th>
      <th>2656</th>
      <th>2657</th>
      <th>2658</th>
      <th>2659</th>
      <th>2660</th>
      <th>2661</th>
      <th>2662</th>
      <th>2663</th>
      <th>2664</th>
      <th>2665</th>
      <th>2666</th>
      <th>2667</th>
      <th>2668</th>
      <th>2669</th>
      <th>2670</th>
      <th>2671</th>
      <th>2672</th>
      <th>2673</th>
      <th>2674</th>
      <th>2675</th>
      <th>2676</th>
      <th>2677</th>
      <th>2678</th>
      <th>2679</th>
      <th>2680</th>
      <th>2681</th>
      <th>2682</th>
      <th>2683</th>
      <th>2684</th>
      <th>2685</th>
      <th>2686</th>
      <th>2687</th>
      <th>2688</th>
      <th>2689</th>
      <th>2690</th>
      <th>2691</th>
      <th>2692</th>
      <th>2693</th>
      <th>2694</th>
      <th>2695</th>
      <th>2696</th>
      <th>2697</th>
      <th>2698</th>
      <th>2699</th>
      <th>2700</th>
      <th>2701</th>
      <th>2702</th>
      <th>2703</th>
      <th>2704</th>
      <th>2705</th>
      <th>2706</th>
      <th>2707</th>
      <th>2708</th>
      <th>2709</th>
      <th>2710</th>
      <th>2711</th>
      <th>2712</th>
      <th>2713</th>
      <th>2714</th>
      <th>2715</th>
      <th>2716</th>
      <th>2717</th>
      <th>2718</th>
      <th>2719</th>
      <th>2720</th>
      <th>2721</th>
      <th>2722</th>
      <th>2723</th>
      <th>2724</th>
      <th>2725</th>
      <th>2726</th>
      <th>2727</th>
      <th>2728</th>
      <th>2729</th>
      <th>2730</th>
      <th>2731</th>
      <th>2732</th>
      <th>2733</th>
      <th>2734</th>
      <th>2735</th>
      <th>2736</th>
      <th>2737</th>
      <th>2738</th>
      <th>2739</th>
      <th>2740</th>
      <th>2741</th>
      <th>2742</th>
      <th>2743</th>
      <th>2744</th>
      <th>2745</th>
      <th>2746</th>
      <th>2747</th>
      <th>2748</th>
      <th>2749</th>
      <th>2750</th>
      <th>2751</th>
      <th>2752</th>
      <th>2753</th>
      <th>2754</th>
      <th>2755</th>
      <th>2756</th>
      <th>2757</th>
      <th>2758</th>
      <th>2759</th>
      <th>2760</th>
      <th>2761</th>
      <th>2762</th>
      <th>2763</th>
      <th>2764</th>
      <th>2765</th>
      <th>2766</th>
      <th>2767</th>
      <th>2768</th>
      <th>2769</th>
      <th>2770</th>
      <th>2771</th>
      <th>2772</th>
      <th>2773</th>
      <th>2774</th>
      <th>2775</th>
      <th>2776</th>
      <th>2777</th>
      <th>2778</th>
      <th>2779</th>
      <th>2780</th>
      <th>2781</th>
      <th>2782</th>
      <th>2783</th>
      <th>2784</th>
      <th>2785</th>
      <th>2786</th>
      <th>2787</th>
      <th>2788</th>
      <th>2789</th>
      <th>2790</th>
      <th>2791</th>
      <th>2792</th>
      <th>2793</th>
      <th>2794</th>
      <th>2795</th>
      <th>2796</th>
      <th>2797</th>
      <th>2798</th>
      <th>2799</th>
      <th>2800</th>
      <th>2801</th>
      <th>2802</th>
      <th>2803</th>
      <th>2804</th>
      <th>2805</th>
      <th>2806</th>
      <th>2807</th>
      <th>2808</th>
      <th>2809</th>
      <th>2810</th>
      <th>2811</th>
      <th>2812</th>
      <th>2813</th>
      <th>2814</th>
      <th>2815</th>
      <th>2816</th>
      <th>2817</th>
      <th>2818</th>
      <th>2819</th>
      <th>2820</th>
      <th>2821</th>
      <th>2822</th>
      <th>2823</th>
      <th>2824</th>
      <th>2825</th>
      <th>2826</th>
      <th>2827</th>
      <th>2828</th>
      <th>2829</th>
      <th>2830</th>
      <th>2831</th>
      <th>2832</th>
      <th>2833</th>
      <th>2834</th>
      <th>2835</th>
      <th>2836</th>
      <th>2837</th>
      <th>2838</th>
      <th>2839</th>
      <th>2840</th>
      <th>2841</th>
      <th>2842</th>
      <th>2843</th>
      <th>2844</th>
      <th>2845</th>
      <th>2846</th>
      <th>2847</th>
      <th>2848</th>
      <th>2849</th>
      <th>2850</th>
      <th>2851</th>
      <th>2852</th>
      <th>2853</th>
      <th>2854</th>
      <th>2855</th>
      <th>2856</th>
      <th>2857</th>
      <th>2858</th>
      <th>2859</th>
      <th>2860</th>
      <th>2861</th>
      <th>2862</th>
      <th>2863</th>
      <th>2864</th>
      <th>2865</th>
      <th>2866</th>
      <th>2867</th>
      <th>2868</th>
      <th>2869</th>
      <th>2870</th>
      <th>2871</th>
      <th>2872</th>
      <th>2873</th>
      <th>2874</th>
      <th>2875</th>
      <th>2876</th>
      <th>2877</th>
      <th>2878</th>
      <th>2879</th>
      <th>2880</th>
      <th>2881</th>
      <th>2882</th>
      <th>2883</th>
      <th>2884</th>
      <th>2885</th>
      <th>2886</th>
      <th>2887</th>
      <th>2888</th>
      <th>2889</th>
      <th>2890</th>
      <th>2891</th>
      <th>2892</th>
      <th>2893</th>
      <th>2894</th>
      <th>2895</th>
      <th>2896</th>
      <th>2897</th>
      <th>2898</th>
      <th>2899</th>
      <th>2900</th>
      <th>2901</th>
      <th>2902</th>
      <th>2903</th>
      <th>2904</th>
      <th>2905</th>
      <th>2906</th>
      <th>2907</th>
      <th>2908</th>
      <th>2909</th>
      <th>2910</th>
      <th>2911</th>
      <th>2912</th>
      <th>2913</th>
      <th>2914</th>
      <th>2915</th>
      <th>2916</th>
      <th>2917</th>
      <th>2918</th>
      <th>2919</th>
      <th>2920</th>
      <th>2921</th>
      <th>2922</th>
      <th>2923</th>
      <th>2924</th>
      <th>2925</th>
      <th>2926</th>
      <th>2927</th>
      <th>2928</th>
      <th>2929</th>
      <th>2930</th>
      <th>2931</th>
      <th>2932</th>
      <th>2933</th>
      <th>2934</th>
      <th>2935</th>
      <th>2936</th>
      <th>2937</th>
      <th>2938</th>
      <th>2939</th>
      <th>2940</th>
      <th>2941</th>
      <th>2942</th>
      <th>2943</th>
      <th>2944</th>
      <th>2945</th>
      <th>2946</th>
      <th>2947</th>
      <th>2948</th>
      <th>2949</th>
      <th>2950</th>
      <th>2951</th>
      <th>2952</th>
      <th>2953</th>
      <th>2954</th>
      <th>2955</th>
      <th>2956</th>
      <th>2957</th>
      <th>2958</th>
      <th>2959</th>
      <th>2960</th>
      <th>2961</th>
      <th>2962</th>
      <th>2963</th>
      <th>2964</th>
      <th>2965</th>
      <th>2966</th>
      <th>2967</th>
      <th>2968</th>
      <th>2969</th>
      <th>2970</th>
      <th>2971</th>
      <th>2972</th>
      <th>2973</th>
      <th>2974</th>
      <th>2975</th>
      <th>2976</th>
      <th>2977</th>
      <th>2978</th>
      <th>2979</th>
      <th>2980</th>
      <th>2981</th>
      <th>2982</th>
      <th>2983</th>
      <th>2984</th>
      <th>2985</th>
      <th>2986</th>
      <th>2987</th>
      <th>2988</th>
      <th>2989</th>
      <th>2990</th>
      <th>2991</th>
      <th>2992</th>
      <th>2993</th>
      <th>2994</th>
      <th>2995</th>
      <th>2996</th>
      <th>2997</th>
      <th>2998</th>
      <th>2999</th>
      <th>3000</th>
      <th>3001</th>
      <th>3002</th>
      <th>3003</th>
      <th>3004</th>
      <th>3005</th>
      <th>3006</th>
      <th>3007</th>
      <th>3008</th>
      <th>3009</th>
      <th>3010</th>
      <th>3011</th>
      <th>3012</th>
      <th>3013</th>
      <th>3014</th>
      <th>3015</th>
      <th>3016</th>
      <th>3017</th>
      <th>3018</th>
      <th>3019</th>
      <th>3020</th>
      <th>3021</th>
      <th>3022</th>
      <th>3023</th>
      <th>3024</th>
      <th>3025</th>
      <th>3026</th>
      <th>3027</th>
      <th>3028</th>
      <th>3029</th>
      <th>3030</th>
      <th>3031</th>
      <th>3032</th>
      <th>3033</th>
      <th>3034</th>
      <th>3035</th>
      <th>3036</th>
      <th>3037</th>
      <th>3038</th>
      <th>3039</th>
      <th>3040</th>
      <th>3041</th>
      <th>3042</th>
      <th>3043</th>
      <th>3044</th>
      <th>3045</th>
      <th>3046</th>
      <th>3047</th>
      <th>3048</th>
      <th>3049</th>
      <th>3050</th>
      <th>3051</th>
      <th>3052</th>
      <th>3053</th>
      <th>3054</th>
      <th>3055</th>
      <th>3056</th>
      <th>3057</th>
      <th>3058</th>
      <th>3059</th>
      <th>3060</th>
      <th>3061</th>
      <th>3062</th>
      <th>3063</th>
      <th>3064</th>
      <th>3065</th>
      <th>3066</th>
      <th>3067</th>
      <th>3068</th>
      <th>3069</th>
      <th>3070</th>
      <th>3071</th>
      <th>3072</th>
      <th>3073</th>
      <th>3074</th>
      <th>3075</th>
      <th>3076</th>
      <th>3077</th>
      <th>3078</th>
      <th>3079</th>
      <th>3080</th>
      <th>3081</th>
      <th>3082</th>
      <th>3083</th>
      <th>3084</th>
      <th>3085</th>
      <th>3086</th>
      <th>3087</th>
      <th>3088</th>
      <th>3089</th>
      <th>3090</th>
      <th>3091</th>
      <th>3092</th>
      <th>3093</th>
      <th>3094</th>
      <th>3095</th>
      <th>3096</th>
      <th>3097</th>
      <th>3098</th>
      <th>3099</th>
      <th>3100</th>
      <th>3101</th>
      <th>3102</th>
      <th>3103</th>
      <th>3104</th>
      <th>3105</th>
      <th>3106</th>
      <th>3107</th>
      <th>3108</th>
      <th>3109</th>
      <th>3110</th>
      <th>3111</th>
      <th>3112</th>
      <th>3113</th>
      <th>3114</th>
      <th>3115</th>
      <th>3116</th>
      <th>3117</th>
      <th>3118</th>
      <th>3119</th>
      <th>3120</th>
      <th>3121</th>
      <th>3122</th>
      <th>3123</th>
      <th>3124</th>
      <th>3125</th>
      <th>3126</th>
      <th>3127</th>
      <th>3128</th>
      <th>3129</th>
      <th>3130</th>
      <th>3131</th>
      <th>3132</th>
      <th>3133</th>
      <th>3134</th>
      <th>3135</th>
      <th>3136</th>
      <th>3137</th>
      <th>3138</th>
      <th>3139</th>
      <th>3140</th>
      <th>3141</th>
      <th>3142</th>
      <th>3143</th>
      <th>3144</th>
      <th>3145</th>
      <th>3146</th>
      <th>3147</th>
      <th>3148</th>
      <th>3149</th>
      <th>3150</th>
      <th>3151</th>
      <th>3152</th>
      <th>3153</th>
      <th>3154</th>
      <th>3155</th>
      <th>3156</th>
      <th>3157</th>
      <th>3158</th>
      <th>3159</th>
      <th>3160</th>
      <th>3161</th>
      <th>3162</th>
      <th>3163</th>
      <th>3164</th>
      <th>3165</th>
      <th>3166</th>
      <th>3167</th>
      <th>3168</th>
      <th>3169</th>
      <th>3170</th>
      <th>3171</th>
      <th>3172</th>
      <th>3173</th>
      <th>3174</th>
      <th>3175</th>
      <th>3176</th>
      <th>3177</th>
      <th>3178</th>
      <th>3179</th>
      <th>3180</th>
      <th>3181</th>
      <th>3182</th>
      <th>3183</th>
      <th>3184</th>
      <th>3185</th>
      <th>3186</th>
      <th>3187</th>
      <th>3188</th>
      <th>3189</th>
      <th>3190</th>
      <th>3191</th>
      <th>3192</th>
      <th>3193</th>
      <th>3194</th>
      <th>3195</th>
      <th>3196</th>
      <th>3197</th>
      <th>3198</th>
      <th>3199</th>
      <th>3200</th>
      <th>3201</th>
      <th>3202</th>
      <th>3203</th>
      <th>3204</th>
      <th>3205</th>
      <th>3206</th>
      <th>3207</th>
      <th>3208</th>
      <th>3209</th>
      <th>3210</th>
      <th>3211</th>
      <th>3212</th>
      <th>3213</th>
      <th>3214</th>
      <th>3215</th>
      <th>3216</th>
      <th>3217</th>
      <th>3218</th>
      <th>3219</th>
      <th>3220</th>
      <th>3221</th>
      <th>3222</th>
      <th>3223</th>
      <th>3224</th>
      <th>3225</th>
      <th>3226</th>
      <th>3227</th>
      <th>3228</th>
      <th>3229</th>
      <th>3230</th>
      <th>3231</th>
      <th>3232</th>
      <th>3233</th>
      <th>3234</th>
      <th>3235</th>
      <th>3236</th>
      <th>3237</th>
      <th>3238</th>
      <th>3239</th>
      <th>3240</th>
      <th>3241</th>
      <th>3242</th>
      <th>3243</th>
      <th>3244</th>
      <th>3245</th>
      <th>3246</th>
      <th>3247</th>
      <th>3248</th>
      <th>3249</th>
      <th>3250</th>
      <th>3251</th>
      <th>3252</th>
      <th>3253</th>
      <th>3254</th>
      <th>3255</th>
      <th>3256</th>
      <th>3257</th>
      <th>3258</th>
      <th>3259</th>
      <th>3260</th>
      <th>3261</th>
      <th>3262</th>
      <th>3263</th>
      <th>3264</th>
      <th>3265</th>
      <th>3266</th>
      <th>3267</th>
      <th>3268</th>
      <th>3269</th>
      <th>3270</th>
      <th>3271</th>
      <th>3272</th>
      <th>3273</th>
      <th>3274</th>
      <th>3275</th>
      <th>3276</th>
      <th>3277</th>
      <th>3278</th>
      <th>3279</th>
      <th>3280</th>
      <th>3281</th>
      <th>3282</th>
      <th>3283</th>
      <th>3284</th>
      <th>3285</th>
      <th>3286</th>
      <th>3287</th>
      <th>3288</th>
      <th>3289</th>
      <th>3290</th>
      <th>3291</th>
      <th>3292</th>
      <th>3293</th>
      <th>3294</th>
      <th>3295</th>
      <th>3296</th>
      <th>3297</th>
      <th>3298</th>
      <th>3299</th>
      <th>3300</th>
      <th>3301</th>
      <th>3302</th>
      <th>3303</th>
      <th>3304</th>
      <th>3305</th>
      <th>3306</th>
      <th>3307</th>
      <th>3308</th>
      <th>3309</th>
      <th>3310</th>
      <th>3311</th>
      <th>3312</th>
      <th>3313</th>
      <th>3314</th>
      <th>3315</th>
      <th>3316</th>
      <th>3317</th>
      <th>3318</th>
      <th>3319</th>
      <th>3320</th>
      <th>3321</th>
      <th>3322</th>
      <th>3323</th>
      <th>3324</th>
      <th>3325</th>
      <th>3326</th>
      <th>3327</th>
      <th>3328</th>
      <th>3329</th>
      <th>3330</th>
      <th>3331</th>
      <th>3332</th>
      <th>3333</th>
      <th>3334</th>
      <th>3335</th>
      <th>3336</th>
      <th>3337</th>
      <th>3338</th>
      <th>3339</th>
      <th>3340</th>
      <th>3341</th>
      <th>3342</th>
      <th>3343</th>
      <th>3344</th>
      <th>3345</th>
      <th>3346</th>
      <th>3347</th>
      <th>3348</th>
      <th>3349</th>
      <th>3350</th>
      <th>3351</th>
      <th>3352</th>
      <th>3353</th>
      <th>3354</th>
      <th>3355</th>
      <th>3356</th>
      <th>3357</th>
      <th>3358</th>
      <th>3359</th>
      <th>3360</th>
      <th>3361</th>
      <th>3362</th>
      <th>3363</th>
      <th>3364</th>
      <th>3365</th>
      <th>3366</th>
      <th>3367</th>
      <th>3368</th>
      <th>3369</th>
      <th>3370</th>
      <th>3371</th>
      <th>3372</th>
      <th>3373</th>
      <th>3374</th>
      <th>3375</th>
      <th>3376</th>
      <th>3377</th>
      <th>3378</th>
      <th>3379</th>
      <th>3380</th>
      <th>3381</th>
      <th>3382</th>
      <th>3383</th>
      <th>3384</th>
      <th>3385</th>
      <th>3386</th>
      <th>3387</th>
      <th>3388</th>
      <th>3389</th>
      <th>3390</th>
      <th>3391</th>
      <th>3392</th>
      <th>3393</th>
      <th>3394</th>
      <th>3395</th>
      <th>3396</th>
      <th>3397</th>
      <th>3398</th>
      <th>3399</th>
      <th>3400</th>
      <th>3401</th>
      <th>3402</th>
      <th>3403</th>
      <th>3404</th>
      <th>3405</th>
      <th>3406</th>
      <th>3407</th>
      <th>3408</th>
      <th>3409</th>
      <th>3410</th>
      <th>3411</th>
      <th>3412</th>
      <th>3413</th>
      <th>3414</th>
      <th>3415</th>
      <th>3416</th>
      <th>3417</th>
      <th>3418</th>
      <th>3419</th>
      <th>3420</th>
      <th>3421</th>
      <th>3422</th>
      <th>3423</th>
      <th>3424</th>
      <th>3425</th>
      <th>3426</th>
      <th>3427</th>
      <th>3428</th>
      <th>3429</th>
      <th>3430</th>
      <th>3431</th>
      <th>3432</th>
      <th>3433</th>
      <th>3434</th>
      <th>3435</th>
      <th>3436</th>
      <th>3437</th>
      <th>3438</th>
      <th>3439</th>
      <th>3440</th>
      <th>3441</th>
      <th>3442</th>
      <th>3443</th>
      <th>3444</th>
      <th>3445</th>
      <th>3446</th>
      <th>3447</th>
      <th>3448</th>
      <th>3449</th>
      <th>3450</th>
      <th>3451</th>
      <th>3452</th>
      <th>3453</th>
      <th>3454</th>
      <th>3455</th>
      <th>3456</th>
      <th>3457</th>
      <th>3458</th>
      <th>3459</th>
      <th>3460</th>
      <th>3461</th>
      <th>3462</th>
      <th>3463</th>
      <th>3464</th>
      <th>3465</th>
      <th>3466</th>
      <th>3467</th>
      <th>3468</th>
      <th>3469</th>
      <th>3470</th>
      <th>3471</th>
      <th>3472</th>
      <th>3473</th>
      <th>3474</th>
      <th>3475</th>
      <th>3476</th>
      <th>3477</th>
      <th>3478</th>
      <th>3479</th>
      <th>3480</th>
      <th>3481</th>
      <th>3482</th>
      <th>3483</th>
      <th>3484</th>
      <th>3485</th>
      <th>3486</th>
      <th>3487</th>
      <th>3488</th>
      <th>3489</th>
      <th>3490</th>
      <th>3491</th>
      <th>3492</th>
      <th>3493</th>
      <th>3494</th>
      <th>3495</th>
      <th>3496</th>
      <th>3497</th>
      <th>3498</th>
      <th>3499</th>
      <th>3500</th>
      <th>3501</th>
      <th>3502</th>
      <th>3503</th>
      <th>3504</th>
      <th>3505</th>
      <th>3506</th>
      <th>3507</th>
      <th>3508</th>
      <th>3509</th>
      <th>3510</th>
      <th>3511</th>
      <th>3512</th>
      <th>3513</th>
      <th>3514</th>
      <th>3515</th>
      <th>3516</th>
      <th>3517</th>
      <th>3518</th>
      <th>3519</th>
      <th>3520</th>
      <th>3521</th>
      <th>3522</th>
      <th>3523</th>
      <th>3524</th>
      <th>3525</th>
      <th>3526</th>
      <th>3527</th>
      <th>3528</th>
      <th>3529</th>
      <th>3530</th>
      <th>3531</th>
      <th>3532</th>
      <th>3533</th>
      <th>3534</th>
      <th>3535</th>
      <th>3536</th>
      <th>3537</th>
      <th>3538</th>
      <th>3539</th>
      <th>3540</th>
      <th>3541</th>
      <th>3542</th>
      <th>3543</th>
      <th>3544</th>
      <th>3545</th>
      <th>3546</th>
      <th>3547</th>
      <th>3548</th>
      <th>3549</th>
      <th>3550</th>
      <th>3551</th>
      <th>3552</th>
      <th>3553</th>
      <th>3554</th>
      <th>3555</th>
      <th>3556</th>
      <th>3557</th>
      <th>3558</th>
      <th>3559</th>
      <th>3560</th>
      <th>3561</th>
      <th>3562</th>
      <th>3563</th>
      <th>3564</th>
      <th>3565</th>
      <th>3566</th>
      <th>3567</th>
      <th>3568</th>
      <th>3569</th>
      <th>3570</th>
      <th>3571</th>
      <th>3572</th>
      <th>3573</th>
      <th>3574</th>
      <th>3575</th>
      <th>3576</th>
      <th>3577</th>
      <th>3578</th>
      <th>3579</th>
      <th>3580</th>
      <th>3581</th>
      <th>3582</th>
      <th>3583</th>
      <th>3584</th>
      <th>3585</th>
      <th>3586</th>
      <th>3587</th>
      <th>3588</th>
      <th>3589</th>
      <th>3590</th>
      <th>3591</th>
      <th>3592</th>
      <th>3593</th>
      <th>3594</th>
      <th>3595</th>
      <th>3596</th>
      <th>3597</th>
      <th>3598</th>
      <th>3599</th>
      <th>3600</th>
      <th>3601</th>
      <th>3602</th>
      <th>3603</th>
      <th>3604</th>
      <th>3605</th>
      <th>3606</th>
      <th>3607</th>
      <th>3608</th>
      <th>3609</th>
      <th>3610</th>
      <th>3611</th>
      <th>3612</th>
      <th>3613</th>
      <th>3614</th>
      <th>3615</th>
      <th>3616</th>
      <th>3617</th>
      <th>3618</th>
      <th>3619</th>
      <th>3620</th>
      <th>3621</th>
      <th>3622</th>
      <th>3623</th>
      <th>3624</th>
      <th>3625</th>
      <th>3626</th>
      <th>3627</th>
      <th>3628</th>
      <th>3629</th>
      <th>3630</th>
      <th>3631</th>
      <th>3632</th>
      <th>3633</th>
      <th>3634</th>
      <th>3635</th>
      <th>3636</th>
      <th>3637</th>
      <th>3638</th>
      <th>3639</th>
      <th>3640</th>
      <th>3641</th>
      <th>3642</th>
      <th>3643</th>
      <th>3644</th>
      <th>3645</th>
      <th>3646</th>
      <th>3647</th>
      <th>3648</th>
      <th>3649</th>
      <th>3650</th>
      <th>3651</th>
      <th>3652</th>
      <th>3653</th>
      <th>3654</th>
      <th>3655</th>
      <th>3656</th>
      <th>3657</th>
      <th>3658</th>
      <th>3659</th>
      <th>3660</th>
      <th>3661</th>
      <th>3662</th>
      <th>3663</th>
      <th>3664</th>
      <th>3665</th>
      <th>3666</th>
      <th>3667</th>
      <th>3668</th>
      <th>3669</th>
      <th>3670</th>
      <th>3671</th>
      <th>3672</th>
      <th>3673</th>
      <th>3674</th>
      <th>3675</th>
      <th>3676</th>
      <th>3677</th>
      <th>3678</th>
      <th>3679</th>
      <th>3680</th>
      <th>3681</th>
      <th>3682</th>
      <th>3683</th>
      <th>3684</th>
      <th>3685</th>
      <th>3686</th>
      <th>3687</th>
      <th>3688</th>
      <th>3689</th>
      <th>3690</th>
      <th>3691</th>
      <th>3692</th>
      <th>3693</th>
      <th>3694</th>
      <th>3695</th>
      <th>3696</th>
      <th>3697</th>
      <th>3698</th>
      <th>3699</th>
      <th>3700</th>
      <th>3701</th>
      <th>3702</th>
      <th>3703</th>
      <th>3704</th>
      <th>3705</th>
      <th>3706</th>
      <th>3707</th>
      <th>3708</th>
      <th>3709</th>
      <th>3710</th>
      <th>3711</th>
      <th>3712</th>
      <th>3713</th>
      <th>3714</th>
      <th>3715</th>
      <th>3716</th>
      <th>3717</th>
      <th>3718</th>
      <th>3719</th>
      <th>3720</th>
      <th>3721</th>
      <th>3722</th>
      <th>3723</th>
      <th>3724</th>
      <th>3725</th>
      <th>3726</th>
      <th>3727</th>
      <th>3728</th>
      <th>3729</th>
      <th>3730</th>
      <th>3731</th>
      <th>3732</th>
      <th>3733</th>
      <th>3734</th>
      <th>3735</th>
      <th>3736</th>
      <th>3737</th>
      <th>3738</th>
      <th>3739</th>
      <th>3740</th>
      <th>3741</th>
      <th>3742</th>
      <th>3743</th>
      <th>3744</th>
      <th>3745</th>
      <th>3746</th>
      <th>3747</th>
      <th>3748</th>
      <th>3749</th>
      <th>3750</th>
      <th>3751</th>
      <th>3752</th>
      <th>3753</th>
      <th>3754</th>
      <th>3755</th>
      <th>3756</th>
      <th>3757</th>
      <th>3758</th>
      <th>3759</th>
      <th>3760</th>
      <th>3761</th>
      <th>3762</th>
      <th>3763</th>
      <th>3764</th>
      <th>3765</th>
      <th>3766</th>
      <th>3767</th>
      <th>3768</th>
      <th>3769</th>
      <th>3770</th>
      <th>3771</th>
      <th>3772</th>
      <th>3773</th>
      <th>3774</th>
      <th>3775</th>
      <th>3776</th>
      <th>3777</th>
      <th>3778</th>
      <th>3779</th>
      <th>3780</th>
      <th>3781</th>
      <th>3782</th>
      <th>3783</th>
      <th>3784</th>
      <th>3785</th>
      <th>3786</th>
      <th>3787</th>
      <th>3788</th>
      <th>3789</th>
      <th>3790</th>
      <th>3791</th>
      <th>3792</th>
      <th>3793</th>
      <th>3794</th>
      <th>3795</th>
      <th>3796</th>
      <th>3797</th>
      <th>3798</th>
      <th>3799</th>
      <th>3800</th>
      <th>3801</th>
      <th>3802</th>
      <th>3803</th>
      <th>3804</th>
      <th>3805</th>
      <th>3806</th>
      <th>3807</th>
      <th>3808</th>
      <th>3809</th>
      <th>3810</th>
      <th>3811</th>
      <th>3812</th>
      <th>3813</th>
      <th>3814</th>
      <th>3815</th>
      <th>3816</th>
      <th>3817</th>
      <th>3818</th>
      <th>3819</th>
      <th>3820</th>
      <th>3821</th>
      <th>3822</th>
      <th>3823</th>
      <th>3824</th>
      <th>3825</th>
      <th>3826</th>
      <th>3827</th>
      <th>3828</th>
      <th>3829</th>
      <th>3830</th>
      <th>3831</th>
      <th>3832</th>
      <th>3833</th>
      <th>3834</th>
      <th>3835</th>
      <th>3836</th>
      <th>3837</th>
      <th>3838</th>
      <th>3839</th>
      <th>3840</th>
      <th>3841</th>
      <th>3842</th>
      <th>3843</th>
      <th>3844</th>
      <th>3845</th>
      <th>3846</th>
      <th>3847</th>
      <th>3848</th>
      <th>3849</th>
      <th>3850</th>
      <th>3851</th>
      <th>3852</th>
      <th>3853</th>
      <th>3854</th>
      <th>3855</th>
      <th>3856</th>
      <th>3857</th>
      <th>3858</th>
      <th>3859</th>
      <th>3860</th>
      <th>3861</th>
      <th>3862</th>
      <th>3863</th>
      <th>3864</th>
      <th>3865</th>
      <th>3866</th>
      <th>3867</th>
      <th>3868</th>
      <th>3869</th>
      <th>3870</th>
      <th>3871</th>
      <th>3872</th>
      <th>3873</th>
      <th>3874</th>
      <th>3875</th>
      <th>3876</th>
      <th>3877</th>
      <th>3878</th>
      <th>3879</th>
      <th>3880</th>
      <th>3881</th>
      <th>3882</th>
      <th>3883</th>
      <th>3884</th>
      <th>3885</th>
      <th>3886</th>
      <th>3887</th>
      <th>3888</th>
      <th>3889</th>
      <th>3890</th>
      <th>3891</th>
      <th>3892</th>
      <th>3893</th>
      <th>3894</th>
      <th>3895</th>
      <th>3896</th>
      <th>3897</th>
      <th>3898</th>
      <th>3899</th>
      <th>3900</th>
      <th>3901</th>
      <th>3902</th>
      <th>3903</th>
      <th>3904</th>
      <th>3905</th>
      <th>3906</th>
      <th>3907</th>
      <th>3908</th>
      <th>3909</th>
      <th>3910</th>
      <th>3911</th>
      <th>3912</th>
      <th>3913</th>
      <th>3914</th>
      <th>3915</th>
      <th>3916</th>
      <th>3917</th>
      <th>3918</th>
      <th>3919</th>
      <th>3920</th>
      <th>3921</th>
      <th>3922</th>
      <th>3923</th>
      <th>3924</th>
      <th>3925</th>
      <th>3926</th>
      <th>3927</th>
      <th>3928</th>
      <th>3929</th>
      <th>3930</th>
      <th>3931</th>
      <th>3932</th>
      <th>3933</th>
      <th>3934</th>
      <th>3935</th>
      <th>3936</th>
      <th>3937</th>
      <th>3938</th>
      <th>3939</th>
      <th>3940</th>
      <th>3941</th>
      <th>3942</th>
      <th>3943</th>
      <th>3944</th>
      <th>3945</th>
      <th>3946</th>
      <th>3947</th>
      <th>3948</th>
      <th>3949</th>
      <th>3950</th>
      <th>3951</th>
      <th>3952</th>
      <th>3953</th>
      <th>3954</th>
      <th>3955</th>
      <th>3956</th>
      <th>3957</th>
      <th>3958</th>
      <th>3959</th>
      <th>3960</th>
      <th>3961</th>
      <th>3962</th>
      <th>3963</th>
      <th>3964</th>
      <th>3965</th>
      <th>3966</th>
      <th>3967</th>
      <th>3968</th>
      <th>3969</th>
      <th>3970</th>
      <th>3971</th>
      <th>3972</th>
      <th>3973</th>
      <th>3974</th>
      <th>3975</th>
      <th>3976</th>
      <th>3977</th>
      <th>3978</th>
      <th>3979</th>
      <th>3980</th>
      <th>3981</th>
      <th>3982</th>
      <th>3983</th>
      <th>3984</th>
      <th>3985</th>
      <th>3986</th>
      <th>3987</th>
      <th>3988</th>
      <th>3989</th>
      <th>3990</th>
      <th>3991</th>
      <th>3992</th>
      <th>3993</th>
      <th>3994</th>
      <th>3995</th>
      <th>3996</th>
      <th>3997</th>
      <th>3998</th>
      <th>3999</th>
      <th>4000</th>
      <th>4001</th>
      <th>4002</th>
      <th>4003</th>
      <th>4004</th>
      <th>4005</th>
      <th>4006</th>
      <th>4007</th>
      <th>4008</th>
      <th>4009</th>
      <th>4010</th>
      <th>4011</th>
      <th>4012</th>
      <th>4013</th>
      <th>4014</th>
      <th>4015</th>
      <th>4016</th>
      <th>4017</th>
      <th>4018</th>
      <th>4019</th>
      <th>4020</th>
      <th>4021</th>
      <th>4022</th>
      <th>4023</th>
      <th>4024</th>
      <th>4025</th>
      <th>4026</th>
      <th>4027</th>
      <th>4028</th>
      <th>4029</th>
      <th>4030</th>
      <th>4031</th>
      <th>4032</th>
      <th>4033</th>
      <th>4034</th>
      <th>4035</th>
      <th>4036</th>
      <th>4037</th>
      <th>4038</th>
      <th>4039</th>
      <th>4040</th>
      <th>4041</th>
      <th>4042</th>
      <th>4043</th>
      <th>4044</th>
      <th>4045</th>
      <th>4046</th>
      <th>4047</th>
      <th>4048</th>
      <th>4049</th>
      <th>4050</th>
      <th>4051</th>
      <th>4052</th>
      <th>4053</th>
      <th>4054</th>
      <th>4055</th>
      <th>4056</th>
      <th>4057</th>
      <th>4058</th>
      <th>4059</th>
      <th>4060</th>
      <th>4061</th>
      <th>4062</th>
      <th>4063</th>
      <th>4064</th>
      <th>4065</th>
      <th>4066</th>
      <th>4067</th>
      <th>4068</th>
      <th>4069</th>
      <th>4070</th>
      <th>4071</th>
      <th>4072</th>
      <th>4073</th>
      <th>4074</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>（2）目标变量提取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = df[<span class="string">&#x27;评价&#x27;</span>]</span><br><span class="line">y.head()</span><br></pre></td></tr></table></figure>




<pre><code>0    1
1    1
2    1
3    1
4    1
Name: 评价, dtype: int64
</code></pre>
<h3 id="16-2-3-神经网络模型的搭建与使用"><a href="#16-2-3-神经网络模型的搭建与使用" class="headerlink" title="16.2.3 神经网络模型的搭建与使用"></a>16.2.3 神经网络模型的搭建与使用</h3><p>1.划分训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.1</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>2.搭建神经网络模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line">mlp =MLPClassifier()  <span class="comment"># 因为模型运行具有随机性，如果想让每次运行结果一致，可以设置random_state随机参数为任一数字，如MLPClassifier(random_state=123)</span></span><br><span class="line">mlp.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>



<p><img src="%E4%B8%8B%E8%BD%BD%20(28).png" alt="下载 (28)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(29).png" alt="下载 (29)"></p>
<p>3.模型使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred = mlp.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred)  <span class="comment"># 因为模型运行具有随机性，所以这里得到的结果可能和书上的略有不同，如果想让每次运行结果一致，可以设置random_state随机参数为任一数字，如MLPClassifier(random_state=123)</span></span><br></pre></td></tr></table></figure>

<pre><code>[1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0
 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1
 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取预测准确度</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line">score</span><br></pre></td></tr></table></figure>




<pre><code>0.9814814814814815
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过模型自带的score()函数也可以获取预测准确度</span></span><br><span class="line">mlp.score(X_test, y_test)</span><br></pre></td></tr></table></figure>




<pre><code>0.9814814814814815
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自我体验</span></span><br><span class="line">comment = <span class="built_in">input</span>(<span class="string">&#x27;请输入您对本商品的评价：&#x27;</span>)</span><br><span class="line">comment = [<span class="string">&#x27; &#x27;</span>.join(jieba.cut(comment))]</span><br><span class="line"><span class="built_in">print</span>(comment)</span><br><span class="line">X_try = vect.transform(comment)</span><br><span class="line">y_pred = mlp.predict(X_try.toarray())</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure>

<pre><code>请输入您对本商品的评价：物流为什么这么慢，到手里都已经一周了。五星给手机，一星给物流。
[&#39;物流 为什么 这么 慢 ， 到 手里 都 已经 一周 了 。 五星 给 手机 ， 一星 给 物流 。&#39;]
[0]
</code></pre>
<p>预测结果为0，代表差评</p>
<p>4.模型对比</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 朴素贝叶斯模型对比</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line">nb_clf = GaussianNB()</span><br><span class="line">nb_clf.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line">y_pred = nb_clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>[1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1
 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1
 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1]
0.8703703703703703
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">数据科学</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/机器学习与深度学习//" class="article-tag-list-link color5">机器学习与深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/02/03/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part5/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-深入浅出python量化交易实战-part1" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/02/03/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApython%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98-part1/">深入浅出python量化交易实战-part1</a>
    </h1>
  

        
        <a href="/2023/02/03/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApython%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98-part1/" class="archive-article-date">
  	<time datetime="2023-02-03T11:56:03.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-02-03</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-从零开始"><a href="#1-从零开始" class="headerlink" title="1 从零开始"></a>1 从零开始</h1><h2 id="1-1-高频交易"><a href="#1-1-高频交易" class="headerlink" title="1.1 高频交易"></a>1.1 高频交易</h2><p>高频交易（HFT）是指在微秒范围内以极低延迟执行的金融工具的自动交易，美国，欧洲超过一半的股市交易采用高频交易</p>
<h2 id="1-2-因子投资"><a href="#1-2-因子投资" class="headerlink" title="1.2 因子投资"></a>1.2 因子投资</h2><p>1.资本资产定价模型（CAPM），其中最重要的组成部分是β系数，它用于表示某项资产的<strong>系统性风险</strong>，这种风险并非孤立却决于该资产，而是该资产相对于其他资产及整个市场的走向。但是只使用β这一个因子对资产回报率的预测并不准确，需要额外的风险因子。————因子投资</p>
<p>2.因子投资（多因子模型）中，因子定义为**<span class="burk">可以量化的信号、特征或其他变量</span><strong>，这些因子与资产回报率呈现明显的</strong>相关性**，并且在未来还要保持这种相关性。</p>
<p>3.多因子模型介绍：</p>
<p><strong>套利定价理论</strong>（APT）：认为证券的回报率与一组因子线性相关</p>
<p><strong>Fama-French三因子模型</strong>：指出证券回报率可以用市场资产组合、市值及账面市值比这三个因子来解释。</p>
<p><strong>Carhart四因子模型</strong>：在三因子模型的基础上，添加了动量因子</p>
<h2 id="1-3-数据"><a href="#1-3-数据" class="headerlink" title="1.3 数据"></a>1.3 数据</h2><p>1.这里的数据是包括任何可以使用机器学习提取交易信号的信息，即<strong>经济统计数据</strong>，<strong>市场交易数据</strong>，<strong>上市公司财报</strong>，<strong>信用卡销售数据</strong>，<strong>股民情绪分析</strong>，<strong>手机地理位置定位</strong>，<strong>爬虫抓取</strong>等等</p>
<p>2.例子：如果在某家上市公司公布财报数据之前，我们可以获取该公司在招聘网站上发布的<strong>招聘岗位数量</strong>，就可以先于财报数据发布了解到该公司的<strong>运营状况</strong>。假如该公司的招聘人数在上升，则可能说明该公司业绩良好，自然股票的价格也可能上涨；反之，加入该公司的招聘人数锐减，则说明该公司的经营状况可能有困难，则可能会导致该公司股价下跌。当然最直接有效的数据还是哪些能够直接体现<strong>用户消费</strong>的数据，如<strong>支付数据</strong>。</p>
<h2 id="1-4-交易策略和α因子"><a href="#1-4-交易策略和α因子" class="headerlink" title="1.4 交易策略和α因子"></a>1.4 交易策略和α因子</h2><p>通过各种数据源提取出<strong>有效信息</strong>，并且通过<strong>特征工程</strong>将数据转换成为<strong>阿尔法因子</strong>，再将这些因子拿来训练模型，使<strong>模型</strong>可以对交易品未来的趋势或价格变动做出<strong>预测</strong>，并触发买单或者卖单。例如，模型预测次日股价大涨，则下单买入，反之则卖出。</p>
<h2 id="1-5-用真实股票数据练手"><a href="#1-5-用真实股票数据练手" class="headerlink" title="1.5 用真实股票数据练手"></a>1.5 用真实股票数据练手</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">! pip install tushare <span class="comment"># 安装tushare库</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入tushare并重命名为ts</span></span><br><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line"><span class="comment">#查看tushare的版本信息</span></span><br><span class="line">ts.__version__</span><br></pre></td></tr></table></figure>




<pre><code>&#39;1.2.89&#39;
</code></pre>
<p>1.下载股票数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#指定一下获取股票数据的起始日期和截止日期</span></span><br><span class="line"><span class="comment">#这里就用2020年1月1日至3月18日的数据</span></span><br><span class="line">start_date = <span class="string">&#x27;2020-01-01&#x27;</span></span><br><span class="line">end_date = <span class="string">&#x27;2020-03-18&#x27;</span></span><br><span class="line"><span class="comment">#创建数据表，这里选择下载的股票代码为601318</span></span><br><span class="line"><span class="comment">#并把我们把设定的开始日期和截止日期作为参数传入</span></span><br><span class="line">data = ts.get_k_data(<span class="string">&#x27;601318&#x27;</span>,</span><br><span class="line">                          start = start_date,</span><br><span class="line">                          end = end_date)</span><br><span class="line">data = data.set_index(<span class="string">&#x27;date&#x27;</span>)</span><br><span class="line"><span class="comment">#下面来检查一下数据表的前5行</span></span><br><span class="line">data.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># date日期，open开盘价，close收盘价，high最高价，low最低价，volume成交量，code股票代码</span></span><br></pre></td></tr></table></figure>

<pre><code>本接口即将停止更新，请尽快使用Pro版接口：https://tushare.pro/document/2
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>volume</th>
      <th>code</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-01-02</th>
      <td>79.149</td>
      <td>79.369</td>
      <td>80.039</td>
      <td>79.129</td>
      <td>778252.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2020-01-03</th>
      <td>80.059</td>
      <td>79.449</td>
      <td>80.129</td>
      <td>79.149</td>
      <td>594980.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2020-01-06</th>
      <td>79.169</td>
      <td>78.849</td>
      <td>80.119</td>
      <td>78.749</td>
      <td>636448.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2020-01-07</th>
      <td>79.259</td>
      <td>79.399</td>
      <td>79.709</td>
      <td>78.919</td>
      <td>452188.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2020-01-08</th>
      <td>79.229</td>
      <td>78.249</td>
      <td>79.229</td>
      <td>77.749</td>
      <td>628053.0</td>
      <td>601318</td>
    </tr>
  </tbody>
</table>
</div>



<p>2.最简单的数据处理</p>
<p>股票每日的涨跌是用当日的收盘价减去前一个交易日的收盘价来计算的，可以在数据表中添加一个字段，用来表示当日股价较前一日的变化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#给新的字段命名为diff，代表difference</span></span><br><span class="line"><span class="comment">#用.diff()方法来计算每日股价变化情况</span></span><br><span class="line">data[<span class="string">&#x27;diff&#x27;</span>] = data[<span class="string">&#x27;close&#x27;</span>].diff()</span><br><span class="line"><span class="comment">#检查一下前5行</span></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>volume</th>
      <th>code</th>
      <th>diff</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-01-02</th>
      <td>79.149</td>
      <td>79.369</td>
      <td>80.039</td>
      <td>79.129</td>
      <td>778252.0</td>
      <td>601318</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2020-01-03</th>
      <td>80.059</td>
      <td>79.449</td>
      <td>80.129</td>
      <td>79.149</td>
      <td>594980.0</td>
      <td>601318</td>
      <td>0.08</td>
    </tr>
    <tr>
      <th>2020-01-06</th>
      <td>79.169</td>
      <td>78.849</td>
      <td>80.119</td>
      <td>78.749</td>
      <td>636448.0</td>
      <td>601318</td>
      <td>-0.60</td>
    </tr>
    <tr>
      <th>2020-01-07</th>
      <td>79.259</td>
      <td>79.399</td>
      <td>79.709</td>
      <td>78.919</td>
      <td>452188.0</td>
      <td>601318</td>
      <td>0.55</td>
    </tr>
    <tr>
      <th>2020-01-08</th>
      <td>79.229</td>
      <td>78.249</td>
      <td>79.229</td>
      <td>77.749</td>
      <td>628053.0</td>
      <td>601318</td>
      <td>-1.15</td>
    </tr>
  </tbody>
</table>
</div>



<p>3.设计最简单的交易策略</p>
<p>最简单的交易策略：如果当日股价<strong>下跌</strong>，我们就在下一个交易日开盘前挂单<strong>买入</strong>；反之，如果当日股价<strong>上涨</strong>，我们就在下一个交易日开盘前挂单<strong>卖出</strong>，循环这个步骤</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#此处会用到numpy，故导入</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#创建交易信号字段，命名为Signal</span></span><br><span class="line"><span class="comment">#如果diff值大于0，则Signal为1，否则为0</span></span><br><span class="line">data[<span class="string">&#x27;Signal&#x27;</span>] = np.where(data[<span class="string">&#x27;diff&#x27;</span>] &gt; <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"><span class="comment">#检查是否成功</span></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>volume</th>
      <th>code</th>
      <th>diff</th>
      <th>Signal</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-01-02</th>
      <td>79.149</td>
      <td>79.369</td>
      <td>80.039</td>
      <td>79.129</td>
      <td>778252.0</td>
      <td>601318</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-01-03</th>
      <td>80.059</td>
      <td>79.449</td>
      <td>80.129</td>
      <td>79.149</td>
      <td>594980.0</td>
      <td>601318</td>
      <td>0.08</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2020-01-06</th>
      <td>79.169</td>
      <td>78.849</td>
      <td>80.119</td>
      <td>78.749</td>
      <td>636448.0</td>
      <td>601318</td>
      <td>-0.60</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-01-07</th>
      <td>79.259</td>
      <td>79.399</td>
      <td>79.709</td>
      <td>78.919</td>
      <td>452188.0</td>
      <td>601318</td>
      <td>0.55</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2020-01-08</th>
      <td>79.229</td>
      <td>78.249</td>
      <td>79.229</td>
      <td>77.749</td>
      <td>628053.0</td>
      <td>601318</td>
      <td>-1.15</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>使用np.where()可以让程序判断每日股价是上涨还是下跌；如果<strong>上涨</strong>，交易信号为为<strong>1</strong>，代表<strong>卖出</strong>；否则交易信号为<strong>0</strong>，代表<strong>买入</strong></p>
<p>4.交易信号可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入画图工具matplotlib</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#设置画布的尺寸为10*5</span></span><br><span class="line">plt.figure(figsize = (<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">#使用折线图绘制出每天的收盘价</span></span><br><span class="line">plt.plot(data[<span class="string">&#x27;close&#x27;</span>],linewidth=<span class="number">2</span>, color=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line"><span class="comment">#如果当天股价上涨，标出卖出信号，用倒三角表示</span></span><br><span class="line">plt.scatter(data[<span class="string">&#x27;close&#x27;</span>].loc[data.Signal==<span class="number">1</span>].index,</span><br><span class="line">        data[<span class="string">&#x27;close&#x27;</span>][data.Signal==<span class="number">1</span>],</span><br><span class="line">        marker = <span class="string">&#x27;v&#x27;</span>, s=<span class="number">80</span>, c=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line"><span class="comment">#如果当天股价下跌给出买入信号，用正三角表示</span></span><br><span class="line">plt.scatter(data[<span class="string">&#x27;close&#x27;</span>].loc[data.Signal==<span class="number">0</span>].index,</span><br><span class="line">        data[<span class="string">&#x27;close&#x27;</span>][data.Signal==<span class="number">0</span>],</span><br><span class="line">        marker = <span class="string">&#x27;^&#x27;</span>, s=<span class="number">80</span>, c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.xticks([<span class="number">0</span>,<span class="number">12</span>,<span class="number">24</span>,<span class="number">36</span>,<span class="number">48</span>])</span><br><span class="line">plt.grid()</span><br><span class="line"><span class="comment">#将图像进行展示</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_22_0.png" alt="output_22_0"></p>
<p>【结果分析】<strong>倒三角</strong>所处的位置是股票收盘价较上一个交易日<strong>上涨</strong>的时刻，代表<strong>卖出</strong>；<strong>正三角</strong>所处的位置是股票收盘价较上一个交易日<strong>下跌</strong>的时刻，代表<strong>买入</strong>。直观来看，在这个时间范围内，股价整体呈现出<strong>下跌</strong>的天数大于<strong>上涨</strong>天数的情况。使用这个交易策略，且每次买入和卖出的股票数量相同的话，那么持有这只股票的仓位会越来越<strong>高</strong>（越买越多）</p>
<h1 id="2-回测与经典策略"><a href="#2-回测与经典策略" class="headerlink" title="2 回测与经典策略"></a>2 回测与经典策略</h1><h2 id="2-1-对“低买高卖”策略进行简单回测"><a href="#2-1-对“低买高卖”策略进行简单回测" class="headerlink" title="2.1 对“低买高卖”策略进行简单回测"></a>2.1 对“低买高卖”策略进行简单回测</h2><h3 id="2-1-1-下载数据并创建交易信号"><a href="#2-1-1-下载数据并创建交易信号" class="headerlink" title="2.1.1 下载数据并创建交易信号"></a>2.1.1 下载数据并创建交易信号</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入必要的库</span></span><br><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#指定下载股票的日期范围</span></span><br><span class="line">start_date = <span class="string">&#x27;2020-01-01&#x27;</span></span><br><span class="line">end_date = <span class="string">&#x27;2020-03-20&#x27;</span></span><br><span class="line"><span class="comment">#使用ts获取数据</span></span><br><span class="line"><span class="comment">#将时间范围作为参数传入</span></span><br><span class="line">zgpa = ts.get_k_data(<span class="string">&#x27;601318&#x27;</span>,</span><br><span class="line">                    start_date, end_date)</span><br><span class="line">zgpa = zgpa.set_index(<span class="string">&#x27;date&#x27;</span>)</span><br><span class="line"><span class="comment">#检查是否下载成功</span></span><br><span class="line">zgpa.head()</span><br></pre></td></tr></table></figure>

<pre><code>本接口即将停止更新，请尽快使用Pro版接口：https://tushare.pro/document/2
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>volume</th>
      <th>code</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-01-02</th>
      <td>79.149</td>
      <td>79.369</td>
      <td>80.039</td>
      <td>79.129</td>
      <td>778252.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2020-01-03</th>
      <td>80.059</td>
      <td>79.449</td>
      <td>80.129</td>
      <td>79.149</td>
      <td>594980.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2020-01-06</th>
      <td>79.169</td>
      <td>78.849</td>
      <td>80.119</td>
      <td>78.749</td>
      <td>636448.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2020-01-07</th>
      <td>79.259</td>
      <td>79.399</td>
      <td>79.709</td>
      <td>78.919</td>
      <td>452188.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2020-01-08</th>
      <td>79.229</td>
      <td>78.249</td>
      <td>79.229</td>
      <td>77.749</td>
      <td>628053.0</td>
      <td>601318</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下面我们来创建交易信号</span></span><br><span class="line"><span class="comment">#为了不影响原始数据，这里创建一个新的数据表</span></span><br><span class="line"><span class="comment">#只保留原始数据中的日期index</span></span><br><span class="line">zgpa_signal = pd.DataFrame(index = zgpa.index)</span><br><span class="line"><span class="comment">#为了更能体现股票的真实价值</span></span><br><span class="line"><span class="comment">#使用Adj Close调整价格作为股票价格</span></span><br><span class="line">zgpa_signal[<span class="string">&#x27;price&#x27;</span>] = zgpa[<span class="string">&#x27;close&#x27;</span>]</span><br><span class="line"><span class="comment">#增加一个字段，来存储股价的变化</span></span><br><span class="line">zgpa_signal[<span class="string">&#x27;diff&#x27;</span>] = zgpa_signal[<span class="string">&#x27;price&#x27;</span>].diff()</span><br><span class="line"><span class="comment">#增加diff字段后，第一行会出现空值，我们使用0来进行填补</span></span><br><span class="line">zgpa_signal = zgpa_signal.fillna(<span class="number">0.0</span>)</span><br><span class="line"><span class="comment">#如果股价上涨或不变，则标记为0</span></span><br><span class="line"><span class="comment">#如果股价下跌，则标记为1</span></span><br><span class="line">zgpa_signal[<span class="string">&#x27;signal&#x27;</span>] = np.where(zgpa_signal[<span class="string">&#x27;diff&#x27;</span>] &gt;= <span class="number">0</span>, <span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#接下来，根据交易信号的变化进行下单</span></span><br><span class="line"><span class="comment">#一般情况下，在A股市场，买入或卖出至少为100股，即1手</span></span><br><span class="line">zgpa_signal[<span class="string">&#x27;order&#x27;</span>] = zgpa_signal[<span class="string">&#x27;signal&#x27;</span>].diff()*<span class="number">100</span></span><br><span class="line"><span class="comment">#检查一下下单的情况</span></span><br><span class="line">zgpa_signal.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>diff</th>
      <th>signal</th>
      <th>order</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-01-02</th>
      <td>79.369</td>
      <td>0.00</td>
      <td>0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2020-01-03</th>
      <td>79.449</td>
      <td>0.08</td>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2020-01-06</th>
      <td>78.849</td>
      <td>-0.60</td>
      <td>1</td>
      <td>100.0</td>
    </tr>
    <tr>
      <th>2020-01-07</th>
      <td>79.399</td>
      <td>0.55</td>
      <td>0</td>
      <td>-100.0</td>
    </tr>
    <tr>
      <th>2020-01-08</th>
      <td>78.249</td>
      <td>-1.15</td>
      <td>1</td>
      <td>100.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>结果分析：在2020年1月6日这一天，股价下跌了大约6角，程序给出交易信号“1”，这时下单买入100股；而到了1月7日，股价上涨了0.55元，程序给出交易信号“0”，此时交易信号的变化为0-1&#x3D;-1，因此下单卖出100股。经过这一买一卖的交易，可赚到大约55元，看起来不错</p>
<h3 id="2-1-2-对交易策略进行简单回测"><a href="#2-1-2-对交易策略进行简单回测" class="headerlink" title="2.1.2 对交易策略进行简单回测"></a>2.1.2 对交易策略进行简单回测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#考虑到股价较高，我们初始给小瓦2万块钱让她去交易</span></span><br><span class="line">initial_cash = <span class="number">20000.00</span></span><br><span class="line"><span class="comment">#增加一个字段，代表小瓦交易的股票的市值</span></span><br><span class="line">zgpa_signal[<span class="string">&#x27;stock&#x27;</span>] = zgpa_signal[<span class="string">&#x27;order&#x27;</span>]*zgpa_signal[<span class="string">&#x27;price&#x27;</span>]</span><br><span class="line"><span class="comment">#两次买卖的订单变化之差就是某一时刻小瓦仓位的变化情况</span></span><br><span class="line"><span class="comment">#持仓股票的数量变化乘以现价，就是代表小瓦交易产生的现金流</span></span><br><span class="line"><span class="comment">#用初始资金减去现金流变化的累加，就是小瓦剩余的现金</span></span><br><span class="line">zgpa_signal[<span class="string">&#x27;cash&#x27;</span>] = initial_cash -\</span><br><span class="line">(zgpa_signal[<span class="string">&#x27;order&#x27;</span>].diff()*zgpa_signal[<span class="string">&#x27;price&#x27;</span>]).cumsum()</span><br><span class="line"><span class="comment">#而最股票的市值加上剩余的现金，就是小瓦的总资产</span></span><br><span class="line">zgpa_signal[<span class="string">&#x27;total&#x27;</span>] = zgpa_signal[<span class="string">&#x27;stock&#x27;</span>] + zgpa_signal[<span class="string">&#x27;cash&#x27;</span>]</span><br><span class="line"><span class="comment">#为了让小瓦直观看到自己的总资产变化</span></span><br><span class="line"><span class="comment">#我们用图形来进行展示</span></span><br><span class="line"><span class="comment">#设置图形的尺寸是10*6</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line"><span class="comment">#分别绘制总资产和持仓股票市值的变化</span></span><br><span class="line">plt.plot(zgpa_signal[<span class="string">&#x27;total&#x27;</span>])</span><br><span class="line">plt.plot(zgpa_signal[<span class="string">&#x27;order&#x27;</span>].cumsum()*zgpa_signal[<span class="string">&#x27;price&#x27;</span>],<span class="string">&#x27;--&#x27;</span>,</span><br><span class="line">        label=<span class="string">&#x27;stock value&#x27;</span>)</span><br><span class="line"><span class="comment">#增加网格，调整一下图注的位置，就可以显示图像了</span></span><br><span class="line">plt.xticks([<span class="number">0</span>,<span class="number">12</span>,<span class="number">24</span>,<span class="number">36</span>,<span class="number">48</span>])</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;center right&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_32_0.png" alt="output_32_0"></p>
<p>何为回测？使用这个策略模拟交易一段时间后总资产的变化情况</p>
<p>分析：从一月初到3.20，使用该低买高卖策略进行交易，总资产最后略微减少了。虽然从1月中旬到2月中旬，总资产也曾经有一定的增长，但涨幅也并不明显。当然，考虑在此期间，股市整体表现都不好，在这样的背景下，其策略没有让总资产大幅缩水，对一个新手来说，很不错</p>
<h3 id="2-1-3-关于回测更多"><a href="#2-1-3-关于回测更多" class="headerlink" title="2.1.3 关于回测更多"></a>2.1.3 关于回测更多</h3><p>回测：通过模拟算法进行交易的过程，用一些指标对交易策略进行评估。在我们所做的回测中，所测量的指标就是<strong>利润和损失</strong>（PnL）。不过在刚才的回测中，我们并没有考虑交易手续费和税费等成本，如果将这些附加成本也添加到模型中参与计算，算出来的就是<strong>净利润和损失</strong>（net PnL）。常用的指标还有年化收益、交易手数、风险敞口以及夏普指数等。</p>
<p><strong>风险敞口</strong>：指的是<strong>未加保护</strong>的风险，在股市中，就是指投资股票的资金。比如我有一万元，其中5000买了股票，另外5000买了保本的理财产品，那么买股票的5000元钱面临着股价下跌的风险，即风险敞口为5000元</p>
<p><strong>夏普指数</strong>（夏普比率）：将一组<strong>投资组合的回报率</strong>与<strong>无风险投资的回报率</strong>（如银行存款或国债）进行对比，看投资组合的回报会<strong>超过</strong>无风险投资回报率多少。夏普指数越高，说明投资组合的回报率越高；相反，如果投资组合的回报率不及无风险投资的回报，就说明这项投资是不应该进行的。</p>
<h2 id="2-2-经典策略之移动平均策略-趋势跟随策略之一"><a href="#2-2-经典策略之移动平均策略-趋势跟随策略之一" class="headerlink" title="2.2 经典策略之移动平均策略(趋势跟随策略之一)"></a>2.2 经典策略之移动平均策略(趋势跟随策略之一)</h2><h3 id="2-2-1-单一移动平均指标"><a href="#2-2-1-单一移动平均指标" class="headerlink" title="2.2.1 单一移动平均指标"></a>2.2.1 单一移动平均指标</h3><p><strong>移动平均策略</strong>：当股价上升且<strong>向上穿过</strong>N日的均线时，说明股价在向上突破，此时<strong>下单买入</strong>；当股价下降且<strong>向下穿过</strong>N日均线时，说明股价整体出现下跌的趋势，此时下单卖出。或者当M日均线上升穿过N日均线（M&lt;N）时，说明股票处于上升的趋势，应该下单买入；反之应下单卖出。</p>
<p>本节需要用到指标为均线，使用2.1节中的股票数据，选取10个交易日的股票均价作为均线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这里使用10日均线</span></span><br><span class="line">period = <span class="number">10</span></span><br><span class="line"><span class="comment">#设置一个空列表，用来存储每10天的价格</span></span><br><span class="line">avg_10 = []</span><br><span class="line"><span class="comment">#再设置一个空列表，用来存储每10天价格的均值</span></span><br><span class="line">avg_value = []</span><br><span class="line"><span class="comment">#设置一个循环</span></span><br><span class="line"><span class="keyword">for</span> price <span class="keyword">in</span> zgpa[<span class="string">&#x27;close&#x27;</span>]:</span><br><span class="line">    <span class="comment">#把每天的价格传入到avg_10列表</span></span><br><span class="line">    avg_10.append(price)</span><br><span class="line">    <span class="comment">#当列表中存储的数值多于10个时</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(avg_10) &gt; period:</span><br><span class="line">        <span class="comment">#就把前面传入的价格数据删掉，确保列表中只有10天的数据</span></span><br><span class="line">        <span class="keyword">del</span> avg_10[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">#将10天数据的均值传入到avg_value列表中</span></span><br><span class="line">    avg_value.append(np.mean(avg_10))</span><br><span class="line"><span class="comment">#把计算好的10日均价写到股票价格数据表中</span></span><br><span class="line">zgpa = zgpa.assign(avg_10 = pd.Series(avg_value, index = zgpa.index))</span><br><span class="line"><span class="comment">#检查一下是否添加成功</span></span><br><span class="line">zgpa.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>volume</th>
      <th>code</th>
      <th>avg_10</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-01-02</th>
      <td>79.149</td>
      <td>79.369</td>
      <td>80.039</td>
      <td>79.129</td>
      <td>778252.0</td>
      <td>601318</td>
      <td>79.369000</td>
    </tr>
    <tr>
      <th>2020-01-03</th>
      <td>80.059</td>
      <td>79.449</td>
      <td>80.129</td>
      <td>79.149</td>
      <td>594980.0</td>
      <td>601318</td>
      <td>79.409000</td>
    </tr>
    <tr>
      <th>2020-01-06</th>
      <td>79.169</td>
      <td>78.849</td>
      <td>80.119</td>
      <td>78.749</td>
      <td>636448.0</td>
      <td>601318</td>
      <td>79.222333</td>
    </tr>
    <tr>
      <th>2020-01-07</th>
      <td>79.259</td>
      <td>79.399</td>
      <td>79.709</td>
      <td>78.919</td>
      <td>452188.0</td>
      <td>601318</td>
      <td>79.266500</td>
    </tr>
    <tr>
      <th>2020-01-08</th>
      <td>79.229</td>
      <td>78.249</td>
      <td>79.229</td>
      <td>77.749</td>
      <td>628053.0</td>
      <td>601318</td>
      <td>79.063000</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以看到，数据表中多出一个字段avg_10。该字段存储的是10日内股票的均价。而在前9天中，由于数据不足10天，均价计算的是自有数据以来，截至到当前的股票均价</p>
<p>为了直观地展示股价与均价的关系，可以进行数据可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># SimHei表示简体黑体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span>  <span class="comment"># 为了正确显示负号</span></span><br><span class="line"><span class="comment">#设置图像尺寸为10*6</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line"><span class="comment">#绘制股价的变化</span></span><br><span class="line">plt.plot(zgpa[<span class="string">&#x27;close&#x27;</span>],lw=<span class="number">2</span>, c=<span class="string">&#x27;k&#x27;</span>,label=<span class="string">&#x27;股价&#x27;</span>)</span><br><span class="line"><span class="comment">#绘制10日均线</span></span><br><span class="line">plt.plot(zgpa[<span class="string">&#x27;avg_10&#x27;</span>], <span class="string">&#x27;--&#x27;</span>,lw=<span class="number">2</span>, c=<span class="string">&#x27;b&#x27;</span>,label=<span class="string">&#x27;10日均价&#x27;</span>)</span><br><span class="line"><span class="comment">#添加图注和网格</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xticks([<span class="number">0</span>,<span class="number">12</span>,<span class="number">24</span>,<span class="number">36</span>,<span class="number">48</span>])</span><br><span class="line"><span class="comment">#将图像进行显示</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_46_0.png" alt="output_46_0"></p>
<p>整体来看，在此期间，该股的整体趋势处于下行，不过不要紧，我们就基于这种“逆境”来尝试创建交易策略</p>
<h3 id="2-2-2-双移动平均策略的实现"><a href="#2-2-2-双移动平均策略的实现" class="headerlink" title="2.2.2 双移动平均策略的实现"></a>2.2.2 双移动平均策略的实现</h3><p>双移动平均策略就是使用<strong>两条均线</strong>来判断股价未来的走势。两条均线中，一条是<strong>长期均线</strong>（如10日均线），另一条是<strong>短期均线</strong>（如5日均线）。</p>
<p>这种策略基于这样一种假设：股票价格的动量会朝着<strong>短期均线</strong>的方向移动。当<strong>短期均线穿过长期均线</strong>，超过长期均线时，动量将向上，此时股价可能会<strong>上涨</strong>。然而，如果短期均线的移动方向相反，则股价可能<strong>下跌</strong>。</p>
<p>根据此理论，创建一个双移动平均交易策略</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#新建一个数据表，命名为strategy（策略）</span></span><br><span class="line"><span class="comment">#序号保持和原始数据一致</span></span><br><span class="line">strategy = pd.DataFrame(index = zgpa.index)</span><br><span class="line"><span class="comment">#添加一个signal字段，用来存储交易信号</span></span><br><span class="line">strategy[<span class="string">&#x27;signal&#x27;</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment">#将5日均价保存到avg_5这个字段</span></span><br><span class="line">strategy[<span class="string">&#x27;avg_5&#x27;</span>] = zgpa[<span class="string">&#x27;close&#x27;</span>].rolling(<span class="number">5</span>).mean()</span><br><span class="line"><span class="comment">#同样，将10日均价保存到avg_10</span></span><br><span class="line">strategy[<span class="string">&#x27;avg_10&#x27;</span>] = zgpa[<span class="string">&#x27;close&#x27;</span>].rolling(<span class="number">10</span>).mean()</span><br><span class="line"><span class="comment">#当5日均价大于10日均价时，标记为1</span></span><br><span class="line"><span class="comment">#反之标记为0</span></span><br><span class="line">strategy[<span class="string">&#x27;signal&#x27;</span>] = np.where(strategy[<span class="string">&#x27;avg_5&#x27;</span>]&gt;strategy[<span class="string">&#x27;avg_10&#x27;</span>], <span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment">#根据交易信号的变化下单，当交易信号从0变成1时买入</span></span><br><span class="line"><span class="comment">#交易信号从1变成0时卖出</span></span><br><span class="line"><span class="comment">#交易信号不变时不下单</span></span><br><span class="line">strategy[<span class="string">&#x27;order&#x27;</span>] = strategy[<span class="string">&#x27;signal&#x27;</span>].diff()</span><br><span class="line"><span class="comment">#查看数据表后10行</span></span><br><span class="line">strategy.tail(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>signal</th>
      <th>avg_5</th>
      <th>avg_10</th>
      <th>order</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-03-09</th>
      <td>1</td>
      <td>73.563</td>
      <td>73.117</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2020-03-10</th>
      <td>1</td>
      <td>73.501</td>
      <td>73.016</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2020-03-11</th>
      <td>1</td>
      <td>73.015</td>
      <td>72.848</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2020-03-12</th>
      <td>0</td>
      <td>71.855</td>
      <td>72.534</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <th>2020-03-13</th>
      <td>0</td>
      <td>70.615</td>
      <td>72.233</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2020-03-16</th>
      <td>0</td>
      <td>69.387</td>
      <td>71.475</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2020-03-17</th>
      <td>0</td>
      <td>67.979</td>
      <td>70.740</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2020-03-18</th>
      <td>0</td>
      <td>66.323</td>
      <td>69.669</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2020-03-19</th>
      <td>0</td>
      <td>64.555</td>
      <td>68.205</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2020-03-20</th>
      <td>0</td>
      <td>63.413</td>
      <td>67.014</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>3.9这一天，5日均线大于10日均线，故程序给出的交易信号是1；在3.10这一天，5日均线依然大于10日均线，交易信号不变，仍然是1，所以这一天不进行任何交易；但到了3.12，5日均线小于10日均线，交易信号变为0，与前一天相比，交易信号的变化为-1，所以下单卖出一手股票。</p>
<p>可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建尺寸为10*5的画布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">#使用实线绘制股价</span></span><br><span class="line">plt.plot(zgpa[<span class="string">&#x27;close&#x27;</span>],lw=<span class="number">2</span>,label=<span class="string">&#x27;price&#x27;</span>)</span><br><span class="line"><span class="comment">#使用虚线绘制5日均线</span></span><br><span class="line">plt.plot(strategy[<span class="string">&#x27;avg_5&#x27;</span>],lw=<span class="number">2</span>,ls=<span class="string">&#x27;--&#x27;</span>,label=<span class="string">&#x27;avg5&#x27;</span>)</span><br><span class="line"><span class="comment">#使用-.风格绘制10日均线</span></span><br><span class="line">plt.plot(strategy[<span class="string">&#x27;avg_10&#x27;</span>],lw=<span class="number">2</span>,ls=<span class="string">&#x27;-.&#x27;</span>,label=<span class="string">&#x27;avg10&#x27;</span>)</span><br><span class="line"><span class="comment">#将买入信号用正三角进行标示</span></span><br><span class="line">plt.scatter(strategy.loc[strategy.order==<span class="number">1</span>].index,</span><br><span class="line">           zgpa[<span class="string">&#x27;close&#x27;</span>][strategy.order==<span class="number">1</span>],</span><br><span class="line">           marker = <span class="string">&#x27;^&#x27;</span>, s=<span class="number">80</span>,color=<span class="string">&#x27;r&#x27;</span>,label=<span class="string">&#x27;Buy&#x27;</span>)</span><br><span class="line"><span class="comment">#将卖出信号用倒三角进行标示</span></span><br><span class="line">plt.scatter(strategy.loc[strategy.order==-<span class="number">1</span>].index,</span><br><span class="line">           zgpa[<span class="string">&#x27;close&#x27;</span>][strategy.order==-<span class="number">1</span>],</span><br><span class="line">           marker = <span class="string">&#x27;v&#x27;</span>, s=<span class="number">80</span>,color=<span class="string">&#x27;g&#x27;</span>,label=<span class="string">&#x27;Sell&#x27;</span>)</span><br><span class="line"><span class="comment">#添加图注</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xticks([<span class="number">0</span>,<span class="number">12</span>,<span class="number">24</span>,<span class="number">36</span>,<span class="number">48</span>])</span><br><span class="line"><span class="comment">#添加网格以便于观察</span></span><br><span class="line">plt.grid()</span><br><span class="line"><span class="comment">#显示图像</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_54_0.png" alt="output_54_0"></p>
<p>结果分析：使用移动平均策略，在选取的时间范围内一共进行了<strong>6笔交易，其中3笔买入，3笔卖出</strong>。由于在该时间范围内，该股的价格一直处于下跌的趋势，通过肉眼也可以看出，每次卖出的价格都要低于买入的价格，总体应该是亏损的状态。</p>
<h3 id="2-2-3-对双移动平均策略进行回测"><a href="#2-2-3-对双移动平均策略进行回测" class="headerlink" title="2.2.3 对双移动平均策略进行回测"></a>2.2.3 对双移动平均策略进行回测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这次我们还是给小瓦2万块钱的启动资金</span></span><br><span class="line">initial_cash = <span class="number">20000</span></span><br><span class="line"><span class="comment">#新建一个数据表positions，序号保持和strategy数据表一致</span></span><br><span class="line"><span class="comment">#用0将空值进行替换</span></span><br><span class="line">positions = pd.DataFrame(index = strategy.index).fillna(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#因为A股买卖都是最低100股</span></span><br><span class="line"><span class="comment">#因此设置stock字段为交易信号的100倍</span></span><br><span class="line">positions[<span class="string">&#x27;stock&#x27;</span>] = strategy[<span class="string">&#x27;signal&#x27;</span>] * <span class="number">100</span></span><br><span class="line"><span class="comment">#创建投资组合数据表，用持仓的股票数量乘股价得出持仓的股票市值</span></span><br><span class="line">portfolio = pd.DataFrame(index = strategy.index)</span><br><span class="line">portfolio[<span class="string">&#x27;stock value&#x27;</span>] = positions.multiply(zgpa[<span class="string">&#x27;close&#x27;</span>], axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#同样仓位的变化就是下单的数量</span></span><br><span class="line">order = positions.diff()</span><br><span class="line"><span class="comment">#用初始资金减去下单金额的总和就是剩余的资金</span></span><br><span class="line">portfolio[<span class="string">&#x27;cash&#x27;</span>] = initial_cash - order.multiply(zgpa[<span class="string">&#x27;close&#x27;</span>],</span><br><span class="line">                                                 axis=<span class="number">0</span>).cumsum()</span><br><span class="line"><span class="comment">#剩余的资金+持仓股票市值即为总资产</span></span><br><span class="line">portfolio[<span class="string">&#x27;total&#x27;</span>] = portfolio[<span class="string">&#x27;cash&#x27;</span>] + portfolio[<span class="string">&#x27;stock value&#x27;</span>]</span><br><span class="line"><span class="comment">#检查一下后10行</span></span><br><span class="line">portfolio.tail(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>stock value</th>
      <th>cash</th>
      <th>total</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-03-09</th>
      <td>7138.9</td>
      <td>12316.1</td>
      <td>19455.0</td>
    </tr>
    <tr>
      <th>2020-03-10</th>
      <td>7242.9</td>
      <td>12316.1</td>
      <td>19559.0</td>
    </tr>
    <tr>
      <th>2020-03-11</th>
      <td>7139.9</td>
      <td>12316.1</td>
      <td>19456.0</td>
    </tr>
    <tr>
      <th>2020-03-12</th>
      <td>0.0</td>
      <td>19306.0</td>
      <td>19306.0</td>
    </tr>
    <tr>
      <th>2020-03-13</th>
      <td>0.0</td>
      <td>19306.0</td>
      <td>19306.0</td>
    </tr>
    <tr>
      <th>2020-03-16</th>
      <td>0.0</td>
      <td>19306.0</td>
      <td>19306.0</td>
    </tr>
    <tr>
      <th>2020-03-17</th>
      <td>0.0</td>
      <td>19306.0</td>
      <td>19306.0</td>
    </tr>
    <tr>
      <th>2020-03-18</th>
      <td>0.0</td>
      <td>19306.0</td>
      <td>19306.0</td>
    </tr>
    <tr>
      <th>2020-03-19</th>
      <td>0.0</td>
      <td>19306.0</td>
      <td>19306.0</td>
    </tr>
    <tr>
      <th>2020-03-20</th>
      <td>0.0</td>
      <td>19306.0</td>
      <td>19306.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>结果分析：截至2020年3月20日，持仓仓位为0，此时的总资产只剩19306元，相比于初始的20000元，总资产缩水了694元。虽然没有赚到钱，但也没有亏损太多</p>
<p>可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建10*5的画布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">#绘制总资产曲线</span></span><br><span class="line">plt.plot(portfolio[<span class="string">&#x27;total&#x27;</span>], lw=<span class="number">2</span>, label=<span class="string">&#x27;total&#x27;</span>)</span><br><span class="line"><span class="comment">#绘制持仓股票市值曲线</span></span><br><span class="line">plt.plot(portfolio[<span class="string">&#x27;stock value&#x27;</span>],lw=<span class="number">2</span>,ls=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&#x27;stock value&#x27;</span>)</span><br><span class="line"><span class="comment">#添加图注</span></span><br><span class="line">plt.legend()</span><br><span class="line"><span class="comment">#添加网格</span></span><br><span class="line">plt.grid()</span><br><span class="line">plt.xticks([<span class="number">0</span>,<span class="number">12</span>,<span class="number">24</span>,<span class="number">36</span>,<span class="number">48</span>])</span><br><span class="line"><span class="comment">#展示图像</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_60_0.png" alt="output_60_0"></p>
<p>结果分析：使用双移动平均策略进行交易，在选定的时间范围内，总资产也轻微减少了。其表现也没有“低买高卖”策略更加出色。仔细分析该策略会发现，我们<strong>持仓的时间</strong>要比使用“低买高卖”策略短很多；而在3.12以后，一直保持着<strong>空仓</strong>的状态，避免了股价大幅下跌带来的损失。</p>
<p>经过测试，双移动平均策略作为经典策略之一，有一定的可取之处，但是在股价下行的趋势中，也没有实现“逆势赚钱”</p>
<h2 id="2-3-经典策略之海龟策略"><a href="#2-3-经典策略之海龟策略" class="headerlink" title="2.3 经典策略之海龟策略"></a>2.3 经典策略之海龟策略</h2><p>核心要点：在股价<strong>超过过去N个交易日</strong>的<strong>股价最高点</strong>时买入，在股价<strong>低于过去N个交易日</strong>的<strong>股价最低点</strong>时卖出。上述的若干个最高点和最低点会组成一个通道，称为<strong>唐奇安通道</strong></p>
<h3 id="2-3-1-使用海龟策略生成交易信号"><a href="#2-3-1-使用海龟策略生成交易信号" class="headerlink" title="2.3.1 使用海龟策略生成交易信号"></a>2.3.1 使用海龟策略生成交易信号</h3><p>使用过去N天的股价最高点和过去N天的股价最低点生成唐奇安通道。一般来说，N会设置为20.不过因为我们下载的股票数据时间范围跨度比较小，所以选择了使用过去5日的股价最高点和最低点来进行演示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建一个名为turtle的数据表，使用原始数据表的日期序号</span></span><br><span class="line">turtle = pd.DataFrame(index = zgpa.index)</span><br><span class="line"><span class="comment">#设置唐奇安通道的上沿为前5天股价的最高点</span></span><br><span class="line">turtle[<span class="string">&#x27;high&#x27;</span>] = zgpa[<span class="string">&#x27;close&#x27;</span>].shift(<span class="number">1</span>).rolling(<span class="number">5</span>).<span class="built_in">max</span>()</span><br><span class="line"><span class="comment">#设置唐奇安通道的下沿为过去5天的最低点</span></span><br><span class="line">turtle[<span class="string">&#x27;low&#x27;</span>] = zgpa[<span class="string">&#x27;close&#x27;</span>].shift(<span class="number">1</span>).rolling(<span class="number">5</span>).<span class="built_in">min</span>()</span><br><span class="line"><span class="comment">#当股价突破上沿时，发出买入信号</span></span><br><span class="line">turtle[<span class="string">&#x27;buy&#x27;</span>] = zgpa[<span class="string">&#x27;close&#x27;</span>] &gt; turtle[<span class="string">&#x27;high&#x27;</span>]</span><br><span class="line"><span class="comment">#当股价突破下沿时，发出卖出信号</span></span><br><span class="line">turtle[<span class="string">&#x27;sell&#x27;</span>] = zgpa[<span class="string">&#x27;close&#x27;</span>] &lt; turtle[<span class="string">&#x27;low&#x27;</span>]</span><br><span class="line"><span class="comment">#检查信号创建情况</span></span><br><span class="line">turtle.tail()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>high</th>
      <th>low</th>
      <th>buy</th>
      <th>sell</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-03-16</th>
      <td>72.429</td>
      <td>67.959</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2020-03-17</th>
      <td>72.429</td>
      <td>65.249</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2020-03-18</th>
      <td>71.399</td>
      <td>65.249</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2020-03-19</th>
      <td>69.899</td>
      <td>63.119</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2020-03-20</th>
      <td>67.959</td>
      <td>61.059</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>



<p>结果分析：high存储的是唐奇安通道的上沿数据，low中存储的是唐奇安通道的下沿；buy如果为True，则为买入信号；sell如果为True，则为卖出信号；而当buy和sell都是False时，则不进行下单。</p>
<p>实际上，在唐奇安通道中，还有一条中线，中线的值是上沿和下沿的均值，本例进行了简化处理</p>
<h3 id="2-3-2-根据交易信号和仓位进行下单"><a href="#2-3-2-根据交易信号和仓位进行下单" class="headerlink" title="2.3.2 根据交易信号和仓位进行下单"></a>2.3.2 根据交易信号和仓位进行下单</h3><p>我们根据生成的交易信号来下单。注意：当程序给出交易信号时，还要结合仓位来判断：</p>
<p>当交易信号为<strong>买入</strong>且<strong>空仓</strong>时，我们才会下<strong>买入</strong>订单；</p>
<p>而交易信号为<strong>卖出</strong>且有<strong>持仓股票</strong>时，我们才会下<strong>卖出</strong>订单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始的订单状态为0</span></span><br><span class="line">turtle[<span class="string">&#x27;orders&#x27;</span>]=<span class="number">0</span></span><br><span class="line"><span class="comment">#初始的仓位为0</span></span><br><span class="line">position = <span class="number">0</span></span><br><span class="line"><span class="comment">#设置循环，遍历turtle数据表</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(turtle)):</span><br><span class="line">    <span class="comment">#当买入信号为True且仓位为0时下单买入1手</span></span><br><span class="line">    <span class="keyword">if</span> turtle.buy[k] <span class="keyword">and</span> position ==<span class="number">0</span>:</span><br><span class="line">        <span class="comment">#修改对应的orders值为1</span></span><br><span class="line">        turtle.orders.values[k] = <span class="number">1</span></span><br><span class="line">        <span class="comment">#仓位也增加1手</span></span><br><span class="line">        position = <span class="number">1</span></span><br><span class="line">    <span class="comment">#而当卖出信号为True且有持仓时买出1手</span></span><br><span class="line">    <span class="keyword">elif</span> turtle.sell[k] <span class="keyword">and</span> position &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment">#orders的值修改为-1</span></span><br><span class="line">        turtle.orders.values[k] = -<span class="number">1</span></span><br><span class="line">        <span class="comment">#仓位相应清零</span></span><br><span class="line">        position = <span class="number">0</span>   </span><br><span class="line"><span class="comment">#检查是否成功</span></span><br><span class="line">turtle.tail(<span class="number">15</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>high</th>
      <th>low</th>
      <th>buy</th>
      <th>sell</th>
      <th>orders</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-03-02</th>
      <td>73.969</td>
      <td>70.969</td>
      <td>False</td>
      <td>False</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-03</th>
      <td>73.439</td>
      <td>70.969</td>
      <td>False</td>
      <td>False</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-04</th>
      <td>73.079</td>
      <td>70.969</td>
      <td>True</td>
      <td>False</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2020-03-05</th>
      <td>73.829</td>
      <td>70.969</td>
      <td>True</td>
      <td>False</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-06</th>
      <td>75.699</td>
      <td>70.969</td>
      <td>False</td>
      <td>False</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-09</th>
      <td>75.699</td>
      <td>72.739</td>
      <td>False</td>
      <td>True</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>2020-03-10</th>
      <td>75.699</td>
      <td>71.389</td>
      <td>False</td>
      <td>False</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-11</th>
      <td>75.699</td>
      <td>71.389</td>
      <td>False</td>
      <td>False</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-12</th>
      <td>75.699</td>
      <td>71.389</td>
      <td>False</td>
      <td>True</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-13</th>
      <td>74.159</td>
      <td>69.899</td>
      <td>False</td>
      <td>True</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-16</th>
      <td>72.429</td>
      <td>67.959</td>
      <td>False</td>
      <td>True</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-17</th>
      <td>72.429</td>
      <td>65.249</td>
      <td>False</td>
      <td>False</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-18</th>
      <td>71.399</td>
      <td>65.249</td>
      <td>False</td>
      <td>True</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-19</th>
      <td>69.899</td>
      <td>63.119</td>
      <td>False</td>
      <td>True</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2020-03-20</th>
      <td>67.959</td>
      <td>61.059</td>
      <td>False</td>
      <td>False</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>结果分析：在3.4日，程序下了买入单，在3.9日，程序下了卖出单</p>
<p>可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># SimHei表示简体黑体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span>  <span class="comment"># 为了正确显示负号</span></span><br><span class="line"><span class="comment">#创建10*5的画布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">#绘制股价的折线图</span></span><br><span class="line">plt.plot(zgpa[<span class="string">&#x27;close&#x27;</span>],lw=<span class="number">2</span>,label=<span class="string">&#x27;股价&#x27;</span>)</span><br><span class="line"><span class="comment">#绘制唐奇安通道上沿</span></span><br><span class="line">plt.plot(turtle[<span class="string">&#x27;high&#x27;</span>],lw=<span class="number">2</span>, ls=<span class="string">&#x27;--&#x27;</span>,c=<span class="string">&#x27;r&#x27;</span>,label=<span class="string">&#x27;上沿&#x27;</span>)</span><br><span class="line"><span class="comment">#绘制唐奇安通道下沿</span></span><br><span class="line">plt.plot(turtle[<span class="string">&#x27;low&#x27;</span>],lw=<span class="number">2</span>,ls=<span class="string">&#x27;--&#x27;</span>,c=<span class="string">&#x27;g&#x27;</span>,label=<span class="string">&#x27;下沿&#x27;</span>)</span><br><span class="line"><span class="comment">#标出买入订单，用正三角标记</span></span><br><span class="line">plt.scatter(turtle.loc[turtle.orders==<span class="number">1</span>].index,</span><br><span class="line">           zgpa[<span class="string">&#x27;close&#x27;</span>][turtle.orders==<span class="number">1</span>],</span><br><span class="line">           marker=<span class="string">&#x27;^&#x27;</span>,s=<span class="number">80</span>,color=<span class="string">&#x27;r&#x27;</span>,label=<span class="string">&#x27;Buy&#x27;</span>)</span><br><span class="line"><span class="comment">#标出卖出订单，用倒三角标记</span></span><br><span class="line">plt.scatter(turtle.loc[turtle.orders==-<span class="number">1</span>].index,</span><br><span class="line">           zgpa[<span class="string">&#x27;close&#x27;</span>][turtle.orders==-<span class="number">1</span>],</span><br><span class="line">           marker=<span class="string">&#x27;v&#x27;</span>,s=<span class="number">80</span>,color=<span class="string">&#x27;g&#x27;</span>,label=<span class="string">&#x27;Sell&#x27;</span>)</span><br><span class="line"><span class="comment">#添加网格、图注并显示</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xticks([<span class="number">0</span>,<span class="number">12</span>,<span class="number">24</span>,<span class="number">36</span>,<span class="number">48</span>])</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_74_0.png" alt="output_74_0"></p>
<p>结果分析：当股价第一次突破唐奇安通道<strong>上沿</strong>时，程序进性了<strong>买入</strong>，但随后的几天中，股价再次突破了上沿，但由于此时已经有1手持仓，故没有再次买入。之后股价<strong>急转直下</strong>，突破了通道<strong>下沿</strong>，程序下单<strong>卖出</strong>。以此类推，在选定的时间范围内，程序进行了6笔交易。</p>
<h3 id="2-3-3-对海龟交易进行回测"><a href="#2-3-3-对海龟交易进行回测" class="headerlink" title="2.3.3 对海龟交易进行回测"></a>2.3.3 对海龟交易进行回测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># SimHei表示简体黑体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span>  <span class="comment"># 为了正确显示负号</span></span><br><span class="line"><span class="comment">#再次给小瓦2万块初始资金</span></span><br><span class="line">initial_cash = <span class="number">20000</span></span><br><span class="line"><span class="comment">#创建新的数据表，序号和turtle数据表一致</span></span><br><span class="line">positions = pd.DataFrame(index=turtle.index).fillna(<span class="number">0.0</span>)</span><br><span class="line"><span class="comment">#每次交易为1手，即100股，仓位即买单和卖单的累积加和</span></span><br><span class="line">positions[<span class="string">&#x27;stock&#x27;</span>] = <span class="number">100</span> * turtle[<span class="string">&#x27;orders&#x27;</span>].cumsum()</span><br><span class="line"><span class="comment">#创建投资组合数据表</span></span><br><span class="line">portfolio = positions.multiply(zgpa[<span class="string">&#x27;close&#x27;</span>], axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#持仓市值为持仓股票数乘以股价</span></span><br><span class="line">portfolio[<span class="string">&#x27;holding_values&#x27;</span>] = (positions.multiply(zgpa[<span class="string">&#x27;close&#x27;</span>], axis=<span class="number">0</span>))</span><br><span class="line"><span class="comment">#计算出仓位的变化</span></span><br><span class="line">pos_diff = positions.diff()</span><br><span class="line"><span class="comment">#剩余的现金是初始资金减去仓位变化产生的现金流累计加和</span></span><br><span class="line">portfolio[<span class="string">&#x27;cash&#x27;</span>] = initial_cash - (pos_diff.multiply(zgpa[<span class="string">&#x27;close&#x27;</span>], axis=<span class="number">0</span>)).cumsum()</span><br><span class="line"><span class="comment">#总资产即为持仓股票市值加剩余现金</span></span><br><span class="line">portfolio[<span class="string">&#x27;total&#x27;</span>] = portfolio[<span class="string">&#x27;cash&#x27;</span>] + portfolio[<span class="string">&#x27;holding_values&#x27;</span>]</span><br><span class="line"><span class="comment">#使用可视化的方式展示</span></span><br><span class="line"><span class="comment">#下面的代码都很熟悉了，就不逐行注释了</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(portfolio[<span class="string">&#x27;total&#x27;</span>],label=<span class="string">&#x27;总资产&#x27;</span>)</span><br><span class="line">plt.plot(portfolio[<span class="string">&#x27;holding_values&#x27;</span>],<span class="string">&#x27;--&#x27;</span>,label=<span class="string">&#x27;持仓价值&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xticks([<span class="number">0</span>,<span class="number">12</span>,<span class="number">24</span>,<span class="number">36</span>,<span class="number">48</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_77_0.png" alt="output_77_0"></p>
<p>结果分析：与使用双移动均线相似，在整个股价变动明显的情况下，总资产略有减少</p>
<p>为了对比海龟策略和双移动平均策略的业绩表现，我们可以可靠使用海龟策略后，我们的总资产究竟少了多少</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#检查最后若干天小瓦的资产情况</span></span><br><span class="line">portfolio.tail(<span class="number">13</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>stock</th>
      <th>holding_values</th>
      <th>cash</th>
      <th>total</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-03-04</th>
      <td>7382.9</td>
      <td>7382.9</td>
      <td>12327.1</td>
      <td>19710.0</td>
    </tr>
    <tr>
      <th>2020-03-05</th>
      <td>7569.9</td>
      <td>7569.9</td>
      <td>12327.1</td>
      <td>19897.0</td>
    </tr>
    <tr>
      <th>2020-03-06</th>
      <td>7415.9</td>
      <td>7415.9</td>
      <td>12327.1</td>
      <td>19743.0</td>
    </tr>
    <tr>
      <th>2020-03-09</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>19466.0</td>
      <td>19466.0</td>
    </tr>
    <tr>
      <th>2020-03-10</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>19466.0</td>
      <td>19466.0</td>
    </tr>
    <tr>
      <th>2020-03-11</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>19466.0</td>
      <td>19466.0</td>
    </tr>
    <tr>
      <th>2020-03-12</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>19466.0</td>
      <td>19466.0</td>
    </tr>
    <tr>
      <th>2020-03-13</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>19466.0</td>
      <td>19466.0</td>
    </tr>
    <tr>
      <th>2020-03-16</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>19466.0</td>
      <td>19466.0</td>
    </tr>
    <tr>
      <th>2020-03-17</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>19466.0</td>
      <td>19466.0</td>
    </tr>
    <tr>
      <th>2020-03-18</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>19466.0</td>
      <td>19466.0</td>
    </tr>
    <tr>
      <th>2020-03-19</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>19466.0</td>
      <td>19466.0</td>
    </tr>
    <tr>
      <th>2020-03-20</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>19466.0</td>
      <td>19466.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>结果分析：最后总资产为19466元，相比于初始资金20000元，减少了544元；而使用双移动平均策略进行交易，我们的总资产缩水694元。可见，本例中，海龟策略略胜一筹。</p>
<p>其中，双移动平均策略和海龟策略，都是基于市场<strong>动量的变化</strong>而设计的，核心思想如下：如果股价上涨并超过某个点位，说明其<strong>上升</strong>的动量变强，这时应该<strong>买入</strong>；反之，则<strong>下行</strong>的动量增强，此时应该卖出。</p>
<h1 id="3-机器学习在交易中的简单应用"><a href="#3-机器学习在交易中的简单应用" class="headerlink" title="3 机器学习在交易中的简单应用"></a>3 机器学习在交易中的简单应用</h1><h2 id="3-1-机器学习的基本概念"><a href="#3-1-机器学习的基本概念" class="headerlink" title="3.1 机器学习的基本概念"></a>3.1 机器学习的基本概念</h2><h3 id="3-1-1-有监督学习和无监督学习"><a href="#3-1-1-有监督学习和无监督学习" class="headerlink" title="3.1.1 有监督学习和无监督学习"></a>3.1.1 有监督学习和无监督学习</h3><p>有监督学习：已知标签的任务</p>
<p>无监督学习：没有已知标签，但是让模型通过观察特征将它们放入不同类别的过程，就是无监督学习的一种</p>
<h3 id="3-1-2-分类和回归"><a href="#3-1-2-分类和回归" class="headerlink" title="3.1.2 分类和回归"></a>3.1.2 分类和回归</h3><p>分类的任务：给定样本的分类标签，训练模型使其可以将新的样本归入正确的分类中——这时模型的目标是离散的</p>
<p>回归的任务：给定样本的目标值，训练模型使其可以预测出新样本对应的数值——这时模型的目标是连续的</p>
<p>在股票领域，假如预测某只股票在未来会“涨”还是会“跌”，这时模型所做的就是分类的工作，但假如要预测某只股票未来会涨1元，还是8角8分，这时模型所做的就是回归的工作</p>
<h3 id="3-1-3-模型性能的评估"><a href="#3-1-3-模型性能的评估" class="headerlink" title="3.1.3 模型性能的评估"></a>3.1.3 模型性能的评估</h3><p>将掌握的<strong>数据集</strong>拆分为<strong>训练集</strong>和<strong>验证集</strong>，使用训练集训练模型，并使用验证集来评估模型是否可用</p>
<p>例如，某只股票有100天的价格数据，就可以将前80天的数据作为训练集，将后20天的数据作为验证集，同时评估模型分别在<strong>训练集</strong>与<strong>验证集</strong>中的准确率。</p>
<p>如果模型在训练集中的得分很高，而在验证集中的得分很低，说明模型出现了<strong>过拟合</strong>的问题；而如果模型在训练集和验证集中的得分都很低，说明模型出现了<strong>欠拟合</strong>的问题。</p>
<p><strong>要想深入解决这些问题，就需要调整模型的参数、补充数据，或者进行更细致的特征工程</strong>。</p>
<h2 id="3-2-机器学习工具的基本使用方法"><a href="#3-2-机器学习工具的基本使用方法" class="headerlink" title="3.2 机器学习工具的基本使用方法"></a>3.2 机器学习工具的基本使用方法</h2><h3 id="3-2-1-KNN算法的基本原理"><a href="#3-2-1-KNN算法的基本原理" class="headerlink" title="3.2.1 KNN算法的基本原理"></a>3.2.1 KNN算法的基本原理</h3><p>K最近邻，既可以用于分类也可以用于回归<br>它识别<strong>k个最近</strong>的数据点（基于欧几里得距离）来进行预测，它分别预测邻域中<strong>最频繁</strong>的分类或是回归情况下的平均结果。</p>
<h3 id="3-2-2-KNN算法用于分类"><a href="#3-2-2-KNN算法用于分类" class="headerlink" title="3.2.2 KNN算法用于分类"></a>3.2.2 KNN算法用于分类</h3><p>1.载入数据集并查看</p>
<p>scikit-learn内置了一些供大家学习的玩具数据集，有些是分类任务的数据，有些是回归任务的数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先导入鸢尾花数据载入工具</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="comment">#导入KNN分类模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="comment">#为了方便可视化，我们再导入matplotlib和seaborn</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载鸢尾花数据集，赋值给iris变量</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"><span class="comment">#查看数据集的键名</span></span><br><span class="line">iris.keys()</span><br></pre></td></tr></table></figure>




<pre><code>dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;])
</code></pre>
<p>该数据集存储了若干个健，我们重点关注一下其中的target和feature_names，因为这两个健对应的分别是样本的<strong>分类标签</strong>和<strong>特征名称</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看数据集的特征名称</span></span><br><span class="line">iris.feature_names</span><br></pre></td></tr></table></figure>




<pre><code>[&#39;sepal length (cm)&#39;,
 &#39;sepal width (cm)&#39;,
 &#39;petal length (cm)&#39;,
 &#39;petal width (cm)&#39;]
</code></pre>
<p>数据集样本有四个特征：sepal length（萼片长度）、sepal width（萼片宽度）、petal lenght（花瓣长度）和petal width（花瓣宽度）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看数据集中的样本分类</span></span><br><span class="line">iris.target</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
</code></pre>
<p>说明数据集中的样本分为3类，分别用0、1、2这三个数字来表示。</p>
<p>这个数据集的目的是：根据样本鸢尾花萼片和花瓣的长度及宽度，结合分类标签来训练模型，以便让模型可以预测出某一种鸢尾花属于哪个分类</p>
<p>2.拆分数据集</p>
<p>下面我们把数据集拆分为训练集和验证集，以便验证模型的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将样本的特征和标签分别赋值给X和y</span></span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line"><span class="comment">#查看是否成功</span></span><br><span class="line">X.shape</span><br></pre></td></tr></table></figure>




<pre><code>(150, 4)
</code></pre>
<p>可知，样本数量共有150个，每个样本有4个特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入数据集拆分工具</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment">#将X和y拆分为训练集与验证集</span></span><br><span class="line">X_train, X_test, y_train, y_test =\</span><br><span class="line">train_test_split(X, y)</span><br><span class="line"><span class="comment">#查看拆分情况</span></span><br><span class="line">X_train.shape</span><br></pre></td></tr></table></figure>




<pre><code>(112, 4)
</code></pre>
<p>通过拆分，训练集中的样本数量为112个，其余的38个样本则进入了验证集</p>
<p>3.训练模型并评估准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建knn分类器,参数保持默认</span></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line"><span class="comment">#使用训练集拟合模型</span></span><br><span class="line">knn_clf.fit(X_train, y_train)</span><br><span class="line"><span class="comment">#查看模型在训练集和验证集中的准确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集准确率：%.2f&#x27;</span>%knn_clf.score(X_train, y_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;验证集准确率：%.2f&#x27;</span>%knn_clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure>

<pre><code>训练集准确率：0.96
验证集准确率：0.97
</code></pre>
<p>从上面的代码可以看出，使用KNN算法训练的分类模型，在训练集中的准确率达到了96%，在验证集中的准确率达到了97%。非常不错</p>
<p>在scikit-learn中，KNN可以通过调节n_neighbors参数来改进模型的性能。在不手动指定的情况下，KNN默认的近邻参数n_neighbors为5。我们可以使用网格搜索法来寻找到模型的最优参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf</span><br></pre></td></tr></table></figure>




<pre><code>KNeighborsClassifier()
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入网格搜索</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="comment">#定义一个从1到10的n_neighbors</span></span><br><span class="line">n_neighbors = <span class="built_in">tuple</span>(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment">#创建网格搜索实例，estimator用KNN分类器</span></span><br><span class="line"><span class="comment">#把刚刚定义的n_neighbors传入给param_grid参数</span></span><br><span class="line"><span class="comment">#cv参数指交叉验证次数为5</span></span><br><span class="line">cv = GridSearchCV(estimator=KNeighborsClassifier(),</span><br><span class="line">                 param_grid = &#123;<span class="string">&#x27;n_neighbors&#x27;</span>:n_neighbors&#125;,</span><br><span class="line">                 cv = <span class="number">5</span>)</span><br><span class="line"><span class="comment">#使用网格搜索拟合数据集</span></span><br><span class="line">cv.fit(X,y)</span><br><span class="line"><span class="comment">#查看最优参数</span></span><br><span class="line">cv.best_params_</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;n_neighbors&#39;: 6&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n_neighbors</span><br></pre></td></tr></table></figure>




<pre><code>(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
</code></pre>
<p>当n_neighbors参数为6时，模型的准确率是最高的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建knn分类器,n_neighbors设置为6</span></span><br><span class="line">knn_clf = KNeighborsClassifier(n_neighbors=<span class="number">6</span>)</span><br><span class="line"><span class="comment">#使用模型拟合训练集数据</span></span><br><span class="line">knn_clf.fit(X_train, y_train)</span><br><span class="line"><span class="comment">#查看模型在训练集和验证集中的准确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集准确率：%.2f&#x27;</span>%knn_clf.score(X_train, y_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;验证集准确率：%.2f&#x27;</span>%knn_clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure>

<pre><code>训练集准确率：0.96
验证集准确率：0.97
</code></pre>
<h3 id="3-2-3-KNN算法用于回归"><a href="#3-2-3-KNN算法用于回归" class="headerlink" title="3.2.3 KNN算法用于回归"></a>3.2.3 KNN算法用于回归</h3><p>1.载入数据集并查看</p>
<p>波士顿房价数据集，该数据集中有506个样本，每个样本有13个特征，以及对应的价格（target）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#载入波士顿房价数据集导入工具</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="comment">#将数据导入</span></span><br><span class="line">boston = load_boston()</span><br><span class="line"><span class="comment">#查看数据集的键名</span></span><br><span class="line">boston.keys()</span><br></pre></td></tr></table></figure>




<pre><code>dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;feature_names&#39;, &#39;DESCR&#39;, &#39;filename&#39;])
</code></pre>
<p>数据集中存储了5个健，我们重点关注target（房屋的售价）及feature_names（房屋的特征），也就是说，我们需要训练模型，让它学习房屋特征和售价的关系，并且可以自动预测出新房屋的售价</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看样本的特征名称</span></span><br><span class="line">boston.feature_names</span><br></pre></td></tr></table></figure>




<pre><code>array([&#39;CRIM&#39;, &#39;ZN&#39;, &#39;INDUS&#39;, &#39;CHAS&#39;, &#39;NOX&#39;, &#39;RM&#39;, &#39;AGE&#39;, &#39;DIS&#39;, &#39;RAD&#39;,
       &#39;TAX&#39;, &#39;PTRATIO&#39;, &#39;B&#39;, &#39;LSTAT&#39;], dtype=&#39;&lt;U7&#39;)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选取前十套房屋，查看售价</span></span><br><span class="line">boston.target[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9])
</code></pre>
<p>2.拆分数据集并训练模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将样本特征和售价赋值给X，y</span></span><br><span class="line">X, y = boston.data, boston.target</span><br><span class="line"><span class="comment">#使用train_test_split拆分为训练集和验证集</span></span><br><span class="line">X_train, X_test, y_train, y_test =\</span><br><span class="line">train_test_split(X, y)</span><br><span class="line"><span class="comment">#查看拆分的结果</span></span><br><span class="line">X_train.shape</span><br></pre></td></tr></table></figure>




<pre><code>(379, 13)
</code></pre>
<p>训练集中有379个样本，其余127个样本进入了验证集</p>
<p>下面开始模型的训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入KNN回归算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line"><span class="comment">#创建一个实例，参数保持默认</span></span><br><span class="line">knn_reg = KNeighborsRegressor()</span><br><span class="line"><span class="comment">#拟合训练集数据</span></span><br><span class="line">knn_reg.fit(X_train, y_train)</span><br><span class="line"><span class="comment">#查看模型在训练集和验证集的性能表现</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集准确率：%.2f&#x27;</span>%knn_reg.score(X_train, y_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;验证集准确率：%.2f&#x27;</span>%knn_reg.score(X_test, y_test))</span><br></pre></td></tr></table></figure>

<pre><code>训练集准确率：0.68
验证集准确率：0.55
</code></pre>
<p>结果分析：缺省参数的KNN回归模型在该数据集中的性能表现差强人意，在训练集中的准确率只有68%，而在验证集中只有55%，这说明模型出现了欠拟合的问题，我们需要对数据集进行处理，或者对模型进行调优</p>
<p>3.模型评估的不同方法和改进</p>
<p>不论是在分类模型中还是回归模型中，我们都使用了**.score()方法**，来评估模型的性能。然和，在两种模型中，.score()方法所进行的计算是不一样的。分类模型中，.score()返回的是模型预测的准确率，其计算公式为:</p>
<p><strong>acc&#x3D;(TP+TN)&#x2F;(TP+FP+TN+FN)</strong></p>
<p>在上面公式中，TP(True Positive)表示模型预测正确的正样本数量；TN(True Negative)表示模型预测正确的负样本数量；FP(False Positive)表示原本是负样本，却被模型预测为正样本的数量。也就是我们平时说的“假阳性”；FN(False Negative)表示原本是正样本，却被模型预测为负样本的数量，也就是“假阴性”。TP、FP、TN、FN的和就是所有样本的样本数量。也就是说，<strong>分类模型的准确率是模型预测正确的样本数量，除以全部参与预测的样本数量</strong>。当然除了准确率外，我们还可以用Precision、Recall、F1score等方法来对模型进行性能评估。</p>
<p>在回归模型中，.score()方法返回的是模型的R^2，它是描述模型预测数值与真实值差距的指标，计算公式为：</p>
<p><img src="%E4%B8%8B%E8%BD%BD.png" alt="下载"></p>
<p>其中，y^代表模型对样本的估计值，y-可代表的是样本真实值的均值。也就是说，R^2是样本真实值减模型估计值，再进行平方并求和，除以样本真实值减样本平均值的平方和，最后用1减去这个结果。因此R^2取值为0-1，并且越接近1，说明模型的性能越好。</p>
<p>除了R^2外，回归模型还可以用均方误差MSE、绝对中位差MAE等指标来进行评估</p>
<p>前面说了，缺省参数的KNN模型再波士顿房价预测这个任务中表现并不理想。下面尝试对KNN回归的参数进行调整，看是否可以改进模型的性能。与分类模型一样，我们先使用网格搜索来寻找模型的最优参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这次让n_neighbors参数从1到20遍历</span></span><br><span class="line">n_neighbors = <span class="built_in">tuple</span>(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">21</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment">#创建KNN回归的网格搜索实例</span></span><br><span class="line">cv_reg = GridSearchCV(estimator = KNeighborsRegressor(),</span><br><span class="line">                     param_grid = &#123;<span class="string">&#x27;n_neighbors&#x27;</span>:n_neighbors&#125;,</span><br><span class="line">                     cv = <span class="number">5</span>)</span><br><span class="line"><span class="comment">#用网格搜索拟合数据集</span></span><br><span class="line">cv_reg.fit(X, y)</span><br><span class="line"><span class="comment">#返回最佳参数</span></span><br><span class="line">cv_reg.best_params_</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;n_neighbors&#39;: 10&#125;
</code></pre>
<p>可以看出，KNN回归模型的最佳n_neighbors参数是10，也就是说，当n_neighbors取10时，模型的R^2最高</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看最佳参数对应的最佳模型R平方分数</span></span><br><span class="line">cv.best_score_</span><br></pre></td></tr></table></figure>




<pre><code>0.9800000000000001
</code></pre>
<p>R^2提高到了98%，说明在性能方面有了显著提升</p>
<h2 id="3-3-基于机器学习的简单交易策略"><a href="#3-3-基于机器学习的简单交易策略" class="headerlink" title="3.3 基于机器学习的简单交易策略"></a>3.3 基于机器学习的简单交易策略</h2><h3 id="3-3-1-获取股票数据"><a href="#3-3-1-获取股票数据" class="headerlink" title="3.3.1 获取股票数据"></a>3.3.1 获取股票数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入Pandas</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment">#导入金融数据获取模块datareader</span></span><br><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line"><span class="comment">#导入numpy，一会儿会用到</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先我们来定义一个函数，用来获取数据</span></span><br><span class="line"><span class="comment">#传入的三个参数分别是开始日期，结束日期和输出的文件名</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_stock</span>(<span class="params">start_date, end_date, output_file</span>):</span><br><span class="line">    <span class="comment">#首先让程序尝试读取已下载并保存的文件</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        df = pd.read_pickle(output_file)</span><br><span class="line">        <span class="comment">#如果文件已存在，则打印载入股票数据文件完毕</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;载入股票数据文件完毕&#x27;</span>)</span><br><span class="line">    <span class="comment">#如果没有找到文件，则重新进行下载</span></span><br><span class="line">    <span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;文件未找到，重新下载中&#x27;</span>)</span><br><span class="line">        <span class="comment">#这里制定下载中国平安（601318）的交易数据</span></span><br><span class="line">        <span class="comment">#下载源为yahoo</span></span><br><span class="line">        df = ts.get_k_data(<span class="string">&#x27;601318&#x27;</span>, start_date, end_date)</span><br><span class="line">        df = df.set_index(<span class="string">&#x27;date&#x27;</span>)</span><br><span class="line">        <span class="comment">#下载成功后保存为pickle文件</span></span><br><span class="line">        df.to_pickle(output_file)</span><br><span class="line">        <span class="comment">#并通知我们下载完成</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;下载完成&#x27;</span>)</span><br><span class="line">    <span class="comment">#最后将下载的数据表进行返回</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下面使用我们定义好的函数来获取中国平安的交易数据</span></span><br><span class="line"><span class="comment">#获取三年的数据，从2017年3月9日至2020年的3月5日</span></span><br><span class="line"><span class="comment">#保存为名为601318的pickle文件</span></span><br><span class="line">zgpa = load_stock(start_date = <span class="string">&#x27;2017-03-09&#x27;</span>, </span><br><span class="line">                  end_date = <span class="string">&#x27;2020-03-05&#x27;</span>,</span><br><span class="line">                 output_file = <span class="string">&#x27;601318.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>文件未找到，重新下载中
本接口即将停止更新，请尽快使用Pro版接口：https://tushare.pro/document/2
下载完成
</code></pre>
<p>因为这里是第一次使用load_stock函数来获取数据，所以程序会提示没有找到文件，并重新开始下载文件。稍等片刻后，我们可以看到程序告知数据下载完成</p>
<p>如果想要查看自己已经下载好的数据，可以：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看一下数据的前五行</span></span><br><span class="line">zgpa.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>volume</th>
      <th>code</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2017-03-09</th>
      <td>24.301</td>
      <td>24.311</td>
      <td>24.331</td>
      <td>24.031</td>
      <td>377966.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2017-03-10</th>
      <td>24.241</td>
      <td>24.131</td>
      <td>24.301</td>
      <td>24.111</td>
      <td>207446.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2017-03-13</th>
      <td>24.131</td>
      <td>24.501</td>
      <td>24.571</td>
      <td>24.091</td>
      <td>359990.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2017-03-14</th>
      <td>24.521</td>
      <td>24.471</td>
      <td>24.661</td>
      <td>24.341</td>
      <td>276964.0</td>
      <td>601318</td>
    </tr>
    <tr>
      <th>2017-03-15</th>
      <td>24.411</td>
      <td>24.491</td>
      <td>24.531</td>
      <td>24.291</td>
      <td>268720.0</td>
      <td>601318</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="3-3-2-创建交易条件"><a href="#3-3-2-创建交易条件" class="headerlink" title="3.3.2 创建交易条件"></a>3.3.2 创建交易条件</h3><p>我们做一点简单的特征工程，以便后续工作。</p>
<p>这里用每日开盘价减去收盘价，并保存为一个新的特征。</p>
<p>用最高价减去最低价，保存为另一个特征。</p>
<p>如果股票次日收盘价高于当日收盘价，则标记为1，代表次日股票价格上涨，反之，如果次日收盘价低于当日收盘价，则标记为-1，代表股票次日价格下跌或者不变。这个过程可以称为创建股票的交易条件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下面我们来定义一个用于分类的函数，给数据表增加三个字段</span></span><br><span class="line"><span class="comment">#首先是开盘价减收盘价，命名为‘Open-Close’</span></span><br><span class="line"><span class="comment">#其次是最高价减最低价，命名为‘High-Low’</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classification_tc</span>(<span class="params">df</span>):</span><br><span class="line">    df[<span class="string">&#x27;Open-Close&#x27;</span>] = df[<span class="string">&#x27;open&#x27;</span>] - df[<span class="string">&#x27;close&#x27;</span>]</span><br><span class="line">    df[<span class="string">&#x27;High-Low&#x27;</span>] = df[<span class="string">&#x27;high&#x27;</span>] - df[<span class="string">&#x27;low&#x27;</span>]</span><br><span class="line">    <span class="comment">#在添加一个target字段，如果次日收盘价高于当日收盘价，则标记为1，反之为-1</span></span><br><span class="line">    df[<span class="string">&#x27;target&#x27;</span>] = np.where(df[<span class="string">&#x27;close&#x27;</span>].shift(-<span class="number">1</span>)&gt;df[<span class="string">&#x27;close&#x27;</span>], <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#去掉有空值的行</span></span><br><span class="line">    df = df.dropna()</span><br><span class="line">    <span class="comment">#将‘Open-Close’和‘High-Low’作为数据集的特征</span></span><br><span class="line">    X = df[[<span class="string">&#x27;Open-Close&#x27;</span>, <span class="string">&#x27;High-Low&#x27;</span>]]</span><br><span class="line">    <span class="comment">#将target赋值给y</span></span><br><span class="line">    y = df[<span class="string">&#x27;target&#x27;</span>]</span><br><span class="line">    <span class="comment">#将处理好的数据表以及X与y进行返回</span></span><br><span class="line">    <span class="keyword">return</span>(df,X,y)</span><br></pre></td></tr></table></figure>

<p>这个交易条件可以用来训练分类模型。让模型预测某只股票在下一个交易日价格上涨与否</p>
<p>现创造一个用于回归模型的交易条件。将次日收盘价减去当日收盘价的差价作为预测的目标。这样就可以训练回归模型，使其预测次日股价上涨（或下跌）的幅度，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下面定义一个用于回归的函数</span></span><br><span class="line"><span class="comment">#特征的添加和分类函数类似</span></span><br><span class="line"><span class="comment">#只不过target字段改为次日收盘价减去当日收盘价</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">regression_tc</span>(<span class="params">df</span>):</span><br><span class="line">    df[<span class="string">&#x27;Open-Close&#x27;</span>] = df[<span class="string">&#x27;open&#x27;</span>] - df[<span class="string">&#x27;close&#x27;</span>]</span><br><span class="line">    df[<span class="string">&#x27;High-Low&#x27;</span>] = df[<span class="string">&#x27;high&#x27;</span>] - df[<span class="string">&#x27;low&#x27;</span>]</span><br><span class="line">    df[<span class="string">&#x27;target&#x27;</span>] = df[<span class="string">&#x27;close&#x27;</span>].shift(-<span class="number">1</span>) - df[<span class="string">&#x27;close&#x27;</span>]</span><br><span class="line">    df = df.dropna()</span><br><span class="line">    X = df[[<span class="string">&#x27;Open-Close&#x27;</span>, <span class="string">&#x27;High-Low&#x27;</span>]]</span><br><span class="line">    y = df[<span class="string">&#x27;target&#x27;</span>]</span><br><span class="line">    <span class="comment">#将处理好的数据表以及X与y进行返回</span></span><br><span class="line">    <span class="keyword">return</span>(df,X,y)</span><br></pre></td></tr></table></figure>

<p>与分类交易条件一样，我们同样是把股票当日的开盘价和收盘价的差，与最高价和最低价的差作为样本的特征。不同的是，预测目标变成了次日收盘价与当日收盘价的差。</p>
<h3 id="3-3-3-使用分类算法制定交易策略"><a href="#3-3-3-使用分类算法制定交易策略" class="headerlink" title="3.3.3 使用分类算法制定交易策略"></a>3.3.3 使用分类算法制定交易策略</h3><p>我们使用上一节中定义的函数来处理下载好的股票数据，生成训练集与验证集，并训练一个简单的模型，以执行我们的交易策略</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用classification_tc函数生成数据集的特征与目标</span></span><br><span class="line">df, X, y = classification_tc(zgpa)</span><br><span class="line"><span class="comment">#将数据集拆分为训练集与验证集</span></span><br><span class="line">X_train, X_test, y_train, y_test =\</span><br><span class="line">train_test_split(X, y, shuffle=<span class="literal">False</span>,train_size=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>volume</th>
      <th>code</th>
      <th>Open-Close</th>
      <th>High-Low</th>
      <th>target</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2017-03-09</th>
      <td>24.301</td>
      <td>24.311</td>
      <td>24.331</td>
      <td>24.031</td>
      <td>377966.0</td>
      <td>601318</td>
      <td>-0.01</td>
      <td>0.30</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>2017-03-10</th>
      <td>24.241</td>
      <td>24.131</td>
      <td>24.301</td>
      <td>24.111</td>
      <td>207446.0</td>
      <td>601318</td>
      <td>0.11</td>
      <td>0.19</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2017-03-13</th>
      <td>24.131</td>
      <td>24.501</td>
      <td>24.571</td>
      <td>24.091</td>
      <td>359990.0</td>
      <td>601318</td>
      <td>-0.37</td>
      <td>0.48</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>2017-03-14</th>
      <td>24.521</td>
      <td>24.471</td>
      <td>24.661</td>
      <td>24.341</td>
      <td>276964.0</td>
      <td>601318</td>
      <td>0.05</td>
      <td>0.32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2017-03-15</th>
      <td>24.411</td>
      <td>24.491</td>
      <td>24.531</td>
      <td>24.291</td>
      <td>268720.0</td>
      <td>601318</td>
      <td>-0.08</td>
      <td>0.24</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建一个knn实例，n_neighbors取95</span></span><br><span class="line">knn_clf = KNeighborsClassifier(n_neighbors=<span class="number">95</span>)</span><br><span class="line"><span class="comment">#使用knn拟合训练集</span></span><br><span class="line">knn_clf.fit(X_train, y_train)</span><br><span class="line"><span class="comment">#打印模型在训练集中的准确率</span></span><br><span class="line"><span class="built_in">print</span>(knn_clf.score(X_train, y_train))</span><br><span class="line"><span class="comment">#打印模型在验证集中的准确率</span></span><br><span class="line"><span class="built_in">print</span>(knn_clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure>

<pre><code>0.5643224699828473
0.4863013698630137
</code></pre>
<p>结果分析：使用经过处理的数据集训练的KNN模型，在训练集中的准确率和验证集中的准确率仍然不高。原因是我们训练模型的样本特征确实<strong>太少</strong>了，无法支撑模型做出正确的判断。</p>
<p>既然模型已经可以做出预测（不论准确率如何），接下来我们就可以来验证一下，使用模型预测作为<strong>交易信号</strong>来进行交易，并且与<strong>基准收益</strong>进行对比。首先我们要计算出<strong>基准收益</strong>和<strong>基于模型预测的策略所带来的收益</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用KNN模型预测每日股票的涨跌，保存为‘Predict_Signal’</span></span><br><span class="line">df[<span class="string">&#x27;Predict_Signal&#x27;</span>] = knn_clf.predict(X)</span><br><span class="line"><span class="comment">#在数据集中添加一个字段，用当日收盘价除以前一日收盘价，并取其自然对数</span></span><br><span class="line">df[<span class="string">&#x27;Return&#x27;</span>] = np.log(df[<span class="string">&#x27;close&#x27;</span>]/df[<span class="string">&#x27;close&#x27;</span>].shift(<span class="number">1</span>))</span><br><span class="line"><span class="comment">#查看一下</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>volume</th>
      <th>code</th>
      <th>Open-Close</th>
      <th>High-Low</th>
      <th>target</th>
      <th>Predict_Signal</th>
      <th>Return</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2017-03-09</th>
      <td>24.301</td>
      <td>24.311</td>
      <td>24.331</td>
      <td>24.031</td>
      <td>377966.0</td>
      <td>601318</td>
      <td>-0.01</td>
      <td>0.30</td>
      <td>-1</td>
      <td>1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2017-03-10</th>
      <td>24.241</td>
      <td>24.131</td>
      <td>24.301</td>
      <td>24.111</td>
      <td>207446.0</td>
      <td>601318</td>
      <td>0.11</td>
      <td>0.19</td>
      <td>1</td>
      <td>1</td>
      <td>-0.007432</td>
    </tr>
    <tr>
      <th>2017-03-13</th>
      <td>24.131</td>
      <td>24.501</td>
      <td>24.571</td>
      <td>24.091</td>
      <td>359990.0</td>
      <td>601318</td>
      <td>-0.37</td>
      <td>0.48</td>
      <td>-1</td>
      <td>1</td>
      <td>0.015217</td>
    </tr>
    <tr>
      <th>2017-03-14</th>
      <td>24.521</td>
      <td>24.471</td>
      <td>24.661</td>
      <td>24.341</td>
      <td>276964.0</td>
      <td>601318</td>
      <td>0.05</td>
      <td>0.32</td>
      <td>1</td>
      <td>1</td>
      <td>-0.001225</td>
    </tr>
    <tr>
      <th>2017-03-15</th>
      <td>24.411</td>
      <td>24.491</td>
      <td>24.531</td>
      <td>24.291</td>
      <td>268720.0</td>
      <td>601318</td>
      <td>-0.08</td>
      <td>0.24</td>
      <td>1</td>
      <td>1</td>
      <td>0.000817</td>
    </tr>
  </tbody>
</table>
</div>



<p>从表中我们可以看出，数据表中的Predict_Signal存储的是KNN模型对股票涨跌的预测，而Return是指当日股票价格变动所带来的收益。</p>
<p>下面我们定义一个函数，计算一下累计的<strong>基准收益</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义一个计算累计回报的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cum_return</span>(<span class="params">df, split_value</span>):</span><br><span class="line">    <span class="comment">#该股票基准收益为‘Return’的总和*100</span></span><br><span class="line">    cum_return = df[split_value:][<span class="string">&#x27;Return&#x27;</span>].cumsum()*<span class="number">100</span></span><br><span class="line">    <span class="comment">#将计算结果进行返回</span></span><br><span class="line">    <span class="keyword">return</span> cum_return</span><br></pre></td></tr></table></figure>

<p>再定义一个函数，计算基于<strong>KNN模型预测</strong>的交易信号所进行的策略交易带来的收益：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#再定义一个计算使用策略交易的收益</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">strategy_return</span>(<span class="params">df, split_value</span>):</span><br><span class="line">    <span class="comment">#使用策略交易的收益为模型‘zgpa_Return’乘以模型预测的涨跌幅</span></span><br><span class="line">    df[<span class="string">&#x27;Strategy_Return&#x27;</span>] = df[<span class="string">&#x27;Return&#x27;</span>]*df[<span class="string">&#x27;Predict_Signal&#x27;</span>].shift(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#将每日策略交易的收益加和并乘以100</span></span><br><span class="line">    cum_strategy_return = df[split_value:][<span class="string">&#x27;Strategy_Return&#x27;</span>].cumsum()*<span class="number">100</span></span><br><span class="line">    <span class="comment">#将计算结果进行返回</span></span><br><span class="line">    <span class="keyword">return</span> cum_strategy_return</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义一个绘图函数，用来对比基准收益和算法交易的收益</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_chart</span>(<span class="params">cum_return, cum_strategy_return, symbol</span>):</span><br><span class="line">    <span class="comment">#首先是定义画布的尺寸</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">9</span>,<span class="number">6</span>))</span><br><span class="line">    <span class="comment">#使用折线图绘制基准收益</span></span><br><span class="line">    plt.plot(cum_return, <span class="string">&#x27;--&#x27;</span>,label=<span class="string">&#x27;%s Returns&#x27;</span>%symbol)</span><br><span class="line">    <span class="comment">#使用折线图绘制算法交易收益</span></span><br><span class="line">    plt.plot(cum_strategy_return, label = <span class="string">&#x27;Strategy Returns&#x27;</span>)</span><br><span class="line">    <span class="comment">#添加图注</span></span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.xticks([<span class="number">0</span>,<span class="number">36</span>,<span class="number">72</span>,<span class="number">108</span>,<span class="number">145</span>])</span><br><span class="line">    <span class="comment">#显示图像</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先来计算基准收益（预测集）</span></span><br><span class="line">cum_return = cum_return(df, split_value=<span class="built_in">len</span>(X_train))</span><br><span class="line"><span class="comment">#然后是计算使用算法交易带来的收益（同样只计算预测集）</span></span><br><span class="line">cum_strategy_return = strategy_return(df, </span><br><span class="line">                                      split_value=<span class="built_in">len</span>(X_train))</span><br><span class="line"><span class="comment">#用图像来进行对比</span></span><br><span class="line">plot_chart(cum_return, cum_strategy_return, <span class="string">&#x27;zgpa&#x27;</span>)</span><br></pre></td></tr></table></figure>


<p><img src="output_158_0.png" alt="output_158_0"></p>
<p>结果分析：虚线部分是该股票的累积基准收益，实线部分是使用算法进行交易的累计收益。虽然这里使用KNN分类模型的准确率并不高，但使用该模型预测涨跌后，进行交易的收益还是高于该股票的基准收益的。如果我们通过补充因子（或者说数据集的特征）的方法来进一步提高模型的准确率的话，则算法交易带来的收益还会显著提高。</p>
<p><strong>注意</strong>：与第二章所使用的回测方式不同，这里我们通过对<strong>算法交易收益</strong>与<strong>基准收益</strong>的对比来评估策略的业绩，而这种方法在实际中更为普遍。</p>
<h1 id="4-多来点数据——借助量化交易平台"><a href="#4-多来点数据——借助量化交易平台" class="headerlink" title="4 多来点数据——借助量化交易平台"></a>4 多来点数据——借助量化交易平台</h1><h2 id="4-1-数据不够，平台来凑"><a href="#4-1-数据不够，平台来凑" class="headerlink" title="4.1 数据不够，平台来凑"></a>4.1 数据不够，平台来凑</h2><h3 id="4-1-1-选择量化交易平台——聚宽"><a href="#4-1-1-选择量化交易平台——聚宽" class="headerlink" title="4.1.1 选择量化交易平台——聚宽"></a>4.1.1 选择量化交易平台——聚宽</h3><h3 id="4-1-2-量化交易平台的研究环境"><a href="#4-1-2-量化交易平台的研究环境" class="headerlink" title="4.1.2 量化交易平台的研究环境"></a>4.1.2 量化交易平台的研究环境</h3><p>后续代码在聚宽上实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">聚宽</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/量化交易//" class="article-tag-list-link color5">量化交易</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/02/03/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApython%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93%E5%AE%9E%E6%88%98-part1/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-python大数据分析与机器学习商业案例实战-part4" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/02/03/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part4/">Python大数据分析与机器学习商业案例实战-part4</a>
    </h1>
  

        
        <a href="/2023/02/03/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part4/" class="archive-article-date">
  	<time datetime="2023-02-03T10:13:14.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-02-03</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="13-数据聚类与分群分析"><a href="#13-数据聚类与分群分析" class="headerlink" title="13 数据聚类与分群分析"></a>13 数据聚类与分群分析</h1><p>机器学习可以分为监督学习和无监督学习两大类，其中非监督学习的数据集只有特征变量，而没有目标变量，我们需要对已有的数据进行建模，根据性质及进行分组。其典型案例是聚类分析问题，例如根据信用卡申请人进行分类（客户分群），根据新闻标题和内容对新闻进行分类等。</p>
<h2 id="13-1-KMeans算法"><a href="#13-1-KMeans算法" class="headerlink" title="13.1 KMeans算法"></a>13.1 KMeans算法</h2><h3 id="13-1-1-KMeans算法的基本原理"><a href="#13-1-1-KMeans算法的基本原理" class="headerlink" title="13.1.1 KMeans算法的基本原理"></a>13.1.1 KMeans算法的基本原理</h3><p>KMeans算法名称中的K代表类别数量，Means代表每个类别内样本的均值，所以又称为K-均值算法。KMeans算法以距离作为样本间相似度的度量标准，将距离相近的样本分配至同一个类别。样本间距离的计算方式可以是欧式距离、曼哈顿距离、余弦相似度等，KMeans算法通常采用欧式距离来度量各样本间的距离</p>
<p>KMeans算法的核心思想是对每个样本点计算到各个中心点的距离，并将该样本点分配给距离最近的中心点代表的类别，一次迭代完成后，根据聚类结果更新每个类别的中心点，然后重复之前操作再次迭代，直到前后两次分类结果没有差别。（下图目的是将8各样本点聚成3各类别K&#x3D;3）<br><img src="%E4%B8%8B%E8%BD%BD%20.png" alt="下载 "></p>
<h3 id="13-1-2-KMeans算法的代码实现"><a href="#13-1-2-KMeans算法的代码实现" class="headerlink" title="13.1.2 KMeans算法的代码实现"></a>13.1.2 KMeans算法的代码实现</h3><p>1.构造数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = np.array([[<span class="number">3</span>, <span class="number">2</span>], [<span class="number">4</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">6</span>], [<span class="number">4</span>, <span class="number">7</span>], [<span class="number">3</span>, <span class="number">9</span>], [<span class="number">6</span>, <span class="number">8</span>], [<span class="number">6</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">7</span>]])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>




<pre><code>array([[3, 2],
       [4, 1],
       [3, 6],
       [4, 7],
       [3, 9],
       [6, 8],
       [6, 6],
       [7, 7]])
</code></pre>
<p>2.可视化展示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], c=<span class="string">&quot;red&quot;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;samples&#x27;</span>)  <span class="comment"># 以红色圆圈样式绘制散点图并加上标签</span></span><br><span class="line">plt.legend()  <span class="comment"># 设置图例，图例内容为上面设置的label参数</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_10_0.png" alt="output_10_0"></p>
<p>第二行代码：data是用numpy库构造的，所以data[:,0]表示两列数的第一列数（第一个元素表示行，冒号表示所有行；第二个元素表示列，0表示第一列），即x坐标，同理，data[:,1]表示y坐标</p>
<p>3.KMeans聚类（聚类成2类）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">kms = KMeans(n_clusters=<span class="number">2</span>) <span class="comment"># K值设置为2</span></span><br><span class="line">kms.fit(data)</span><br></pre></td></tr></table></figure>



<p>4.获取结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">label = kms.labels_  <span class="comment"># 通过模型的labels_属性获取聚类结果</span></span><br><span class="line"><span class="built_in">print</span>(label)</span><br></pre></td></tr></table></figure>

<pre><code>[1 1 0 0 0 0 0 0]
</code></pre>
<p>结果表示原数据中前两个数据聚为一类，其他数据聚为另一类</p>
<p>5.结果可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(data[label == <span class="number">0</span>][:, <span class="number">0</span>], data[label == <span class="number">0</span>][:, <span class="number">1</span>], c=<span class="string">&quot;red&quot;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;class0&#x27;</span>)  <span class="comment"># 以红色圆圈样式绘制散点图并加上标签</span></span><br><span class="line">plt.scatter(data[label == <span class="number">1</span>][:, <span class="number">0</span>], data[label == <span class="number">1</span>][:, <span class="number">1</span>], c=<span class="string">&quot;green&quot;</span>, marker=<span class="string">&#x27;*&#x27;</span>, label=<span class="string">&#x27;class1&#x27;</span>)  <span class="comment"># 以绿色星星样式绘制散点图并加上标签</span></span><br><span class="line">plt.legend()  <span class="comment"># 设置图例</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.legend.Legend at 0x19db372cf10&gt;
</code></pre>
<p><img src="output_18_1.png" alt="output_18_1"></p>
<p>6.聚类成3类，并可视化呈现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kms_3 = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">kms_3.fit(data)</span><br><span class="line">label_3 = kms_3.labels_</span><br><span class="line"><span class="built_in">print</span>(label_3)</span><br></pre></td></tr></table></figure>


<pre><code>[1 1 2 2 2 0 0 0]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(data[label_3 == <span class="number">0</span>][:, <span class="number">0</span>], data[label_3 == <span class="number">0</span>][:, <span class="number">1</span>], c=<span class="string">&quot;red&quot;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;class0&#x27;</span>)  <span class="comment"># 以红色圆圈样式绘制散点图并加上标签</span></span><br><span class="line">plt.scatter(data[label_3 == <span class="number">1</span>][:, <span class="number">0</span>], data[label_3 == <span class="number">1</span>][:, <span class="number">1</span>], c=<span class="string">&quot;green&quot;</span>, marker=<span class="string">&#x27;*&#x27;</span>, label=<span class="string">&#x27;class1&#x27;</span>)  <span class="comment"># 以绿色星星样式绘制散点图并加上标签</span></span><br><span class="line">plt.scatter(data[label_3 == <span class="number">2</span>][:, <span class="number">0</span>], data[label_3 == <span class="number">2</span>][:, <span class="number">1</span>], c=<span class="string">&quot;blue&quot;</span>, marker=<span class="string">&#x27;+&#x27;</span>, label=<span class="string">&#x27;class2&#x27;</span>)  <span class="comment"># 以蓝色加号样式绘制散点图并加上标签</span></span><br><span class="line">plt.legend()  <span class="comment"># 设置图例</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.legend.Legend at 0x19db39903a0&gt;
</code></pre>
<p><img src="output_21_1.png" alt="output_21_1"></p>
<p>说明：因为KMeans算法的初始中心点是随机选取的，所以如果样本数据量比较大，可能会导致每次运行代码得到的聚类结果略有不同，如果希望每次运行的代码得到的聚类结果相同，可以在模型中传入random_state参数，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kms = KMeans(n_clusters=3,random_state=123)</span></span><br></pre></td></tr></table></figure>

<h3 id="13-1-3-案例实战：银行客户分群模型"><a href="#13-1-3-案例实战：银行客户分群模型" class="headerlink" title="13.1.3 案例实战：银行客户分群模型"></a>13.1.3 案例实战：银行客户分群模型</h3><p>1.案例背景</p>
<p>银行拥有海量的客户，对于不同的客户，银行需要采取不同的营销工作策略。例如，对于收入高且风险承受能力强的客户，可以重点挖掘业务机会，如向其推销一些收益率高但周期相对较长的理财产品；对于收入低且风险承受能力较弱的客户，则需要采取其他策略，因此，银行通常需要将客户进行分群处理，以便有的放矢地开展营销工作</p>
<p>2.读取银行客户数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line">data = pd.read_excel(<span class="string">&#x27;客户信息.xlsx&#x27;</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年龄(岁)</th>
      <th>收入(万元)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>50</td>
      <td>66</td>
    </tr>
    <tr>
      <th>1</th>
      <td>44</td>
      <td>51</td>
    </tr>
    <tr>
      <th>2</th>
      <td>30</td>
      <td>56</td>
    </tr>
    <tr>
      <th>3</th>
      <td>46</td>
      <td>50</td>
    </tr>
    <tr>
      <th>4</th>
      <td>32</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter(data.iloc[:, <span class="number">0</span>], data.iloc[:, <span class="number">1</span>], c=<span class="string">&quot;green&quot;</span>, marker=<span class="string">&#x27;*&#x27;</span>)  <span class="comment"># 以绿色星星样式绘制散点图</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;age&#x27;</span>)  <span class="comment"># 添加x轴名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;salary&#x27;</span>)  <span class="comment"># 添加y轴名称</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_29_0.png" alt="output_29_0"></p>
<p>可以看出，年龄越大的人收入相对越高，符合认知</p>
<p>3.模型搭建与使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">kms = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">123</span>)</span><br><span class="line">kms.fit(data)</span><br><span class="line">label = kms.labels_</span><br><span class="line">label = kms.fit_predict(data)</span><br><span class="line"><span class="built_in">print</span>(label)</span><br></pre></td></tr></table></figure>


<pre><code>[1 1 2 1 2 2 1 2 2 1 1 1 1 2 1 1 1 2 1 1 1 2 2 1 1 1 1 2 2 1 2 1 2 2 2 0 2
 1 2 0 1 1 2 1 2 1 2 1 1 2 2 0 1 2 1 1 1 1 2 1 2 2 2 2 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 1 1 1 2 1 2 0 0 0 0 0 0
 2]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(data[label == <span class="number">0</span>].iloc[:, <span class="number">0</span>], data[label == <span class="number">0</span>].iloc[:, <span class="number">1</span>], c=<span class="string">&quot;red&quot;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;class0&#x27;</span>)  <span class="comment"># 以红色圆圈样式绘制散点图并加上标签  </span></span><br><span class="line">plt.scatter(data[label == <span class="number">1</span>].iloc[:, <span class="number">0</span>], data[label == <span class="number">1</span>].iloc[:, <span class="number">1</span>], c=<span class="string">&quot;green&quot;</span>, marker=<span class="string">&#x27;*&#x27;</span>, label=<span class="string">&#x27;class1&#x27;</span>)  <span class="comment"># 以绿色星星样式绘制散点图并加上标签 </span></span><br><span class="line">plt.scatter(data[label == <span class="number">2</span>].iloc[:, <span class="number">0</span>], data[label == <span class="number">2</span>].iloc[:, <span class="number">1</span>], c=<span class="string">&quot;blue&quot;</span>, marker=<span class="string">&#x27;+&#x27;</span>, label=<span class="string">&#x27;class2&#x27;</span>)  <span class="comment"># 以蓝色加号样式绘制散点图并加上标签</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;age&#x27;</span>)  <span class="comment"># 添加x轴名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;salary&#x27;</span>)  <span class="comment"># 添加y轴名称</span></span><br><span class="line">plt.legend()  <span class="comment"># 设置图例</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.legend.Legend at 0x19db375bf70&gt;
</code></pre>
<p><img src="output_33_1.png" alt="output_33_1"></p>
<p>在上图中，class1代表的这部分客户年龄为40-50岁，平均收入58万元，可以视为重点客户，是需要重点营销和推广的对象；class2代表的这部分客户年龄为25-42岁，平均收入46万元，可以视为优质客户，是需要精心维护和营销的对象；class0代表的这部分客户年龄为20-40岁，平均收入21万元，可以视为潜力客户，是需要耐心挖掘和等待都对象</p>
<p><strong>补充知识点，查看各标签人的收入均值</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(data[label == <span class="number">0</span>].iloc[:, <span class="number">1</span>].mean())  <span class="comment"># 看下分类为标签0的人的收入均值，iloc[:, 1]为data表格的第二列，也即“收入”列</span></span><br><span class="line"><span class="built_in">print</span>(data[label == <span class="number">1</span>].iloc[:, <span class="number">1</span>].mean())</span><br><span class="line"><span class="built_in">print</span>(data[label == <span class="number">2</span>].iloc[:, <span class="number">1</span>].mean())</span><br></pre></td></tr></table></figure>

<pre><code>21.125
57.55555555555556
46.285714285714285
</code></pre>
<h2 id="13-2-DBSCAN算法"><a href="#13-2-DBSCAN算法" class="headerlink" title="13.2 DBSCAN算法"></a>13.2 DBSCAN算法</h2><p>DBCAN（Density-Based Apatial Clustering of Applications with Noise）是一种以密度为基础的空间聚类算法，可以用密度的概念剔除不属于任一类别的噪声点。该算法将簇定义为密度相连的点的最大集合，将具有足够密度的区域划分为簇，并可以发现任意形状的簇</p>
<h3 id="13-2-1-DBSCAN算法的基本原理"><a href="#13-2-1-DBSCAN算法的基本原理" class="headerlink" title="13.2.1 DBSCAN算法的基本原理"></a>13.2.1 DBSCAN算法的基本原理</h3><p><img src="%E4%B8%8B%E8%BD%BD.png" alt="下载"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(1).png" alt="下载 (1)"></p>
<p>可视化网站：<a target="_blank" rel="noopener" href="https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/">https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/</a></p>
<h3 id="13-2-2-DBSCAN算法的代码实现"><a href="#13-2-2-DBSCAN算法的代码实现" class="headerlink" title="13.2.2 DBSCAN算法的代码实现"></a>13.2.2 DBSCAN算法的代码实现</h3><p>1.读取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_excel(<span class="string">&#x27;演示数据.xlsx&#x27;</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10.44</td>
      <td>5.74</td>
    </tr>
    <tr>
      <th>1</th>
      <td>11.55</td>
      <td>6.16</td>
    </tr>
    <tr>
      <th>2</th>
      <td>11.36</td>
      <td>5.10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10.62</td>
      <td>6.12</td>
    </tr>
    <tr>
      <th>4</th>
      <td>11.20</td>
      <td>5.39</td>
    </tr>
  </tbody>
</table>
</div>



<p>2.数据可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter(data.iloc[:, <span class="number">0</span>], data.iloc[:, <span class="number">1</span>], c=<span class="string">&quot;green&quot;</span>, marker=<span class="string">&#x27;*&#x27;</span>)  <span class="comment"># 以绿色星星样式绘制散点图</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)  <span class="comment"># 添加x轴名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)  <span class="comment"># 添加y轴名称</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_47_0.png" alt="output_47_0"></p>
<p>3.数据建模&#x2F;4.查看聚类结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line">dbs = DBSCAN()  <span class="comment"># 不设置参数，即所有参数都取默认值：画圆半径参数eps取默认值0.5，园内最小样本数参数min_samples取默认值5</span></span><br><span class="line">dbs.fit(data)</span><br><span class="line">label_dbs = dbs.labels_</span><br><span class="line"><span class="built_in">print</span>(label_dbs)</span><br></pre></td></tr></table></figure>

<pre><code>[0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0
 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1
 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DBSCAN?  # 如果想查看DBSCAN的官方说明，可以在其DBSCAN后面加上?进行查看</span></span><br></pre></td></tr></table></figure>

<p>5.用散点图展示DBSCAN算法的聚类结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(data[label_dbs == <span class="number">0</span>].iloc[:, <span class="number">0</span>], data[label_dbs == <span class="number">0</span>].iloc[:, <span class="number">1</span>], c=<span class="string">&quot;red&quot;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;class0&#x27;</span>)  <span class="comment"># 以红色圆圈样式绘制散点图并加上标签  </span></span><br><span class="line">plt.scatter(data[label_dbs == <span class="number">1</span>].iloc[:, <span class="number">0</span>], data[label_dbs == <span class="number">1</span>].iloc[:, <span class="number">1</span>], c=<span class="string">&quot;green&quot;</span>, marker=<span class="string">&#x27;*&#x27;</span>, label=<span class="string">&#x27;class1&#x27;</span>)  <span class="comment"># 以绿色星星样式绘制散点图并加上标签 </span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)  <span class="comment"># 添加x轴名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)  <span class="comment"># 添加y轴名称</span></span><br><span class="line">plt.legend()  <span class="comment"># 设置图例</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.legend.Legend at 0x19db4f3e1f0&gt;
</code></pre>
<p><img src="output_52_1.png" alt="output_52_1"></p>
<h3 id="13-2-3-KMeans算法与DBSCAN算法的对比"><a href="#13-2-3-KMeans算法与DBSCAN算法的对比" class="headerlink" title="13.2.3 KMeans算法与DBSCAN算法的对比"></a>13.2.3 KMeans算法与DBSCAN算法的对比</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">KMs = KMeans(n_clusters=<span class="number">2</span>)</span><br><span class="line">KMs.fit(data)</span><br><span class="line">label_kms = KMs.labels_</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># KMs # 这样可以查看模型参数，这里没有设置random_state参数，所以可能每次跑出来的结果略有不同（因为每次起始点选的地方不同）</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(label_kms)</span><br></pre></td></tr></table></figure>

<pre><code>[1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1
 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0
 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(data[label_kms == <span class="number">0</span>].iloc[:, <span class="number">0</span>], data[label_kms == <span class="number">0</span>].iloc[:, <span class="number">1</span>], c=<span class="string">&quot;red&quot;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;class0&#x27;</span>)  <span class="comment"># 以红色圆圈样式绘制散点图并加上标签  </span></span><br><span class="line">plt.scatter(data[label_kms == <span class="number">1</span>].iloc[:, <span class="number">0</span>], data[label_kms == <span class="number">1</span>].iloc[:, <span class="number">1</span>], c=<span class="string">&quot;green&quot;</span>, marker=<span class="string">&#x27;*&#x27;</span>, label=<span class="string">&#x27;class1&#x27;</span>)  <span class="comment"># 以绿色星星样式绘制散点图并加上标签 </span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)  <span class="comment"># 添加x轴名称</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)  <span class="comment"># 添加y轴名称</span></span><br><span class="line">plt.legend()  <span class="comment"># 设置图例</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.legend.Legend at 0x19db4d20df0&gt;
</code></pre>
<p><img src="output_57_1.png" alt="output_57_1"></p>
<p>可以看到，对于形状类似同心圆的数据，KMeans算法聚类效果较差，只能机械地将数据分为左右两部分，而无法以外圆内圆的方式进行区分。</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(2).png" alt="下载 (2)"></p>
<h2 id="13-3-案例实战：新闻聚类分群模型"><a href="#13-3-案例实战：新闻聚类分群模型" class="headerlink" title="13.3 案例实战：新闻聚类分群模型"></a>13.3 案例实战：新闻聚类分群模型</h2><h3 id="13-3-1-案例背景"><a href="#13-3-1-案例背景" class="headerlink" title="13.3.1 案例背景"></a>13.3.1 案例背景</h3><p>新闻种类复杂多样，财经、体育、科技、娱乐等等，在本案例中，笔者根据关键词从百度新闻爬取了962条新闻，且每个关键词对应的新闻条数接近，现在需要对每条新闻划分类别，匹配到正确的版面</p>
<h3 id="13-3-2-文本数据的读取与处理"><a href="#13-3-2-文本数据的读取与处理" class="headerlink" title="13.3.2 文本数据的读取与处理"></a>13.3.2 文本数据的读取与处理</h3><p><strong>1.读取数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;新闻.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>关键词</th>
      <th>标题</th>
      <th>网址</th>
      <th>来源</th>
      <th>时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>华能信托</td>
      <td>信托公司2019年上半年经营业绩概览</td>
      <td>http://www.financialnews.com.cn/jrsb_m/xt/zx/2...</td>
      <td>中国金融新闻网</td>
      <td>2019年07月23日 00:00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>华能信托</td>
      <td>首单信托型企业ABS获批</td>
      <td>http://www.jjckb.cn/2018-10/23/c_137552198.htm</td>
      <td>经济参考网</td>
      <td>2018年10月23日 12:21</td>
    </tr>
    <tr>
      <th>2</th>
      <td>华能信托</td>
      <td>华能贵诚信托孙磊:金融科技助力打造开放信托生态</td>
      <td>https://baijiahao.baidu.com/s?id=1639276579449...</td>
      <td>同花顺财经</td>
      <td>2019年07月17日 10:49</td>
    </tr>
    <tr>
      <th>3</th>
      <td>华能信托</td>
      <td>华能贵诚信托孙磊:金融科技已经成为信托行业重要的基础设施</td>
      <td>https://finance.qq.com/a/20190716/007898.htm</td>
      <td>腾讯财经</td>
      <td>2019年07月16日 18:53</td>
    </tr>
    <tr>
      <th>4</th>
      <td>华能信托</td>
      <td>格力电器股权转让意向方闭门开会 华能信托赫然在列</td>
      <td>https://finance.sina.com.cn/trust/roll/2019-05...</td>
      <td>新浪</td>
      <td>2019年05月22日 22:53</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.shape</span><br></pre></td></tr></table></figure>




<pre><code>(962, 5)
</code></pre>
<p><strong>2.中文分词</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 中文分词演示</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">word = jieba.cut(<span class="string">&#x27;我爱北京天安门&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> word:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure>


<pre><code>我
爱
北京
天安门
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一条新闻标题</span></span><br><span class="line">df.iloc[<span class="number">0</span>][<span class="string">&#x27;标题&#x27;</span>]</span><br></pre></td></tr></table></figure>




<pre><code>&#39;信托公司2019年上半年经营业绩概览&#39;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一条新闻标题中文分词</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">word = jieba.cut(df.iloc[<span class="number">0</span>][<span class="string">&#x27;标题&#x27;</span>])</span><br><span class="line">result = <span class="string">&#x27; &#x27;</span>.join(word) <span class="comment"># jion函数将变量word中的各个分词以空格（‘’）为连接符连接在一起</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<pre><code>信托公司 2019 年 上半年 经营 业绩 概览
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过for循环遍历来进行所有标题的分词</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">words = []</span><br><span class="line"><span class="keyword">for</span> i, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    word = jieba.cut(row[<span class="string">&#x27;标题&#x27;</span>])</span><br><span class="line">    result = <span class="string">&#x27; &#x27;</span>.join(word) </span><br><span class="line">    words.append(result)</span><br></pre></td></tr></table></figure>

<p>第三行代码创建一个空列表words来存储每一条新闻标题的分词结果。第四行代码通过for循环遍历整张表格，其中iterrows()是pandas库遍历表格每一行的方法，i对应每一行的行号，row对应每一行的内容。5，6行代码对每一条新闻标题进行分词，并将各个分词用空格连接在一起，roe[‘标题’]表示这一行的“标题”列的内容。第七行代码用append()函数将每一条新闻标题的分词结果添加到works列表中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words[<span class="number">0</span>:<span class="number">3</span>]  <span class="comment"># 展示前三条新闻的分词结果</span></span><br></pre></td></tr></table></figure>




<pre><code>[&#39;信托公司 2019 年 上半年 经营 业绩 概览&#39;,
 &#39;首单 信托 型 企业 ABS 获批&#39;,
 &#39;华能 贵 诚信 托孙磊 : 金融 科技 助力 打造 开放 信托 生态&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 熟悉了上面的过程后，可以把代码合并写成如下形式</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">words = []</span><br><span class="line"><span class="keyword">for</span> i, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    words.append(<span class="string">&#x27; &#x27;</span>.join(jieba.cut(row[<span class="string">&#x27;标题&#x27;</span>])))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words[<span class="number">0</span>:<span class="number">3</span>]  <span class="comment"># 同样展示前三条新闻的分词结果</span></span><br></pre></td></tr></table></figure>




<pre><code>[&#39;信托公司 2019 年 上半年 经营 业绩 概览&#39;,
 &#39;首单 信托 型 企业 ABS 获批&#39;,
 &#39;华能 贵 诚信 托孙磊 : 金融 科技 助力 打造 开放 信托 生态&#39;]
</code></pre>
<p><strong>补充知识点：遍历DataFrame表格的函数 - iterrows()函数</strong></p>
<p>pandas库中的iterrows()函数用于遍历DataFrame的每一行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">    <span class="built_in">print</span>(row)</span><br></pre></td></tr></table></figure>

<pre><code>0
关键词                                                 华能信托
标题                                    信托公司2019年上半年经营业绩概览
网址     http://www.financialnews.com.cn/jrsb_m/xt/zx/2...
来源                                               中国金融新闻网
时间                                     2019年07月23日 00:00
Name: 0, dtype: object
1
关键词                                              华能信托
标题                                       首单信托型企业ABS获批
网址     http://www.jjckb.cn/2018-10/23/c_137552198.htm
来源                                              经济参考网
时间                                  2018年10月23日 12:21
Name: 1, dtype: object
2
关键词                                                 华能信托
标题                               华能贵诚信托孙磊:金融科技助力打造开放信托生态
网址     https://baijiahao.baidu.com/s?id=1639276579449...
来源                                                 同花顺财经
时间                                     2019年07月17日 10:49
Name: 2, dtype: object
3
关键词                                            华能信托
标题                     华能贵诚信托孙磊:金融科技已经成为信托行业重要的基础设施
网址     https://finance.qq.com/a/20190716/007898.htm
来源                                             腾讯财经
时间                                2019年07月16日 18:53
Name: 3, dtype: object
4
关键词                                                 华能信托
标题                              格力电器股权转让意向方闭门开会 华能信托赫然在列
网址     https://finance.sina.com.cn/trust/roll/2019-05...
来源                                                    新浪
时间                                     2019年05月22日 22:53
Name: 4, dtype: object
5
关键词                                      华能信托
标题         直击格力电器意向投资者见面会:参会者华能信托背后现国务院国资委...
网址     http://finance.ifeng.com/c/7mt651IB1rX
来源                                        凤凰网
时间                          2019年05月22日 18:24
Name: 5, dtype: object
6
关键词                                      华能信托
标题             格力电器股权转让意向投资者见面会召开 自称华能信托的人士到场
网址     http://finance.ifeng.com/c/7msujqm5Mcr
来源                                        凤凰网
时间                          2019年05月22日 15:42
Name: 6, dtype: object
7
关键词                                                 华能信托
标题                      2018年信托业人均创利304万元 华润、华能贵诚跌出万亿俱乐部
网址     http://finance.eastmoney.com/a/201905121120104...
来源                                                 东方财富网
时间                                     2019年05月12日 16:00
Name: 7, dtype: object
8
关键词                                                 华能信托
标题                          去年信托业人均创利304万元,华能贵诚信托跌出万亿俱乐部
网址     https://baijiahao.baidu.com/s?id=1633311466139...
来源                                                 财经新鲜事
时间                                     2019年05月12日 15:47
Name: 8, dtype: object
9
关键词                                                华能信托
标题              ...或200亿收购中江信托 50亿爆雷“烫手山芋”如何处置?;华能信托...
网址     http://www.jnlc.com/article/20190417239422.shtml
来源                                                金牛基金网
时间                                    2019年04月17日 09:41
Name: 9, dtype: object
10
关键词                                                华能信托
标题                                 华能信托是外界传言泰禾引进战投的目标之一
网址     http://www.jnlc.com/article/20190417239409.shtml
来源                                                金牛基金网
时间                                    2019年04月17日 08:35
Name: 10, dtype: object
 …………………………………………
 …………………………………………
Name: 961,dtype: object
</code></pre>
<p>可以看到，这里的i就是每一行的行索引序号，row就是每一行的内容，该内容是一个一维的Series对象，它可以根据索引来提取内容，例如，通过roe[‘标题’]可以提取该条新闻的标题内容，通过row[‘网址’]可以提取该条新闻的网址内容</p>
<p><strong>3.文本向量化基础：建立词频矩阵</strong></p>
<p>此时已经把每一条新闻标题分词完毕并存储到words列表中，下面需要将这些文本类型的数据转换成数值类型的数据，以便构造特征变量及训练模型。文本向量化函数CountVectorizer()，通过它可以很方便地将文本转换成数值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CountVectorizer()函数简单演示</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">test = [<span class="string">&#x27;金融 科技 厉害&#x27;</span>, <span class="string">&#x27;华能 信托 厉害&#x27;</span>]</span><br><span class="line">vect = CountVectorizer()</span><br><span class="line">X = vect.fit_transform(test) <span class="comment"># 用fit_transform（）函数进行文本向量化转换</span></span><br><span class="line">X = X.toarray()</span><br><span class="line"><span class="built_in">print</span>(X)</span><br></pre></td></tr></table></figure>

<pre><code>[[0 0 1 1 1]
 [1 1 1 0 0]]
</code></pre>
<p>可以看到，此时2条新闻标题已经变成了由数字0和1组成的2个一维数组，每个数组中各有5个元素</p>
<p>CountVectorizer()函数会先根据空格来识别每一句话中的词语，“金融”，“科技”，“厉害”，“华能”，“信托”这5个不同的词，这5个词便构成了这2条新闻标题的词袋，该函数会自动对词袋中的词进行编号，通过vocabulary_属性便能获取词袋内容及响应编号</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看词袋和对应的顺序</span></span><br><span class="line">words_bag = vect.vocabulary_</span><br><span class="line"><span class="built_in">print</span>(words_bag)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;金融&#39;: 4, &#39;科技&#39;: 3, &#39;厉害&#39;: 2, &#39;华能&#39;: 1, &#39;信托&#39;: 0&#125;
</code></pre>
<p>可以看到，词袋是一个字典，每个词是字典的键，词对应的编号是字典的值。这些不同的词其实就代表着不同的特征，第几个编号就代表第几个特征</p>
<p>有了上面的词袋，就可以构建如下表所示的词频矩阵，表中的数值即为相关标题中对应特征词的出现频数</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(3).png" alt="下载 (3)"></p>
<p>所以标题1对应的数值数组就是[0 0 1 1 1]，此外，CountVectorizer()函数会自动过滤掉一个字的词，这样会过滤掉“的”“之”等没有重要意义的词，不过同时也会过滤掉“爱”“坑”等可能有重要意义的词，因此，这个特点既是一个优势，也是一个劣势</p>
<p><strong>4.文本向量化实战：构造特征变量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将之前所有的新闻标题进行文本向量化</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">vect = CountVectorizer()</span><br><span class="line">X = vect.fit_transform(words)</span><br><span class="line">X = X.toarray()  <span class="comment"># 用toarray()函数将X转换成为数组形式并重新赋给变量X</span></span><br><span class="line"><span class="built_in">print</span>(X)</span><br></pre></td></tr></table></figure>

<pre><code>[[0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为新闻标题的数据居多，分词后得到的不同的词语非常多，导致词频矩阵比较稀疏</span></span><br><span class="line"><span class="comment"># 特征向量X中很多地方都是0（即该词在新闻标题中出现的频次为0）</span></span><br><span class="line"><span class="comment"># 查看所有新闻标题的词袋</span></span><br><span class="line">words_bag = vect.vocabulary_</span><br><span class="line"><span class="built_in">print</span>(words_bag)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;信托公司&#39;: 630, &#39;2019&#39;: 21, &#39;上半年&#39;: 296, &#39;经营&#39;: 2659, &#39;业绩&#39;: 345, &#39;概览&#39;: 2130, &#39;首单&#39;: 3337, &#39;信托&#39;: 628, &#39;企业&#39;: 538, &#39;abs&#39;: 103, &#39;获批&#39;: 2827, &#39;华能&#39;: 896, &#39;诚信&#39;: 2947, &#39;托孙磊&#39;: 1721, &#39;金融&#39;: 3199, &#39;科技&#39;: 2541, &#39;助力&#39;: 854, &#39;打造&#39;: 1720, &#39;开放&#39;: 1553, &#39;生态&#39;: 2408, &#39;已经&#39;: 1467, &#39;成为&#39;: 1673, &#39;行业&#39;: 2860, &#39;重要&#39;: 3191, &#39;基础设施&#39;: 1135, &#39;格力电器&#39;: 2116, &#39;股权&#39;: 2754, &#39;转让&#39;: 3063, &#39;意向&#39;: 1664, &#39;闭门&#39;: 3229, &#39;开会&#39;: 1535, &#39;赫然&#39;: 3013, &#39;直击&#39;: 2465, &#39;投资者&#39;: 1744, &#39;见面会&#39;: 2886, &#39;参会者&#39;: 934, &#39;背后&#39;: 2758, &#39;国务院&#39;: 1091, &#39;国资委&#39;: 1098, &#39;召开&#39;: 972, &#39;自称&#39;: 2784, &#39;人士&#39;: 492, &#39;到场&#39;: 818, &#39;2018&#39;: 20, &#39;信托业&#39;: 629, &#39;人均&#39;: 491, &#39;创利&#39;: 798, &#39;304&#39;: 44, &#39;万元&#39;: 273, &#39;华润&#39;: 895, &#39;贵诚&#39;: 2992, &#39;跌出&#39;: 3039, &#39;万亿&#39;: 272, &#39;俱乐部&#39;: 633, &#39;去年&#39;: 932, &#39;托跌出&#39;: 1724, &#39;200&#39;: 17, &#39;收购&#39;: 1854, &#39;50&#39;: 60, &#39;亿爆雷&#39;: 508, &#39;烫手山芋&#39;: 2323, &#39;如何&#39;: 1269, &#39;处置&#39;: 1152, &#39;外界&#39;: 1159, &#39;传言&#39;: 564, &#39;泰禾&#39;: 2221, &#39;引进&#39;: 1566, &#39;战投&#39;: 1695, &#39;目标&#39;: 2464, &#39;之一&#39;: 407, &#39;行业动态&#39;: 2861, &#39;山东&#39;: 1438, &#39;年报&#39;: 1497, &#39;规模&#39;: 2890, &#39;缩水&#39;: 2686, &#39;曲线&#39;: 2021, &#39;上市&#39;: 297, &#39;浪潮&#39;: 2241, &#39;再度&#39;: 739, &#39;托换帅&#39;: 1722, &#39;孙磊&#39;: 1303, &#39;出任&#39;: 764, &#39;总经理&#39;: 1641, &#39;换帅&#39;: 1794, &#39;任职&#39;: 537, &#39;资格&#39;: 3002, &#39;到期&#39;: 821, &#39;债务&#39;: 638, &#39;590&#39;: 70, &#39;集团&#39;: 3263, &#39;救命&#39;: 1875, &#39;国际&#39;: 1101, &#39;资本&#39;: 3000, &#39;有限公司&#39;: 2059, &#39;股东&#39;: 2750, &#39;东莞&#39;: 351, &#39;昨日&#39;: 1996, &#39;双双&#39;: 940, &#39;增资&#39;: 1143, &#39;今年&#39;: 515, &#39;注册资本&#39;: 2216, &#39;增加&#39;: 1140, &#39;总额&#39;: 1643, &#39;注册&#39;: 2215, &#39;资本金&#39;: 3001, &#39;42&#39;: 55, &#39;亿元&#39;: 505, &#39;61&#39;: 75, &#39;94&#39;: 95, &#39;时隔&#39;: 1983, &#39;近两年&#39;: 3089, &#39;再次&#39;: 740, &#39;增至&#39;: 1142, &#39;95&#39;: 96, &#39;肖钢&#39;: 2749, &#39;密集&#39;: 1373, &#39;调研&#39;: 2965, &#39;资产&#39;: 2999, &#39;证券化&#39;: 2931, &#39;业务&#39;: 343, &#39;访华&#39;: 2929, &#39;中信&#39;: 363, &#39;62&#39;: 79, &#39;净利润&#39;: 756, &#39;排名&#39;: 1803, &#39;平安&#39;: 1491, &#39;位列&#39;: 572, &#39;速睹&#39;: 3148, &#39;金志&#39;: 3196, &#39;五大&#39;: 460, &#39;秘诀&#39;: 2551, &#39;塑造&#39;: 1139, &#39;核心&#39;: 2112, &#39;竞争力&#39;: 2574, &#39;单家&#39;: 902, &#39;公司股票&#39;: 701, &#39;产品&#39;: 477, &#39;净值&#39;: 754, &#39;20&#39;: 16, &#39;持有&#39;: 1778, &#39;股票&#39;: 2756, &#39;超过&#39;: 3030, &#39;参与&#39;: 933, &#39;消费&#39;: 2252, &#39;机会&#39;: 2076, &#39;模式分析&#39;: 2138, &#39;新任&#39;: 1921, &#39;到位&#39;: 817, &#39;五矿&#39;: 464, &#39;突围&#39;: 2566, &#39;首任&#39;: 3335, &#39;辞职&#39;: 3070, &#39;接任&#39;: 1812, &#39;王卓&#39;: 2372, &#39;华夏&#39;: 892, &#39;幸福&#39;: 1505, &#39;关于&#39;: 719, &#39;签署&#39;: 2608, &#39;协议&#39;: 900, &#39;公告&#39;: 702, &#39;普邦&#39;: 2006, &#39;股份&#39;: 2752, &#39;集合&#39;: 3262, &#39;资金&#39;: 3005, &#39;计划&#39;: 2910, &#39;合同&#39;: 988, &#39;试水&#39;: 2944, &#39;不良资产&#39;: 327, &#39;收益权&#39;: 1853, &#39;已有&#39;: 1466, &#39;46&#39;: 58, &#39;银登&#39;: 3209, &#39;中心&#39;: 376, &#39;北京&#39;: 862, &#39;银行&#39;: 3210, &#39;携手&#39;: 1838, &#39;中航&#39;: 385, &#39;创新&#39;: 802, &#39;慈善&#39;: 1670, &#39;模式&#39;: 2137, &#39;acca&#39;: 104, &#39;财经&#39;: 2977, &#39;领袖&#39;: 3312, &#39;培养&#39;: 1129, &#39;第一期&#39;: 2585, &#39;学员&#39;: 1311, &#39;选拔&#39;: 3135, &#39;结果&#39;: 2666, &#39;公布&#39;: 704, &#39;国金&#39;: 1100, &#39;早报&#39;: 1977, &#39;招行&#39;: 1771, &#39;合作&#39;: 986, &#39;发行&#39;: 957, &#39;99&#39;: 99, &#39;用益&#39;: 2419, &#39;日报&#39;: 1971, &#39;江苏&#39;: 2194, &#39;前四&#39;: 828, &#39;58&#39;: 68, &#39;净利&#39;: 755, &#39;排位&#39;: 1802, &#39;杨广伟&#39;: 2093, &#39;未来&#39;: 2071, &#39;10&#39;: 5, &#39;人工智能&#39;: 495, &#39;一定&#39;: 232, &#39;改变&#39;: 1858, &#39;房地产&#39;: 1702, &#39;文献&#39;: 1912, &#39;述评&#39;: 3120, &#39;精神科&#39;: 2623, &#39;应用&#39;: 1517, &#39;严重&#39;: 359, &#39;提醒&#39;: 1830, &#39;骗子&#39;: 3365, &#39;ai&#39;: 105, &#39;技术&#39;: 1737, &#39;知道&#39;: 2490, &#39;时代&#39;: 1979, &#39;征程&#39;: 1595, &#39;机遇&#39;: 2080, &#39;机器&#39;: 2077, &#39;视觉&#39;: 2893, &#39;高峰论坛&#39;: 3367, &#39;如期而至&#39;: 1271, &#39;华为&#39;: 889, &#39;atlas&#39;: 108, &#39;遥感&#39;: 3160, &#39;碧空&#39;: 2511, &#39;慧眼&#39;: 1671, &#39;读心术&#39;: 2958, &#39;新高&#39;: 1946, &#39;基于&#39;: 1131, &#39;血谱&#39;: 2859, &#39;光学&#39;: 658, &#39;成像&#39;: 1674, &#39;情感&#39;: 1653, &#39;伙伴&#39;: 552, &#39;招募&#39;: 1768, &#39;京东&#39;: 479, &#39;炼金&#39;: 2322, &#39;开启&#39;: 1538, &#39;共生&#39;: 715, &#39;融合&#39;: 2857, &#39;商业化&#39;: 1058, &#39;场景&#39;: 1117, &#39;发展&#39;: 951, &#39;实践&#39;: 1357, &#39;落地&#39;: 2834, &#39;注重&#39;: 2218, &#39;三大&#39;: 285, &#39;渗透&#39;: 2283, &#39;年内&#39;: 1494, &#39;占领&#39;: 917, &#39;市场&#39;: 1470, &#39;言通&#39;: 2904, &#39;专注&#39;: 339, &#39;领域&#39;: 3309, &#39;语音&#39;: 2953, &#39;识别&#39;: 2941, &#39;服务&#39;: 2061, &#39;多维度&#39;: 1174, &#39;赋能&#39;: 3007, &#39;智能&#39;: 2012, &#39;营销&#39;: 2833, &#39;阿尔法&#39;: 3238, &#39;现身&#39;: 2392, &#39;教育&#39;: 1879, &#39;数据&#39;: 1885, &#39;峰会&#39;: 1443, &#39;获点&#39;: 2828, &#39;世界&#39;: 341, &#39;大会&#39;: 1181, &#39;看点&#39;: 2474, &#39;聚焦&#39;: 2746, &#39;全球&#39;: 684, &#39;健康&#39;: 646, &#39;智汇&#39;: 2010, &#39;预见&#39;: 3306, &#39;亚马逊&#39;: 469, &#39;stylesnap&#39;: 194, &#39;一款&#39;: 244, &#39;工具&#39;: 1452, &#39;倒计时&#39;: 635, &#39;22&#39;: 29, &#39;众多&#39;: 543, &#39;民生&#39;: 2175, &#39;展商&#39;: 1427, &#39;亮相&#39;: 485, &#39;联想&#39;: 2740, &#39;宣布&#39;: 1363, &#39;开展&#39;: 1544, &#39;为期&#39;: 393, &#39;多年&#39;: 1168, &#39;提升&#39;: 1827, &#39;高性能&#39;: 3370, &#39;计算&#39;: 2911, &#39;百度&#39;: 2449, &#39;副总裁&#39;: 835, &#39;张旭&#39;: 1573, &#39;学好&#39;: 1312, &#39;少儿&#39;: 1413, &#39;编程&#39;: 2683, &#39;轻松&#39;: 3065, &#39;掌控&#39;: 1799, &#39;跨境&#39;: 3041, &#39;电商&#39;: 2423, &#39;交通运输&#39;: 473, &#39;西媒&#39;: 2878, &#39;盘点&#39;: 2456, &#39;2016&#39;: 19, &#39;中国&#39;: 368, &#39;打响&#39;: 1715, &#39;发令枪&#39;: 948, &#39;足迹&#39;: 3037, &#39;惊人&#39;: 1655, &#39;研究&#39;: 2498, &#39;人员&#39;: 490, &#39;呼吁&#39;: 1037, &#39;提高&#39;: 1832, &#39;效率&#39;: 1874, &#39;乐队&#39;: 427, &#39;鼓手&#39;: 3398, &#39;失业&#39;: 1231, &#39;索尼&#39;: 2628, &#39;自动&#39;: 2775, &#39;音乐&#39;: 3294, &#39;节拍&#39;: 2798, &#39;各国&#39;: 983, &#39;战略&#39;: 1697, &#39;构筑&#39;: 2100, &#39;我国&#39;: 1686, &#39;优势&#39;: 546, &#39;赛事&#39;: 3010, &#39;对标&#39;: 1383, &#39;高手&#39;: 3371, &#39;齐聚&#39;: 3400, &#39;鹭岛&#39;: 3386, &#39;竞技&#39;: 2576, &#39;国家&#39;: 1093, &#39;新一代&#39;: 1920, &#39;平台&#39;: 1489, &#39;将近&#39;: 1397, &#39;280&#39;: 36, &#39;高效&#39;: 3373, &#39;还要&#39;: 3094, &#39;符合&#39;: 2581, &#39;伦理&#39;: 567, &#39;怎么&#39;: 1621, &#39;ibm&#39;: 145, &#39;即将&#39;: 921, &#39;站上&#39;: 2571, &#39;长春&#39;: 3223, &#39;旗山&#39;: 1959, &#39;论谈&#39;: 2923, &#39;专场&#39;: 337, &#39;论坛&#39;: 2922, &#39;举办&#39;: 405, &#39;一项&#39;: 263, &#39;确认&#39;: 2510, &#39;避开&#39;: 3163, &#39;行人&#39;: 2862, &#39;开车&#39;: 1557, &#39;睡觉&#39;: 2483, &#39;现实&#39;: 2390, &#39;性感&#39;: 1633, &#39;在线&#39;: 1111, &#39;程序&#39;: 2558, &#39;猴子&#39;: 2365, &#39;网友&#39;: 2688, &#39;小米&#39;: 1408, &#39;关系&#39;: 723, &#39;arm&#39;: 107, &#39;布局&#39;: 1473, &#39;生态圈&#39;: 2409, &#39;瞄准&#39;: 2484, &#39;联网&#39;: 2744, &#39;设备&#39;: 2925, &#39;来看&#39;: 2091, &#39;是否&#39;: 1999, &#39;需要&#39;: 3273, &#39;拥有&#39;: 1774, &#39;智慧&#39;: 2008, &#39;明白&#39;: 1990, &#39;2019china&#39;: 23, &#39;控制&#39;: 1815, &#39;展示会&#39;: 1432, &#39;官网&#39;: 1335, &#39;上海&#39;: 299, &#39;成立&#39;: 1680, &#39;联盟&#39;: 2742, &#39;集成电路&#39;: 3265, &#39;涂国身&#39;: 2251, &#39;关键&#39;: 724, &#39;用例&#39;: 2417, &#39;一天&#39;: 231, &#39;可能&#39;: 978, &#39;使用&#39;: 610, &#39;创造&#39;: 805, &#39;就业机会&#39;: 1419, &#39;破坏&#39;: 2502, &#39;希鸥&#39;: 1475, &#39;专访&#39;: 340, &#39;拓世&#39;: 1766, &#39;创始人&#39;: 800, &#39;火亮&#39;: 2309, &#39;通用&#39;: 3145, &#39;对战&#39;: 1380, &#39;王者&#39;: 2381, &#39;荣耀&#39;: 2820, &#39;职业&#39;: 2733, &#39;战队&#39;: 1698, &#39;表现&#39;: 2868, &#39;出色&#39;: 773, &#39;落实&#39;: 2835, &#39;风潮&#39;: 3326, &#39;科技领域&#39;: 2546, &#39;带来&#39;: 1477, &#39;变革&#39;: 966, &#39;更为&#39;: 2022, &#39;用户&#39;: 2418, &#39;生活&#39;: 2413, &#39;惊喜&#39;: 1656, &#39;所说&#39;: 1705, &#39;不是&#39;: 322, &#39;口中&#39;: 967, &#39;科学&#39;: 2539, &#39;一起&#39;: 259, &#39;看看&#39;: 2475, &#39;艺术&#39;: 2794, &#39;奇特&#39;: 1241, &#39;美妙&#39;: 2706, &#39;媒介&#39;: 1293, &#39;超越&#39;: 3029, &#39;美国&#39;: 2703, &#39;欧洲联盟&#39;: 2142, &#39;生态系统&#39;: 2410, &#39;调查&#39;: 2963, &#39;寻求&#39;: 1388, &#39;硅谷&#39;: 2504, &#39;夺取&#39;: 1238, &#39;宝座&#39;: 1345, &#39;23&#39;: 30, &#39;概念股&#39;: 2129, &#39;湖南卫视&#39;: 2292, &#39;十年&#39;: 873, &#39;匠心&#39;: 865, &#39;几年&#39;: 763, &#39;专业&#39;: 334, &#39;大获全胜&#39;: 1208, &#39;这个&#39;: 3095, &#39;新起&#39;: 1943, &#39;之秀&#39;: 415, &#39;抗衡&#39;: 1746, &#39;究竟&#39;: 2563, &#39;地方&#39;: 1113, &#39;政府&#39;: 1866, &#39;新宠&#39;: 1930, &#39;先进&#39;: 657, &#39;汽车&#39;: 2199, &#39;目前&#39;: 2463, &#39;谷歌&#39;: 2970, &#39;正在&#39;: 2146, &#39;预测&#39;: 3304, &#39;风电场&#39;: 3327, &#39;输出功率&#39;: 3068, &#39;it&#39;: 150, &#39;领导者&#39;: 3310, &#39;希望&#39;: 1474, &#39;更加&#39;: 2023, &#39;关注&#39;: 721, &#39;道德&#39;: 3157, &#39;一直&#39;: 248, &#39;年前&#39;: 1495, &#39;老本&#39;: 2724, &#39;少年&#39;: 1415, &#39;有个&#39;: 2051, &#39;梦想&#39;: 2124, &#39;一种&#39;: 249, &#39;受欢迎&#39;: 960, &#39;推动力&#39;: 1821, &#39;可以&#39;: 975, &#39;gdp&#39;: 135, &#39;数百万美元&#39;: 1888, &#39;法律&#39;: 2213, &#39;十大&#39;: 872, &#39;前沿&#39;: 831, &#39;问题&#39;: 3231, &#39;威盛&#39;: 1281, &#39;展览会&#39;: 1434, &#39;文艺创作&#39;: 1916, &#39;适恰性&#39;: 3133, &#39;之识&#39;: 418, &#39;无界&#39;: 1967, &#39;探索&#39;: 1810, &#39;边界&#39;: 3071, &#39;当大&#39;: 1583, &#39;结合&#39;: 2662, &#39;哪些&#39;: 1048, &#39;gartner&#39;: 133, &#39;增强&#39;: 1141, &#39;最有&#39;: 2041, &#39;价值&#39;: 532, &#39;应用程序&#39;: 1518, &#39;广元&#39;: 1507, &#39;示范&#39;: 2512, &#39;全国&#39;: 676, &#39;首个&#39;: 3334, &#39;职称&#39;: 2734, &#39;评审会&#39;: 2937, &#39;掀起&#39;: 1797, &#39;革命&#39;: 3291, &#39;医学影像&#39;: 868, &#39;交大&#39;: 470, &#39;同济&#39;: 1001, &#39;复旦&#39;: 1155, &#39;大将&#39;: 1195, &#39;加入&#39;: 841, &#39;遍地开花&#39;: 3156, &#39;霍金&#39;: 3275, &#39;生前&#39;: 2406, &#39;告诫&#39;: 1029, &#39;人类&#39;: 502, &#39;小心&#39;: 1406, &#39;他们&#39;: 522, &#39;真的&#39;: 2482, &#39;能够&#39;: 2765, &#39;威胁&#39;: 1282, &#39;引育&#39;: 1564, &#39;人才&#39;: 497, &#39;天津&#39;: 1220, &#39;持续&#39;: 1779, &#39;引领&#39;: 1567, &#39;牢记&#39;: 2346, &#39;嘱托&#39;: 1069, &#39;天津市&#39;: 1222, &#39;能否&#39;: 2764, &#39;获得&#39;: 2826, &#39;永生&#39;: 2184, &#39;或许&#39;: 1693, &#39;告诉&#39;: 1028, &#39;我们&#39;: 1685, &#39;答案&#39;: 2604, &#39;刘庆峰&#39;: 792, &#39;每个&#39;: 2160, &#39;孩子&#39;: 1317, &#39;肩膀&#39;: 2757, &#39;适应&#39;: 3132, &#39;解读&#39;: 2901, &#39;医疗&#39;: 869, &#39;无限&#39;: 1970, &#39;读研&#39;: 2959, &#39;选择&#39;: 3136, &#39;方向&#39;: 1947, &#39;沐盟&#39;: 2202, ………………………………&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看词袋中词的数目</span></span><br><span class="line"><span class="built_in">len</span>(words_bag)</span><br></pre></td></tr></table></figure>




<pre><code>3402
</code></pre>
<p>从上面的输出结果可以看出，词袋的词汇量很大，然而具体到某一条新闻标题，其中只会出现词袋中的少数几个词，其他大部分的词都不会出现，这就是词频矩阵中有很多0的原因</p>
<p><strong>选取前2条新闻标题做一个简单的演示</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看前两条分词完的新闻</span></span><br><span class="line"><span class="built_in">print</span>(words[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(words[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>信托公司 2019 年 上半年 经营 业绩 概览
首单 信托 型 企业 ABS 获批
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文本向量化</span></span><br><span class="line">vect = CountVectorizer()  <span class="comment"># 引入CountVectorizer()函数</span></span><br><span class="line">X_test = vect.fit_transform(words[<span class="number">0</span>:<span class="number">2</span>])  <span class="comment"># 将前两条新闻文本向量化</span></span><br><span class="line">X_test = X_test.toarray()  <span class="comment"># 将X_test转为数组</span></span><br><span class="line"><span class="built_in">print</span>(X_test)  <span class="comment"># 查看生成的二维数组</span></span><br></pre></td></tr></table></figure>

<pre><code>[[1 0 1 1 0 0 1 1 1 0 0]
 [0 1 0 0 1 1 0 0 0 1 1]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看词袋的第一种方式</span></span><br><span class="line">words_bag = vect.vocabulary_  <span class="comment"># 第一种查看词袋的方式</span></span><br><span class="line"><span class="built_in">print</span>(words_bag)  <span class="comment"># 查看词袋</span></span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;信托公司&#39;: 6, &#39;2019&#39;: 0, &#39;上半年&#39;: 2, &#39;经营&#39;: 8, &#39;业绩&#39;: 3, &#39;概览&#39;: 7, &#39;首单&#39;: 10, &#39;信托&#39;: 5, &#39;企业&#39;: 4, &#39;abs&#39;: 1, &#39;获批&#39;: 9&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看词袋的第二种方式</span></span><br><span class="line">words_bag2 = vect.get_feature_names_out()  <span class="comment"># 第二种查看词袋的方法</span></span><br><span class="line"><span class="built_in">print</span>(words_bag2)  <span class="comment"># 第二种查看词袋的方式</span></span><br></pre></td></tr></table></figure>

<pre><code>[&#39;2019&#39; &#39;abs&#39; &#39;上半年&#39; &#39;业绩&#39; &#39;企业&#39; &#39;信托&#39; &#39;信托公司&#39; &#39;概览&#39; &#39;经营&#39; &#39;获批&#39; &#39;首单&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame(X_test, columns=words_bag2)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>2019</th>
      <th>abs</th>
      <th>上半年</th>
      <th>业绩</th>
      <th>企业</th>
      <th>信托</th>
      <th>信托公司</th>
      <th>概览</th>
      <th>经营</th>
      <th>获批</th>
      <th>首单</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将之前所有的新闻标题进行文本向量化并通过pandas展示</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本向量化</span></span><br><span class="line">vect = CountVectorizer()</span><br><span class="line">X = vect.fit_transform(words)  <span class="comment"># 将分词后的内容文本向量化</span></span><br><span class="line">X = X.toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文本向量化的结果</span></span><br><span class="line">words_bag2 = vect.get_feature_names_out()  <span class="comment"># 第二种查看词袋的方法</span></span><br><span class="line">df = pd.DataFrame(X, columns=words_bag2)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>00700</th>
      <th>03</th>
      <th>04</th>
      <th>08s</th>
      <th>09</th>
      <th>10</th>
      <th>100</th>
      <th>11</th>
      <th>12</th>
      <th>150</th>
      <th>...</th>
      <th>黄萍</th>
      <th>黄金</th>
      <th>黑客</th>
      <th>黑灰产</th>
      <th>黑金</th>
      <th>黑马</th>
      <th>鼓手</th>
      <th>鼻祖</th>
      <th>齐聚</th>
      <th>龙风</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 3402 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # 如果想显示pandas中DataFrmae所有行，或者所有列，可以采用下面的代码</span></span><br><span class="line"><span class="comment"># import pandas as pd</span></span><br><span class="line"><span class="comment"># pd.set_option(&#x27;display.max_columns&#x27;, None)  # 显示所有列，如果把None改成100则显示100列</span></span><br><span class="line"><span class="comment"># pd.set_option(&#x27;display.max_rows&#x27;, None)  # 显示所有行，如果把None改成100则显示100行</span></span><br><span class="line"><span class="comment"># df = pd.DataFrame(X, columns=words_bag2)</span></span><br><span class="line"><span class="comment"># df.head()</span></span><br></pre></td></tr></table></figure>

<p><strong>总结</strong>：当有n条新闻标题时，先用jieba库对它们进行分词，然后用CountVectorizer()函数提取所有分词中k个不同的词，用这些词构成一个词袋，每个词对应一个编号，即相应的特征，根据原标题中相关词出现的次数来赋值相关特征为i（即相关词出现的次数），这样就完成了文本数值化的工作，接下来进行模型的搭建与使用</p>
<h3 id="13-3-3-模型的搭建与使用"><a href="#13-3-3-模型的搭建与使用" class="headerlink" title="13.3.3 模型的搭建与使用"></a>13.3.3 模型的搭建与使用</h3><p><strong>1.通过KMeans算法进行聚类分群</strong></p>
<p>本案例的原始数据是根据10个关键词爬取的新闻，下面利用KMeans算法搭建模型进行聚类分群，看看它能否将来自10个不同题材的新闻准确地分类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">kms = KMeans(n_clusters=<span class="number">10</span>, random_state=<span class="number">123</span>) <span class="comment"># 将样本聚成10类</span></span><br><span class="line">k_data = kms.fit_predict(df) <span class="comment"># 用fit_predict()函数将模型拟合训练和聚类结果传递合二为一</span></span><br><span class="line"><span class="built_in">print</span>(k_data)</span><br></pre></td></tr></table></figure>


<pre><code>[0 0 3 3 0 0 0 0 0 8 0 8 8 0 0 0 0 8 0 0 8 0 7 0 0 0 0 0 0 0 8 0 8 0 8 0 0
 0 7 8 0 8 0 8 0 0 0 7 8 0 8 0 8 0 0 0 7 8 0 8 0 8 0 0 0 7 8 0 8 0 8 0 0 0
 7 8 0 8 0 8 0 0 0 7 8 0 8 0 8 0 0 0 7 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 9
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 0 3 3 3 0 3 3 9 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 2 0 0
 2 2 0 2 2 2 0 0 0 2 0 2 2 2 0 2 5 5 2 0 2 2 2 5 5 0 2 0 2 0 0 2 0 0 2 5 2
 2 5 5 2 0 0 0 0 2 2 2 2 0 5 2 2 0 2 2 2 2 5 2 5 2 2 2 5 2 2 0 2 2 2 2 0 0
 5 2 0 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0
 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 5 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1
 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1
 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 6 1 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 4
 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 4 0 0 0 0 0 4 0 0 0 0 0]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">words_ary = np.array(words)</span><br><span class="line"><span class="built_in">print</span>(words_ary[k_data == <span class="number">2</span>])  <span class="comment"># 可以把数字1改成其他数字看看效果</span></span><br></pre></td></tr></table></figure>

<pre><code>[&#39;数字 媒体 的 体育 版权 经营 逻辑&#39; &#39;关心 下一代 华夏 国际 体育 训练营 丨 美国 体育 训练营&#39;
 &#39;左手 优酷 体育 右手 苏宁 体育   阿里 体育 组队 围攻 腾讯 体育&#39;
 &#39;新 赛季 “ 抢人 ” 大战 正酣   优酷 体育 会员 悄然 下架&#39; &#39;学校 体育 资源 开放 , 步子 再 快 一点&#39;
 &#39;PP 体育 独家 呈现 英超 精彩   会员 专享 50 帧 直播 技术&#39; &#39;广州 : 中考 体育 跳绳 满分 标准 逐步提高&#39;
 &#39;2019 重庆 青少年 体育 夏令营 关心 关爱 留守 儿童&#39; &#39;第三届 “ 全国 小学 体育 活力 校园 创新奖 ” 揭晓&#39;
 &#39;二青会 湖北 体育 代表团 誓师大会 胜利 召开&#39; &#39;官宣 了 ! 洲明 体育 正式 成为 国际 篮联 全球 供应商&#39;
 &#39;体育彩票 公益金 支持 湖南 体育 舞蹈 锦标赛&#39; &#39;中国 传媒大学 推出 体育 传播 MBA , 培养 全球 体育产业 人才&#39;
 &#39;记得 《 大 李小李 和 老李 》 吗   演绎 体育 经典电影 底色&#39; &#39;体育 与 健康 _ 20190807&#39;
 &#39;惠州 将 举行 系列 体育 活动 迎接 全民 健身 日 的 到来&#39; &#39;惠州 将 举行 系列 体育 活动 迎接 全民 健身 日 的 到来&#39;
 &#39;新政 | 让 学生 学会 两项 体育 技能 一项 艺术 爱好 , 上海 “ 一条龙 ” 布局 ...&#39;
 &#39;响应 国家 全民 健身 , 超级 猩猩 为 互联网 + 体育 助力 发展&#39; &#39;体育 惠民 , 让 全城 “ 动 起来 ”&#39;
 &#39;新 中国 成立 70 周年 中国 体育 巨变 纵览 之三 : 专业 体育 和 全民 健身&#39; &#39;太原 : 一个 城市 的 体育 成长&#39;
 &#39;婺源 百姓 吃 上 “ 体育 饭 ”&#39; &#39;重庆 两个 社区 体育 文化公园 建成 迎客&#39; &#39;社区 体育 文化公园   漂亮 实用 更 人性化&#39;
 &#39;海看 体育 新 服务 发布&#39; &#39;女排 首夺 大满贯 , 国足 04 亚洲杯 折戟 工体 , 中国 体育 历史 上 的 8 月 7 日&#39;
 &#39;体育 + 旅游 的 有效 尝试   阿坝 分赛区 完美 呈现 !&#39;
 &#39;上海 建工 建设者 连续 奋战   市民 体育 公园 ( 一期 ) 下 月 基本 建成&#39;
 &#39;英超 开赛 在 即   苏宁 体育 王冬 : 苏宁 布局 体育 的 逻辑&#39;
 &#39;跳绳 、 中长跑 更难 拿 满分 ! 广州 中考 体育 改革 2021 年 起 实施&#39;
 &#39;跳绳 、 中长跑 更难 拿 满分 ! 广州 中考 体育 改革 2021 年 起 实施&#39;
 &#39;体育 大 V 和 MCN 如何 在 微博玩 出新 套路 ? 邓亚萍 及 皇马 、 拜仁 如是说&#39;
 &#39;2000 多名 体育 爱好者 共享 “ 全民 健身 日 ”&#39;
 &#39;看见 | 第二批 社区 体育 文化公园 今日 开园   这次 有 很多 便民 的 人性化 设计&#39;
 &#39;重磅 ! PP 体育 全场 次 独家 视频 直播 英超 + 超级 福利&#39;
 &#39;重庆 今年 将 建成 20 个 社区 体育 文化公园   本月 起 陆续 开放&#39;
 &#39;苏州 体育 惠民 消费 行动 火热 进行 中   办卡 可享 诸多 优惠 补贴&#39; &#39;我省 制定 计划 促进 体育 消费&#39;
 &#39;启蒙 《 体育 三字经 》 亚博 体育 俱乐部 公益 开放日&#39; &#39;平湖 老人 诠释 体育 人生 之美 】&#39;
 &#39;王国生 : 守初 心担 使命 强 责任 抓 落实   高质量 办好 民族 体育 盛会&#39;
 &#39;“ 智慧 + 体育 ” 全国 首批 智慧 社区 健身 中心 在 雨花台区 启用&#39;
 &#39;莱昂纳多 转型 记 : 从 球员 到 体育 总监 , 广泛 的 足球圈 人脉助 一臂之力&#39;
 &#39;“ 足球 + 时尚 ” 跨界 升级 球迷 体验   天猫 引领 体育 消费 蓝海&#39;
 &#39;首座 体育场馆 落成 , 阿里 体育 继续 拼 体育 服务&#39; &#39;海看 体育 新 服务 发布会 举行   为 体育产业 发展 提供 新 动能&#39;
 &#39;动因 体育 篮球 技术 总监 麦迪要 来 了 !&#39; &#39;新利 18app 体育&#39; &#39;婺源 : “ 体育 风 ” 吹热 乡村 振兴&#39;
 &#39;中考 体育 培训 大热 , 学业 压力 为何 难成 行业 动力 ?&#39; &#39;山西 代县 的 体育 脉动&#39; &#39;体育 六艺   孔子 故里 发 新芽&#39;
 &#39;体育 还 得 从 娃娃 抓起&#39; &#39;梅列 阳光 体育   让 生命 更 阳光&#39;
 &#39;《 体育 鹅 》 —   体育 鹅 - 体育迷 及 体育 游戏 爱好者 的 娱乐 营地&#39;
 &#39;职业 体育 有 多 残酷 ? 足球 先生 、 NBA 得分王 亦 难 体面 告别&#39;
 &#39;江苏 : 2022 年 体育 消费 总 规模 达 2800 亿元&#39; &#39;以 体育 人   为 爱而赛&#39;
 &#39;姚明 多次 谈 体育 教育 到底 谈 了 什么 ?&#39;
 &#39;体育 、 艺术 科考 将 影响 高考 ! 广东 2018 或 2019 年 入学 新生 开始 试点 !&#39;]
</code></pre>
<p>可以看到，分类为2的新闻标题大多是和体育相关的</p>
<p><strong>2.通过DBSCAN算法进行聚类分群</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line">dbs = DBSCAN(eps=<span class="number">1</span>, min_samples=<span class="number">3</span>)</span><br><span class="line">d_data = dbs.fit_predict(df)</span><br><span class="line"><span class="built_in">print</span>(d_data)</span><br></pre></td></tr></table></figure>

<pre><code>[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8
  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5
  6  7  8  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1  9 10 11  9 10 11  9 10 11  9 10 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 12 13 14 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 12 13 14 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 12 13 14 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1]
</code></pre>
<p>可以看到DBSCAN算法对新闻标题的聚类效果较差，其中有大量的离群点（-1），即不知道这些新闻标题属于什么分类，因为进行文本向量化后，每个新闻标题都有3402个特征，过多的特征容易导致样本点间距离较远，从而产生离群点，因此对于新闻文本而言，KMeans算法的聚类效果很好，DBSCAN算法的聚类效果则不尽如人意，这也说明了对于特征变量较多的数据，KMeans算法的聚类效果要优于DBSCAN算法的聚类效果</p>
<h3 id="13-3-4-模型优化（利用余弦相似度进行优化）"><a href="#13-3-4-模型优化（利用余弦相似度进行优化）" class="headerlink" title="13.3.4 模型优化（利用余弦相似度进行优化）"></a>13.3.4 模型优化（利用余弦相似度进行优化）</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(4).png" alt="下载 (4)"></p>
<p><strong>1.模型误差产生的原因</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(5).png" alt="下载 (5)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(6).png" alt="下载 (6)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">words_test = [<span class="string">&#x27;想去 华能 信托&#x27;</span>, <span class="string">&#x27;华能 信托 很好 想去&#x27;</span>, <span class="string">&#x27;华能 信托 很好 想去 华能 信托 很好 想去&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本向量化</span></span><br><span class="line">vect = CountVectorizer()</span><br><span class="line">X_test = vect.fit_transform(words_test)  <span class="comment"># 将分词后的内容文本向量化</span></span><br><span class="line">X_test = X_test.toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文本向量化的结果</span></span><br><span class="line">words_bag2 = vect.get_feature_names_out()  <span class="comment"># 第二种查看词袋的方法</span></span><br><span class="line">df_test = pd.DataFrame(X_test, columns=words_bag2)</span><br><span class="line">df_test.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>信托</th>
      <th>华能</th>
      <th>很好</th>
      <th>想去</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



<p><img src="%E4%B8%8B%E8%BD%BD%20(7).png" alt="下载 (7)"></p>
<p>这种因为文本长短造成的预测不准确可以通过余弦相似度来解决。余弦相似度是根据向量的夹角而非距离来判断相似度的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 补充知识点：通过numpy库计算欧式距离</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">dist = np.linalg.norm(df_test.iloc[<span class="number">0</span>] - df_test.iloc[<span class="number">1</span>])</span><br><span class="line">dist</span><br></pre></td></tr></table></figure>




<pre><code>1.0
</code></pre>
<p><strong>2.余弦相似度的数学原理</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(8).png" alt="下载 (8)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(9).png" alt="下载 (9)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(10).png" alt="下载 (10)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(11).png" alt="下载 (11)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(12).png" alt="下载 (12)"></p>
<p><strong>3.余弦相似度的python代码实现</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(13).png" alt="下载 (13)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算余弦相似度</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">cosine_similarities  = cosine_similarity(df_test)</span><br><span class="line">cosine_similarities</span><br></pre></td></tr></table></figure>




<pre><code>array([[1.       , 0.8660254, 0.8660254],
       [0.8660254, 1.       , 1.       ],
       [0.8660254, 1.       , 1.       ]])
</code></pre>
<p>上述结果为3行3列的二维数组，第i行第j列的数字表示第i个数据和第j个数据的余弦相似度，如第二行第三列的数字1是第二条新闻标题和第三条新闻标题的余弦相似度</p>
<p><strong>4.余弦相似度实战 - 模型优化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">cosine_similarities  = cosine_similarity(df)</span><br><span class="line"><span class="built_in">print</span>(cosine_similarities)</span><br></pre></td></tr></table></figure>

<pre><code>[[1.         0.         0.         ... 0.         0.         0.        ]
 [0.         1.         0.14142136 ... 0.         0.         0.        ]
 [0.         0.14142136 1.         ... 0.         0.         0.        ]
 ...
 [0.         0.         0.         ... 1.         0.18490007 0.10050378]
 [0.         0.         0.         ... 0.18490007 1.         0.0836242 ]
 [0.         0.         0.         ... 0.10050378 0.0836242  1.        ]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">kms = KMeans(n_clusters=<span class="number">10</span>, random_state=<span class="number">123</span>)</span><br><span class="line">k_data = kms.fit_predict(cosine_similarities)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(k_data)</span><br></pre></td></tr></table></figure>

<pre><code>[1 3 3 3 3 3 3 1 1 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 1 1 3 1 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 1 0 0 0 0 1 1 8 8 8 8 1 8 8 8 8 8 8 8 8 8 8 8 8 8 1 8 8 8 8 8 8 8 8
 8 8 8 8 8 8 8 8 8 8 8 8 8 8 1 8 8 1 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8
 8 8 8 1 8 8 8 1 8 8 8 8 8 8 8 8 1 8 8 8 8 8 8 8 8 8 8 8 8 1 8 8 8 8 7 1 1
 7 7 1 7 7 7 1 1 1 7 1 7 7 7 1 7 7 7 7 1 7 7 7 7 7 1 7 1 7 1 1 7 1 1 7 7 7
 7 7 1 7 1 1 1 1 7 7 7 7 1 7 7 7 1 7 7 7 7 7 7 7 7 7 7 7 7 7 1 7 7 7 7 1 1
 7 7 1 7 7 7 7 7 7 7 7 7 1 7 7 7 7 7 7 7 7 1 7 4 4 4 4 4 4 4 1 4 4 4 4 4 4
 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 8 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2
 2 2 2 1 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 1 1 2 1 1 2 8 2 2 2 1 2 2 2 2 2
 5 2 2 1 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2 6 2 6
 6 6 6 6 6 6 0 6 6 6 6 6 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 5 8 6 6 6 5 6 6 6 6 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 5 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 2 5 5 5 5 2 5 5
 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5
 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5
 5 5 5 5 5 5 5 0 5 5 5 9 9 9 9 9 9 9 9 9 9 1 9 9 9 0 9 1 9 9 9 9 9 9 9 9 9
 9 9 9 9 9 9 9 9 9 9 9 1 9 9 9 9 9 9 9 1 9 9 9 9 1 9 9 9 9 9 9 9 9 9 9 9 9
 9 9 1 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 1 9 9 9 9 9 9]
</code></pre>
<p>可以看到，利用余弦相似度优化后的KMeans模型能够较好地将新闻分成10类，并且每类新闻的数量都比较接近，与原始数据的实际情况比较一致</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看分类结果</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">words_ary = np.array(words)</span><br><span class="line"><span class="built_in">print</span>(words_ary[k_data == <span class="number">3</span>])  <span class="comment"># 可以把数字3改成其他数字看看效果</span></span><br></pre></td></tr></table></figure>

<pre><code>[&#39;首单 信托 型 企业 ABS 获批&#39; &#39;华能 贵 诚信 托孙磊 : 金融 科技 助力 打造 开放 信托 生态&#39;
 &#39;华能 贵 诚信 托孙磊 : 金融 科技 已经 成为 信托 行业 重要 的 基础设施&#39;
 &#39;格力电器 股权 转让 意向 方 闭门 开会   华能 信托 赫然 在 列&#39;
 &#39;直击 格力电器 意向 投资者 见面会 : 参会者 华能 信托 背后 现 国务院 国资委 ...&#39;
 &#39;格力电器 股权 转让 意向 投资者 见面会 召开   自称 华能 信托 的 人士 到场&#39;
 &#39;... 或 200 亿 收购 中 江 信托   50 亿爆雷 “ 烫手山芋 ” 如何 处置 ? ; 华能 信托 ...&#39;
 &#39;华能 信托 是 外界 传言 泰禾 引进 战投 的 目标 之一&#39;
 &#39;... 或 200 亿 收购 中 江 信托   50 亿爆雷 “ 烫手山芋 ” 如何 处置 ? ; 华能 信托 ...&#39;
 &#39;【 行业动态 】 山东 信托 2018 年报   规模 缩水 ; 信托 曲线 上市 浪潮 再度 涌 ...&#39;
 &#39;华能 贵 诚信 托换帅 , 孙磊 出任 总经理&#39; &#39;华能 信托 换帅 总经理 孙磊 任职 资格 获批&#39;
 &#39;2019 年 到期 债务 达 590 亿 ! 泰禾 集团 找 华能 信托 来 救命&#39;
 &#39;华能 国际 : 华能 资本 是 华能 贵 诚信 托 有限公司 大 股东&#39;
 &#39;华能 信托 、 东莞 信托 昨日 双双 增资   今年 信托公司 注册资本 增加 总额 逾 ...&#39;
 &#39;华能 信托 注册 资本金 由 42 亿元 增资 至 61.94 亿&#39;
 &#39;时隔 近两年 再次 增资   华能 信托 注册资本 增至 61.95 亿元&#39;
 &#39;肖钢 密集 调研 资产 证券化 业务   走 访华 能 信托 和 中信 信托&#39;
 &#39;速睹 62 家 信托 上半年 业绩 ! 平安 中信 华能 位列 前 三&#39; &#39;华能 信托 总经理 金志 培 : 五大 秘诀 塑造 核心 竞争力&#39;
 &#39;华能 信托 : 信托公司 参与 消费 金融 的 新 机会 与 模式分析&#39; &#39;华能 信托&#39; &#39;新任 总经理 到位   五矿 信托 谋 突围&#39;
 &#39;五矿 信托 首任 总经理 辞职   接任 者 或 为 华能 信托 王卓&#39;
 &#39;华夏 幸福 关于 拟 与 华能 信托 签署 《 增资 协议 》 的 公告&#39;
 &#39;普邦 股份 : 华能 信托 . 普邦 1 号 集合 资金 信托 计划 信托 合同&#39;
 &#39;华能 信托 试水 首单 不良资产 收益权 转让 已有 46 家 信托公司 与 银登 中心 ...&#39;
 &#39;北京 银行 携手 华能 、 中航 信托 创新 慈善 信托 模式&#39;
 &#39;ACCA - 华能 信托 “ 财经 领袖 培养 计划 ” 第一期 学员 选拔 结果 公布&#39; &#39;华能 贵 诚信 托 有限公司&#39;
 &#39;国金 ABS 云   ·   早报 丨 招行 与 华能 信托 将 合作 发行 99 亿元 ABS&#39;
 &#39;用益 - 信托 日报 : 平安 江苏 中信 华能 位列 前四 ! 58 家 信托 上半年 净利 排位 !&#39;
 &#39;五矿 信托 首任 总经理 辞职   接任 者 或 为 华能 信托 王卓&#39;
 &#39;华夏 幸福 关于 拟 与 华能 信托 签署 《 增资 协议 》 的 公告&#39;
 &#39;普邦 股份 : 华能 信托 . 普邦 1 号 集合 资金 信托 计划 信托 合同&#39;
 &#39;华能 信托 试水 首单 不良资产 收益权 转让 已有 46 家 信托公司 与 银登 中心 ...&#39;
 &#39;北京 银行 携手 华能 、 中航 信托 创新 慈善 信托 模式&#39;
 &#39;ACCA - 华能 信托 “ 财经 领袖 培养 计划 ” 第一期 学员 选拔 结果 公布&#39; &#39;华能 贵 诚信 托 有限公司&#39;
 &#39;国金 ABS 云   ·   早报 丨 招行 与 华能 信托 将 合作 发行 99 亿元 ABS&#39;
 &#39;用益 - 信托 日报 : 平安 江苏 中信 华能 位列 前四 ! 58 家 信托 上半年 净利 排位 !&#39;
 &#39;五矿 信托 首任 总经理 辞职   接任 者 或 为 华能 信托 王卓&#39;
 &#39;华夏 幸福 关于 拟 与 华能 信托 签署 《 增资 协议 》 的 公告&#39;
 &#39;普邦 股份 : 华能 信托 . 普邦 1 号 集合 资金 信托 计划 信托 合同&#39;
 &#39;华能 信托 试水 首单 不良资产 收益权 转让 已有 46 家 信托公司 与 银登 中心 ...&#39;
 &#39;北京 银行 携手 华能 、 中航 信托 创新 慈善 信托 模式&#39;
 &#39;ACCA - 华能 信托 “ 财经 领袖 培养 计划 ” 第一期 学员 选拔 结果 公布&#39; &#39;华能 贵 诚信 托 有限公司&#39;
 &#39;国金 ABS 云   ·   早报 丨 招行 与 华能 信托 将 合作 发行 99 亿元 ABS&#39;
 &#39;用益 - 信托 日报 : 平安 江苏 中信 华能 位列 前四 ! 58 家 信托 上半年 净利 排位 !&#39;
 &#39;五矿 信托 首任 总经理 辞职   接任 者 或 为 华能 信托 王卓&#39;
 &#39;华夏 幸福 关于 拟 与 华能 信托 签署 《 增资 协议 》 的 公告&#39;
 &#39;普邦 股份 : 华能 信托 . 普邦 1 号 集合 资金 信托 计划 信托 合同&#39;
 &#39;华能 信托 试水 首单 不良资产 收益权 转让 已有 46 家 信托公司 与 银登 中心 ...&#39;
 &#39;北京 银行 携手 华能 、 中航 信托 创新 慈善 信托 模式&#39;
 &#39;ACCA - 华能 信托 “ 财经 领袖 培养 计划 ” 第一期 学员 选拔 结果 公布&#39; &#39;华能 贵 诚信 托 有限公司&#39;
 &#39;国金 ABS 云   ·   早报 丨 招行 与 华能 信托 将 合作 发行 99 亿元 ABS&#39;
 &#39;用益 - 信托 日报 : 平安 江苏 中信 华能 位列 前四 ! 58 家 信托 上半年 净利 排位 !&#39;
 &#39;五矿 信托 首任 总经理 辞职   接任 者 或 为 华能 信托 王卓&#39;
 &#39;华夏 幸福 关于 拟 与 华能 信托 签署 《 增资 协议 》 的 公告&#39;
 &#39;普邦 股份 : 华能 信托 . 普邦 1 号 集合 资金 信托 计划 信托 合同&#39;
 &#39;华能 信托 试水 首单 不良资产 收益权 转让 已有 46 家 信托公司 与 银登 中心 ...&#39;
 &#39;北京 银行 携手 华能 、 中航 信托 创新 慈善 信托 模式&#39;
 &#39;ACCA - 华能 信托 “ 财经 领袖 培养 计划 ” 第一期 学员 选拔 结果 公布&#39; &#39;华能 贵 诚信 托 有限公司&#39;
 &#39;国金 ABS 云   ·   早报 丨 招行 与 华能 信托 将 合作 发行 99 亿元 ABS&#39;
 &#39;用益 - 信托 日报 : 平安 江苏 中信 华能 位列 前四 ! 58 家 信托 上半年 净利 排位 !&#39;
 &#39;五矿 信托 首任 总经理 辞职   接任 者 或 为 华能 信托 王卓&#39;
 &#39;华夏 幸福 关于 拟 与 华能 信托 签署 《 增资 协议 》 的 公告&#39;
 &#39;普邦 股份 : 华能 信托 . 普邦 1 号 集合 资金 信托 计划 信托 合同&#39;
 &#39;华能 信托 试水 首单 不良资产 收益权 转让 已有 46 家 信托公司 与 银登 中心 ...&#39;
 &#39;北京 银行 携手 华能 、 中航 信托 创新 慈善 信托 模式&#39;
 &#39;ACCA - 华能 信托 “ 财经 领袖 培养 计划 ” 第一期 学员 选拔 结果 公布&#39; &#39;华能 贵 诚信 托 有限公司&#39;
 &#39;国金 ABS 云   ·   早报 丨 招行 与 华能 信托 将 合作 发行 99 亿元 ABS&#39;
 &#39;用益 - 信托 日报 : 平安 江苏 中信 华能 位列 前四 ! 58 家 信托 上半年 净利 排位 !&#39;
 &#39;五矿 信托 首任 总经理 辞职   接任 者 或 为 华能 信托 王卓&#39;
 &#39;华夏 幸福 关于 拟 与 华能 信托 签署 《 增资 协议 》 的 公告&#39;
 &#39;普邦 股份 : 华能 信托 . 普邦 1 号 集合 资金 信托 计划 信托 合同&#39;
 &#39;华能 信托 试水 首单 不良资产 收益权 转让 已有 46 家 信托公司 与 银登 中心 ...&#39;
 &#39;北京 银行 携手 华能 、 中航 信托 创新 慈善 信托 模式&#39;
 &#39;ACCA - 华能 信托 “ 财经 领袖 培养 计划 ” 第一期 学员 选拔 结果 公布&#39; &#39;华能 贵 诚信 托 有限公司&#39;
 &#39;国金 ABS 云   ·   早报 丨 招行 与 华能 信托 将 合作 发行 99 亿元 ABS&#39;
 &#39;用益 - 信托 日报 : 平安 江苏 中信 华能 位列 前四 ! 58 家 信托 上半年 净利 排位 !&#39;]
</code></pre>
<p>可以看到是有关财经类的新闻</p>
<hr>
<p><strong>补充：大数据分词：jieba库的使用</strong></p>
<p>1.jieba库的安装与简单演示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">word = jieba.cut(<span class="string">&#x27;我爱北京天安门&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> word:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure>

<pre><code>我
爱
北京
天安门
</code></pre>
<p>注意，用cut()函数分词得到的word不是一个列表，而是一个迭代器，所谓迭代器其实和列表很相似，可以把它理解成一个“隐身的列表”，但是迭代器里的元素要通过for循环来访问，所以第3-4行代码不能改写成print(word)</p>
<p>2.读取文本内容并进行分词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">report = <span class="built_in">open</span>(<span class="string">&#x27;信托行业年度报告.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>).read()</span><br><span class="line">words = jieba.cut(report)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">    <span class="built_in">print</span>(word)</span><br></pre></td></tr></table></figure>

<pre><code>2017
年
信托业
面临
着
较为
复杂
的
外部环境
。
一方面
，
全球
经济
逐步
复苏
，
中国
经济
持续
向
好
，
实体
经济
结构调整
，
新兴产业
发展
迅猛
，
居民
财富
不断
增长
，
这些
均
为
信托业
的
发展
带来
新
的
业务
机会
；
另一方面
，
社会
资金
供给
整体
偏紧
，
传统
业务
领域
不可
持续
，
风险
暴露
逐渐
增加
，
金融监管
持续
收紧
，
进入
“
统筹
协调
监管
”
的
新
阶段
，
信托业
的
转型
发展
压力
依然
较大
。
在
此
背景
下
，
信托公司
纷纷
将
战略
制定
和
战略
管理工作
提高
到
更加
重要
的
层面
，
包含
战略
制定
、
战略
分解
、
战略
监督
、
战略
评估
等
在内
的
战略
管理体系
逐步
建成
，
聚焦
公司
核心
竞争力
，
为
企业
的
长远
发展
作出
整体性
、
长期性
、
连续性
、
全局性
的
规划
方案
。
………………………………    
</code></pre>
<p>3.提取分词后的4字词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">words = jieba.cut(report)  <span class="comment"># 这里得重新jieba.cut()一下，因为之前的words用过一次就被清空了</span></span><br><span class="line">report_words = []</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words:  <span class="comment"># 将大于4个字的词语放入列表</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(word) &gt;= <span class="number">4</span>:</span><br><span class="line">        report_words.append(word)</span><br><span class="line"><span class="built_in">print</span>(report_words)</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;2017&#39;, &#39;外部环境&#39;, &#39;结构调整&#39;, &#39;新兴产业&#39;, &#39;另一方面&#39;, &#39;金融监管&#39;, &#39;信托公司&#39;, &#39;管理工作&#39;, &#39;管理体系&#39;, &#39;2017&#39;, &#39;全面落实&#39;, &#39;结构调整&#39;, &#39;初见成效&#39;, &#39;金融监管&#39;, &#39;小康社会&#39;, &#39;深远影响&#39;, &#39;2017&#39;, &#39;多管齐下&#39;, &#39;长效机制&#39;, &#39;长效机制&#39;, &#39;建立健全&#39;, &#39;变化趋势&#39;, &#39;合作伙伴&#39;, &#39;积极探索&#39;, &#39;另一方面&#39;, &#39;积极探索&#39;, &#39;更新改造&#39;, &#39;REITs&#39;, &#39;市场前景&#39;, &#39;REITs&#39;, &#39;信托公司&#39;, &#39;充分发挥&#39;, &#39;REITs&#39;, &#39;REITs&#39;, &#39;REITs&#39;, &#39;2017&#39;, &#39;信托公司&#39;, &#39;信托公司&#39;, &#39;科技园区&#39;, &#39;CMBS&#39;, &#39;住宅建设&#39;, &#39;REITs&#39;, &#39;投资信托&#39;, &#39;基础设施&#39;, &#39;新形势下&#39;, &#39;基础产业&#39;, &#39;基础产业&#39;, &#39;交易方式&#39;, &#39;2017&#39;, &#39;信托公司&#39;, &#39;基础设施&#39;, &#39;基础产业&#39;, &#39;配套工程&#39;, &#39;有限公司&#39;, &#39;国际展览中心&#39;, &#39;管理机构&#39;, &#39;市场需求&#39;, &#39;金融机构&#39;, &#39;快速增长&#39;, &#39;2016&#39;, &#39;金融机构&#39;, &#39;金融机构&#39;, &#39;金融机构&#39;, &#39;信托公司&#39;, &#39;与此同时&#39;, &#39;信托公司&#39;, &#39;保险市场&#39;, &#39;金融工具&#39;, &#39;信托公司&#39;, &#39;传统模式&#39;, &#39;信托公司&#39;, &#39;事务管理&#39;, &#39;主导作用&#39;, &#39;信托公司&#39;, &#39;2017&#39;, &#39;常务会议&#39;, &#39;2025&#39;, &#39;2025&#39;, &#39;与此同时&#39;, &#39;高度重视&#39;, &#39;金融服务&#39;, &#39;明确指出&#39;, &#39;金融体制&#39;, &#39;金融服务&#39;, &#39;贯彻落实&#39;, &#39;2017&#39;, &#39;大力开展&#39;, &#39;新兴产业&#39;, &#39;信托公司&#39;, &#39;航空航天&#39;, &#39;新兴产业&#39;, &#39;贡献力量&#39;, &#39;生物医药&#39;, &#39;基础设施&#39;, &#39;现代农业&#39;, &#39;国家统计局&#39;, &#39;2017&#39;, &#39;58.8%&#39;, &#39;信托公司&#39;, &#39;积极开展&#39;, &#39;2014&#39;, &#39;金融业务&#39;, &#39;管理制度&#39;, &#39;管理系统&#39;, &#39;金融公司&#39;, &#39;金融公司&#39;, &#39;抢占市场&#39;, &#39;纷繁复杂&#39;, &#39;2017&#39;, &#39;金融业务&#39;, &#39;信托公司&#39;, &#39;信托公司&#39;, &#39;高附加值&#39;, &#39;事务管理&#39;, &#39;组成部分&#39;, &#39;2017&#39;, &#39;信托公司&#39;, &#39;友邦保险&#39;, &#39;人寿保险&#39;, &#39;金融机构&#39;, &#39;信托公司&#39;, &#39;保险公司&#39;, &#39;信托公司&#39;, &#39;信托公司&#39;, &#39;信托公司&#39;, &#39;信托公司&#39;, &#39;除此之外&#39;, &#39;2017&#39;, &#39;信托公司&#39;, &#39;美好生活&#39;, &#39;管理体系&#39;, &#39;管理决策&#39;, &#39;绩效考核&#39;, &#39;信息系统&#39;，………………]
</code></pre>
<p>4.统计高频词汇的词频</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">result = Counter(report_words) </span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<pre><code>Counter(&#123;&#39;信托公司&#39;: 1391, &#39;2017&#39;: 577, &#39;2016&#39;: 184, &#39;金融机构&#39;: 148, &#39;投资信托&#39;: 108, &#39;基础产业&#39;: 91, &#39;2018&#39;: 87, &#39;风险管理&#39;: 82, &#39;工商企业&#39;: 77, &#39;QDII&#39;: 70, &#39;金融服务&#39;: 69, &#39;信息系统&#39;: 63, &#39;2015&#39;: 59, &#39;基础设施&#39;: 56, &#39;金融公司&#39;: 47, &#39;另一方面&#39;: 45, &#39;信托投资公司&#39;: 45, &#39;中国人民银行&#39;: 44, &#39;REITs&#39;: 39, &#39;金融业务&#39;: 38, &#39;监管部门&#39;: 35, &#39;客户服务&#39;: 33, &#39;2013&#39;: 32, &#39;新兴产业&#39;: 31, &#39;资金来源&#39;: 31, &#39;商业银行&#39;: 30, &#39;信息技术&#39;: 29, &#39;金融市场&#39;: 29, &#39;2014&#39;: 28, &#39;有限公司&#39;: 24, &#39;债券市场&#39;: 24, &#39;管理体系&#39;: 23, &#39;发展趋势&#39;: 22, &#39;法律法规&#39;: 22, &#39;金融监管&#39;: 21, &#39;宏观经济&#39;: 20, &#39;产品设计&#39;: 19, &#39;对外开放&#39;: 19, &#39;管理系统&#39;: 18, &#39;人工智能&#39;: 18, &#39;事务管理&#39;: 17, &#39;金融风险&#39;: 17, &#39;上市公司&#39;: 16, &#39;积极探索&#39;: 15, &#39;充分发挥&#39;: 15, &#39;与此同时&#39;: 15, &#39;战略规划&#39;: 15, &#39;从业人员&#39;: 15, &#39;消费信贷&#39;: 15, &#39;组成部分&#39;: 14, ………………………………&#125;)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result = Counter(report_words).most_common(<span class="number">50</span>)  <span class="comment"># 取最多的50组</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<pre><code>[(&#39;信托公司&#39;, 1391), (&#39;2017&#39;, 577), (&#39;2016&#39;, 184), (&#39;金融机构&#39;, 148), (&#39;投资信托&#39;, 108), (&#39;基础产业&#39;, 91), (&#39;2018&#39;, 87), (&#39;风险管理&#39;, 82), (&#39;工商企业&#39;, 77), (&#39;QDII&#39;, 70), (&#39;金融服务&#39;, 69), (&#39;信息系统&#39;, 63), (&#39;2015&#39;, 59), (&#39;基础设施&#39;, 56), (&#39;金融公司&#39;, 47), (&#39;另一方面&#39;, 45), (&#39;信托投资公司&#39;, 45), (&#39;中国人民银行&#39;, 44), (&#39;REITs&#39;, 39), (&#39;金融业务&#39;, 38), (&#39;监管部门&#39;, 35), (&#39;客户服务&#39;, 33), (&#39;2013&#39;, 32), (&#39;新兴产业&#39;, 31), (&#39;资金来源&#39;, 31), (&#39;商业银行&#39;, 30), (&#39;信息技术&#39;, 29), (&#39;金融市场&#39;, 29), (&#39;2014&#39;, 28), (&#39;有限公司&#39;, 24), (&#39;债券市场&#39;, 24), (&#39;管理体系&#39;, 23), (&#39;发展趋势&#39;, 22), (&#39;法律法规&#39;, 22), (&#39;金融监管&#39;, 21), (&#39;宏观经济&#39;, 20), (&#39;产品设计&#39;, 19), (&#39;对外开放&#39;, 19), (&#39;管理系统&#39;, 18), (&#39;人工智能&#39;, 18), (&#39;事务管理&#39;, 17), (&#39;金融风险&#39;, 17), (&#39;上市公司&#39;, 16), (&#39;积极探索&#39;, 15), (&#39;充分发挥&#39;, 15), (&#39;与此同时&#39;, 15), (&#39;战略规划&#39;, 15), (&#39;从业人员&#39;, 15), (&#39;消费信贷&#39;, 15), (&#39;组成部分&#39;, 14)]
</code></pre>
<p>完整代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.读取文本内容，并利用jieba.cut功能俩进行自动分词</span></span><br><span class="line">report = <span class="built_in">open</span>(<span class="string">&#x27;信托行业年度报告.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>).read()</span><br><span class="line">words = jieba.cut(report) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.通过for循环来提取words列表中大于4个字的词语</span></span><br><span class="line">report_words = []</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words:  <span class="comment"># 将大于4个字的词语放入列表</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(word) &gt;= <span class="number">4</span>:</span><br><span class="line">        report_words.append(word)</span><br><span class="line"><span class="built_in">print</span>(report_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.获得打印输出高频词的出现次数</span></span><br><span class="line">result = Counter(report_words).most_common(<span class="number">50</span>)  <span class="comment"># 取词频最高的50组词</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<pre><code>Counter(&#123;&#39;信托公司&#39;: 1391, &#39;2017&#39;: 577, &#39;2016&#39;: 184, &#39;金融机构&#39;: 148, &#39;投资信托&#39;: 108, &#39;基础产业&#39;: 91, &#39;2018&#39;: 87, &#39;风险管理&#39;: 82, &#39;工商企业&#39;: 77, &#39;QDII&#39;: 70, &#39;金融服务&#39;: 69, &#39;信息系统&#39;: 63, &#39;2015&#39;: 59, &#39;基础设施&#39;: 56, &#39;金融公司&#39;: 47, &#39;另一方面&#39;: 45, &#39;信托投资公司&#39;: 45, &#39;中国人民银行&#39;: 44, &#39;REITs&#39;: 39, &#39;金融业务&#39;: 38, &#39;监管部门&#39;: 35, &#39;客户服务&#39;: 33, &#39;2013&#39;: 32, &#39;新兴产业&#39;: 31, &#39;资金来源&#39;: 31, &#39;商业银行&#39;: 30, &#39;信息技术&#39;: 29, &#39;金融市场&#39;: 29, &#39;2014&#39;: 28, &#39;有限公司&#39;: 24, &#39;债券市场&#39;: 24, &#39;管理体系&#39;: 23, &#39;发展趋势&#39;: 22, &#39;法律法规&#39;: 22, &#39;金融监管&#39;: 21, &#39;宏观经济&#39;: 20, &#39;产品设计&#39;: 19, &#39;对外开放&#39;: 19, &#39;管理系统&#39;: 18, &#39;人工智能&#39;: 18, &#39;事务管理&#39;: 17, &#39;金融风险&#39;: 17, &#39;上市公司&#39;: 16, &#39;积极探索&#39;: 15, &#39;充分发挥&#39;: 15, &#39;与此同时&#39;: 15, &#39;战略规划&#39;: 15, &#39;从业人员&#39;: 15, &#39;消费信贷&#39;: 15, &#39;组成部分&#39;: 14, ………………………………&#125;)
</code></pre>
<h1 id="14-智能推荐系统"><a href="#14-智能推荐系统" class="headerlink" title="14 智能推荐系统"></a>14 智能推荐系统</h1><h2 id="14-1-智能推荐系统的基本原理"><a href="#14-1-智能推荐系统的基本原理" class="headerlink" title="14.1 智能推荐系统的基本原理"></a>14.1 智能推荐系统的基本原理</h2><h3 id="14-1-1-智能推荐系统的应用场景"><a href="#14-1-1-智能推荐系统的应用场景" class="headerlink" title="14.1.1 智能推荐系统的应用场景"></a>14.1.1 智能推荐系统的应用场景</h3><h3 id="14-1-2-智能推荐系统的基础：协同过滤算法"><a href="#14-1-2-智能推荐系统的基础：协同过滤算法" class="headerlink" title="14.1.2 智能推荐系统的基础：协同过滤算法"></a>14.1.2 智能推荐系统的基础：协同过滤算法</h3><p>协同过滤算法的原理是根据用户群体的产品的偏好数据，发现用户或物品之间的相关性，并基于这些相关性为用户进行推荐。根据原理的不同，协同过滤算法分为两类——基于用户的协同过滤算法和基于物品的协同过滤算法</p>
<p><strong>1.基于用户的协同过滤算法</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(14).png" alt="下载 (14)"></p>
<p><strong>2.基于物品的协同过滤算法</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(15).png" alt="下载 (15)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(16).png" alt="下载 (16)"></p>
<h2 id="14-2-计算相似度的常用方法"><a href="#14-2-计算相似度的常用方法" class="headerlink" title="14.2 计算相似度的常用方法"></a>14.2 计算相似度的常用方法</h2><p>无论是基于用户还是基于产品的协同过滤算法，其本质都是寻找数据之间的相似度，计算相似度的三种方法——欧式距离、余弦值和皮尔逊相关系数<br><img src="%E4%B8%8B%E8%BD%BD%20(17).png" alt="下载 (17)"></p>
<h3 id="14-2-1-欧氏距离"><a href="#14-2-1-欧氏距离" class="headerlink" title="14.2.1 欧氏距离"></a>14.2.1 欧氏距离</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(18).png" alt="下载 (18)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(19).png" alt="下载 (19)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(20).png" alt="下载 (20)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(21).png" alt="下载 (21)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(22).png" alt="下载 (22)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame([[<span class="number">5</span>, <span class="number">1</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">2</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>]], columns=[<span class="string">&#x27;用户1&#x27;</span>, <span class="string">&#x27;用户2&#x27;</span>, <span class="string">&#x27;用户3&#x27;</span>], index=[<span class="string">&#x27;物品A&#x27;</span>, <span class="string">&#x27;物品B&#x27;</span>, <span class="string">&#x27;物品C&#x27;</span>])</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>用户1</th>
      <th>用户2</th>
      <th>用户3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>物品A</th>
      <td>5</td>
      <td>1</td>
      <td>5</td>
    </tr>
    <tr>
      <th>物品B</th>
      <td>4</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>物品C</th>
      <td>4</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">dist = np.linalg.norm(df.iloc[<span class="number">0</span>] - df.iloc[<span class="number">1</span>])</span><br><span class="line">dist</span><br></pre></td></tr></table></figure>




<pre><code>3.3166247903554
</code></pre>
<p><img src="%E4%B8%8B%E8%BD%BD%20(23).png" alt="下载 (23)"></p>
<h3 id="14-2-2-余弦相似度"><a href="#14-2-2-余弦相似度" class="headerlink" title="14.2.2 余弦相似度"></a>14.2.2 余弦相似度</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(24).png" alt="下载 (24)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame([[<span class="number">5</span>, <span class="number">1</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">2</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>]], columns=[<span class="string">&#x27;用户1&#x27;</span>, <span class="string">&#x27;用户2&#x27;</span>, <span class="string">&#x27;用户3&#x27;</span>], index=[<span class="string">&#x27;物品A&#x27;</span>, <span class="string">&#x27;物品B&#x27;</span>, <span class="string">&#x27;物品C&#x27;</span>])</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>用户1</th>
      <th>用户2</th>
      <th>用户3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>物品A</th>
      <td>5</td>
      <td>1</td>
      <td>5</td>
    </tr>
    <tr>
      <th>物品B</th>
      <td>4</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>物品C</th>
      <td>4</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">user_similarity = cosine_similarity(df)</span><br><span class="line">pd.DataFrame(user_similarity, columns=[<span class="string">&#x27;物品A&#x27;</span>, <span class="string">&#x27;物品B&#x27;</span>, <span class="string">&#x27;物品C&#x27;</span>], index=[<span class="string">&#x27;物品A&#x27;</span>, <span class="string">&#x27;物品B&#x27;</span>, <span class="string">&#x27;物品C&#x27;</span>])</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>物品A</th>
      <th>物品B</th>
      <th>物品C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>物品A</th>
      <td>1.000000</td>
      <td>0.914659</td>
      <td>0.825029</td>
    </tr>
    <tr>
      <th>物品B</th>
      <td>0.914659</td>
      <td>1.000000</td>
      <td>0.979958</td>
    </tr>
    <tr>
      <th>物品C</th>
      <td>0.825029</td>
      <td>0.979958</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以看到，物品B和物品C的余弦相似度最大，约为0.98，因此可以认为表中的所有物品中它们最相似</p>
<h3 id="14-2-3-皮尔逊相关系数"><a href="#14-2-3-皮尔逊相关系数" class="headerlink" title="14.2.3 皮尔逊相关系数"></a>14.2.3 皮尔逊相关系数</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(25).png" alt="下载 (25)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line">X = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line">Y = [<span class="number">9</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">2</span>]</span><br><span class="line">corr = pearsonr(X, Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;相关系数r值为&#x27;</span> + <span class="built_in">str</span>(corr[<span class="number">0</span>]) + <span class="string">&#x27;，显著性水平P值为&#x27;</span> + <span class="built_in">str</span>(corr[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>相关系数r值为-0.993883734673619，显著性水平P值为0.0005736731093321903
</code></pre>
<p><img src="%E4%B8%8B%E8%BD%BD%20(26).png" alt="下载 (26)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame([[<span class="number">5</span>, <span class="number">4</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>], [<span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]], columns=[<span class="string">&#x27;物品A&#x27;</span>, <span class="string">&#x27;物品B&#x27;</span>, <span class="string">&#x27;物品C&#x27;</span>], index=[<span class="string">&#x27;用户1&#x27;</span>, <span class="string">&#x27;用户2&#x27;</span>, <span class="string">&#x27;用户3&#x27;</span>])  </span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>物品A</th>
      <th>物品B</th>
      <th>物品C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>用户1</th>
      <td>5</td>
      <td>4</td>
      <td>4</td>
    </tr>
    <tr>
      <th>用户2</th>
      <td>1</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>用户3</th>
      <td>5</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>corrwith()函数可以计算单个物品与其他物品的皮尔逊相关系数，corr()函数可以计算整张表的皮尔逊相关系数，因为这两个函数默认计算的是DataFrame的列与列之间的相关系数，所以需要将之前的表格转置一下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 物品A与其他物品的皮尔逊相关系数</span></span><br><span class="line">A = df[<span class="string">&#x27;物品A&#x27;</span>]</span><br><span class="line">corr_A = df.corrwith(A)</span><br><span class="line">corr_A</span><br></pre></td></tr></table></figure>




<pre><code>物品A    1.000000
物品B    0.500000
物品C    0.188982
dtype: float64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 皮尔逊系数表，获取各物品相关性</span></span><br><span class="line">df.corr()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>物品A</th>
      <th>物品B</th>
      <th>物品C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>物品A</th>
      <td>1.000000</td>
      <td>0.500000</td>
      <td>0.188982</td>
    </tr>
    <tr>
      <th>物品B</th>
      <td>0.500000</td>
      <td>1.000000</td>
      <td>0.944911</td>
    </tr>
    <tr>
      <th>物品C</th>
      <td>0.188982</td>
      <td>0.944911</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>



<p>B和C的r值的绝对值最大，约为0.9449，因此可以认为表中的所有物品中它们最相似，这与使用欧式距离和余弦相似度度量物品间相似度所得到的结论一致</p>
<h2 id="14-3-案例实战：电影智能推荐系统"><a href="#14-3-案例实战：电影智能推荐系统" class="headerlink" title="14.3 案例实战：电影智能推荐系统"></a>14.3 案例实战：电影智能推荐系统</h2><h3 id="14-3-1-案例背景"><a href="#14-3-1-案例背景" class="headerlink" title="14.3.1 案例背景"></a>14.3.1 案例背景</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(27).png" alt="下载 (27)"></p>
<h3 id="14-3-2-数据读取与处理"><a href="#14-3-2-数据读取与处理" class="headerlink" title="14.3.2 数据读取与处理"></a>14.3.2 数据读取与处理</h3><p><strong>1.读取数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line">movies = pd.read_excel(<span class="string">&#x27;电影.xlsx&#x27;</span>)</span><br><span class="line">movies.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>电影编号</th>
      <th>名称</th>
      <th>类别</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>玩具总动员（1995）</td>
      <td>冒险|动画|儿童|喜剧|幻想</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>勇敢者的游戏（1995）</td>
      <td>冒险|儿童|幻想</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>斗气老顽童2（1995）</td>
      <td>喜剧|爱情</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>待到梦醒时分（1995）</td>
      <td>喜剧|剧情|爱情</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>新娘之父2（1995）</td>
      <td>喜剧</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score = pd.read_excel(<span class="string">&#x27;评分.xlsx&#x27;</span>)</span><br><span class="line">score.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>用户编号</th>
      <th>电影编号</th>
      <th>评分</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>3</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>6</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>47</td>
      <td>5.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>50</td>
      <td>5.0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.merge(movies, score, on=<span class="string">&#x27;电影编号&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>电影编号</th>
      <th>名称</th>
      <th>类别</th>
      <th>用户编号</th>
      <th>评分</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>玩具总动员（1995）</td>
      <td>冒险|动画|儿童|喜剧|幻想</td>
      <td>1</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>玩具总动员（1995）</td>
      <td>冒险|动画|儿童|喜剧|幻想</td>
      <td>5</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>玩具总动员（1995）</td>
      <td>冒险|动画|儿童|喜剧|幻想</td>
      <td>7</td>
      <td>4.5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>玩具总动员（1995）</td>
      <td>冒险|动画|儿童|喜剧|幻想</td>
      <td>15</td>
      <td>2.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>玩具总动员（1995）</td>
      <td>冒险|动画|儿童|喜剧|幻想</td>
      <td>17</td>
      <td>4.5</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;评分&#x27;</span>].value_counts()  <span class="comment"># 查看各个评分的出现的次数</span></span><br></pre></td></tr></table></figure>




<pre><code>4.0    26794
3.0    20017
5.0    13180
3.5    13129
4.5     8544
2.0     7545
2.5     5544
1.0     2808
1.5     1791
0.5     1369
Name: 评分, dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">df[<span class="string">&#x27;评分&#x27;</span>].hist(bins=<span class="number">20</span>)  <span class="comment"># hist()函数绘制直方图，竖轴为各评分出现的次数</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x15b3239c610&gt;
</code></pre>
<p><img src="output_195_1.png" alt="output_195_1"></p>
<p><strong>2.数据分析</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ratings = pd.DataFrame(df.groupby(<span class="string">&#x27;名称&#x27;</span>)[<span class="string">&#x27;评分&#x27;</span>].mean()) <span class="comment"># 对合并原始数据得到的DataFrame按“名称”归类，再用mean()函数计算每部电影的评分均值</span></span><br><span class="line">ratings.sort_values(<span class="string">&#x27;评分&#x27;</span>, ascending=<span class="literal">False</span>).head() <span class="comment"># 用sort_values()函数将评分均值从高到低排序</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>评分</th>
    </tr>
    <tr>
      <th>名称</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>假小子（1997）</th>
      <td>5.0</td>
    </tr>
    <tr>
      <th>福尔摩斯和华生医生历险记：讹诈之王（1980）</th>
      <td>5.0</td>
    </tr>
    <tr>
      <th>机器人（2016）</th>
      <td>5.0</td>
    </tr>
    <tr>
      <th>奥斯卡（1967）</th>
      <td>5.0</td>
    </tr>
    <tr>
      <th>人类状况III（1961）</th>
      <td>5.0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ratings[<span class="string">&#x27;评分次数&#x27;</span>] = df.groupby(<span class="string">&#x27;名称&#x27;</span>)[<span class="string">&#x27;评分&#x27;</span>].count() <span class="comment"># 统计每部电影的评分次数，然后为每部电影新增一列“评分次数”</span></span><br><span class="line">ratings.sort_values(<span class="string">&#x27;评分次数&#x27;</span>, ascending=<span class="literal">False</span>).head() <span class="comment"># 将评分次数从高到低排序</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>评分</th>
      <th>评分次数</th>
    </tr>
    <tr>
      <th>名称</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>阿甘正传（1994）</th>
      <td>4.164134</td>
      <td>329</td>
    </tr>
    <tr>
      <th>肖申克的救赎（1994）</th>
      <td>4.429022</td>
      <td>317</td>
    </tr>
    <tr>
      <th>低俗小说（1994）</th>
      <td>4.197068</td>
      <td>307</td>
    </tr>
    <tr>
      <th>沉默的羔羊（1991）</th>
      <td>4.161290</td>
      <td>279</td>
    </tr>
    <tr>
      <th>黑客帝国（1999）</th>
      <td>4.192446</td>
      <td>278</td>
    </tr>
  </tbody>
</table>
</div>



<p>从表中可以看出，排除极少数电影评分次数极低的情况，通常某部电影的评分次数越多，该电影的评分也会越高，假设某个用户给《阿甘正传》打了高分，我们需要寻找与《阿甘正传》相似度高的电影推荐给该用户</p>
<p><strong>3.数据处理</strong></p>
<p>先通过如下代码将原始数据转换为数据透视表。数据透视表是一种交互式表格，我们可以动态调整表格的版面布局，以便通过不同的方式分析数据，如求和、计数等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">user_movie = df.pivot_table(index=<span class="string">&#x27;用户编号&#x27;</span>, columns=<span class="string">&#x27;名称&#x27;</span>, values=<span class="string">&#x27;评分&#x27;</span>)</span><br><span class="line">user_movie.tail()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>名称</th>
      <th>007之黄金眼（1995）</th>
      <th>100个女孩（2000）</th>
      <th>100条街道（2016）</th>
      <th>101忠狗续集:伦敦大冒险（2003）</th>
      <th>101忠狗（1961）</th>
      <th>101雷克雅未克（2000）</th>
      <th>102只斑点狗（2000）</th>
      <th>10件或更少（2006）</th>
      <th>10（1979）</th>
      <th>11:14（2003）</th>
      <th>...</th>
      <th>龙珠：神秘冒险（1988）</th>
      <th>龙珠：血红宝石的诅咒（1986）</th>
      <th>龙珠：魔鬼城堡中的睡公主（1987）</th>
      <th>龙种子（1944）</th>
      <th>龙纹身的女孩（2011）</th>
      <th>龙舌兰日出（1988）</th>
      <th>龙虾（2015）</th>
      <th>龙：夜之怒的礼物（2011）</th>
      <th>龙：李小龙的故事（1993）</th>
      <th>龟日记（1985）</th>
    </tr>
    <tr>
      <th>用户编号</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>606</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>607</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>608</th>
      <td>4.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.5</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>609</th>
      <td>4.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>610</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>4.0</td>
      <td>NaN</td>
      <td>4.5</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 9687 columns</p>
</div>



<p>pivot_table()函数基于变量df创建数据透视表，并赋给变量user_movie。其中设置函数的index参数为“用户编号”，即以用户编号作为数据透视表的索引；设置columns参数为“名称”，即以电影名称作为数据透视表的列；设置values参数作为“评分”，即以电影评分作为数据透视表中显示的数据</p>
<p>其中行代表不同的用户，列代表不同的电影，第i行第j列单元格中的值代表第i个用户对第j部电影的评分，可以看到，绝大部分评分是NAN，数据透视表显得非常稀松，这是因为电影数量过于庞大，每个用户打分的电影数量却很有限</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">user_movie.describe()  <span class="comment"># 因为数据量较大，这个耗时可能会有1分钟左右</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>名称</th>
      <th>007之黄金眼（1995）</th>
      <th>100个女孩（2000）</th>
      <th>100条街道（2016）</th>
      <th>101忠狗续集:伦敦大冒险（2003）</th>
      <th>101忠狗（1961）</th>
      <th>101雷克雅未克（2000）</th>
      <th>102只斑点狗（2000）</th>
      <th>10件或更少（2006）</th>
      <th>10（1979）</th>
      <th>11:14（2003）</th>
      <th>...</th>
      <th>龙珠：神秘冒险（1988）</th>
      <th>龙珠：血红宝石的诅咒（1986）</th>
      <th>龙珠：魔鬼城堡中的睡公主（1987）</th>
      <th>龙种子（1944）</th>
      <th>龙纹身的女孩（2011）</th>
      <th>龙舌兰日出（1988）</th>
      <th>龙虾（2015）</th>
      <th>龙：夜之怒的礼物（2011）</th>
      <th>龙：李小龙的故事（1993）</th>
      <th>龟日记（1985）</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>132.000000</td>
      <td>4.00</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>44.000000</td>
      <td>1.0</td>
      <td>9.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>4.00</td>
      <td>...</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.000000</td>
      <td>1.0</td>
      <td>42.000000</td>
      <td>13.000000</td>
      <td>7.000000</td>
      <td>1.0</td>
      <td>8.00000</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>3.496212</td>
      <td>3.25</td>
      <td>2.5</td>
      <td>2.5</td>
      <td>3.431818</td>
      <td>3.5</td>
      <td>2.777778</td>
      <td>2.666667</td>
      <td>3.375000</td>
      <td>3.75</td>
      <td>...</td>
      <td>3.5</td>
      <td>3.5</td>
      <td>3.250000</td>
      <td>3.5</td>
      <td>3.488095</td>
      <td>3.038462</td>
      <td>4.000000</td>
      <td>5.0</td>
      <td>2.81250</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.859381</td>
      <td>0.50</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.751672</td>
      <td>NaN</td>
      <td>0.833333</td>
      <td>1.040833</td>
      <td>1.030776</td>
      <td>0.50</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.353553</td>
      <td>NaN</td>
      <td>1.327422</td>
      <td>0.431158</td>
      <td>0.707107</td>
      <td>NaN</td>
      <td>1.03294</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.500000</td>
      <td>2.50</td>
      <td>2.5</td>
      <td>2.5</td>
      <td>1.500000</td>
      <td>3.5</td>
      <td>2.000000</td>
      <td>1.500000</td>
      <td>2.000000</td>
      <td>3.00</td>
      <td>...</td>
      <td>3.5</td>
      <td>3.5</td>
      <td>3.000000</td>
      <td>3.5</td>
      <td>0.500000</td>
      <td>2.000000</td>
      <td>3.000000</td>
      <td>5.0</td>
      <td>0.50000</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>3.000000</td>
      <td>3.25</td>
      <td>2.5</td>
      <td>2.5</td>
      <td>3.000000</td>
      <td>3.5</td>
      <td>2.000000</td>
      <td>2.250000</td>
      <td>3.125000</td>
      <td>3.75</td>
      <td>...</td>
      <td>3.5</td>
      <td>3.5</td>
      <td>3.125000</td>
      <td>3.5</td>
      <td>2.625000</td>
      <td>3.000000</td>
      <td>3.500000</td>
      <td>5.0</td>
      <td>2.87500</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>3.500000</td>
      <td>3.50</td>
      <td>2.5</td>
      <td>2.5</td>
      <td>3.500000</td>
      <td>3.5</td>
      <td>2.500000</td>
      <td>3.000000</td>
      <td>3.500000</td>
      <td>4.00</td>
      <td>...</td>
      <td>3.5</td>
      <td>3.5</td>
      <td>3.250000</td>
      <td>3.5</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>5.0</td>
      <td>3.00000</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>4.000000</td>
      <td>3.50</td>
      <td>2.5</td>
      <td>2.5</td>
      <td>4.000000</td>
      <td>3.5</td>
      <td>3.000000</td>
      <td>3.250000</td>
      <td>3.750000</td>
      <td>4.00</td>
      <td>...</td>
      <td>3.5</td>
      <td>3.5</td>
      <td>3.375000</td>
      <td>3.5</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>4.500000</td>
      <td>5.0</td>
      <td>3.12500</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>5.000000</td>
      <td>3.50</td>
      <td>2.5</td>
      <td>2.5</td>
      <td>5.000000</td>
      <td>3.5</td>
      <td>4.500000</td>
      <td>3.500000</td>
      <td>4.500000</td>
      <td>4.00</td>
      <td>...</td>
      <td>3.5</td>
      <td>3.5</td>
      <td>3.500000</td>
      <td>3.5</td>
      <td>5.000000</td>
      <td>4.000000</td>
      <td>5.000000</td>
      <td>5.0</td>
      <td>4.00000</td>
      <td>4.0</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 9687 columns</p>
</div>



<p>上表中的count是该电影被评分的次数，mean是评分的均值，std是评分的标准差，min是最低评分，25%，50%，75%是相应的分位数，max是最高评分</p>
<h3 id="14-3-3-系统搭建"><a href="#14-3-3-系统搭建" class="headerlink" title="14.3.3 系统搭建"></a>14.3.3 系统搭建</h3><p>本小节利用之前处理好的数据进行相关性分析，以《阿甘正传》为例，分析应该向观看了《阿甘正传》的用户推荐什么样的电影</p>
<p>首先从数据透视表中提取各用户对《阿甘正传》的评分，使用head()函数显示前5行，代码如下，其中FG是《阿甘正传》的英文名首字母缩写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FG = user_movie[<span class="string">&#x27;阿甘正传（1994）&#x27;</span>]  <span class="comment"># FG是Forrest Gump（），阿甘英文名称的缩写</span></span><br><span class="line">pd.DataFrame(FG).head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>阿甘正传（1994）</th>
    </tr>
    <tr>
      <th>用户编号</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>4.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>NaN</td>
    </tr>
    <tr>
      <th>5</th>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># axis默认为0，计算user_movie各列与FG的相关系数</span></span><br><span class="line">corr_FG = user_movie.corrwith(FG)</span><br><span class="line">similarity = pd.DataFrame(corr_FG, columns=[<span class="string">&#x27;相关系数&#x27;</span>])</span><br><span class="line">similarity.head()</span><br></pre></td></tr></table></figure>





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>相关系数</th>
    </tr>
    <tr>
      <th>名称</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>007之黄金眼（1995）</th>
      <td>0.217441</td>
    </tr>
    <tr>
      <th>100个女孩（2000）</th>
      <td>NaN</td>
    </tr>
    <tr>
      <th>100条街道（2016）</th>
      <td>NaN</td>
    </tr>
    <tr>
      <th>101忠狗续集:伦敦大冒险（2003）</th>
      <td>NaN</td>
    </tr>
    <tr>
      <th>101忠狗（1961）</th>
      <td>0.141023</td>
    </tr>
  </tbody>
</table>
</div>



<p>表中有些相关系数是空值，这是因为计算变量user_movie的列向量和变量FG的皮尔逊相关系数时，其实是在计算某部电影的所有评分和《阿甘正传》的所有评分的皮尔逊相关系数。如果某列的空值过多，与《阿甘正传》的所有用户的评分一个交叉项也没有，即没有一个用户同时对这两部电影进行打分，那么就无法计算皮尔逊相关系数中的协方差，导致表中出现了很多空值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">similarity.dropna(inplace=<span class="literal">True</span>)  <span class="comment"># 或写成similarity=similarity.dropna()</span></span><br><span class="line">similarity.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>相关系数</th>
    </tr>
    <tr>
      <th>名称</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>007之黄金眼（1995）</th>
      <td>0.217441</td>
    </tr>
    <tr>
      <th>101忠狗（1961）</th>
      <td>0.141023</td>
    </tr>
    <tr>
      <th>102只斑点狗（2000）</th>
      <td>-0.857589</td>
    </tr>
    <tr>
      <th>10件或更少（2006）</th>
      <td>-1.000000</td>
    </tr>
    <tr>
      <th>11:14（2003）</th>
      <td>0.500000</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">similarity_new = pd.merge(similarity, ratings[<span class="string">&#x27;评分次数&#x27;</span>], left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>)</span><br><span class="line">similarity_new.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>相关系数</th>
      <th>评分次数</th>
    </tr>
    <tr>
      <th>名称</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>007之黄金眼（1995）</th>
      <td>0.217441</td>
      <td>132</td>
    </tr>
    <tr>
      <th>101忠狗（1961）</th>
      <td>0.141023</td>
      <td>44</td>
    </tr>
    <tr>
      <th>102只斑点狗（2000）</th>
      <td>-0.857589</td>
      <td>9</td>
    </tr>
    <tr>
      <th>10件或更少（2006）</th>
      <td>-1.000000</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11:14（2003）</th>
      <td>0.500000</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二种合并方式</span></span><br><span class="line">similarity_new = similarity.join(ratings[<span class="string">&#x27;评分次数&#x27;</span>])</span><br><span class="line">similarity_new.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>相关系数</th>
      <th>评分次数</th>
    </tr>
    <tr>
      <th>名称</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>007之黄金眼（1995）</th>
      <td>0.217441</td>
      <td>132</td>
    </tr>
    <tr>
      <th>101忠狗（1961）</th>
      <td>0.141023</td>
      <td>44</td>
    </tr>
    <tr>
      <th>102只斑点狗（2000）</th>
      <td>-0.857589</td>
      <td>9</td>
    </tr>
    <tr>
      <th>10件或更少（2006）</th>
      <td>-1.000000</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11:14（2003）</th>
      <td>0.500000</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>



<p>因为电影数量庞大，每个用户评过分的电影数量却是有限的，导致许多电影的评分次数很少，所以可能有偶然的因素导致部分电影的评分偏高或偏低，无法反映真实水平，此时需要设置阈值，只有当评分次数大于该阈值时才认为该电影的总体评分有效，这里简单设置阈值为20</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">similarity_new[similarity_new[<span class="string">&#x27;评分次数&#x27;</span>] &gt; <span class="number">20</span>].sort_values(by=<span class="string">&#x27;相关系数&#x27;</span>, ascending=<span class="literal">False</span>).head()  <span class="comment"># 选取阈值</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>相关系数</th>
      <th>评分次数</th>
    </tr>
    <tr>
      <th>名称</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>阿甘正传（1994）</th>
      <td>1.000000</td>
      <td>329</td>
    </tr>
    <tr>
      <th>抓狂双宝（1996）</th>
      <td>0.723238</td>
      <td>31</td>
    </tr>
    <tr>
      <th>雷神：黑暗世界（2013）</th>
      <td>0.715809</td>
      <td>21</td>
    </tr>
    <tr>
      <th>致命吸引力（1987）</th>
      <td>0.701856</td>
      <td>36</td>
    </tr>
    <tr>
      <th>X战警：未来的日子（2014）</th>
      <td>0.682284</td>
      <td>30</td>
    </tr>
  </tbody>
</table>
</div>



<p><strong>补充知识点：groupby()函数的使用</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.DataFrame([[<span class="string">&#x27;战狼2&#x27;</span>, <span class="string">&#x27;丁一&#x27;</span>, <span class="number">6</span>, <span class="number">8</span>], [<span class="string">&#x27;攀登者&#x27;</span>, <span class="string">&#x27;王二&#x27;</span>, <span class="number">8</span>, <span class="number">6</span>], [<span class="string">&#x27;攀登者&#x27;</span>, <span class="string">&#x27;张三&#x27;</span>, <span class="number">10</span>, <span class="number">8</span>], [<span class="string">&#x27;卧虎藏龙&#x27;</span>, <span class="string">&#x27;李四&#x27;</span>, <span class="number">8</span>, <span class="number">8</span>], [<span class="string">&#x27;卧虎藏龙&#x27;</span>, <span class="string">&#x27;赵五&#x27;</span>, <span class="number">8</span>, <span class="number">10</span>]], columns=[<span class="string">&#x27;电影名称&#x27;</span>, <span class="string">&#x27;影评师&#x27;</span>, <span class="string">&#x27;观前评分&#x27;</span>, <span class="string">&#x27;观后评分&#x27;</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>电影名称</th>
      <th>影评师</th>
      <th>观前评分</th>
      <th>观后评分</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>战狼2</td>
      <td>丁一</td>
      <td>6</td>
      <td>8</td>
    </tr>
    <tr>
      <th>1</th>
      <td>攀登者</td>
      <td>王二</td>
      <td>8</td>
      <td>6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>攀登者</td>
      <td>张三</td>
      <td>10</td>
      <td>8</td>
    </tr>
    <tr>
      <th>3</th>
      <td>卧虎藏龙</td>
      <td>李四</td>
      <td>8</td>
      <td>8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>卧虎藏龙</td>
      <td>赵五</td>
      <td>8</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">means = data.groupby(<span class="string">&#x27;电影名称&#x27;</span>)[[<span class="string">&#x27;观后评分&#x27;</span>]].mean()</span><br><span class="line">means</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>观后评分</th>
    </tr>
    <tr>
      <th>电影名称</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>卧虎藏龙</th>
      <td>9</td>
    </tr>
    <tr>
      <th>战狼2</th>
      <td>8</td>
    </tr>
    <tr>
      <th>攀登者</th>
      <td>7</td>
    </tr>
  </tbody>
</table>
</div>



<p>这行代码的含义是为现根据电影名称进行分组，然后选取分组后的观影评分，并用mean()函数计算每个组的观后评分的平均值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">means = data.groupby(<span class="string">&#x27;电影名称&#x27;</span>)[[<span class="string">&#x27;观前评分&#x27;</span>, <span class="string">&#x27;观后评分&#x27;</span>]].mean()</span><br><span class="line">means</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>观前评分</th>
      <th>观后评分</th>
    </tr>
    <tr>
      <th>电影名称</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>卧虎藏龙</th>
      <td>8</td>
      <td>9</td>
    </tr>
    <tr>
      <th>战狼2</th>
      <td>6</td>
      <td>8</td>
    </tr>
    <tr>
      <th>攀登者</th>
      <td>9</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">means = data.groupby([<span class="string">&#x27;电影名称&#x27;</span>, <span class="string">&#x27;影评师&#x27;</span>])[[<span class="string">&#x27;观后评分&#x27;</span>]].mean()</span><br><span class="line">means</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>观后评分</th>
    </tr>
    <tr>
      <th>电影名称</th>
      <th>影评师</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2" valign="top">卧虎藏龙</th>
      <th>李四</th>
      <td>8</td>
    </tr>
    <tr>
      <th>赵五</th>
      <td>10</td>
    </tr>
    <tr>
      <th>战狼2</th>
      <th>丁一</th>
      <td>8</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">攀登者</th>
      <th>张三</th>
      <td>8</td>
    </tr>
    <tr>
      <th>王二</th>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>



<p>这里设置了多重索引，其中第一重索引为电影名称，第二重索引为影评师</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">count = data.groupby(<span class="string">&#x27;电影名称&#x27;</span>)[[<span class="string">&#x27;观后评分&#x27;</span>]].count()</span><br><span class="line">count</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>观后评分</th>
    </tr>
    <tr>
      <th>电影名称</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>卧虎藏龙</th>
      <td>2</td>
    </tr>
    <tr>
      <th>战狼2</th>
      <td>1</td>
    </tr>
    <tr>
      <th>攀登者</th>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">count = count.rename(columns=&#123;<span class="string">&#x27;观后评分&#x27;</span>:<span class="string">&#x27;评分次数&#x27;</span>&#125;)</span><br><span class="line">count</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>评分次数</th>
    </tr>
    <tr>
      <th>电影名称</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>卧虎藏龙</th>
      <td>2</td>
    </tr>
    <tr>
      <th>战狼2</th>
      <td>1</td>
    </tr>
    <tr>
      <th>攀登者</th>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>


      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">数据科学</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/机器学习与深度学习//" class="article-tag-list-link color5">机器学习与深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/02/03/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part4/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-python大数据分析与机器学习商业案例实战-part3" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/02/03/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part3/">Python大数据分析与机器学习商业案例实战-part3</a>
    </h1>
  

        
        <a href="/2023/02/03/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part3/" class="archive-article-date">
  	<time datetime="2023-02-03T09:13:14.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-02-03</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="11-特征工程之数据预处理"><a href="#11-特征工程之数据预处理" class="headerlink" title="11 特征工程之数据预处理"></a>11 特征工程之数据预处理</h1><p>在实际工作中获取到的数据往往不那么理想，可能会存在非数值类型的文本数据、重复值、缺失值、异常值及数据分布不均衡等问题，因此，在进行数学建模前还需要对这些问题进行处理，这项工作称为特征工程。特征工程通常分为特征使用方案、特征获取方案、特征处理、特征监控几大部分，其中特征处理是特征工程的核心内容，有时称为数据预处理。</p>
<p>本章可以当成工具手册进行查阅</p>
<h2 id="11-1-非数值类型数据处理"><a href="#11-1-非数值类型数据处理" class="headerlink" title="11.1 非数值类型数据处理"></a>11.1 非数值类型数据处理</h2><p>有时会包含一些非数值数据，其中最常见的为文本数据，例如性别中的男女，处理时可替换为1和0</p>
<p>下面介绍python中两种常用的数值类型数据处理方法——Get_dummies哑变量处理和Label Encoding编号处理</p>
<h3 id="11-1-1-Get-dummies哑变量处理"><a href="#11-1-1-Get-dummies哑变量处理" class="headerlink" title="11.1.1 Get_dummies哑变量处理"></a>11.1.1 Get_dummies哑变量处理</h3><p>哑变量也叫虚拟变量，通常取值为0或1，python中用get_dummies()函数进行哑变量处理，不仅可以处理只有两个分类的简单问题，还可以处理含多个分类的问题</p>
<p>1.简单示例：“男”和“女”的数值转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;客户编号&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">&#x27;性别&#x27;</span>: [<span class="string">&#x27;男&#x27;</span>, <span class="string">&#x27;女&#x27;</span>, <span class="string">&#x27;男&#x27;</span>]&#125;)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>客户编号</th>
      <th>性别</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>男</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>女</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>男</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.get_dummies(df, columns=[<span class="string">&#x27;性别&#x27;</span>])</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>客户编号</th>
      <th>性别_女</th>
      <th>性别_男</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以看到，原来的性别列变为“性别_女”和“性别_男”两列，这两列中的数字1表示符合列名，数字0表示不符合列名</p>
<p>虽然现在已经将文本类型的数据转换成了数字，但是“性别_女”和“性别_男”这两列存在多重共线性，即知道其中一列的内容，就能知道另一列的内容，用公式表达就是：性别_男&#x3D;1-性别_女</p>
<p>多重共线性会带来一些问题，因此需要用drop()函数删去其中一列</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = df.drop(columns=<span class="string">&#x27;性别_女&#x27;</span>) </span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>客户编号</th>
      <th>性别_男</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>用rename()函数更改列名</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = df.rename(columns=&#123;<span class="string">&#x27;性别_男&#x27;</span>:<span class="string">&#x27;性别&#x27;</span>&#125;)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>客户编号</th>
      <th>性别</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>2.稍复杂点的案例：房屋朝向的数值转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;房屋编号&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="string">&#x27;朝向&#x27;</span>: [<span class="string">&#x27;东&#x27;</span>, <span class="string">&#x27;南&#x27;</span>, <span class="string">&#x27;西&#x27;</span>, <span class="string">&#x27;北&#x27;</span>, <span class="string">&#x27;南&#x27;</span>]&#125;)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>房屋编号</th>
      <th>朝向</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>东</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>南</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>西</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>北</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>南</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.get_dummies(df, columns=[<span class="string">&#x27;朝向&#x27;</span>])</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>房屋编号</th>
      <th>朝向_东</th>
      <th>朝向_北</th>
      <th>朝向_南</th>
      <th>朝向_西</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>上表还是同样存在多重共线性（即根据3个朝向的数字就能判断第4个朝向的数字是0还是1），因此需要从新构造出来的4个哑变量中删去一个</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = df.drop(columns=<span class="string">&#x27;朝向_西&#x27;</span>) </span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>房屋编号</th>
      <th>朝向_东</th>
      <th>朝向_北</th>
      <th>朝向_南</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p><strong>注意</strong>：构造哑变量容易产生高维数据，因此，哑变量常常和PCA主成分分析一起使用</p>
<h3 id="11-1-2-Label-Encoding编号处理"><a href="#11-1-2-Label-Encoding编号处理" class="headerlink" title="11.1.2 Label Encoding编号处理"></a>11.1.2 Label Encoding编号处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;编号&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="string">&#x27;城市&#x27;</span>: [<span class="string">&#x27;北京&#x27;</span>, <span class="string">&#x27;上海&#x27;</span>, <span class="string">&#x27;广州&#x27;</span>, <span class="string">&#x27;深圳&#x27;</span>, <span class="string">&#x27;北京&#x27;</span>]&#125;)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>编号</th>
      <th>城市</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>北京</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>上海</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>广州</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>深圳</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>北京</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">le = LabelEncoder()</span><br><span class="line">label = le.fit_transform(df[<span class="string">&#x27;城市&#x27;</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(label)</span><br></pre></td></tr></table></figure>

<pre><code>[1 0 2 3 1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;城市&#x27;</span>] = label</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>编号</th>
      <th>城市</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p><strong>注意</strong>：可以看到上海和广州的平均值是北京，这个现象其实是没有现实意义的，这也是Label Encoding的一个缺点——可能会产生一些没有意义的关系。不过树模型（如决策树、随机森林及XGBoost等集成算法）能很好地处理这种转化，因此对于树模型来说，这种奇怪的现象是不会影响结果的。</p>
<p><strong>补充知识点</strong>：pandas库中的replace()函数</p>
<p>上述Label Encoding函数生成的数字是随机的，如果想按特定的内容进行替换，可以使用replace()函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;编号&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="string">&#x27;城市&#x27;</span>: [<span class="string">&#x27;北京&#x27;</span>, <span class="string">&#x27;上海&#x27;</span>, <span class="string">&#x27;广州&#x27;</span>, <span class="string">&#x27;深圳&#x27;</span>, <span class="string">&#x27;北京&#x27;</span>]&#125;)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;城市&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure>




<pre><code>北京    2
深圳    1
广州    1
上海    1
Name: 城市, dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;城市&#x27;</span>] = df[<span class="string">&#x27;城市&#x27;</span>].replace(&#123;<span class="string">&#x27;北京&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;上海&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;广州&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;深圳&#x27;</span>:<span class="number">3</span>&#125;)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>编号</th>
      <th>城市</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>总结来说，Get_dummies的优点就是它的值只有0和1，缺点是当类别的数量很多时，特征维度会很高，我们可以配合使用下一章即将讲到的PCA主成分分析来减少维度。所以如果Get_dummies类别数目不多时可以优先考虑，其次考虑Label Encoding或replace()函数，但如果是基于树模型的机器学习模型，则是用Label Encoding编号处理则没有太大关系。</p>
<h2 id="11-2-重复值、缺失值及异常值处理"><a href="#11-2-重复值、缺失值及异常值处理" class="headerlink" title="11.2 重复值、缺失值及异常值处理"></a>11.2 重复值、缺失值及异常值处理</h2><h3 id="11-2-1-重复值处理"><a href="#11-2-1-重复值处理" class="headerlink" title="11.2.1 重复值处理"></a>11.2.1 重复值处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里首先创建一个含有重复值的DataFrame，代码如下：</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.DataFrame([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], columns=[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c2&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时的data二维列表如下所示，可以看到第一行和第二行是重复的。</span></span><br><span class="line">data</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果数据量较大，我们可以通过duplicated()函数来查询重复的内容，代码如下：</span></span><br><span class="line">data[data.duplicated()]</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想统计重复行的数量，可以通过sum()函数进行查看，代码如下，本案例结果为1。</span></span><br><span class="line">data.duplicated().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>




<pre><code>1
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 发现有重复行的时候，可以通过drop_duplicates()函数删除重复行，代码如下：</span></span><br><span class="line">data = data.drop_duplicates()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想按列进行去重，比如说如果c1列出现相同的内容，就把那行代码删掉，可以采用如下代码。这样的筛选条件则不如之前要全部一样才删除严格。</span></span><br><span class="line">data = pd.DataFrame([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], columns=[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c2&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>])</span><br><span class="line">data = data.drop_duplicates(<span class="string">&#x27;c1&#x27;</span>)</span><br><span class="line">data</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="11-2-2-缺失值处理"><a href="#11-2-2-缺失值处理" class="headerlink" title="11.2.2 缺失值处理"></a>11.2.2 缺失值处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里先构造一个含有缺失值的DataFrame，代码如下：</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = pd.DataFrame([[<span class="number">1</span>, np.nan, <span class="number">3</span>], [np.nan, <span class="number">2</span>, np.nan], [<span class="number">1</span>, np.nan, <span class="number">0</span>]], columns=[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c2&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>])</span><br><span class="line">data</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>NaN</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>NaN</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以用isnull()函数或isna()函数（两者作用类似）来查看空值，代码如下：</span></span><br><span class="line">data.isnull()  <span class="comment"># 或者写data.isna()</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>True</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以对单列查看缺失值情况，代码如下：</span></span><br><span class="line">data[<span class="string">&#x27;c1&#x27;</span>].isnull()</span><br></pre></td></tr></table></figure>




<pre><code>0    False
1     True
2    False
Name: c1, dtype: bool
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果数据量较大，可以通过如下代码筛选某列内容为空值的行，代码如下：</span></span><br><span class="line">data[data[<span class="string">&#x27;c1&#x27;</span>].isnull()]</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于空值有两种常见的处理方式：删除空值和填补空值。常用方法</span></span><br><span class="line"><span class="comment"># 通过dropna()函数可以删除空值，代码如下：</span></span><br><span class="line">a = data.dropna()</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果觉得该删除方法过于激进，可以设置thresh参数，比如将其设置为n，那么其含义是如果该行的非空值少于n个则删除该行，演示代码如下：</span></span><br><span class="line">a = data.dropna(thresh=<span class="number">2</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>NaN</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>NaN</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>第一行和第三行都有两个非空值，因此不会被删除该行，而第二行只有一个非空值，少于两个，因此会被删除</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过finllna()函数可以填补空值，这里采用的是均值填充法，通过每一列的均值对该列的空值进行填充，也可以把其中的data.mean()换成data.meian()则变为中位数填充。</span></span><br><span class="line">b = data.fillna(data.mean())</span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此处method=&#x27;pad&#x27;代表用缺失值所在列的前一个值填充，如果前一个值不存在或也缺失，则结果不变。运行结果如下：</span></span><br><span class="line">c = data.fillna(method=<span class="string">&#x27;pad&#x27;</span>)</span><br><span class="line">c</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>NaN</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 还可以采用method=&#x27;backfill&#x27;或method=&#x27;bfill&#x27;用缺失值所在列的后一个值填充，如果后一个值不存在或也缺失，则结果不变。</span></span><br><span class="line">d = data.fillna(method=<span class="string">&#x27;backfill&#x27;</span>)</span><br><span class="line">e = data.fillna(method=<span class="string">&#x27;bfill&#x27;</span>)</span><br><span class="line">e</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>NaN</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="11-2-3-异常值处理"><a href="#11-2-3-异常值处理" class="headerlink" title="11.2.3 异常值处理"></a>11.2.3 异常值处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里先构造一个含有异常值的数据集：</span></span><br><span class="line">data = pd.DataFrame(&#123;<span class="string">&#x27;c1&#x27;</span>: [<span class="number">3</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">69</span>], <span class="string">&#x27;c2&#x27;</span>: [<span class="number">15</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">100</span>, <span class="number">19</span>, <span class="number">11</span>, <span class="number">8</span>], <span class="string">&#x27;c3&#x27;</span>: [<span class="number">20</span>, <span class="number">15</span>, <span class="number">18</span>, <span class="number">21</span>, <span class="number">120</span>, <span class="number">27</span>, <span class="number">29</span>]&#125;, columns=[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c2&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>])</span><br><span class="line">data</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>15</td>
      <td>20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10</td>
      <td>16</td>
      <td>15</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td>14</td>
      <td>18</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7</td>
      <td>100</td>
      <td>21</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>19</td>
      <td>120</td>
    </tr>
    <tr>
      <th>5</th>
      <td>9</td>
      <td>11</td>
      <td>27</td>
    </tr>
    <tr>
      <th>6</th>
      <td>69</td>
      <td>8</td>
      <td>29</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以看到第一列的数字69，第二列的数字100，第三列的数字120为比较明显的异常值，那么该如何利用Python来进行异常值的检测呢？下面我们主要通过两种方法来进行检测：利用箱体图观察和利用标准差检测。</p>
<p><strong>1.利用箱型图观察</strong></p>
<p>箱体图是一种用于显示一组数据分散情况资料的统计图，可以通过设定标准，将大于或小于箱体图上下界的数值识别为异常值</p>
<p>如下图所示，将数据的下四分位数记为Q1，即样本中仅有25%的数据小于Q1；将数据的上四分位数记为Q3，即样本中仅有25%的数据大于Q3；将上四分位数和下四分位数的差值记为IQR，记IQR&#x3D;Q3-Q1；令箱体图上界为Q3+1.5xIQR，下界为Q1-1.5xIQR<br><img src="%E4%B8%8B%E8%BD%BD.png" alt="下载"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.boxplot()  <span class="comment"># 画箱型图</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1f64cdd69a0&gt;
</code></pre>
<p><img src="output_57_1.png" alt="output_57_1"></p>
<p>可以明显看到每列数据各有一个异常值</p>
<p><strong>2.利用标准差检测</strong></p>
<p>当数据服从标准正态分布时，99%的数值与均值的距离应该在3个标准差之内，95%的数值与均值的距离应该在2个标准差之内，因为三个标准差过于严格，此处将阈值设定为2个标准差，即认为当数值与均值的距离超出2个标准差，则可以认为它是异常值</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(1).png" alt="下载 (1)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data.columns:  <span class="comment"># 用for循环以此对数据的每列进行操作</span></span><br><span class="line">    z = (data[i] - data[i].mean()) / data[i].std()  <span class="comment"># 用均值函数和标准差函数将每列数据进行Z-score标准化</span></span><br><span class="line">    a[i] = <span class="built_in">abs</span>(z) &gt; <span class="number">2</span>  <span class="comment"># 如果标准化后的数值大于标准正态分布的标准差1的2倍，那么数值为异常值，返回True，否则返回False</span></span><br></pre></td></tr></table></figure>

<p>Z-score标准化：<br><img src="%E4%B8%8B%E8%BD%BD%20(2).png" alt="下载 (2)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a  <span class="comment"># 打印来看，每一列都有一个异常值</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>5</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>6</th>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>



<p><img src="%E4%B8%8B%E8%BD%BD%20(3).png" alt="下载 (3)"><br>检测到异常后，如果异常值较少或影响不大，也可以不处理。如果需要处理，可以采用如下几种常见的方式：</p>
<p>删除含有异常值的记录</p>
<p>将异常值视为缺失值，利用11.2.2小节介绍的方法进行处理</p>
<p>利用11.4节讲解的数据分箱的方法进行处理</p>
<h2 id="11-3-数据标准化"><a href="#11-3-数据标准化" class="headerlink" title="11.3 数据标准化"></a>11.3 数据标准化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">X = pd.DataFrame(&#123;<span class="string">&#x27;酒精含量(%)&#x27;</span>: [<span class="number">50</span>, <span class="number">60</span>, <span class="number">40</span>, <span class="number">80</span>, <span class="number">90</span>], <span class="string">&#x27;苹果酸含量(%)&#x27;</span>: [<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>]&#125;)</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">X  <span class="comment"># 查看X</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>酒精含量(%)</th>
      <th>苹果酸含量(%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>50</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>60</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>40</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>80</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>90</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="11-3-1-min-max标准化"><a href="#11-3-1-min-max标准化" class="headerlink" title="11.3.1 min-max标准化"></a>11.3.1 min-max标准化</h3><p>min-max标准化也称为离差标准化，它利用原始数据的最大值和最小值把原始数据转换到[0,1]区间内，转换公式如下：<br><img src="%E4%B8%8B%E8%BD%BD%20(4).png" alt="下载 (4)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">X_new = MinMaxScaler().fit_transform(X)  <span class="comment"># 用fit_transform()函数对原始数据进行min-max标准化</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X_new)  <span class="comment"># 查看X_new</span></span><br></pre></td></tr></table></figure>

<pre><code>[[0.2 0.5]
 [0.4 0. ]
 [0.  0. ]
 [0.8 1. ]
 [1.  0.5]]
</code></pre>
<p>第1列为“酒精含量”标准化后的值，第二列为“苹果酸含量”标准化后的值，可以看到它们都在[0,1]区间内。在实际应用中，通常将所有数据都归一化后，再进行训练集和测试集划分，演示代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br></pre></td></tr></table></figure>

<h3 id="11-3-2-Z-score标准化"><a href="#11-3-2-Z-score标准化" class="headerlink" title="11.3.2 Z-score标准化"></a>11.3.2 Z-score标准化</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(5).png" alt="下载 (5)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">X_new = StandardScaler().fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X_new)  <span class="comment"># 查看X_new</span></span><br></pre></td></tr></table></figure>

<pre><code>[[-0.75482941  0.26726124]
 [-0.21566555 -1.06904497]
 [-1.29399328 -1.06904497]
 [ 0.86266219  1.60356745]
 [ 1.40182605  0.26726124]]
</code></pre>
<p><img src="%E4%B8%8B%E8%BD%BD%20(6).png" alt="下载 (6)"></p>
<h2 id="11-4-数据分箱"><a href="#11-4-数据分箱" class="headerlink" title="11.4 数据分箱"></a>11.4 数据分箱</h2><p>各种贷款业务机构普遍使用信用评分对客户进行评估，相应的模型称为信用评分卡模型。在构建信用评分卡模型的过程中，需要利用WOE值和IV值进行特征筛选，而计算这两个值的第一步就是要进行数据分箱，所以本节先来学习如何对连续型变量进行分箱处理</p>
<p>数据分箱就是将一个连续型变量离散化，可分为<strong>等宽分箱</strong>和<strong>等深分箱</strong>。<strong>等宽分箱是指每个分箱的差值相等</strong>。例如年龄这一个连续型变量，其取值范围为0~100的连续数值，可以将“年龄”分为0-20、20-40、40-60、60-80、80-100共五个分箱，这五个分箱就可以当成离散的分类样本，每个分箱的年龄差相等都相差20岁。<strong>等深分箱是指每个分箱中的样本数一致</strong>，同样按年龄这一个特征变量进行分箱，例如500个样本分成5箱，那么每个分箱中都是100人，此时对应的5个分箱可能就是0-20、20-25、25-30、30-50、50-100，确保每个分箱中的人数一致。</p>
<p>实战中用等宽分箱较多</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.DataFrame([[<span class="number">22</span>,<span class="number">1</span>],[<span class="number">25</span>,<span class="number">1</span>],[<span class="number">20</span>,<span class="number">0</span>],[<span class="number">35</span>,<span class="number">0</span>],[<span class="number">32</span>,<span class="number">1</span>],[<span class="number">38</span>,<span class="number">0</span>],[<span class="number">50</span>,<span class="number">0</span>],[<span class="number">46</span>,<span class="number">1</span>]], columns=[<span class="string">&#x27;年龄&#x27;</span>, <span class="string">&#x27;是否违约&#x27;</span>])</span><br><span class="line">data</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年龄</th>
      <th>是否违约</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>25</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>38</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>50</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>46</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下代码即可进行等宽数据分箱：</span></span><br><span class="line">data_cut = pd.cut(data[<span class="string">&#x27;年龄&#x27;</span>], <span class="number">3</span>) <span class="comment"># 第一个参数是待分箱的列，第二个参数是分箱个数</span></span><br><span class="line"><span class="built_in">print</span>(data_cut)</span><br></pre></td></tr></table></figure>

<pre><code>0    (19.97, 30.0]
1    (19.97, 30.0]
2    (19.97, 30.0]
3     (30.0, 40.0]
4     (30.0, 40.0]
5     (30.0, 40.0]
6     (40.0, 50.0]
7     (40.0, 50.0]
Name: 年龄, dtype: category
Categories (3, interval[float64]): [(19.97, 30.0] &lt; (30.0, 40.0] &lt; (40.0, 50.0]]
</code></pre>
<p>年龄列中数据范围是20-50岁，分为三组恰好为20-30岁，30-40岁，40-50岁，即等宽分箱</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过groupby()函数进行分组，count()函数（详见14.3节补充知识点）进行计数可以获取每个分箱中的样本数目，代码如下：</span></span><br><span class="line">data[<span class="string">&#x27;年龄&#x27;</span>].groupby(data_cut).count()</span><br></pre></td></tr></table></figure>




<pre><code>年龄
(19.97, 30.0]    3
(30.0, 40.0]     3
(40.0, 50.0]     2
Name: 年龄, dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 补充知识点，分箱并进行编号</span></span><br><span class="line"><span class="built_in">print</span>(pd.cut(data[<span class="string">&#x27;年龄&#x27;</span>], <span class="number">3</span>, labels=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>0    1
1    1
2    1
3    2
4    2
5    2
6    3
7    3
Name: 年龄, dtype: category
Categories (3, int64): [1 &lt; 2 &lt; 3]
</code></pre>
<h2 id="11-5-特征筛选：WOE值与IV值——根据该指标大小进行特征变量的筛选"><a href="#11-5-特征筛选：WOE值与IV值——根据该指标大小进行特征变量的筛选" class="headerlink" title="11.5 特征筛选：WOE值与IV值——根据该指标大小进行特征变量的筛选"></a>11.5 特征筛选：WOE值与IV值——根据该指标大小进行特征变量的筛选</h2><p>在使用逻辑回归、决策树等模型算法构建分类模型的时候，经常需要对<strong>特征变量</strong>进行筛选。因为有时可能会获得100多个候选特征变量，通常不会直接把这些特征变量放到模型中去进行拟合训练，而是从这些特征变量中<strong>挑选一些</strong>放进模型，构成<strong>入模变量列表</strong>。那么该如何挑选入模变量模型呢？</p>
<p>挑选入模变量模型需要考虑很多因素，如变量的预测能力、简单性（容易生成和使用）、可解释性等。其中最主要的衡量标准就是<strong>变量的预测能力</strong>，对分类模型来说，即希望变量具有较好的<strong>特征区分度</strong>，可以较准确地将样本进行分类</p>
<p><strong>WOE和IV值</strong>就是这样的指标，它们可以用来衡量特征变量的<strong>预测能力</strong>，或者说特征变量的特征区分度，类似的指标还有5.1.2小节中提到的基尼系数和信息增益。对于决策树等树模型来说，可以通过基尼系数或信息增益来衡量变量的特征区分度，而对逻辑回归等没有基尼系数等指标的模型而言，可以通过WOE值和IV值进行变量选择。IV值的计算是以WOE值为基础的，而要计算一个变量的WOE值，需要先用上一节所讲的知识对这个变量进行分箱处理。</p>
<h3 id="11-5-1-WOE值的定义与计算"><a href="#11-5-1-WOE值的定义与计算" class="headerlink" title="11.5.1 WOE值的定义与计算"></a>11.5.1 WOE值的定义与计算</h3><p><strong>1.WOE值的定义</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(7).png" alt="下载 (7)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(8).png" alt="下载 (8)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(9).png" alt="下载 (9)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(10).png" alt="下载 (10)"></p>
<p>实际应用中，因为数据量通常较大，所以不太可能出现WOE值为+∞或-∞的情况，如果出现了无穷大的WOE值，也是我们不希望看到的，这样会导致基于WOE值的IV值也变成无穷大，不利于进行特征筛选。此时的处理方法有两种：第一种方法是对数据重新进行更合理的分箱，使各个分箱的WOE值不再无穷大；第二种方法是忽略这些无穷大的值，直接让它变为0，这一思路比较简单且易于实现，在11.5.3节会有应用</p>
<p><strong>2.WOE值的计算过程演示</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(11).png" alt="下载 (11)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(12).png" alt="下载 (12)"></p>
<h3 id="11-5-2-IV值的定义与计算"><a href="#11-5-2-IV值的定义与计算" class="headerlink" title="11.5.2 IV值的定义与计算"></a>11.5.2 IV值的定义与计算</h3><p>在实战应用中，通过IV值可以评判特征变量的<strong>预测能力</strong>，从而进行特征筛选</p>
<p><strong>1.IV值的定义</strong></p>
<p>IV值是Information Value（信息量）的缩写。在进行特征筛选时，IV值能较好地反映特征变量的预测能力，特征变量对预测结果的贡献越大，其价值就越大，对应的IV值就越大，因此，我们可以根据IV值的大小筛选出需要的特征变量</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(13).png" alt="下载 (13)"></p>
<p><strong>2.IV值的计算过程演示</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(14).png" alt="下载 (14)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(15).png" alt="下载 (15)"></p>
<p>汇总所有数据，可以得到下表：<br><img src="%E4%B8%8B%E8%BD%BD%20(16).png"></p>
<p>通过这种方式对样本数据的每个特征变量进行IV值计算并排序后，就可以获得特征变量的决策树能力强弱信息</p>
<p>一个特征变量的<strong>IV值越高</strong>，说明该特征变量越具有<strong>区分度</strong>。不过IV值也不是越大越好，如果一个特征变量的IV值大于0.5，有时需要对这个特征变量持有疑问，因为它有点过好而显得不够真实。通常会选择IV值在0.1~0.5这个范围内的特征范围。不同应用场景的取值也会有所不同，例如，有些风控团队会将IV值大于0.5的特征变量也纳入考量，这个其实也需要根据实际的建模效果来做进一步判断</p>
<p><strong>补充：使用IV值而不直接使用WOE值的原因</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(17).png" alt="下载 (17)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(18).png" alt="下载 (18)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(19).png" alt="下载 (19)"></p>
<h3 id="11-5-3-WOE值与IV值的代码实现"><a href="#11-5-3-WOE值与IV值的代码实现" class="headerlink" title="11.5.3 WOE值与IV值的代码实现"></a>11.5.3 WOE值与IV值的代码实现</h3><p>1.数据分箱</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先通过如下代码构造数据：</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.DataFrame([[<span class="number">22</span>,<span class="number">1</span>],[<span class="number">25</span>,<span class="number">1</span>],[<span class="number">20</span>,<span class="number">0</span>],[<span class="number">35</span>,<span class="number">0</span>],[<span class="number">32</span>,<span class="number">1</span>],[<span class="number">38</span>,<span class="number">0</span>],[<span class="number">50</span>,<span class="number">0</span>],[<span class="number">46</span>,<span class="number">1</span>]], columns=[<span class="string">&#x27;年龄&#x27;</span>, <span class="string">&#x27;是否违约&#x27;</span>])</span><br><span class="line">data</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年龄</th>
      <th>是否违约</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>25</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>38</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>50</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>46</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有了数据之后，根据“年龄”这一特征变量进行数据分箱，代码如下：</span></span><br><span class="line">data_cut = pd.cut(data[<span class="string">&#x27;年龄&#x27;</span>], <span class="number">3</span>)</span><br><span class="line">data_cut</span><br></pre></td></tr></table></figure>




<pre><code>0    (19.97, 30.0]
1    (19.97, 30.0]
2    (19.97, 30.0]
3     (30.0, 40.0]
4     (30.0, 40.0]
5     (30.0, 40.0]
6     (40.0, 50.0]
7     (40.0, 50.0]
Name: 年龄, dtype: category
Categories (3, interval[float64]): [(19.97, 30.0] &lt; (30.0, 40.0] &lt; (40.0, 50.0]]
</code></pre>
<p>2.统计各个分箱样本总数、坏样本数和好样本数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计总客户数</span></span><br><span class="line">cut_group_all = data[<span class="string">&#x27;是否违约&#x27;</span>].groupby(data_cut).count() <span class="comment"># 用groupby()函数根据分箱内容进行分类，用count()函数进行计数，可以获得各个分箱中的总客户数</span></span><br><span class="line"><span class="comment"># 统计违约客户</span></span><br><span class="line">cut_y = data[<span class="string">&#x27;是否违约&#x27;</span>].groupby(data_cut).<span class="built_in">sum</span>() <span class="comment"># 用groupby()函数根据分箱内容进行归类，用sum()函数进行求和，因为违约客户的数字是1，未违约客户数字标识为0，所以sum()函数求和结果为违约客户数</span></span><br><span class="line"><span class="comment"># 统计未违约客户</span></span><br><span class="line">cut_n = cut_group_all - cut_y <span class="comment"># 总客户数减去违约客户数得到未违约客户数</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里展示下cut_group_all的结果，如下所示：</span></span><br><span class="line">cut_group_all</span><br></pre></td></tr></table></figure>




<pre><code>年龄
(19.97, 30.0]    3
(30.0, 40.0]     3
(40.0, 50.0]     2
Name: 是否违约, dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过2.2.1节相关知识点将cut_group_all、cut_y、cut_n进行汇总，代码如下，这里我们将违约客户命名为“坏样本”，非违约客户命名为“好样本”。</span></span><br><span class="line">df = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame用来汇总数据</span></span><br><span class="line">df[<span class="string">&#x27;总数&#x27;</span>] = cut_group_all</span><br><span class="line">df[<span class="string">&#x27;坏样本&#x27;</span>] = cut_y</span><br><span class="line">df[<span class="string">&#x27;好样本&#x27;</span>] = cut_n</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>总数</th>
      <th>坏样本</th>
      <th>好样本</th>
    </tr>
    <tr>
      <th>年龄</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(19.97, 30.0]</th>
      <td>3</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>(30.0, 40.0]</th>
      <td>3</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>(40.0, 50.0]</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>3.统计各分箱中坏样本比率和好样本比率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算坏样本%和好样本%</span></span><br><span class="line">df[<span class="string">&#x27;坏样本%&#x27;</span>] = df[<span class="string">&#x27;坏样本&#x27;</span>] / df[<span class="string">&#x27;坏样本&#x27;</span>].<span class="built_in">sum</span>()</span><br><span class="line">df[<span class="string">&#x27;好样本%&#x27;</span>] = df[<span class="string">&#x27;好样本&#x27;</span>] / df[<span class="string">&#x27;好样本&#x27;</span>].<span class="built_in">sum</span>()</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>总数</th>
      <th>坏样本</th>
      <th>好样本</th>
      <th>坏样本%</th>
      <th>好样本%</th>
    </tr>
    <tr>
      <th>年龄</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(19.97, 30.0]</th>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>0.50</td>
      <td>0.25</td>
    </tr>
    <tr>
      <th>(30.0, 40.0]</th>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>0.25</td>
      <td>0.50</td>
    </tr>
    <tr>
      <th>(40.0, 50.0]</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0.25</td>
      <td>0.25</td>
    </tr>
  </tbody>
</table>
</div>



<p>4.计算WOE值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">df[<span class="string">&#x27;WOE&#x27;</span>] = np.log(df[<span class="string">&#x27;坏样本%&#x27;</span>] / df[<span class="string">&#x27;好样本%&#x27;</span>])  <span class="comment"># 使用对数函数np.log()</span></span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>总数</th>
      <th>坏样本</th>
      <th>好样本</th>
      <th>坏样本%</th>
      <th>好样本%</th>
      <th>WOE</th>
    </tr>
    <tr>
      <th>年龄</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(19.97, 30.0]</th>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>0.50</td>
      <td>0.25</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>(30.0, 40.0]</th>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>0.25</td>
      <td>0.50</td>
      <td>-0.693147</td>
    </tr>
    <tr>
      <th>(40.0, 50.0]</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0.25</td>
      <td>0.25</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>



<p>此外，我们在11.5.1节第一部分也讲过，在实际应用中，我们不希望WOE值出现无穷大（这样会导致之后计算的IV值也变为无穷大，丧失了IV值的意义），但是有的时候可能由于数据特殊性及分箱的原因，它还是出现了WOE值为无穷大的情况（某个分箱中只含有一种类别的数据），此时解决办法是当WOE值为无穷大时，将它替换为0，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.replace(&#123;<span class="string">&#x27;WOE&#x27;</span>: &#123;np.inf: <span class="number">0</span>, -np.inf: <span class="number">0</span>&#125;&#125;)</span><br></pre></td></tr></table></figure>

<p>这行代码用pandas库中replace()函数将无穷大替换为0，其中np.inf是利用Numpy库构造的正无穷，-np.inf则是负无穷</p>
<p>5.计算IV值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;IV&#x27;</span>] = df[<span class="string">&#x27;WOE&#x27;</span>] * (df[<span class="string">&#x27;坏样本%&#x27;</span>] - df[<span class="string">&#x27;好样本%&#x27;</span>])</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>总数</th>
      <th>坏样本</th>
      <th>好样本</th>
      <th>坏样本%</th>
      <th>好样本%</th>
      <th>WOE</th>
      <th>IV</th>
    </tr>
    <tr>
      <th>年龄</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(19.97, 30.0]</th>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>0.50</td>
      <td>0.25</td>
      <td>0.693147</td>
      <td>0.173287</td>
    </tr>
    <tr>
      <th>(30.0, 40.0]</th>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>0.25</td>
      <td>0.50</td>
      <td>-0.693147</td>
      <td>0.173287</td>
    </tr>
    <tr>
      <th>(40.0, 50.0]</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0.25</td>
      <td>0.25</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iv = df[<span class="string">&#x27;IV&#x27;</span>].<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(iv)</span><br></pre></td></tr></table></figure>

<pre><code>0.34657359027997264
</code></pre>
<p>整理上面计算WOE值和IV值的内容，完整代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.构造数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.DataFrame([[<span class="number">22</span>,<span class="number">1</span>],[<span class="number">25</span>,<span class="number">1</span>],[<span class="number">20</span>,<span class="number">0</span>],[<span class="number">35</span>,<span class="number">0</span>],[<span class="number">32</span>,<span class="number">1</span>],[<span class="number">38</span>,<span class="number">0</span>],[<span class="number">50</span>,<span class="number">0</span>],[<span class="number">46</span>,<span class="number">1</span>]], columns=[<span class="string">&#x27;年龄&#x27;</span>, <span class="string">&#x27;是否违约&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.数据分箱</span></span><br><span class="line">data_cut = pd.cut(data[<span class="string">&#x27;年龄&#x27;</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.统计各个分箱样本总数、坏样本数和好样本数并汇总数据</span></span><br><span class="line"><span class="comment"># 统计总客户数</span></span><br><span class="line">cut_group_all = data[<span class="string">&#x27;是否违约&#x27;</span>].groupby(data_cut).count()</span><br><span class="line"><span class="comment"># 统计违约客户</span></span><br><span class="line">cut_y = data[<span class="string">&#x27;是否违约&#x27;</span>].groupby(data_cut).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># 统计未违约客户</span></span><br><span class="line">cut_n = cut_group_all - cut_y</span><br><span class="line"><span class="comment"># 汇总基础数据</span></span><br><span class="line">df = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame用来汇总数据</span></span><br><span class="line">df[<span class="string">&#x27;总数&#x27;</span>] = cut_group_all</span><br><span class="line">df[<span class="string">&#x27;坏样本&#x27;</span>] = cut_y</span><br><span class="line">df[<span class="string">&#x27;好样本&#x27;</span>] = cut_n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.统计坏样本%和好样本%</span></span><br><span class="line">df[<span class="string">&#x27;坏样本%&#x27;</span>] = df[<span class="string">&#x27;坏样本&#x27;</span>] / df[<span class="string">&#x27;坏样本&#x27;</span>].<span class="built_in">sum</span>()</span><br><span class="line">df[<span class="string">&#x27;好样本%&#x27;</span>] = df[<span class="string">&#x27;好样本&#x27;</span>] / df[<span class="string">&#x27;好样本&#x27;</span>].<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.计算WOE值</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">df[<span class="string">&#x27;WOE&#x27;</span>] = np.log(df[<span class="string">&#x27;坏样本%&#x27;</span>] / df[<span class="string">&#x27;好样本%&#x27;</span>])</span><br><span class="line">df = df.replace(&#123;<span class="string">&#x27;WOE&#x27;</span>: &#123;np.inf: <span class="number">0</span>, -np.inf: <span class="number">0</span>&#125;&#125;)  <span class="comment"># 替换可能存在的无穷大</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.计算各个分箱的IV值</span></span><br><span class="line">df[<span class="string">&#x27;IV&#x27;</span>] = df[<span class="string">&#x27;WOE&#x27;</span>] * (df[<span class="string">&#x27;坏样本%&#x27;</span>] - df[<span class="string">&#x27;好样本%&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7.汇总各个分箱的IV值，获得特征变量的IV值</span></span><br><span class="line">iv = df[<span class="string">&#x27;IV&#x27;</span>].<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(iv)</span><br></pre></td></tr></table></figure>

<pre><code>0.34657359027997264
</code></pre>
<p>在实际应用中，通过类似上面的代码计算出各个特征变量的IV值，然后根据IV值从高到低排序，即可筛选出需要的特征变量</p>
<h3 id="11-5-4-案例实战：客户流失预警模型的IV值计算"><a href="#11-5-4-案例实战：客户流失预警模型的IV值计算" class="headerlink" title="11.5.4 案例实战：客户流失预警模型的IV值计算"></a>11.5.4 案例实战：客户流失预警模型的IV值计算</h3><p>4.2节中讲解了一个客户流失预警模型，下面就以它作为案例来计算各个特征变量的IV值，并筛选出合适的特征变量</p>
<p>为了提高代码的通用性，这里将上一节的代码稍加修改，写成如下的自定义函数形式。该函数一共有4个参数：data(原始数据)、cut_num（数据分箱步骤中的分箱的个数）、feature（需要计算IV值的特征变量名称）、target（目标变量名称）。有了这个函数，就能方便地对任意一个数据集计算各个特征变量的IV值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将上面的内容首先定义为一个函数</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_iv</span>(<span class="params">data, cut_num, feature, target</span>):</span><br><span class="line">    <span class="comment"># 1.数据分箱</span></span><br><span class="line">    data_cut = pd.cut(data[feature], cut_num)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.统计各个分箱样本总数、坏样本数和好样本数</span></span><br><span class="line">    cut_group_all = data[target].groupby(data_cut).count()  <span class="comment"># 总客户数</span></span><br><span class="line">    cut_y = data[target].groupby(data_cut).<span class="built_in">sum</span>()  <span class="comment"># 坏样本数</span></span><br><span class="line">    cut_n = cut_group_all - cut_y  <span class="comment"># 好样本数</span></span><br><span class="line">    <span class="comment"># 汇总基础数据</span></span><br><span class="line">    df = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame用来汇总数据</span></span><br><span class="line">    df[<span class="string">&#x27;总数&#x27;</span>] = cut_group_all</span><br><span class="line">    df[<span class="string">&#x27;坏样本&#x27;</span>] = cut_y</span><br><span class="line">    df[<span class="string">&#x27;好样本&#x27;</span>] = cut_n</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.统计坏样本%和好样本%</span></span><br><span class="line">    df[<span class="string">&#x27;坏样本%&#x27;</span>] = df[<span class="string">&#x27;坏样本&#x27;</span>] / df[<span class="string">&#x27;坏样本&#x27;</span>].<span class="built_in">sum</span>()</span><br><span class="line">    df[<span class="string">&#x27;好样本%&#x27;</span>] = df[<span class="string">&#x27;好样本&#x27;</span>] / df[<span class="string">&#x27;好样本&#x27;</span>].<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.计算WOE值</span></span><br><span class="line">    df[<span class="string">&#x27;WOE&#x27;</span>] = np.log(df[<span class="string">&#x27;坏样本%&#x27;</span>] / df[<span class="string">&#x27;好样本%&#x27;</span>])</span><br><span class="line">    df = df.replace(&#123;<span class="string">&#x27;WOE&#x27;</span>: &#123;np.inf: <span class="number">0</span>, -np.inf: <span class="number">0</span>&#125;&#125;) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.计算各个分箱的IV值</span></span><br><span class="line">    df[<span class="string">&#x27;IV&#x27;</span>] = df[<span class="string">&#x27;WOE&#x27;</span>] * (df[<span class="string">&#x27;坏样本%&#x27;</span>] - df[<span class="string">&#x27;好样本%&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6.汇总各个分箱的IV值，获得特征变量的IV值</span></span><br><span class="line">    iv = df[<span class="string">&#x27;IV&#x27;</span>].<span class="built_in">sum</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(iv)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有了上面的自动计算IV值的函数后，通过如下代码来读取客户流失预警模型中的相关数据：</span></span><br><span class="line">data = pd.read_excel(<span class="string">&#x27;股票客户流失.xlsx&#x27;</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>账户资金（元）</th>
      <th>最后一次交易距今时间（天）</th>
      <th>上月交易佣金（元）</th>
      <th>本券商使用时长（年）</th>
      <th>是否流失</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22686.5</td>
      <td>297</td>
      <td>149.25</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>190055.0</td>
      <td>42</td>
      <td>284.75</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>29733.5</td>
      <td>233</td>
      <td>269.25</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>185667.5</td>
      <td>44</td>
      <td>211.50</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>33648.5</td>
      <td>213</td>
      <td>353.50</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们利用刚刚编好的函数进行第一个特征变量“账户资金（元）”的IV值计算，代码如下：</span></span><br><span class="line">cal_iv(data, <span class="number">4</span>, <span class="string">&#x27;账户资金（元）&#x27;</span>, <span class="string">&#x27;是否流失&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>0.15205722409339645
</code></pre>
<p>其中，第一个参数data就是刚刚读取的股票客户流失数据；设置第二个参数cut_num为4，即将数据分为4个箱；设置第三个参数feature为’账户资金（元）’，即要计算IV值的特征变量；设置第四个参数target为‘是否流失’，即原始表格中的目标变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data.columns[:-<span class="number">1</span>]:</span><br><span class="line">    <span class="built_in">print</span>(i + <span class="string">&#x27;的IV值为：&#x27;</span>)</span><br><span class="line">    cal_iv(data, <span class="number">4</span>, i, <span class="string">&#x27;是否流失&#x27;</span>)  <span class="comment"># 调用函数</span></span><br></pre></td></tr></table></figure>

<pre><code>账户资金（元）的IV值为：
0.15205722409339645
最后一次交易距今时间（天）的IV值为：
0.2508468300174099
上月交易佣金（元）的IV值为：
0.30811632146662304
本券商使用时长（年）的IV值为：
0.6144219248359752
</code></pre>
<p>将上述IV值从高到低排序，结果为：本券商使用时长（年）&gt;上月交易佣金（元）&gt;最后一次交易距今时间（天）&gt;账户资金（元）。可以得出结论：“本券商使用时长（年）”的信息量最大，而“账户资金（元）”的信息量最小，预测能力最低。这其实也是搭建逻辑回归模型时判断特征重要性的一个方式</p>
<h2 id="11-6-多重共线性的分析与处理"><a href="#11-6-多重共线性的分析与处理" class="headerlink" title="11.6 多重共线性的分析与处理"></a>11.6 多重共线性的分析与处理</h2><h3 id="11-6-1-多重共线性的定义"><a href="#11-6-1-多重共线性的定义" class="headerlink" title="11.6.1 多重共线性的定义"></a>11.6.1 多重共线性的定义</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(20).png" alt="下载 (20)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(21).png" alt="下载 (21)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(22).png" alt="下载 (22)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(23).png" alt="下载 (23)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(24).png" alt="下载 (24)"></p>
<h3 id="11-6-2-多重共线性的分析与检验"><a href="#11-6-2-多重共线性的分析与检验" class="headerlink" title="11.6.2 多重共线性的分析与检验"></a>11.6.2 多重共线性的分析与检验</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用Pandas库读入一组存在多重共线性的数据，并对其回归作为示例：</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;数据.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X1</th>
      <th>X2</th>
      <th>X3</th>
      <th>Y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8</td>
      <td>16</td>
      <td>-32</td>
      <td>77</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7</td>
      <td>14</td>
      <td>-31</td>
      <td>52</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>9</td>
      <td>-12</td>
      <td>42</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>2</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>2</td>
      <td>8</td>
      <td>19</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以发现X2列中绝大部分数据是X1列中数据的2倍</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对数据集划分特征变量和目标变量：</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line">Y = df[<span class="string">&#x27;Y&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>下面我们需要做的就是分析与检验这三个特征变量是否存在多重共线性，这里主要讲解两种判别方法：相关系数判断以及方差膨胀因子法（VIF检验）来检验多重共线性。</p>
<p><strong>1.相关系数判断</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.corr()  <span class="comment"># corr()函数可以快速计算不同变量间的相关系数</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X1</th>
      <th>X2</th>
      <th>X3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>X1</th>
      <td>1.000000</td>
      <td>0.992956</td>
      <td>-0.422788</td>
    </tr>
    <tr>
      <th>X2</th>
      <td>0.992956</td>
      <td>1.000000</td>
      <td>-0.410412</td>
    </tr>
    <tr>
      <th>X3</th>
      <td>-0.422788</td>
      <td>-0.410412</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>



<p><img src="%E4%B8%8B%E8%BD%BD%20(25).png"></p>
<p>这里用到的相关系数其实是皮尔逊相关系数</p>
<p>相关系数的判断使用起来非常简单，结论也比较清洗，不过它有一个缺点：简单相关系数只是多重共线性的充分条件，不是必要条件。在有多个特征变量时，相关系数较小的特征变量间也可能存在较严重的多重共线性。为了更加严谨，实战中还经常用到方差膨胀系数（VIF检验）</p>
<p><strong>2.方差膨胀因子法（VIF检验）</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(26).png" alt="下载 (26)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(27).png" alt="下载 (27)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了检验上述回归中是否存在严重的多重共线性，我们使用Python的VIF检验模块来验证：</span></span><br><span class="line"><span class="keyword">from</span> statsmodels.stats.outliers_influence <span class="keyword">import</span> variance_inflation_factor</span><br><span class="line">vif = [variance_inflation_factor(X.values, X.columns.get_loc(i)) <span class="keyword">for</span> i <span class="keyword">in</span> X.columns]</span><br></pre></td></tr></table></figure>

<p>第一行代码从statsmodels模块引入variance_inflation_factor()函数；第二行代码通过for循环循环依次求得每个特征变量的方差膨胀系数并将结果放入列表中，该行可以看成求VIF值的固定写法，其中X.columns.get_loc(i)返回的是指定列的序号数字，如第一列返回的就是数字0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vif</span><br></pre></td></tr></table></figure>




<pre><code>[259.6430487184967, 257.6315718292196, 1.302330632715429]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果对上面的快捷写法不太理解，上面的代码也可以写成：</span></span><br><span class="line">vif = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> X.columns:  <span class="comment"># i对应的是每一列的列名</span></span><br><span class="line">    vif.append(variance_inflation_factor(X.values, X.columns.get_loc(i)))</span><br><span class="line">    </span><br><span class="line">vif</span><br></pre></td></tr></table></figure>




<pre><code>[259.6430487184967, 257.6315718292196, 1.302330632715429]
</code></pre>
<p>因为特征变量X2是X1的2倍,所以使用X1对X2和X3回归和使用X2对X1和X3回归时所得的方差膨胀系数会很大，从上述计算结果也可以看出，前2个VIF值均大于100，暗示多重共线性十分严重，应该删掉X1或X2</p>
<p>下面删掉X2再进行一次回归和VIF检验，看看结果变化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对数据集重新划分特征变量和目标变量：</span></span><br><span class="line">X = df[[<span class="string">&#x27;X1&#x27;</span>, <span class="string">&#x27;X3&#x27;</span>]]</span><br><span class="line">Y = df[<span class="string">&#x27;Y&#x27;</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行检验VIF检验：</span></span><br><span class="line"><span class="keyword">from</span> statsmodels.stats.outliers_influence <span class="keyword">import</span> variance_inflation_factor</span><br><span class="line">vif = [variance_inflation_factor(X.values, X.columns.get_loc(i)) <span class="keyword">for</span> i <span class="keyword">in</span> X.columns]</span><br><span class="line"></span><br><span class="line">vif</span><br></pre></td></tr></table></figure>




<pre><code>[1.289349054516766, 1.2893490545167656]
</code></pre>
<p>可以看到，此时两个特征变量的方差膨胀系数都小于10，说明它们之间不存在多重共线性</p>
<p><strong>总结来说，对于线性回归模型和逻辑回归模型等以线性方程表达式为基础的机器学习模型，需要注意多重共线性的影响。如果存在多重共线性，则需要进行响应处理，如删去某个引起多重共线性的特征变量</strong></p>
<h2 id="11-7-过采样和欠采样"><a href="#11-7-过采样和欠采样" class="headerlink" title="11.7 过采样和欠采样"></a>11.7 过采样和欠采样</h2><p>建立模型时，可能会遇到正负样本比例极度不均衡的情况。例如，建立信用违约模型时，违约样本的比例远小于不违约样本的比例，此时模型会花更多的精力去拟合不违约的样本，但实际上找出违约样本更为重要。这会导致模型可能在训练集上表现良好，但测试时表现不佳。为了改善样本比例不均衡的问题，可以使用过采样和欠采样的方法。假设建立信用违约模型时，样本数据中有1000个不违约样本和100个违约样本</p>
<h3 id="11-7-1-过采样"><a href="#11-7-1-过采样" class="headerlink" title="11.7.1 过采样"></a>11.7.1 过采样</h3><p><strong>1.过采样的原理</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(28).png" alt="下载 (28)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(29).png" alt="下载 (29)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(30).png" alt="下载 (30)"></p>
<p><strong>2.过采样的代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_excel(<span class="string">&quot;信用卡数据.xlsx&quot;</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>编号</th>
      <th>年龄</th>
      <th>负债比率</th>
      <th>月收入</th>
      <th>贷款数量</th>
      <th>家属人数</th>
      <th>分类</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>29</td>
      <td>0.22</td>
      <td>7800</td>
      <td>1</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>52</td>
      <td>0.46</td>
      <td>4650</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>28</td>
      <td>0.10</td>
      <td>3000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>29</td>
      <td>0.20</td>
      <td>5916</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>27</td>
      <td>1.28</td>
      <td>1300</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>分类指的是是否违约，1代表违约，0代表不违约</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下代码提取特征变量并将其赋值给变量X，提取目标变量并将其赋值给变量y：。</span></span><br><span class="line">X = data.drop(columns=<span class="string">&#x27;分类&#x27;</span>)</span><br><span class="line">y = data[<span class="string">&#x27;分类&#x27;</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 然后使用collections库中的Counter()方法，对目标变量进行计数：</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">Counter(y)</span><br></pre></td></tr></table></figure>




<pre><code>Counter(&#123;0: 1000, 1: 100&#125;)
</code></pre>
<p>不违约的样本数有1000个，远远大于违约的样本数100。为了防止建立信用违约模型时，模型着重拟合不违约的样本，而无法找出违约的样本，我们采用过采样的方法来改善样本比例不均衡的问题，这里我们将通过上面讲到的随机过采样和SMOTE法过采样来进行代码实现。</p>
<p>（1）随机过采样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> RandomOverSampler</span><br><span class="line">ros = RandomOverSampler(random_state=<span class="number">0</span>)</span><br><span class="line">X_oversampled, y_oversampled = ros.fit_resample(X, y)</span><br></pre></td></tr></table></figure>

<p>第一行代码从imblearn库中引入用来进行随机过采样的RandomOverSampler()函数，第二行代码设置函数参数random_state为0（此数字没有特殊含义，可以换成其他数字），使得每次代码运行的结果保持一致，第三行代码使用原始数据的特征变量和目标变量生成过采样数据集，并分别赋给变量X_oversampled和y_oversampled</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用Counter()方法检验一下SMOTE过采样的效果。</span></span><br><span class="line">Counter(y_oversampled)</span><br></pre></td></tr></table></figure>




<pre><code>Counter(&#123;0: 1000, 1: 1000&#125;)
</code></pre>
<p>违约的样本数从100上升至不违约的样本数1000，这证明我们的随机过采样有效。同时我们可以打印特征变量X_oversampled的shape来看看特征变量的变化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_oversampled.shape</span><br></pre></td></tr></table></figure>




<pre><code>(2000, 6)
</code></pre>
<p>这里的2000就是1000个违约样本和1000个不违约样本相加得到的，可以看到，随机过采样后特征变量的数据也随之增多。</p>
<p>（2）SMOTE过采样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下代码即可进行SMOTE过采样：</span></span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</span><br><span class="line">smote = SMOTE(random_state=<span class="number">0</span>)</span><br><span class="line">X_smotesampled, y_smotesampled = smote.fit_resample(X, y)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用Counter()方法检验一下SMOTE过采样的效果。</span></span><br><span class="line">Counter(y_smotesampled)</span><br></pre></td></tr></table></figure>




<pre><code>Counter(&#123;0: 1000, 1: 1000&#125;)
</code></pre>
<h3 id="11-7-2-欠采样"><a href="#11-7-2-欠采样" class="headerlink" title="11.7.2 欠采样"></a>11.7.2 欠采样</h3><p><strong>1.欠采样的原理</strong></p>
<p>欠采样是从1000个不违约样本中随机选取100个样本，和100个违约样本一起构成新的训练集。欠采样抛弃了大部分不违约样本，在搭建模型时有可能产生欠拟合。</p>
<p><strong>2.欠采样的代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 仍然采用上方的信用违约数据进行欠采样代码的展示：</span></span><br><span class="line"><span class="keyword">from</span> imblearn.under_sampling <span class="keyword">import</span> RandomUnderSampler</span><br><span class="line">rus = RandomUnderSampler(random_state=<span class="number">0</span>)</span><br><span class="line">X_undersampled, y_undersampled = rus.fit_resample(X, y)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用Counter()方法检验一下随机欠采样的效果。</span></span><br><span class="line">Counter(y_undersampled)</span><br></pre></td></tr></table></figure>




<pre><code>Counter(&#123;0: 100, 1: 100&#125;)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不违约的样本数从1000下降至违约的样本数100，这证明我们的随机欠采样有效。同时我们可以打印特征变量X_undersampled的shape来看看特征变量的变化：</span></span><br><span class="line">X_undersampled.shape</span><br></pre></td></tr></table></figure>




<pre><code>(200, 6)
</code></pre>
<p>在实战中处理样本不均衡问题时，如果样本数据量不大，通常使用过采样，因为这样能够更好地利用数据，不会像欠采样那样狠多数据都没有使用到；如果数据量充足，则过采样和欠采样都可以考虑使用</p>
<h1 id="12-数据降维之PCA"><a href="#12-数据降维之PCA" class="headerlink" title="12 数据降维之PCA"></a>12 数据降维之PCA</h1><p>建立模型分析特征数据时，很可能会面临特征数据维度过大的问题。如收入、年龄、性别、婚姻状况、工作单位等数百个维度的特征。如果将所有特征数据都用来拟合模型，会提高模型的复杂度，造成过拟合风险显著增大，且不同的特征维数间可能存在共线性。此时就需要对数据进行降维，以浓缩特征向量。</p>
<h2 id="12-1-数据降维"><a href="#12-1-数据降维" class="headerlink" title="12.1 数据降维"></a>12.1 数据降维</h2><p>如果特征变量的数量非常多（如成百上千个特征变量），我们往往需要进行数据降维。</p>
<p>降维的方法主要有两种：<strong>选择特征</strong>和<strong>抽取特征</strong>两种：选择特征是从原有的特征中挑选出<strong>最佳的特征</strong>；抽取特征是将数据从高维向低维投影，进行坐标的线性转换。PCA（主成分分析）即典型的抽取特征的方法，它不仅是对高维数据进行降维，更重要的是经过降维<strong>去除噪声，发现数据中的模式</strong>。</p>
<h3 id="12-1-1-PCA的基本原理"><a href="#12-1-1-PCA的基本原理" class="headerlink" title="12.1.1 PCA的基本原理"></a>12.1.1 PCA的基本原理</h3><p>1.二维空间降维</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(31).png" alt="下载 (31)"></p>
<p>在实际进行数据降维前，需要先对特征数据做零均值化处理，即将每个特征维度的数据减去该特征的均值。例如，此处的二维数据（1，1）、（2，2）、（3，3），其特征X（1，2，3）和特征Y（1，2，3）的均值都是（1+2+3）&#x2F;3&#x3D;2，每个特征都减去均值后，数据被转化为（-1，-1）、（0，0）、（1，1）.再对零均值化后的数据进行线性组合，二维空间中的3个点就依次转化为数轴上的-根号2，0和根号2<br><img src="%E4%B8%8B%E8%BD%BD%20(32).png" alt="下载 (32)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(33).png" alt="下载 (33)"></p>
<p>2.n维空间降维</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(34).png" alt="下载 (34)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(35).png" alt="下载 (35)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(36).png" alt="下载 (36)"></p>
<h3 id="12-1-2-PCA的代码实现"><a href="#12-1-2-PCA的代码实现" class="headerlink" title="12.1.2 PCA的代码实现"></a>12.1.2 PCA的代码实现</h3><p><strong>1.二维空间降维的代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>]])</span><br><span class="line">X</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 1],
       [2, 2],
       [3, 3]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以通过pandas库来构造数据，效果一样</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">X = pd.DataFrame([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>]])</span><br><span class="line">X</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据降维，由二维降至一维</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">1</span>) <span class="comment"># 该参数为保留的成分个数，设置为1</span></span><br><span class="line">pca.fit(X)  <span class="comment"># 进行降维模型训练</span></span><br><span class="line">X_transformed = pca.transform(X)  <span class="comment"># 进行数据降维，并赋值给X_transformed</span></span><br><span class="line"></span><br><span class="line">X_transformed  <span class="comment"># 查看降维后的结果</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[-1.41421356],
       [ 0.        ],
       [ 1.41421356]])
</code></pre>
<p><strong>说明</strong>：n_components设置为1，也就是二维数据降为一维数据。该参数不仅可以设置成降维后成分的个数，还可以设置成降维后保留的信息的百分比，例如，将其设置成0.9就是在降维后保留原特征90%的信息。注意，如果将参数设置为降维后成分的个数，其值不能大于min(n_samples,n_features)，即样本数和特征变量数两者之中的最小值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看此时的维度</span></span><br><span class="line">X_transformed.shape</span><br></pre></td></tr></table></figure>




<pre><code>(3, 1)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看降维的系数</span></span><br><span class="line">pca.components_  </span><br></pre></td></tr></table></figure>




<pre><code>array([[0.70710678, 0.70710678]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看线性组合表达式</span></span><br><span class="line">a = pca.components_[<span class="number">0</span>][<span class="number">0</span>] </span><br><span class="line">b = pca.components_[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span>(a) + <span class="string">&#x27; * X + &#x27;</span> +  <span class="built_in">str</span>(b) + <span class="string">&#x27; * Y&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>0.7071067811865476 * X + 0.7071067811865475 * Y
</code></pre>
<p><strong>1.三维空间降维的代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">X = pd.DataFrame([[<span class="number">45</span>, <span class="number">0.8</span>, <span class="number">9120</span>], [<span class="number">40</span>, <span class="number">0.12</span>, <span class="number">2600</span>], [<span class="number">38</span>, <span class="number">0.09</span>, <span class="number">3042</span>], [<span class="number">30</span>, <span class="number">0.04</span>, <span class="number">3300</span>], [<span class="number">39</span>, <span class="number">0.21</span>, <span class="number">3500</span>]], columns=[<span class="string">&#x27;年龄(岁)&#x27;</span>, <span class="string">&#x27;负债比率&#x27;</span>, <span class="string">&#x27;月收入(元)&#x27;</span>])</span><br><span class="line">X</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年龄(岁)</th>
      <th>负债比率</th>
      <th>月收入(元)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>45</td>
      <td>0.80</td>
      <td>9120</td>
    </tr>
    <tr>
      <th>1</th>
      <td>40</td>
      <td>0.12</td>
      <td>2600</td>
    </tr>
    <tr>
      <th>2</th>
      <td>38</td>
      <td>0.09</td>
      <td>3042</td>
    </tr>
    <tr>
      <th>3</th>
      <td>30</td>
      <td>0.04</td>
      <td>3300</td>
    </tr>
    <tr>
      <th>4</th>
      <td>39</td>
      <td>0.21</td>
      <td>3500</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为三个指标数据的量级相差较大，所以可以先进行数据归一化处理</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">X_new = StandardScaler().fit_transform(X)</span><br><span class="line"></span><br><span class="line">X_new  <span class="comment"># 查看归一化后的数据</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[ 1.36321743,  1.96044639,  1.98450514],
       [ 0.33047695, -0.47222431, -0.70685302],
       [-0.08261924, -0.57954802, -0.52440206],
       [-1.73500401, -0.75842087, -0.41790353],
       [ 0.12392886, -0.15025319, -0.33534653]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)  <span class="comment"># 将三维数据降为二维数据</span></span><br><span class="line">pca.fit(X_new)  <span class="comment"># 进行降维模型训练</span></span><br><span class="line">X_transformed = pca.transform(X_new)  <span class="comment"># 进行数据降维，并赋值给X_transformed</span></span><br><span class="line"></span><br><span class="line">X_transformed  <span class="comment"># 查看降维后的结果</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[ 3.08724247,  0.32991205],
       [-0.52888635, -0.74272137],
       [-0.70651782, -0.33057258],
       [-1.62877292,  1.05218639],
       [-0.22306538, -0.30880449]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看降维的系数</span></span><br><span class="line">pca.components_  </span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0.52952108,  0.61328179,  0.58608264],
       [-0.82760701,  0.22182579,  0.51561609]])
</code></pre>
<p><img src="%E4%B8%8B%E8%BD%BD%20(37).png" alt="下载 (37)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dim = [<span class="string">&#x27;年龄(岁)&#x27;</span>, <span class="string">&#x27;负债比率&#x27;</span>, <span class="string">&#x27;月收入(元)&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> pca.components_:</span><br><span class="line">    formula = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(i)):</span><br><span class="line">        formula.append(<span class="built_in">str</span>(i[j]) + <span class="string">&#x27; * &#x27;</span> + dim[j])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; + &quot;</span>.join(formula))</span><br></pre></td></tr></table></figure>

<pre><code>0.529521083916554 * 年龄(岁) + 0.6132817922410683 * 负债比率 + 0.5860826434841946 * 月收入(元)
-0.8276070105929828 * 年龄(岁) + 0.2218257919336098 * 负债比率 + 0.5156160917294703 * 月收入(元)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果不想显示具体的特征名称，可以采用如下的写法</span></span><br><span class="line">dim = [<span class="string">&#x27;X&#x27;</span>, <span class="string">&#x27;Y&#x27;</span>, <span class="string">&#x27;Z&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> pca.components_:</span><br><span class="line">    formula = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(i)):</span><br><span class="line">        formula.append(<span class="built_in">str</span>(i[j]) + <span class="string">&#x27; * &#x27;</span> + dim[j])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; + &quot;</span>.join(formula))</span><br></pre></td></tr></table></figure>

<pre><code>0.529521083916554 * X + 0.6132817922410683 * Y + 0.5860826434841946 * Z
-0.8276070105929828 * X + 0.2218257919336098 * Y + 0.5156160917294703 * Z
</code></pre>
<h2 id="12-2-案例实战：人脸识别模型"><a href="#12-2-案例实战：人脸识别模型" class="headerlink" title="12.2 案例实战：人脸识别模型"></a>12.2 案例实战：人脸识别模型</h2><h3 id="12-2-1-案例背景"><a href="#12-2-1-案例背景" class="headerlink" title="12.2.1 案例背景"></a>12.2.1 案例背景</h3><p>人脸识别的本质是根据每张人脸图像中不同像素点的颜色进行数据建模与判断。人脸图像的每个像素点的颜色都有不同的值，这些值可以组成人脸的特征向量，不过因为人脸图片的像素点很多，所以特征变量也很多，需要利用PCA进行数据降维</p>
<h3 id="12-2-2-人脸数据读取、处理与变量提取"><a href="#12-2-2-人脸数据读取、处理与变量提取" class="headerlink" title="12.2.2 人脸数据读取、处理与变量提取"></a>12.2.2 人脸数据读取、处理与变量提取</h3><p>1.读取人脸照片数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">names = os.listdir(<span class="string">&#x27;olivettifaces&#x27;</span>)</span><br><span class="line"></span><br><span class="line">names[<span class="number">0</span>:<span class="number">5</span>]  <span class="comment"># 查看前5项读取的文件名</span></span><br></pre></td></tr></table></figure>




<pre><code>[&#39;10_0.jpg&#39;, &#39;10_1.jpg&#39;, &#39;10_2.jpg&#39;, &#39;10_3.jpg&#39;, &#39;10_4.jpg&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取到文件名称后，便可以通过如下代码在Python中查看这些图片</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img0 = Image.<span class="built_in">open</span>(<span class="string">&#x27;olivettifaces\\&#x27;</span> + names[<span class="number">0</span>])</span><br><span class="line">img0.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img0  <span class="comment"># 在Jupyter Notebook中可以直接输入变量名查看图像</span></span><br></pre></td></tr></table></figure>




<p><img src="output_235_0.png" alt="output_235_0"></p>
<p><strong>2.人脸数据处理 - 特征变量提取</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图像灰度处理及数值化处理</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img0 = img0.convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">img0 = img0.resize((<span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">arr = np.array(img0)</span><br><span class="line"></span><br><span class="line">arr  <span class="comment"># 查看数值化后的结果</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[186,  76,  73, ..., 100, 103, 106],
       [196,  85,  68, ...,  85, 106, 103],
       [193,  69,  79, ...,  82,  99, 100],
       ...,
       [196,  87, 193, ..., 103,  66,  52],
       [219, 179, 202, ..., 150, 127, 109],
       [244, 228, 230, ..., 198, 202, 206]], dtype=uint8)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果觉得numpy格式的arr不好观察，则可以通过pandas库将其转为DataFrame格式进行观察</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.DataFrame(arr)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
      <th>26</th>
      <th>27</th>
      <th>28</th>
      <th>29</th>
      <th>30</th>
      <th>31</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>186</td>
      <td>76</td>
      <td>73</td>
      <td>87</td>
      <td>89</td>
      <td>88</td>
      <td>75</td>
      <td>81</td>
      <td>100</td>
      <td>102</td>
      <td>...</td>
      <td>71</td>
      <td>75</td>
      <td>75</td>
      <td>73</td>
      <td>76</td>
      <td>85</td>
      <td>95</td>
      <td>100</td>
      <td>103</td>
      <td>106</td>
    </tr>
    <tr>
      <th>1</th>
      <td>196</td>
      <td>85</td>
      <td>68</td>
      <td>78</td>
      <td>104</td>
      <td>97</td>
      <td>100</td>
      <td>94</td>
      <td>83</td>
      <td>87</td>
      <td>...</td>
      <td>52</td>
      <td>59</td>
      <td>70</td>
      <td>85</td>
      <td>62</td>
      <td>82</td>
      <td>89</td>
      <td>85</td>
      <td>106</td>
      <td>103</td>
    </tr>
    <tr>
      <th>2</th>
      <td>193</td>
      <td>69</td>
      <td>79</td>
      <td>92</td>
      <td>105</td>
      <td>102</td>
      <td>112</td>
      <td>117</td>
      <td>106</td>
      <td>94</td>
      <td>...</td>
      <td>41</td>
      <td>45</td>
      <td>50</td>
      <td>76</td>
      <td>59</td>
      <td>74</td>
      <td>83</td>
      <td>82</td>
      <td>99</td>
      <td>100</td>
    </tr>
    <tr>
      <th>3</th>
      <td>186</td>
      <td>67</td>
      <td>71</td>
      <td>75</td>
      <td>85</td>
      <td>99</td>
      <td>114</td>
      <td>115</td>
      <td>109</td>
      <td>109</td>
      <td>...</td>
      <td>42</td>
      <td>43</td>
      <td>40</td>
      <td>52</td>
      <td>41</td>
      <td>61</td>
      <td>69</td>
      <td>76</td>
      <td>76</td>
      <td>108</td>
    </tr>
    <tr>
      <th>4</th>
      <td>179</td>
      <td>46</td>
      <td>41</td>
      <td>50</td>
      <td>53</td>
      <td>69</td>
      <td>80</td>
      <td>91</td>
      <td>108</td>
      <td>104</td>
      <td>...</td>
      <td>43</td>
      <td>37</td>
      <td>30</td>
      <td>31</td>
      <td>35</td>
      <td>43</td>
      <td>59</td>
      <td>61</td>
      <td>56</td>
      <td>101</td>
    </tr>
    <tr>
      <th>5</th>
      <td>173</td>
      <td>33</td>
      <td>43</td>
      <td>49</td>
      <td>48</td>
      <td>53</td>
      <td>64</td>
      <td>69</td>
      <td>72</td>
      <td>75</td>
      <td>...</td>
      <td>38</td>
      <td>36</td>
      <td>33</td>
      <td>32</td>
      <td>39</td>
      <td>45</td>
      <td>68</td>
      <td>60</td>
      <td>45</td>
      <td>83</td>
    </tr>
    <tr>
      <th>6</th>
      <td>173</td>
      <td>30</td>
      <td>37</td>
      <td>41</td>
      <td>42</td>
      <td>57</td>
      <td>81</td>
      <td>88</td>
      <td>77</td>
      <td>64</td>
      <td>...</td>
      <td>31</td>
      <td>32</td>
      <td>35</td>
      <td>32</td>
      <td>35</td>
      <td>49</td>
      <td>65</td>
      <td>64</td>
      <td>53</td>
      <td>87</td>
    </tr>
    <tr>
      <th>7</th>
      <td>171</td>
      <td>24</td>
      <td>32</td>
      <td>36</td>
      <td>42</td>
      <td>55</td>
      <td>77</td>
      <td>101</td>
      <td>107</td>
      <td>102</td>
      <td>...</td>
      <td>54</td>
      <td>64</td>
      <td>63</td>
      <td>51</td>
      <td>53</td>
      <td>60</td>
      <td>56</td>
      <td>46</td>
      <td>49</td>
      <td>89</td>
    </tr>
    <tr>
      <th>8</th>
      <td>170</td>
      <td>21</td>
      <td>31</td>
      <td>29</td>
      <td>28</td>
      <td>35</td>
      <td>47</td>
      <td>62</td>
      <td>76</td>
      <td>83</td>
      <td>...</td>
      <td>105</td>
      <td>101</td>
      <td>89</td>
      <td>63</td>
      <td>45</td>
      <td>42</td>
      <td>41</td>
      <td>37</td>
      <td>61</td>
      <td>101</td>
    </tr>
    <tr>
      <th>9</th>
      <td>172</td>
      <td>21</td>
      <td>22</td>
      <td>27</td>
      <td>28</td>
      <td>30</td>
      <td>33</td>
      <td>43</td>
      <td>46</td>
      <td>44</td>
      <td>...</td>
      <td>129</td>
      <td>118</td>
      <td>103</td>
      <td>74</td>
      <td>39</td>
      <td>27</td>
      <td>36</td>
      <td>34</td>
      <td>68</td>
      <td>101</td>
    </tr>
    <tr>
      <th>10</th>
      <td>171</td>
      <td>23</td>
      <td>30</td>
      <td>21</td>
      <td>30</td>
      <td>36</td>
      <td>44</td>
      <td>51</td>
      <td>46</td>
      <td>41</td>
      <td>...</td>
      <td>136</td>
      <td>126</td>
      <td>120</td>
      <td>102</td>
      <td>70</td>
      <td>40</td>
      <td>30</td>
      <td>35</td>
      <td>78</td>
      <td>101</td>
    </tr>
    <tr>
      <th>11</th>
      <td>175</td>
      <td>21</td>
      <td>33</td>
      <td>31</td>
      <td>42</td>
      <td>54</td>
      <td>71</td>
      <td>73</td>
      <td>64</td>
      <td>59</td>
      <td>...</td>
      <td>142</td>
      <td>133</td>
      <td>122</td>
      <td>114</td>
      <td>111</td>
      <td>77</td>
      <td>31</td>
      <td>36</td>
      <td>95</td>
      <td>104</td>
    </tr>
    <tr>
      <th>12</th>
      <td>192</td>
      <td>42</td>
      <td>27</td>
      <td>37</td>
      <td>63</td>
      <td>87</td>
      <td>99</td>
      <td>111</td>
      <td>116</td>
      <td>116</td>
      <td>...</td>
      <td>105</td>
      <td>108</td>
      <td>108</td>
      <td>107</td>
      <td>109</td>
      <td>104</td>
      <td>58</td>
      <td>59</td>
      <td>100</td>
      <td>97</td>
    </tr>
    <tr>
      <th>13</th>
      <td>196</td>
      <td>82</td>
      <td>41</td>
      <td>58</td>
      <td>102</td>
      <td>112</td>
      <td>110</td>
      <td>110</td>
      <td>107</td>
      <td>108</td>
      <td>...</td>
      <td>70</td>
      <td>71</td>
      <td>86</td>
      <td>102</td>
      <td>107</td>
      <td>116</td>
      <td>84</td>
      <td>77</td>
      <td>95</td>
      <td>99</td>
    </tr>
    <tr>
      <th>14</th>
      <td>192</td>
      <td>88</td>
      <td>78</td>
      <td>95</td>
      <td>96</td>
      <td>87</td>
      <td>81</td>
      <td>72</td>
      <td>51</td>
      <td>40</td>
      <td>...</td>
      <td>62</td>
      <td>73</td>
      <td>73</td>
      <td>101</td>
      <td>116</td>
      <td>116</td>
      <td>99</td>
      <td>80</td>
      <td>88</td>
      <td>105</td>
    </tr>
    <tr>
      <th>15</th>
      <td>190</td>
      <td>88</td>
      <td>102</td>
      <td>114</td>
      <td>99</td>
      <td>76</td>
      <td>55</td>
      <td>55</td>
      <td>50</td>
      <td>37</td>
      <td>...</td>
      <td>100</td>
      <td>112</td>
      <td>120</td>
      <td>120</td>
      <td>125</td>
      <td>125</td>
      <td>105</td>
      <td>97</td>
      <td>101</td>
      <td>98</td>
    </tr>
    <tr>
      <th>16</th>
      <td>189</td>
      <td>106</td>
      <td>111</td>
      <td>113</td>
      <td>137</td>
      <td>124</td>
      <td>113</td>
      <td>109</td>
      <td>103</td>
      <td>96</td>
      <td>...</td>
      <td>134</td>
      <td>137</td>
      <td>142</td>
      <td>146</td>
      <td>134</td>
      <td>120</td>
      <td>96</td>
      <td>115</td>
      <td>122</td>
      <td>93</td>
    </tr>
    <tr>
      <th>17</th>
      <td>188</td>
      <td>106</td>
      <td>132</td>
      <td>119</td>
      <td>142</td>
      <td>160</td>
      <td>158</td>
      <td>153</td>
      <td>148</td>
      <td>145</td>
      <td>...</td>
      <td>163</td>
      <td>158</td>
      <td>156</td>
      <td>142</td>
      <td>124</td>
      <td>110</td>
      <td>100</td>
      <td>114</td>
      <td>108</td>
      <td>86</td>
    </tr>
    <tr>
      <th>18</th>
      <td>193</td>
      <td>83</td>
      <td>140</td>
      <td>130</td>
      <td>122</td>
      <td>141</td>
      <td>153</td>
      <td>160</td>
      <td>168</td>
      <td>177</td>
      <td>...</td>
      <td>163</td>
      <td>158</td>
      <td>148</td>
      <td>129</td>
      <td>112</td>
      <td>103</td>
      <td>99</td>
      <td>99</td>
      <td>78</td>
      <td>77</td>
    </tr>
    <tr>
      <th>19</th>
      <td>190</td>
      <td>81</td>
      <td>117</td>
      <td>127</td>
      <td>107</td>
      <td>120</td>
      <td>134</td>
      <td>146</td>
      <td>163</td>
      <td>166</td>
      <td>...</td>
      <td>157</td>
      <td>145</td>
      <td>132</td>
      <td>117</td>
      <td>105</td>
      <td>103</td>
      <td>90</td>
      <td>64</td>
      <td>67</td>
      <td>72</td>
    </tr>
    <tr>
      <th>20</th>
      <td>193</td>
      <td>83</td>
      <td>84</td>
      <td>106</td>
      <td>104</td>
      <td>113</td>
      <td>122</td>
      <td>134</td>
      <td>138</td>
      <td>143</td>
      <td>...</td>
      <td>142</td>
      <td>129</td>
      <td>116</td>
      <td>109</td>
      <td>105</td>
      <td>102</td>
      <td>86</td>
      <td>55</td>
      <td>60</td>
      <td>63</td>
    </tr>
    <tr>
      <th>21</th>
      <td>194</td>
      <td>78</td>
      <td>87</td>
      <td>91</td>
      <td>92</td>
      <td>108</td>
      <td>113</td>
      <td>122</td>
      <td>127</td>
      <td>140</td>
      <td>...</td>
      <td>140</td>
      <td>128</td>
      <td>118</td>
      <td>113</td>
      <td>109</td>
      <td>101</td>
      <td>68</td>
      <td>56</td>
      <td>56</td>
      <td>56</td>
    </tr>
    <tr>
      <th>22</th>
      <td>191</td>
      <td>80</td>
      <td>89</td>
      <td>88</td>
      <td>90</td>
      <td>104</td>
      <td>114</td>
      <td>120</td>
      <td>131</td>
      <td>141</td>
      <td>...</td>
      <td>130</td>
      <td>129</td>
      <td>119</td>
      <td>108</td>
      <td>103</td>
      <td>101</td>
      <td>50</td>
      <td>53</td>
      <td>55</td>
      <td>53</td>
    </tr>
    <tr>
      <th>23</th>
      <td>189</td>
      <td>77</td>
      <td>89</td>
      <td>91</td>
      <td>86</td>
      <td>93</td>
      <td>111</td>
      <td>122</td>
      <td>133</td>
      <td>129</td>
      <td>...</td>
      <td>102</td>
      <td>113</td>
      <td>111</td>
      <td>107</td>
      <td>101</td>
      <td>85</td>
      <td>53</td>
      <td>51</td>
      <td>54</td>
      <td>55</td>
    </tr>
    <tr>
      <th>24</th>
      <td>190</td>
      <td>86</td>
      <td>88</td>
      <td>87</td>
      <td>87</td>
      <td>87</td>
      <td>104</td>
      <td>115</td>
      <td>127</td>
      <td>115</td>
      <td>...</td>
      <td>105</td>
      <td>108</td>
      <td>104</td>
      <td>102</td>
      <td>97</td>
      <td>55</td>
      <td>53</td>
      <td>50</td>
      <td>53</td>
      <td>59</td>
    </tr>
    <tr>
      <th>25</th>
      <td>187</td>
      <td>74</td>
      <td>89</td>
      <td>81</td>
      <td>93</td>
      <td>130</td>
      <td>103</td>
      <td>96</td>
      <td>110</td>
      <td>108</td>
      <td>...</td>
      <td>111</td>
      <td>105</td>
      <td>103</td>
      <td>102</td>
      <td>83</td>
      <td>50</td>
      <td>49</td>
      <td>56</td>
      <td>51</td>
      <td>49</td>
    </tr>
    <tr>
      <th>26</th>
      <td>190</td>
      <td>79</td>
      <td>81</td>
      <td>107</td>
      <td>166</td>
      <td>206</td>
      <td>119</td>
      <td>88</td>
      <td>94</td>
      <td>105</td>
      <td>...</td>
      <td>102</td>
      <td>104</td>
      <td>99</td>
      <td>100</td>
      <td>111</td>
      <td>111</td>
      <td>57</td>
      <td>48</td>
      <td>52</td>
      <td>53</td>
    </tr>
    <tr>
      <th>27</th>
      <td>192</td>
      <td>78</td>
      <td>83</td>
      <td>173</td>
      <td>211</td>
      <td>158</td>
      <td>114</td>
      <td>100</td>
      <td>87</td>
      <td>94</td>
      <td>...</td>
      <td>101</td>
      <td>98</td>
      <td>98</td>
      <td>96</td>
      <td>116</td>
      <td>123</td>
      <td>119</td>
      <td>52</td>
      <td>49</td>
      <td>55</td>
    </tr>
    <tr>
      <th>28</th>
      <td>188</td>
      <td>70</td>
      <td>136</td>
      <td>177</td>
      <td>198</td>
      <td>108</td>
      <td>101</td>
      <td>119</td>
      <td>86</td>
      <td>81</td>
      <td>...</td>
      <td>98</td>
      <td>98</td>
      <td>93</td>
      <td>82</td>
      <td>80</td>
      <td>123</td>
      <td>145</td>
      <td>73</td>
      <td>43</td>
      <td>51</td>
    </tr>
    <tr>
      <th>29</th>
      <td>196</td>
      <td>87</td>
      <td>193</td>
      <td>187</td>
      <td>179</td>
      <td>113</td>
      <td>123</td>
      <td>123</td>
      <td>110</td>
      <td>81</td>
      <td>...</td>
      <td>95</td>
      <td>90</td>
      <td>96</td>
      <td>77</td>
      <td>53</td>
      <td>160</td>
      <td>124</td>
      <td>103</td>
      <td>66</td>
      <td>52</td>
    </tr>
    <tr>
      <th>30</th>
      <td>219</td>
      <td>179</td>
      <td>202</td>
      <td>196</td>
      <td>198</td>
      <td>146</td>
      <td>122</td>
      <td>118</td>
      <td>119</td>
      <td>94</td>
      <td>...</td>
      <td>92</td>
      <td>90</td>
      <td>87</td>
      <td>57</td>
      <td>89</td>
      <td>126</td>
      <td>140</td>
      <td>150</td>
      <td>127</td>
      <td>109</td>
    </tr>
    <tr>
      <th>31</th>
      <td>244</td>
      <td>228</td>
      <td>230</td>
      <td>231</td>
      <td>233</td>
      <td>213</td>
      <td>188</td>
      <td>195</td>
      <td>193</td>
      <td>189</td>
      <td>...</td>
      <td>179</td>
      <td>184</td>
      <td>177</td>
      <td>161</td>
      <td>202</td>
      <td>182</td>
      <td>207</td>
      <td>198</td>
      <td>202</td>
      <td>206</td>
    </tr>
  </tbody>
</table>
<p>32 rows × 32 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上面获得的32*32的二维数组，还不利于数据建模，所以我们还需要通过reshape(1, -1)方法将其转换成一行(若reshape(-1,1)则转为一列），也即1*1024格式</span></span><br><span class="line">arr = arr.reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(arr)  <span class="comment"># 查看转换后的结果，这一行数就是代表那张人脸图片了，其共有32*32=1024列数</span></span><br></pre></td></tr></table></figure>

<pre><code>[[186  76  73 ... 198 202 206]]
</code></pre>
<p>因为总共有400张照片需要处理，若将400个二维数组堆叠起来会形成三维数组，因为我们需要使用flatten()函数将1*1024的二维数组降维成一维数组，并通过tolist()函数将其转为列表方便之后和其他图片的颜色数值信息一起处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(arr.flatten().tolist())  <span class="comment"># 下面这一行数就是那张人脸转换后的结果了</span></span><br></pre></td></tr></table></figure>

<pre><code>[186, 76, 73, 87, 89, 88, 75, 81, 100, 102, 105, 92, 74, 65, 65, 53, 43, 55, 53, 42, 58, 77, 71, 75, 75, 73, 76, 85, 95, 100, 103, 106, 196, 85, 68, 78, 104, 97, 100, 94, 83, 87, 88, 89, 86, 70, 65, 61, 55, 52, 38, 32, 52, 66, 52, 59, 70, 85, 62, 82, 89, 85, 106, 103, 193, 69, 79, 92, 105, 102, 112, 117, 106, 94, 91, 112, 101, 87, 75, 61, 58, 54, 49, 48, 44, 41, 41, 45, 50, 76, 59, 74, 83, 82, 99, 100, 186, 67, 71, 75, 85, 99, 114, 115, 109, 109, 98, 101, 86, 68, 74, 65, 58, 53, 51, 52, 42, 40, 42, 43, 40, 52, 41, 61, 69, 76, 76, 108, 179, 46, 41, 50, 53, 69, 80, 91, 108, 104, 98, 93, 91, 88, 73, 60, 56, 55, 51, 49, 53, 55, 43, 37, 30, 31, 35, 43, 59, 61, 56, 101, 173, 33, 43, 49, 48, 53, 64, 69, 72, 75, 82, 84, 84, 82, 72, 75, 69, 71, 67, 56, 58, 55, 38, 36, 33, 32, 39, 45, 68, 60, 45, 83, 173, 30, 37, 41, 42, 57, 81, 88, 77, 64, 63, 64, 65, 64, 48, 48, 68, 62, 47, 50, 45, 31, 31, 32, 35, 32, 35, 49, 65, 64, 53, 87, 171, 24, 32, 36, 42, 55, 77, 101, 107, 102, 98, 83, 71, 64, 44, 48, 54, 64, 76, 65, 49, 48, 54, 64, 63, 51, 53, 60, 56, 46, 49, 89, 170, 21, 31, 29, 28, 35, 47, 62, 76, 83, 87, 78, 53, 58, 65, 83, 90, 97, 108, 101, 97, 105, 105, 101, 89, 63, 45, 42, 41, 37, 61, 101, 172, 21, 22, 27, 28, 30, 33, 43, 46, 44, 43, 46, 50, 63, 76, 87, 95, 104, 114, 120, 122, 125, 129, 118, 103, 74, 39, 27, 36, 34, 68, 101, 171, 23, 30, 21, 30, 36, 44, 51, 46, 41, 45, 52, 61, 70, 84, 101, 112, 119, 126, 131, 134, 138, 136, 126, 120, 102, 70, 40, 30, 35, 78, 101, 175, 21, 33, 31, 42, 54, 71, 73, 64, 59, 65, 78, 95, 106, 110, 116, 122, 130, 140, 143, 141, 142, 142, 133, 122, 114, 111, 77, 31, 36, 95, 104, 192, 42, 27, 37, 63, 87, 99, 111, 116, 116, 117, 117, 121, 122, 120, 119, 120, 121, 120, 113, 107, 106, 105, 108, 108, 107, 109, 104, 58, 59, 100, 97, 196, 82, 41, 58, 102, 112, 110, 110, 107, 108, 107, 107, 115, 126, 126, 118, 98, 87, 69, 57, 55, 55, 70, 71, 86, 102, 107, 116, 84, 77, 95, 99, 192, 88, 78, 95, 96, 87, 81, 72, 51, 40, 52, 53, 68, 89, 116, 111, 83, 61, 44, 55, 61, 42, 62, 73, 73, 101, 116, 116, 99, 80, 88, 105, 190, 88, 102, 114, 99, 76, 55, 55, 50, 37, 60, 53, 49, 84, 161, 155, 109, 94, 83, 76, 84, 88, 100, 112, 120, 120, 125, 125, 105, 97, 101, 98, 189, 106, 111, 113, 137, 124, 113, 109, 103, 96, 84, 84, 99, 123, 172, 176, 139, 130, 127, 120, 116, 115, 134, 137, 142, 146, 134, 120, 96, 115, 122, 93, 188, 106, 132, 119, 142, 160, 158, 153, 148, 145, 156, 151, 139, 139, 160, 177, 151, 138, 141, 153, 162, 164, 163, 158, 156, 142, 124, 110, 100, 114, 108, 86, 193, 83, 140, 130, 122, 141, 153, 160, 168, 177, 171, 154, 135, 137, 161, 176, 158, 146, 143, 142, 159, 164, 163, 158, 148, 129, 112, 103, 99, 99, 78, 77, 190, 81, 117, 127, 107, 120, 134, 146, 163, 166, 155, 134, 138, 137, 153, 164, 141, 132, 132, 127, 148, 156, 157, 145, 132, 117, 105, 103, 90, 64, 67, 72, 193, 83, 84, 106, 104, 113, 122, 134, 138, 143, 141, 138, 93, 69, 84, 86, 71, 62, 75, 106, 142, 146, 142, 129, 116, 109, 105, 102, 86, 55, 60, 63, 194, 78, 87, 91, 92, 108, 113, 122, 127, 140, 148, 154, 121, 80, 72, 66, 80, 96, 106, 112, 137, 147, 140, 128, 118, 113, 109, 101, 68, 56, 56, 56, 191, 80, 89, 88, 90, 104, 114, 120, 131, 141, 145, 150, 153, 142, 144, 124, 105, 111, 119, 121, 128, 128, 130, 129, 119, 108, 103, 101, 50, 53, 55, 53, 189, 77, 89, 91, 86, 93, 111, 122, 133, 129, 125, 126, 118, 117, 115, 111, 100, 85, 87, 86, 76, 88, 102, 113, 111, 107, 101, 85, 53, 51, 54, 55, 190, 86, 88, 87, 87, 87, 104, 115, 127, 115, 101, 88, 83, 84, 91, 96, 108, 114, 101, 98, 106, 103, 105, 108, 104, 102, 97, 55, 53, 50, 53, 59, 187, 74, 89, 81, 93, 130, 103, 96, 110, 108, 108, 129, 126, 112, 119, 108, 96, 98, 105, 110, 117, 118, 111, 105, 103, 102, 83, 50, 49, 56, 51, 49, 190, 79, 81, 107, 166, 206, 119, 88, 94, 105, 112, 116, 113, 106, 96, 92, 95, 102, 103, 107, 113, 108, 102, 104, 99, 100, 111, 111, 57, 48, 52, 53, 192, 78, 83, 173, 211, 158, 114, 100, 87, 94, 108, 114, 115, 119, 129, 146, 151, 144, 140, 133, 121, 112, 101, 98, 98, 96, 116, 123, 119, 52, 49, 55, 188, 70, 136, 177, 198, 108, 101, 119, 86, 81, 94, 105, 115, 123, 127, 128, 126, 122, 119, 109, 97, 97, 98, 98, 93, 82, 80, 123, 145, 73, 43, 51, 196, 87, 193, 187, 179, 113, 123, 123, 110, 81, 73, 81, 88, 96, 97, 95, 91, 91, 89, 86, 91, 99, 95, 90, 96, 77, 53, 160, 124, 103, 66, 52, 219, 179, 202, 196, 198, 146, 122, 118, 119, 94, 76, 73, 72, 74, 77, 73, 74, 79, 77, 83, 92, 94, 92, 90, 87, 57, 89, 126, 140, 150, 127, 109, 244, 228, 230, 231, 233, 213, 188, 195, 193, 189, 181, 173, 171, 171, 171, 168, 168, 171, 173, 178, 182, 179, 179, 184, 177, 161, 202, 182, 207, 198, 202, 206]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造所有图片的特征变量</span></span><br><span class="line">X = []  <span class="comment"># 特征变量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> names:</span><br><span class="line">    img = Image.<span class="built_in">open</span>(<span class="string">&#x27;olivettifaces\\&#x27;</span> + i)</span><br><span class="line">    img = img.convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">    img = img.resize((<span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">    arr = np.array(img)</span><br><span class="line">    X.append(arr.reshape(<span class="number">1</span>, -<span class="number">1</span>).flatten().tolist())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">X = pd.DataFrame(X)</span><br><span class="line"></span><br><span class="line">X  <span class="comment"># 查看400张图片转换后的结果</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>1014</th>
      <th>1015</th>
      <th>1016</th>
      <th>1017</th>
      <th>1018</th>
      <th>1019</th>
      <th>1020</th>
      <th>1021</th>
      <th>1022</th>
      <th>1023</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>186</td>
      <td>76</td>
      <td>73</td>
      <td>87</td>
      <td>89</td>
      <td>88</td>
      <td>75</td>
      <td>81</td>
      <td>100</td>
      <td>102</td>
      <td>...</td>
      <td>179</td>
      <td>184</td>
      <td>177</td>
      <td>161</td>
      <td>202</td>
      <td>182</td>
      <td>207</td>
      <td>198</td>
      <td>202</td>
      <td>206</td>
    </tr>
    <tr>
      <th>1</th>
      <td>196</td>
      <td>90</td>
      <td>97</td>
      <td>98</td>
      <td>98</td>
      <td>87</td>
      <td>101</td>
      <td>89</td>
      <td>65</td>
      <td>73</td>
      <td>...</td>
      <td>181</td>
      <td>167</td>
      <td>190</td>
      <td>188</td>
      <td>203</td>
      <td>209</td>
      <td>205</td>
      <td>198</td>
      <td>190</td>
      <td>190</td>
    </tr>
    <tr>
      <th>2</th>
      <td>193</td>
      <td>89</td>
      <td>97</td>
      <td>99</td>
      <td>75</td>
      <td>74</td>
      <td>83</td>
      <td>64</td>
      <td>77</td>
      <td>86</td>
      <td>...</td>
      <td>178</td>
      <td>178</td>
      <td>156</td>
      <td>185</td>
      <td>195</td>
      <td>201</td>
      <td>206</td>
      <td>201</td>
      <td>189</td>
      <td>190</td>
    </tr>
    <tr>
      <th>3</th>
      <td>192</td>
      <td>84</td>
      <td>93</td>
      <td>89</td>
      <td>97</td>
      <td>89</td>
      <td>66</td>
      <td>60</td>
      <td>60</td>
      <td>57</td>
      <td>...</td>
      <td>173</td>
      <td>151</td>
      <td>199</td>
      <td>189</td>
      <td>203</td>
      <td>200</td>
      <td>196</td>
      <td>186</td>
      <td>182</td>
      <td>184</td>
    </tr>
    <tr>
      <th>4</th>
      <td>194</td>
      <td>72</td>
      <td>49</td>
      <td>45</td>
      <td>56</td>
      <td>37</td>
      <td>44</td>
      <td>62</td>
      <td>71</td>
      <td>71</td>
      <td>...</td>
      <td>192</td>
      <td>194</td>
      <td>192</td>
      <td>176</td>
      <td>174</td>
      <td>224</td>
      <td>200</td>
      <td>218</td>
      <td>176</td>
      <td>168</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>395</th>
      <td>114</td>
      <td>115</td>
      <td>115</td>
      <td>119</td>
      <td>115</td>
      <td>120</td>
      <td>117</td>
      <td>118</td>
      <td>113</td>
      <td>112</td>
      <td>...</td>
      <td>190</td>
      <td>193</td>
      <td>169</td>
      <td>141</td>
      <td>142</td>
      <td>144</td>
      <td>143</td>
      <td>141</td>
      <td>143</td>
      <td>215</td>
    </tr>
    <tr>
      <th>396</th>
      <td>115</td>
      <td>118</td>
      <td>117</td>
      <td>117</td>
      <td>116</td>
      <td>118</td>
      <td>117</td>
      <td>119</td>
      <td>117</td>
      <td>116</td>
      <td>...</td>
      <td>187</td>
      <td>189</td>
      <td>183</td>
      <td>216</td>
      <td>189</td>
      <td>193</td>
      <td>148</td>
      <td>144</td>
      <td>142</td>
      <td>212</td>
    </tr>
    <tr>
      <th>397</th>
      <td>113</td>
      <td>116</td>
      <td>113</td>
      <td>117</td>
      <td>114</td>
      <td>121</td>
      <td>121</td>
      <td>120</td>
      <td>121</td>
      <td>114</td>
      <td>...</td>
      <td>184</td>
      <td>188</td>
      <td>185</td>
      <td>221</td>
      <td>203</td>
      <td>192</td>
      <td>144</td>
      <td>143</td>
      <td>137</td>
      <td>212</td>
    </tr>
    <tr>
      <th>398</th>
      <td>110</td>
      <td>109</td>
      <td>109</td>
      <td>110</td>
      <td>110</td>
      <td>112</td>
      <td>112</td>
      <td>113</td>
      <td>113</td>
      <td>111</td>
      <td>...</td>
      <td>172</td>
      <td>171</td>
      <td>209</td>
      <td>212</td>
      <td>175</td>
      <td>136</td>
      <td>142</td>
      <td>141</td>
      <td>137</td>
      <td>213</td>
    </tr>
    <tr>
      <th>399</th>
      <td>105</td>
      <td>107</td>
      <td>111</td>
      <td>112</td>
      <td>113</td>
      <td>113</td>
      <td>113</td>
      <td>116</td>
      <td>116</td>
      <td>107</td>
      <td>...</td>
      <td>181</td>
      <td>184</td>
      <td>220</td>
      <td>188</td>
      <td>140</td>
      <td>139</td>
      <td>142</td>
      <td>141</td>
      <td>138</td>
      <td>213</td>
    </tr>
  </tbody>
</table>
<p>400 rows × 1024 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(X.shape)  <span class="comment"># 查看此时的表格结构</span></span><br></pre></td></tr></table></figure>

<pre><code>(400, 1024)
</code></pre>
<p><strong>3.人脸数据处理 - 目标变量提取</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取目标变量y：第一张图片演示</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">int</span>(names[<span class="number">0</span>].split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>10
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 批量获取所有图片的目标变量y</span></span><br><span class="line">y = []  <span class="comment"># 目标变量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> names:</span><br><span class="line">    img = Image.<span class="built_in">open</span>(<span class="string">&#x27;olivettifaces\\&#x27;</span> + i)</span><br><span class="line">    y.append(<span class="built_in">int</span>(i.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>]))</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(y)  <span class="comment"># 查看目标变量,也就是对应的人员编号</span></span><br></pre></td></tr></table></figure>

<pre><code>[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]
</code></pre>
<h3 id="12-2-3-数据划分与降维"><a href="#12-2-3-数据划分与降维" class="headerlink" title="12.2.3 数据划分与降维"></a>12.2.3 数据划分与降维</h3><p>1.划分训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>2.PCA数据降维</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据降维模型训练</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">100</span>)</span><br><span class="line">pca.fit(X_train)</span><br></pre></td></tr></table></figure>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对训练集和测试集进行数据降维</span></span><br><span class="line">X_train_pca = pca.transform(X_train)</span><br><span class="line">X_test_pca = pca.transform(X_test)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们通过如下代码验证PCA是否降维：</span></span><br><span class="line"><span class="built_in">print</span>(X_train_pca.shape)</span><br><span class="line"><span class="built_in">print</span>(X_test_pca.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(320, 100)
(80, 100)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想查看此时降维后的X_train_pca和X_test_pca，可以直接将它们打印出来查看，也可以将它们转为DataFrame格式进行查看，代码如下：</span></span><br><span class="line">pd.DataFrame(X_train_pca).head()</span><br><span class="line"><span class="comment"># pd.DataFrame(X_test_pca).head()</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>90</th>
      <th>91</th>
      <th>92</th>
      <th>93</th>
      <th>94</th>
      <th>95</th>
      <th>96</th>
      <th>97</th>
      <th>98</th>
      <th>99</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>843.848468</td>
      <td>-117.990083</td>
      <td>341.041562</td>
      <td>-45.240791</td>
      <td>-265.560411</td>
      <td>243.732770</td>
      <td>280.603780</td>
      <td>-259.737066</td>
      <td>216.102416</td>
      <td>192.380199</td>
      <td>...</td>
      <td>-17.946320</td>
      <td>31.326442</td>
      <td>34.625376</td>
      <td>-8.882940</td>
      <td>24.183872</td>
      <td>-40.728981</td>
      <td>8.891023</td>
      <td>-10.077005</td>
      <td>-41.382655</td>
      <td>-15.945479</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-495.124726</td>
      <td>-937.701751</td>
      <td>-305.851409</td>
      <td>4.351306</td>
      <td>-127.743015</td>
      <td>504.793986</td>
      <td>389.144515</td>
      <td>5.118525</td>
      <td>-32.353105</td>
      <td>103.516952</td>
      <td>...</td>
      <td>-56.858041</td>
      <td>-21.676102</td>
      <td>-37.577951</td>
      <td>8.156807</td>
      <td>11.915498</td>
      <td>-59.731644</td>
      <td>50.641757</td>
      <td>-49.858187</td>
      <td>-19.619541</td>
      <td>-19.917775</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201.200905</td>
      <td>623.575255</td>
      <td>130.691212</td>
      <td>22.627082</td>
      <td>-427.373676</td>
      <td>-85.786784</td>
      <td>531.627875</td>
      <td>95.691712</td>
      <td>-176.386340</td>
      <td>-70.070452</td>
      <td>...</td>
      <td>-46.574460</td>
      <td>4.225610</td>
      <td>-1.518801</td>
      <td>-28.003457</td>
      <td>99.331340</td>
      <td>-33.898140</td>
      <td>-12.756052</td>
      <td>-53.757395</td>
      <td>46.329195</td>
      <td>-36.555657</td>
    </tr>
    <tr>
      <th>3</th>
      <td>603.867640</td>
      <td>-744.880158</td>
      <td>-626.408564</td>
      <td>-598.649870</td>
      <td>-400.044101</td>
      <td>7.220209</td>
      <td>-246.688151</td>
      <td>58.416884</td>
      <td>-417.199241</td>
      <td>-121.976573</td>
      <td>...</td>
      <td>42.639400</td>
      <td>8.357066</td>
      <td>28.521712</td>
      <td>28.323359</td>
      <td>39.183447</td>
      <td>-8.003523</td>
      <td>-85.881704</td>
      <td>30.512104</td>
      <td>-18.756975</td>
      <td>-24.699085</td>
    </tr>
    <tr>
      <th>4</th>
      <td>935.882937</td>
      <td>132.602933</td>
      <td>441.577563</td>
      <td>145.548390</td>
      <td>-260.280190</td>
      <td>248.876747</td>
      <td>235.116995</td>
      <td>-333.232316</td>
      <td>98.692073</td>
      <td>75.838782</td>
      <td>...</td>
      <td>-33.393869</td>
      <td>-36.531246</td>
      <td>43.010484</td>
      <td>-0.482193</td>
      <td>-33.761494</td>
      <td>2.675790</td>
      <td>-61.477543</td>
      <td>87.388430</td>
      <td>-20.888620</td>
      <td>-82.934200</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 100 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在PCA后面加个“？”运行可以可以看看官方的一些提示</span></span><br><span class="line"><span class="comment"># PCA?</span></span><br></pre></td></tr></table></figure>

<h3 id="12-2-4-模型的搭建与使用"><a href="#12-2-4-模型的搭建与使用" class="headerlink" title="12.2.4 模型的搭建与使用"></a>12.2.4 模型的搭建与使用</h3><p>1.模型搭建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn = KNeighborsClassifier()  <span class="comment"># 建立KNN模型  </span></span><br><span class="line">knn.fit(X_train_pca, y_train)  <span class="comment"># 用降维后的训练集进行训练模型</span></span><br></pre></td></tr></table></figure>





<p>2.模型预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred = knn.predict(X_test_pca)  <span class="comment"># 用降维后的测试集进行测试</span></span><br><span class="line"><span class="built_in">print</span>(y_pred)  <span class="comment"># 将对测试集的预测结果打印出来</span></span><br></pre></td></tr></table></figure>

<pre><code>[ 9 21  3 40 26  4 28 37 12 36 26  7 27 21  3 24  7  2 17 24 21 32  8  2
 11 19  6 29  6 29 18 10 25 35 10 18 15  5  9 22 34 29  2 16  8 18  8 38
 39 35 16 30 30 11 37 36 35 20 33  6  1 16 31 32  5 30  1 39 35 39  2 19
  5  8 11  4 14 27 22 24]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过和之前章节类似的代码，我们可以将预测值和实际值进行对比：</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line"></span><br><span class="line">a.head()  <span class="comment"># 查看表格前5行</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9</td>
      <td>9</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21</td>
      <td>21</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>40</td>
      <td>40</td>
    </tr>
    <tr>
      <th>4</th>
      <td>26</td>
      <td>26</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看预测准确度 - 方法1</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.9125
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看预测准确度 - 方法2</span></span><br><span class="line">score = knn.score(X_test_pca, y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.9125
</code></pre>
<p>3.模型对比（数据降维与不降维）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn = KNeighborsClassifier()  <span class="comment"># 建立KNN模型  </span></span><br><span class="line">knn.fit(X_train, y_train)  <span class="comment"># 不使用数据降维，直接训练</span></span><br><span class="line">y_pred = knn.predict(X_test)  <span class="comment"># 不使用数据降维，直接测试</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.9125
</code></pre>
<p>此时获得的准确度评分score为0.91，可以看到使用数据降维对提高模型预测效果还是有一些效果的，这里的数据量并不大，当数据量更大的时候，利用PCA主成分分析进行数据降维则会发挥更大的作用。</p>
<p>其实看出，在数据量不大的时候，PCA效果不明显</p>
<h2 id="12-3-人脸识别外部接口调用（学习了解）"><a href="#12-3-人脸识别外部接口调用（学习了解）" class="headerlink" title="12.3 人脸识别外部接口调用（学习了解）"></a>12.3 人脸识别外部接口调用（学习了解）</h2><h3 id="12-3-1-baidu-aip安装"><a href="#12-3-1-baidu-aip安装" class="headerlink" title="12.3.1 baidu-aip安装"></a>12.3.1 baidu-aip安装</h3><h3 id="12-3-2-调用接口进行人脸识别和打分"><a href="#12-3-2-调用接口进行人脸识别和打分" class="headerlink" title="12.3.2 调用接口进行人脸识别和打分"></a>12.3.2 调用接口进行人脸识别和打分</h3><p>进入网址<a target="_blank" rel="noopener" href="https://ai.baidu.com/tech/face">https://ai.baidu.com/tech/face</a> 点击立即使用，点击创建应用，选择响应的选项，点击立即创建，在应用列表中可以看到如下：<br><img src="%E4%B8%8B%E8%BD%BD%20(38).png" alt="下载 (38)"></p>
<p>这些数据在调用接口时会用到</p>
<p>AppID：29907061</p>
<p>API Key：0ascf98uSFUgGHBcbG1GEr79</p>
<p>Secret Key：C6S8SFnC6zsxwqOZsgd0gnx7ssWZ1xBz</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> aip <span class="keyword">import</span> AipFace</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面3行内容为自己的APP_ID,API_KEY,SECRET_KEY</span></span><br><span class="line">APP_ID = <span class="string">&#x27;29907061&#x27;</span></span><br><span class="line">API_KEY = <span class="string">&#x27;0ascf98uSFUgGHBcbG1GEr79&#x27;</span></span><br><span class="line">SECRET_KEY = <span class="string">&#x27;C6S8SFnC6zsxwqOZsgd0gnx7ssWZ1xBz&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把上面输入的账号信息传入接口</span></span><br><span class="line">aipFace = AipFace(APP_ID, API_KEY, SECRET_KEY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面一行内容为需要识别的人脸图片的地址，其他地方就不用改了</span></span><br><span class="line">filePath = <span class="string">r&#x27;王宇涵.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义打开文件的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_file_content</span>(<span class="params">filePath</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        content = base64.b64encode(fp.read())</span><br><span class="line">        <span class="keyword">return</span> content.decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">imageType = <span class="string">&quot;BASE64&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择最后要展示的内容，这里展示age（年龄）；gender（性别）；beauty（颜值）</span></span><br><span class="line">options = &#123;&#125;</span><br><span class="line">options[<span class="string">&quot;face_field&quot;</span>] = <span class="string">&quot;age,gender,beauty&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用接口aipFace的detect()函数进行人脸识别，打印结果</span></span><br><span class="line">result = aipFace.detect(get_file_content(filePath), imageType, options)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印具体信息，本质就是列表索引和字典的键值对应</span></span><br><span class="line">age = result[<span class="string">&#x27;result&#x27;</span>][<span class="string">&#x27;face_list&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;age&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;年龄预测为：&#x27;</span> + <span class="built_in">str</span>(age))</span><br><span class="line">gender = result[<span class="string">&#x27;result&#x27;</span>][<span class="string">&#x27;face_list&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;gender&#x27;</span>][<span class="string">&#x27;type&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;性别预测为：&#x27;</span> + gender)</span><br><span class="line">beauty = result[<span class="string">&#x27;result&#x27;</span>][<span class="string">&#x27;face_list&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;beauty&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;颜值评分为：&#x27;</span> + <span class="built_in">str</span>(beauty))</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;error_code&#39;: 0, &#39;error_msg&#39;: &#39;SUCCESS&#39;, &#39;log_id&#39;: 1258602420, &#39;timestamp&#39;: 1674483658, &#39;cached&#39;: 0, &#39;result&#39;: &#123;&#39;face_num&#39;: 1, &#39;face_list&#39;: [&#123;&#39;face_token&#39;: &#39;7c0ca661081a579b2eb47a372a4be16a&#39;, &#39;location&#39;: &#123;&#39;left&#39;: 338.66, &#39;top&#39;: 291.83, &#39;width&#39;: 612, &#39;height&#39;: 637, &#39;rotation&#39;: -1&#125;, &#39;face_probability&#39;: 1, &#39;angle&#39;: &#123;&#39;yaw&#39;: -5.65, &#39;pitch&#39;: 5.96, &#39;roll&#39;: -2.13&#125;, &#39;age&#39;: 22, &#39;gender&#39;: &#123;&#39;type&#39;: &#39;male&#39;, &#39;probability&#39;: 1&#125;, &#39;beauty&#39;: 66.19&#125;]&#125;&#125;
年龄预测为：22
性别预测为：male
颜值评分为：66.19
</code></pre>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">数据科学</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/机器学习与深度学习//" class="article-tag-list-link color5">机器学习与深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/02/03/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part3/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-python大数据分析与机器学习商业案例实战-part2" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/02/02/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part2/">Python大数据分析与机器学习商业案例实战-part2</a>
    </h1>
  

        
        <a href="/2023/02/02/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part2/" class="archive-article-date">
  	<time datetime="2023-02-02T13:13:14.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-02-02</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="5-决策树模型"><a href="#5-决策树模型" class="headerlink" title="5. 决策树模型"></a>5. 决策树模型</h1><h2 id="5-1-决策树模型的基本原理"><a href="#5-1-决策树模型的基本原理" class="headerlink" title="5.1 决策树模型的基本原理"></a>5.1 决策树模型的基本原理</h2><h3 id="5-1-1-决策树模型简介"><a href="#5-1-1-决策树模型简介" class="headerlink" title="5.1.1 决策树模型简介"></a>5.1.1 决策树模型简介</h3><p>决策树是通过对一系列问题进行If&#x2F;else的推导，最终实现相关决策<br><img src="%E4%B8%8B%E8%BD%BD%20(46).png" alt="下载 (46)"><br>商业实战中是根据多个特征来预测离职概率，再根据相应的阈值来判断是否离职，例如，离职概率超过50%即认为员工会离职</p>
<p>几个概念：父节点、子节点、根节点、叶子节点</p>
<p>决策树主要是通过连续的逻辑判断得出最后的结论，其关键在于如何建立起这样一棵“树”，例如，根节点应该选哪一个特征，选择不同特征会收到不同的效果。其次，收入是一个连续变量。选择收入&lt;10000元或选择收入&lt;100000元作为节点其结果也是不同的</p>
<h3 id="5-1-2-决策树模型的建树依据"><a href="#5-1-2-决策树模型的建树依据" class="headerlink" title="5.1.2 决策树模型的建树依据"></a>5.1.2 决策树模型的建树依据</h3><p><img src="%E4%B8%8B%E8%BD%BD.png" alt="下载"></p>
<p>例如，一个全部都是离职员工的样本中只有一个类别——离职员工，其出现的频率是100%，所以该系统的基尼系数为$1-1^2&#x3D;0$，表示该系统没有混乱，或者说该系统“纯度很高”。而如果样本中一半是离职员工，另一半是未离职员工，那么两个类别个数为2，每个类别出现的频率都为50%，所以基尼系数为$1-(0.5^2+0.5^2)&#x3D;0.5$,其混乱程度很高</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(1).png" alt="下载 (1)"></p>
<p>例如，一个初始样本中有1000个员工，其中已知有400人离职，600人不离职。划分前该系统的基尼系数为$1-(0.4^2+0.6^2)&#x3D;0.48$,下面采用两种方式决定根节点：一是根据“满意度&lt;5”进行分类；二是根据“收入&lt;10000元”进行分类</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(2).png" alt="下载 (2)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(3).png" alt="下载 (3)"></p>
<p>可以看到，划分前的基尼系数为0.48，以满意度&lt;5为根节点进行划分后的基尼系数为0.3，而以收入&lt;10000元为根节点进行划分后的基尼系数为0.45.基尼系数越低表示系统的混乱程度越低，区分度越高，越适合用于分类预测，<strong>因此这里选择满意度&lt;5作为根节点</strong></p>
<p>根节点下面的节点也是也是用类似方法来选择。例如，对于变量“收入”来说，是选择收入&lt;10000元，还是选择收入&lt;100000元作为划分依据，同样通过计算这两种情况下划分后的<strong>基尼系数</strong>来进行判断。若还有其他变量，如“工龄”，“月工时”等，也是通过类似方法计算划分后的基尼系数，<strong>再根据基尼系数判断如何划分节点</strong>，从而搭建出一个较为完善的决策树模型。<strong>采用基尼系数进行运算的决策树也称为CART决策树</strong></p>
<p><strong>补充知识点：信息熵</strong></p>
<p>除了基尼系数，还有一种衡量系统伦乱程度的经典手段——信息熵<br><img src="%E4%B8%8B%E8%BD%BD%20(4).png" alt="下载 (4)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(5).png" alt="下载 (5)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(6).png" alt="下载 (6)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(7).png" alt="下载 (7)"></p>
<p>基尼系数涉及平方运算，而信息熵涉及对数函数运算，因此目前决策树模型默认使用基尼系数作为建树依据，运算速度较快</p>
<h3 id="5-1-3-决策树模型的代码实现"><a href="#5-1-3-决策树模型的代码实现" class="headerlink" title="5.1.3 决策树模型的代码实现"></a>5.1.3 决策树模型的代码实现</h3><p>决策树模型既能做分类分析（<strong>即预测分类变量值</strong>），又能做回归分析（<strong>即预测连续变量值</strong>），对应的模型分别为分类决策树模型（DecisionTreeClassifier）和回归决策树模型（DecisonTreeRegressor）</p>
<p><strong>1.分类决策树模型（DecisionTreeClassifier）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier <span class="comment"># 引入分类决策树模型DecisionTreeClassifier</span></span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]] <span class="comment"># X是特征变量，共有5个训练数据，每个数据有两个特征</span></span><br><span class="line">y = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>] <span class="comment"># 目标变量，共有两个类别——0和1</span></span><br><span class="line"></span><br><span class="line">model = DecisionTreeClassifier(random_state=<span class="number">0</span>) <span class="comment"># 引入模型并设置随机状态参数random_state为0，0无意义可以换成其他数字，种子参数，可使每次运行结果一致</span></span><br><span class="line">model.fit(X, y) <span class="comment"># 用fit()函数训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]])) <span class="comment"># 用predict()函数进行预测</span></span><br></pre></td></tr></table></figure>

<pre><code>[0]
</code></pre>
<p>如果要同时预测多个数据，则可以写成如下形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>], [<span class="number">7</span>, <span class="number">7</span>], [<span class="number">9</span>, <span class="number">9</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[0 0 1]
</code></pre>
<p><img src="%E4%B8%8B%E8%BD%BD%20(8).png" alt="下载 (8)"><br>决策树可视化,</p>
<p>X[0]表示数据的第一个特征</p>
<p>X[1]表示数据的第二个特征</p>
<p>gini表示该节点的基尼系数，以根节点为列,$1-(0.4^2+0.6^2)&#x3D;0.48$</p>
<p>sanples表示该节点的样本数</p>
<p>value表示各分类的样本数，例如，根节点中的[2,3]表示分类为0的样本数为2，分类为1的样本数为3</p>
<p>class表示该区块被划分为的类别，它是由value中样本数较多的类别决定的</p>
<p>以数据[5,5]为例，在根节点时，它满足X[1]（即第2个特征数值）小于等于7的条件，所以被划分到左边的子节点。在孩子节点又进行一次判断，判断X[0]是否小于等于2，因为X[0]为5，不满足该条件，所以划分到孩子节点的右边的子节点，而孩子节点的类别class为0，所以[5,5]在该决策树模型中被预测为类别0</p>
<p><strong>决策树可视化不重要，不用看了</strong></p>
<p><strong>2.回归决策树模型（DecisionTreeRegressor）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]] <span class="comment"># X是特征变量，共有两个特征</span></span><br><span class="line">y = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]  <span class="comment"># 目标变量，为一个连续变量</span></span><br><span class="line"></span><br><span class="line">model = DecisionTreeRegressor(max_depth=<span class="number">2</span>, random_state=<span class="number">0</span>) <span class="comment"># 设置决策树最大深度为2，随机状态参数为0</span></span><br><span class="line">model.fit(X, y) <span class="comment"># 训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">9</span>, <span class="number">9</span>]])) <span class="comment"># 对数据进行预测</span></span><br></pre></td></tr></table></figure>

<pre><code>[4.5]
</code></pre>
<p><img src="%E4%B8%8B%E8%BD%BD%20(9).png" alt="下载 (9)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(10).png" alt="下载 (10)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(11).png" alt="下载 (11)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(12).png" alt="下载 (12)"></p>
<h2 id="5-2-案例实战：员工离职预测模型搭建"><a href="#5-2-案例实战：员工离职预测模型搭建" class="headerlink" title="5.2 案例实战：员工离职预测模型搭建"></a>5.2 案例实战：员工离职预测模型搭建</h2><h3 id="5-2-1-模型搭建"><a href="#5-2-1-模型搭建" class="headerlink" title="5.2.1 模型搭建"></a>5.2.1 模型搭建</h3><p>通过已有的员工信息和离职表现来搭建相应的员工离职预测模型，可以预测之后的员工是否会离职</p>
<p><strong>1.数据读取与预处理</strong></p>
<p>首先读取员工信息以及其交易离职表现，即是否离职记录，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;员工离职预测模型.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>工资</th>
      <th>满意度</th>
      <th>考核得分</th>
      <th>工程数量</th>
      <th>月工时</th>
      <th>工龄</th>
      <th>离职</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>低</td>
      <td>3.8</td>
      <td>0.53</td>
      <td>2</td>
      <td>157</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>中</td>
      <td>8.0</td>
      <td>0.86</td>
      <td>5</td>
      <td>262</td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>中</td>
      <td>1.1</td>
      <td>0.88</td>
      <td>7</td>
      <td>272</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>低</td>
      <td>7.2</td>
      <td>0.87</td>
      <td>5</td>
      <td>223</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>低</td>
      <td>3.7</td>
      <td>0.52</td>
      <td>2</td>
      <td>159</td>
      <td>3</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>数据解读：离职列中，1表示离职，0表示未离职，该表格共有15000组历史数据，前3571组为离职数据，后11429组为非离职员工数据。我们的目的就是根据这些历史数据搭建决策树模型来预测之后员工的离职可能性</p>
<p>处理文本内容，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = df.replace(&#123;<span class="string">&#x27;工资&#x27;</span>: &#123;<span class="string">&#x27;低&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;中&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;高&#x27;</span>: <span class="number">2</span>&#125;&#125;)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>工资</th>
      <th>满意度</th>
      <th>考核得分</th>
      <th>工程数量</th>
      <th>月工时</th>
      <th>工龄</th>
      <th>离职</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3.8</td>
      <td>0.53</td>
      <td>2</td>
      <td>157</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>8.0</td>
      <td>0.86</td>
      <td>5</td>
      <td>262</td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1.1</td>
      <td>0.88</td>
      <td>7</td>
      <td>272</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>7.2</td>
      <td>0.87</td>
      <td>5</td>
      <td>223</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3.7</td>
      <td>0.52</td>
      <td>2</td>
      <td>159</td>
      <td>3</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>将表格中的“离职”列表作为目标变量，剩下的字段作为特征变量，通过一个员工的特征来判断其是否会离职</p>
<p><strong>2.提取特征变量和目标变量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df.drop(columns=<span class="string">&#x27;离职&#x27;</span>) </span><br><span class="line">y = df[<span class="string">&#x27;离职&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p><strong>3.划分训练集和测试集</strong></p>
<p>提取完特征变量和目标变量后，还需要将原来的15000组数据划分为训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment"># 导入train_test_split()函数</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br><span class="line"><span class="comment"># X和y就是之前提取的特征变量和目标变量，test_size是测试集数据所占的比例，这里设置为0.2</span></span><br><span class="line"><span class="comment"># 如果数据量多，也可以将其设置为0.1，即分配更少比例的数据用于测试，分配更多比例的</span></span><br><span class="line"><span class="comment"># 数据用于训练，设置random_state参数是为了每次划分数据的结果保持一致</span></span><br></pre></td></tr></table></figure>

<p><strong>4.模型训练及搭建</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">model = DecisionTreeClassifier(max_depth=<span class="number">3</span>, random_state=<span class="number">123</span>) <span class="comment"># 设置树的最大深度参数为3 </span></span><br><span class="line">model.fit(X_train, y_train) <span class="comment"># 训练函数</span></span><br></pre></td></tr></table></figure>




<pre><code>DecisionTreeClassifier(max_depth=3, random_state=123)
</code></pre>
<p>至此，模型训练完成</p>
<h3 id="5-2-2-模型预测及评估"><a href="#5-2-2-模型预测及评估" class="headerlink" title="5.2.2 模型预测及评估"></a>5.2.2 模型预测及评估</h3><p><strong>1.直接预测是否离职</strong></p>
<p>这里把测试集中的数据导入模型中进行预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred[<span class="number">0</span>:<span class="number">100</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0
 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0
 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0]
</code></pre>
<p>y_pred是一个numpy.ndarray类型的数组结构，y_test为Series类型的一维序列结构，都把它们转换成列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过构造DataFrame进行对比</span></span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果要查看整体的预测准确度，可以采用如下代码：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.9573333333333334
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 或者用模型自带的score函数查看预测准确度</span></span><br><span class="line">model.score(X_test, y_test)</span><br></pre></td></tr></table></figure>




<pre><code>0.9573333333333334
</code></pre>
<p><strong>2.预测不离职&amp;离职概率</strong></p>
<p>其实分类决策树模型本质预测的并不是准确的0或1的分类，而是预测其属于某一分类的概率，可以通过如下代码查看预测属于各个分类的概率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred_proba = model.predict_proba(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred_proba[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[[0.98526077 0.01473923]
 [0.98526077 0.01473923]
 [0.28600613 0.71399387]
 [0.98526077 0.01473923]
 [0.92283214 0.07716786]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = pd.DataFrame(y_pred_proba, columns=[<span class="string">&#x27;不离职概率&#x27;</span>, <span class="string">&#x27;离职概率&#x27;</span>]) </span><br><span class="line">b.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>不离职概率</th>
      <th>离职概率</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.985261</td>
      <td>0.014739</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.985261</td>
      <td>0.014739</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.286006</td>
      <td>0.713994</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.985261</td>
      <td>0.014739</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.922832</td>
      <td>0.077168</td>
    </tr>
  </tbody>
</table>
</div>



<p>如果想查看离职概率，即查看y_pred_proba的第二列，可以采用如下代码，这个是二维数组选取列的方法，其中逗号前的“:”表示所有行，逗号后面的数字1则表示第二列，如果把数字1改成数字0，则提取第一列不离职概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred_proba[:,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([0.01473923, 0.01473923, 0.71399387, ..., 0.01473923, 0.94594595,
       0.01473923])
</code></pre>
<p><strong>3.模型预测效果评估</strong></p>
<p>在Python实现上，通过4.3节讲过的代码就可以求出在不同阈值下的命中率（TPR）以及假警报率（FPR）的值，从而可以绘制ROC曲线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>通过4.3节相关代码可以查看不同阈值下的假警报率和命中率，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;阈值&#x27;</span>] = <span class="built_in">list</span>(thres)</span><br><span class="line">a[<span class="string">&#x27;假警报率&#x27;</span>] = <span class="built_in">list</span>(fpr)</span><br><span class="line">a[<span class="string">&#x27;命中率&#x27;</span>] = <span class="built_in">list</span>(tpr)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>阈值</th>
      <th>假警报率</th>
      <th>命中率</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.247110</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.945946</td>
      <td>0.008232</td>
      <td>0.677746</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.713994</td>
      <td>0.038128</td>
      <td>0.942197</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.077168</td>
      <td>0.159879</td>
      <td>0.969653</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.059406</td>
      <td>0.171577</td>
      <td>0.972543</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.045763</td>
      <td>0.240035</td>
      <td>0.976879</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.014739</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>



<p>第二行表示只有当某员工被预测为离职的概率≥100%（因为概率不会超过100%，所以其实就是被预测为离职的概率等于100%），才判定其离职，此时命中率为24.7%，即所有的实际离职的员工中被预测为离职的员工占24.7%，在这种极端的阈值条件下，该命中率已经算是很高了。第三行表示只有当某员工被预测为离职的概率≥94.6%，才判定其会离职，此时命中率为67.8%，假警报率为0.82%，以此类推</p>
<p>已知了不同阈值下的假警报率和命中率，可通过matplotlib库可绘制ROC曲线，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># 当时运行代码时找不到指定文件，升级一下matplotlib包就好了，</span></span><br><span class="line">                                 <span class="comment"># 可能时是当初jupyternotebook版本太老，它自带的三个库版本也老</span></span><br><span class="line">plt.plot(fpr, tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_71_0.png" alt="output_71_0"></p>
<p>通过如下代码则可以快速求出模型的AUC值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">score = roc_auc_score(y_test, y_pred_proba[:,<span class="number">1</span>]) <span class="comment"># 传入测试集的目标变量y_test及预测的离职概率</span></span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.9736722483245008
</code></pre>
<p><strong>4.特征重要性评估</strong></p>
<p>搭建完模型后，有时还要知道各个特征变量的重要程度，即哪些特征变量在模型中发挥的作用更大，这个重要性程度称为特征重要性。在决策树模型中，一个特征变量对模型整体基尼系数下降的贡献越大，它的特征重要性就越大</p>
<p>通过如下代码可以查看决策树模型中各个特征变量的重要性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.feature_importances_</span><br></pre></td></tr></table></figure>




<pre><code>array([0.        , 0.59810862, 0.14007392, 0.10638659, 0.00456495,
       0.15086592])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过DataFrame进行展示，并根据重要性进行倒序排列</span></span><br><span class="line">features = X.columns  <span class="comment"># 获取特征名称</span></span><br><span class="line">importances = model.feature_importances_  <span class="comment"># 获取特征重要性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过二维表格形式显示</span></span><br><span class="line">importances_df = pd.DataFrame()</span><br><span class="line">importances_df[<span class="string">&#x27;特征名称&#x27;</span>] = features</span><br><span class="line">importances_df[<span class="string">&#x27;特征重要性&#x27;</span>] = importances</span><br><span class="line">importances_df.sort_values(<span class="string">&#x27;特征重要性&#x27;</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>特征名称</th>
      <th>特征重要性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>满意度</td>
      <td>0.598109</td>
    </tr>
    <tr>
      <th>5</th>
      <td>工龄</td>
      <td>0.150866</td>
    </tr>
    <tr>
      <th>2</th>
      <td>考核得分</td>
      <td>0.140074</td>
    </tr>
    <tr>
      <th>3</th>
      <td>工程数量</td>
      <td>0.106387</td>
    </tr>
    <tr>
      <th>4</th>
      <td>月工时</td>
      <td>0.004565</td>
    </tr>
    <tr>
      <th>0</th>
      <td>工资</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以看到，特征重要性最高的是“满意度”，而“工资”在模型中特征重要性为0，也就是说它没有发挥作用，这并不符合常理，之所以会有这个结果，在某种程度上是因为我们限制了决策树的最大深度为3层，所以“工资”没有发挥作用的机会，如果增大决策树的最大深度，那么它可能会发挥作用，这一点在5.3.2小节会进行验证。另一个重要的原因是本案例中“工资”不是具体的数值，而是“高”“中”“低”三个档次，这种划分过于宽泛，使得特征变量在决策树模型中发挥的作用较小，如果“工资”是具体的数值，如10000元，那么该特征变量应该会发挥更大的作用</p>
<h2 id="5-3-参数调优-K折交叉验证-amp-GridSearch网格搜索"><a href="#5-3-参数调优-K折交叉验证-amp-GridSearch网格搜索" class="headerlink" title="5.3 参数调优 - K折交叉验证 &amp; GridSearch网格搜索"></a>5.3 参数调优 - K折交叉验证 &amp; GridSearch网格搜索</h2><p>机器学习的各个模型其实都有一些内置的参数，如max_depth（树的最大深度），这种参数又称为超参数，除了这个还有一些参数：</p>
<p><strong>分类决策树模型DecisionTreeClassifier（）的常用超参数</strong>：</p>
<p><strong>criterion</strong>：特征选择标准，取值为’entropy’（信息熵）和’gini’（基尼系数），默认值为’gini’</p>
<p><strong>splitter</strong>：取值为’best’和’random’。’best’指在特征的所有划分点中找出最优的划分点，适合样本量不大的情况；random指随机地在部分划分点中寻找局部最优的划分点，适合样本量非常大的情况，默认值为best</p>
<p><strong>max_depth</strong>：决策树最大深度，取值为int型或None，默认值为None，一般数据或特征较少的时候可以不设置，如果数据或特征较多，可以设置最大深度进行限制</p>
<p><strong>min_samples_split</strong>：子节点往下分裂所需的最小样本数，默认值为2，如果子节点中的样本数小于该值则停止分裂</p>
<p><strong>min_samples_leaf</strong>：叶子节点的最小样本数，默认值为1.如果叶子节点中的样本数小于该值，该叶子节点会和兄弟节点一起被剪枝，即剔除该叶子节点和其兄弟节点，并停止分类</p>
<p><strong>min_weight_fraction_leaf</strong>：叶子节点最小的样本权重和，默认值为0，即不考虑权重问题，如果小于该值，该叶子节点会和兄弟节点一起被剪枝。如果较多样本有缺失值或者样本的分布类别偏差很大，则需要考虑样本权重的问题</p>
<p><strong>max_features</strong>：在划分节点时所考虑的特征值数量的最大值，默认值为None，可以传入int型或float型数据，如果传入的是float型数据，则表示百分比</p>
<p><strong>max_leaf_nodes</strong>：最大叶子节点数，默认值为None，可以传入int型数据</p>
<p><strong>class_weight</strong>：指定类别权重，默认值为None，可以取balanced，代表样本量少的类别所对应的样本权重更高，也可以传入字典来指定权重，该参数主要是为防止训练集中某些类别的样本过多，导致训练的决策树过于偏向这些类别。处理指定参数，还可以使用过采样和欠采样的方法处理样本类别不平衡的问题，在11章讲解</p>
<p><strong>random_state</strong>：当数据量较大或特征变量较多，可能在某个节点划分时，会遇到两个特征变量的信息增益或基尼系数下降值相同的情况，此时决策树模型会默认从中随机选择一个特征变量进行划分，这样可能会导致每次运行程序后生成的决策树不一致。设置random_state参数可以保证每次运行程序后各节点的分裂结果都是一致的，这在特征变量较多、树的深度较深时较为重要</p>
<p>大多数情况下，使用模型的默认参数也能获得较好的结果及预测准确度，然而如果想要获得更准确的结果，就需要对模型的超参数进行调优。例如max_depth取3还是取默认值None（即不限制最大深度，分裂到所有叶子节点的基尼系数都为0）是有讲究的，如果取值过小，可能会导致模型欠拟合，如果取值过大，则会容易过拟合，因此需要一个手段来合理地调节模型参数。</p>
<p>本节介绍调节模型参数的常用方法GridSearch网格搜索，以及常与其搭配使用的K折交叉验证</p>
<h3 id="5-3-1-K折交叉验证"><a href="#5-3-1-K折交叉验证" class="headerlink" title="5.3.1 K折交叉验证"></a>5.3.1 K折交叉验证</h3><p>在机器学习中，因为训练集和测试集的数据划分是随机的，所以有时候会<strong>重复</strong>地使用数据，以便更好地评估模型的有效性，并选出最好的模型，该做法称为<strong>交叉验证</strong>。具体而言就是对原始样本数据进行切分，然后组合成为<strong>多组不同</strong>的训练集和测试集，用训练集训练模型，用测试集评估模型。某次的训练集可能是下次的测试集，故而称为交叉验证。</p>
<p>交叉验证的方法有三种：简单交叉验证，K折交叉验证和留一交叉验证。K折交叉验证应用较为广泛，它是指将数据集随机等分为K份，每一次选取K-1份作为训练集，用剩下的的1份作为测试集，得到K个模型后将者K个模型的平均测试效果作为最终的模型效果<br><img src="%E4%B8%8B%E8%BD%BD%20(13).png"><br>通常来说，如果<strong>训练集相对较小</strong>，则<strong>增大K值</strong>，这样每次迭代过程中将会有更多的数据用于模型训练，同时算法的时间延迟；如果<strong>训练集相对较大</strong>，则<strong>减小K值</strong>，这样可以降低模型在不同的数据块上进行重复拟合性能评估的计算成本，在平均性能的基础上获得模型的准确评估。</p>
<p>除了更精确地评估模型，交叉验证的另一个重要的作用就是利用更精确的评估结果对模型进行<strong>参数调优</strong>，它经常与GridSeatch网格搜索配合使用</p>
<p><strong>前情提要 - 5.2节的模型搭建代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.读取数据与简单预处理</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;员工离职预测模型.xlsx&#x27;</span>)</span><br><span class="line">df = df.replace(&#123;<span class="string">&#x27;工资&#x27;</span>: &#123;<span class="string">&#x27;低&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;中&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;高&#x27;</span>: <span class="number">2</span>&#125;&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.提取特征变量和目标变量</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;离职&#x27;</span>) </span><br><span class="line">y = df[<span class="string">&#x27;离职&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.划分训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.模型训练及搭建</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">model = DecisionTreeClassifier(max_depth=<span class="number">3</span>, random_state=<span class="number">123</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>DecisionTreeClassifier(max_depth=3, random_state=123)
</code></pre>
<p>通过以下代码可以实现K折交叉验证，并获得每次验证的得分结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score  <span class="comment"># 引入交叉验证函数</span></span><br><span class="line">acc = cross_val_score(model, X, y, cv=<span class="number">5</span>) <span class="comment"># 传入参数为模型名称model，特征变量数据X，目标变量数据y，交叉验证的次数cv（即K值）</span></span><br><span class="line">                                         <span class="comment"># 每次随机取4/5的数据用于训练，1/5的数据用于测试，默认交叉验证3次</span></span><br><span class="line">                                         <span class="comment"># 这里没有设置scoring参数，表示默认值accuracy（精确度）作为评估标准</span></span><br><span class="line">acc  <span class="comment"># 打印acc来查看5次交叉验证得到的打分，结果如下</span></span><br></pre></td></tr></table></figure>




<pre><code>array([0.96666667, 0.96066667, 0.959     , 0.96233333, 0.91366667])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">acc.mean()</span><br></pre></td></tr></table></figure>




<pre><code>0.9524666666666667
</code></pre>
<p>以ROC曲线的AUC值作为评估标准，则可以设置scoring参数为roc_auc</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">acc = cross_val_score(model, X, y, scoring=<span class="string">&#x27;roc_auc&#x27;</span>, cv=<span class="number">5</span>)</span><br><span class="line">acc</span><br></pre></td></tr></table></figure>




<pre><code>array([0.97146884, 0.9674637 , 0.96641351, 0.97047305, 0.95030156])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">acc.mean()</span><br></pre></td></tr></table></figure>




<pre><code>0.9652241309284616
</code></pre>
<h3 id="5-3-2-GridSearch网格搜索"><a href="#5-3-2-GridSearch网格搜索" class="headerlink" title="5.3.2 GridSearch网格搜索"></a>5.3.2 GridSearch网格搜索</h3><p>GridSearch网格搜索时一种<strong>穷举搜索</strong>的参数调优手段：遍历所有的候选参数，循环建立模型并评估模型的有效性和准确性，选取<strong>表现最好的参数</strong>作为最终结果。以决策树最大深度参数max_depth为例，我们可以在[1,3,5,7,9]这些值中遍历，以准确度或ROC曲线的AUC值作为评估标准来搜索最合适的max_depth值，如果要同时调节多个模型参数，例如，模型有2个参数，第一个参数有4种可能，第2个参数有5种可能，所有的可能性可以表示成4x5的网格，那么遍历的过程就像是在网格里搜索</p>
<p><strong>1.单参数的参数调优</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV  <span class="comment"># 网格搜索合适的超参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定参数k的范围</span></span><br><span class="line">parameters = &#123;<span class="string">&#x27;max_depth&#x27;</span>: [<span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>]&#125;</span><br><span class="line"><span class="comment"># 构建决策树分类器</span></span><br><span class="line">model = DecisionTreeClassifier()  <span class="comment"># 这里因为要进行参数调优，所以不需要传入固定的参数了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网格搜索</span></span><br><span class="line">grid_search = GridSearchCV(model, parameters, scoring=<span class="string">&#x27;roc_auc&#x27;</span>, cv=<span class="number">5</span>)   <span class="comment"># cv=5表示交叉验证5次，默认值为3；scoring=&#x27;roc_auc&#x27;表示通过ROC曲线的AUC值来进行评分，默认通过准确度评分</span></span><br><span class="line">grid_search.fit(X_train, y_train) <span class="comment"># 传入测试数据并开始进行参数调优</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出参数的最优值</span></span><br><span class="line">grid_search.best_params_</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;max_depth&#39;: 7&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过取消如下代码的注释可以查看GridSearchCV函数的官方介绍</span></span><br><span class="line">GridSearchCV?</span><br></pre></td></tr></table></figure>

<p>因为max_depth参数设置了5个候选值，又设置了5折交叉验证，所以对于每个候选值，模型都会运行5遍（公运行5x5&#x3D;25遍），每个候选值都通过5折交叉验证获得一个平均分，根据平均分进行排序，得到决策树的最大深度设置为7时最优</p>
<p><strong>补充知识点：批量生成调参所需数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">parameters = &#123;<span class="string">&#x27;max_depth&#x27;</span>: np.arange(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>)&#125;</span><br></pre></td></tr></table></figure>

<p><strong>参数调优的效果检验</strong></p>
<p>根据max_depth&#x3D;7来重新搭建模型，并进行检测查看新模型的预测准确度及ROC曲线的AUC值来验证参数调优后是否提高了模型的有效性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据max_depth=7来重新搭建模型</span></span><br><span class="line">model = DecisionTreeClassifier(max_depth=<span class="number">7</span>)  <span class="comment"># 这个max_depth参数是可以调节的，之后讲</span></span><br><span class="line">model.fit(X_train, y_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看整体预测准确度</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.982
</code></pre>
<p>与原模型在测试集上的预测准确度0.957相比，参数调优之后预测准确度有所上升。其实预测准确度也可能下降，因为参数调优时是以ROC曲线的AUC值作为评估标准的，而非预测准确度</p>
<p><strong>查看新模型的ROC曲线和AUC值</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看新的AUC值</span></span><br><span class="line"><span class="comment"># 预测不违约&amp;违约概率</span></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)</span><br><span class="line">y_pred_proba[:,<span class="number">1</span>]  <span class="comment"># 如果想单纯的查看违约概率，即查看y_pred_proba的第二列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制ROC曲线，计算AUC值</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制ROC曲线</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(fpr, tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_107_0.png" alt="output_107_0"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算AUC值，得到0.987，与参数调优前的AUC值0.973相比，模型有效性的确有所提高</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">score = roc_auc_score(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.9878194468097895
</code></pre>
<p>总结：原来获得的AUC值为0.9736，现在获得的AUC值为0.9877，的确提高了模型的预测水平</p>
<p><strong>补充：决策树深度增加时特征重要性的改变</strong></p>
<p>参数调优后，决策树模型的深度从3增加到7，树的子节点和叶子节点都会有所增加，特征重要性也可能发生变化，通过如下代码可以查看参数调优后新模型中各个特征变量的特征重要性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看此时的变量重要性</span></span><br><span class="line">model.feature_importances_</span><br></pre></td></tr></table></figure>




<pre><code>array([0.00059222, 0.52718305, 0.13201648, 0.1116004 , 0.07731135,
       0.1512965 ])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一一对应</span></span><br><span class="line">features = X.columns</span><br><span class="line">importances = model.feature_importances_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过表格形式显示</span></span><br><span class="line">importances_df = pd.DataFrame()  <span class="comment"># 创建空二维表格，为之后准备</span></span><br><span class="line">importances_df[<span class="string">&#x27;特征名称&#x27;</span>] = features</span><br><span class="line">importances_df[<span class="string">&#x27;特征重要性&#x27;</span>] = importances</span><br><span class="line"></span><br><span class="line">importances_df.sort_values(<span class="string">&#x27;特征重要性&#x27;</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>特征名称</th>
      <th>特征重要性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>满意度</td>
      <td>0.527183</td>
    </tr>
    <tr>
      <th>5</th>
      <td>工龄</td>
      <td>0.151297</td>
    </tr>
    <tr>
      <th>2</th>
      <td>考核得分</td>
      <td>0.132016</td>
    </tr>
    <tr>
      <th>3</th>
      <td>工程数量</td>
      <td>0.111600</td>
    </tr>
    <tr>
      <th>4</th>
      <td>月工时</td>
      <td>0.077311</td>
    </tr>
    <tr>
      <th>0</th>
      <td>工资</td>
      <td>0.000592</td>
    </tr>
  </tbody>
</table>
</div>



<p>对比原模型的特征变量的特征重要性，可以发现“工资”的特征重要性从0上升到0.000592，这是因为决策树增加了节点使用“工资”作为分裂的依据，也验证了5.2.2小节末尾的猜测：如果增大决策树的最大深度，那么“工资”可能会发挥作用</p>
<p><strong>2.多参数调优</strong></p>
<p>除了可以进行单参数调优，网格搜索还可以进行多参数调优。下面选择DecisionTreeClassifier()函数的3个超参数max_depth（最大深度），criterion（特征选择标准）和min_samples_split（子节点往下分裂所需的最小样本数），使用GridSearchCV（）函数进行多参数调优</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定决策树分类器中各个参数的范围</span></span><br><span class="line">parameters = &#123;<span class="string">&#x27;max_depth&#x27;</span>: [<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>, <span class="number">13</span>], <span class="string">&#x27;criterion&#x27;</span>:[<span class="string">&#x27;gini&#x27;</span>, <span class="string">&#x27;entropy&#x27;</span>], <span class="string">&#x27;min_samples_split&#x27;</span>:[<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">15</span>]&#125;</span><br><span class="line"><span class="comment"># 构建决策树分类器</span></span><br><span class="line">model = DecisionTreeClassifier()  <span class="comment"># 这里因为要进行参数调优，所以不需要传入固定的参数了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网格搜索</span></span><br><span class="line">grid_search = GridSearchCV(model, parameters, scoring=<span class="string">&#x27;roc_auc&#x27;</span>, cv=<span class="number">5</span>)</span><br><span class="line">grid_search.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得参数的最优值</span></span><br><span class="line">grid_search.best_params_</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 11, &#39;min_samples_split&#39;: 13&#125;
</code></pre>
<p>将criterion设置为entropy信息熵，max_depth设置为11，min_samples_split设置为13时，模型最优，将这些参数的最优值引入模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据多参数调优的结果来重新搭建模型</span></span><br><span class="line">model = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>, max_depth=<span class="number">11</span>, min_samples_split=<span class="number">13</span>)</span><br><span class="line">model.fit(X_train, y_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看整体预测准确度</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.9823333333333333
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看新的AUC值</span></span><br><span class="line"><span class="comment"># 预测不违约&amp;违约概率</span></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)</span><br><span class="line">y_pred_proba[:,<span class="number">1</span>]  <span class="comment"># 如果想单纯的查看违约概率，即查看y_pred_proba的第二列</span></span><br><span class="line"></span><br><span class="line">score = roc_auc_score(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.9880075960970136
</code></pre>
<p>总结：这里多参数调优后发现，模型效果的确有所优化</p>
<p><strong>注意点1：多参数调优和分别单参数调优的区别</strong></p>
<p>多参数调优和单参数分别调优是有区别的，比如有的读者为了省事，对上面的3个参数进行3次单独的单参数调优，然后将结果汇总，这样的做法其实是不严谨的。因为在进行单参数调优的时候，是默认其他参数取默认值的，那么该参数和其他参数都不取默认值的情况就没有考虑进来，也即忽略了多个参数对模型的组合影响。以上面的代码示例来说，使用多参数调优时，它是5<em>2</em>6&#x3D;60种组合可能，而如果是进行3次单参数调优，则只是5+2+6&#x3D;13种组合可能。<br>因此，如果只需要调节一个参数，那么可以使用单参数调优，如果需要调节多个参数，则推荐使用多参数调优。</p>
<p><strong>注意点2：参数取值是给定范围的边界</strong></p>
<p>另外一点需要需要注意的是，<strong>如果使用GridSearchCV()方法所得到的参数取值是给定范围的边界，那么有可能存在范围以外的取值使得模型效果更好</strong>，因此需要我们<strong>额外增加范围，继续调参</strong>。举例来说，倘若上述代码中获得的最佳max_depth值为设定的最大值13，那么实际真正合适的max_depth可能更大，此时便需要将搜索网格重新调整，如将max_depth的搜索范围变成[9, 11, 13, 15, 17]，再重新参数调优。</p>
<p><strong>补充：决策树的前剪枝和后剪枝</strong></p>
<p>决策树剪枝的目的是防止构建的决策树出现<strong>过拟合</strong>。决策树剪枝分为前剪枝和后剪枝</p>
<p><strong>前剪枝</strong>：从上往下剪枝，通常利用超参数进行剪枝。例如，通过限制树的最大深度便能减去该最大深度下面的节点</p>
<p><strong>后剪枝</strong>：从下往上剪枝，大多是根据业务需求剪枝，例如，在违约预测模型中，认为违约概率为45%和50%的两个叶子节点都是高危人群，那么就把这两个叶子节点合并成一个节点</p>
<p>在商业实战中，前剪枝应用的更广泛，参数调优其实也起到了一定的前剪枝作用</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(14).png" alt="下载 (14)"></p>
<h1 id="6-朴素贝叶斯模型"><a href="#6-朴素贝叶斯模型" class="headerlink" title="6 朴素贝叶斯模型"></a>6 朴素贝叶斯模型</h1><h2 id="6-1-朴素贝叶斯模型的算法原理"><a href="#6-1-朴素贝叶斯模型的算法原理" class="headerlink" title="6.1 朴素贝叶斯模型的算法原理"></a>6.1 朴素贝叶斯模型的算法原理</h2><p><img src="%E4%B8%8B%E8%BD%BD%20(15).png" alt="下载 (15)"></p>
<h3 id="6-1-1-一维特征变量下的贝叶斯模型"><a href="#6-1-1-一维特征变量下的贝叶斯模型" class="headerlink" title="6.1.1 一维特征变量下的贝叶斯模型"></a>6.1.1 一维特征变量下的贝叶斯模型</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(16).png" alt="下载 (16)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(17).png" alt="下载 (17)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(18).png" alt="下载 (18)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(19).png" alt="下载 (19)"></p>
<h3 id="6-1-2-二维特征向量下单贝叶斯模型"><a href="#6-1-2-二维特征向量下单贝叶斯模型" class="headerlink" title="6.1.2 二维特征向量下单贝叶斯模型"></a>6.1.2 二维特征向量下单贝叶斯模型</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(20).png" alt="下载 (20)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(21).png" alt="下载 (21)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(22).png" alt="下载 (22)"></p>
<p>（1）独立性假设</p>
<p>在计算该概率之前，我们首先引入朴素贝叶斯模型的独立性假设：朴素贝叶斯模型中各特征之间相互独立，即，因此上式可以写作<br><img src="%E4%B8%8B%E8%BD%BD%20(23).png" alt="下载 (23)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(24).png" alt="下载 (24)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(25).png" alt="下载 (25)"></p>
<h3 id="6-1-3-n维特征变量下的贝叶斯模型"><a href="#6-1-3-n维特征变量下的贝叶斯模型" class="headerlink" title="6.1.3 n维特征变量下的贝叶斯模型"></a>6.1.3 n维特征变量下的贝叶斯模型</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(26).png" alt="下载 (26)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(27).png" alt="下载 (27)"></p>
<h3 id="6-1-4-朴素贝叶斯模型简单代码演示"><a href="#6-1-4-朴素贝叶斯模型简单代码演示" class="headerlink" title="6.1.4 朴素贝叶斯模型简单代码演示"></a>6.1.4 朴素贝叶斯模型简单代码演示</h3><p>通过如下代码引入朴素贝叶斯模型（这里用的是高斯贝叶斯分类器）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]] <span class="comment"># 特征变量，共有两个特征</span></span><br><span class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>] <span class="comment"># 目标变量，共有两个类别0，1</span></span><br><span class="line"></span><br><span class="line">model = GaussianNB()</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[0]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GaussianNB?</span><br></pre></td></tr></table></figure>

<h2 id="6-2-案例实战：肿瘤预测模型"><a href="#6-2-案例实战：肿瘤预测模型" class="headerlink" title="6.2 案例实战：肿瘤预测模型"></a>6.2 案例实战：肿瘤预测模型</h2><h3 id="6-2-1-案例背景"><a href="#6-2-1-案例背景" class="headerlink" title="6.2.1 案例背景"></a>6.2.1 案例背景</h3><h3 id="6-2-2-数据读取与划分"><a href="#6-2-2-数据读取与划分" class="headerlink" title="6.2.2 数据读取与划分"></a>6.2.2 数据读取与划分</h3><p>1.数据读取</p>
<p>首先通过如下代码导入某医院乳腺肿瘤患者的6个特征维度及肿瘤性质的数据。共569个患者，其中良性肿瘤358例，恶性肿瘤211例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;肿瘤数据.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>最大周长</th>
      <th>最大凹陷度</th>
      <th>平均凹陷度</th>
      <th>最大面积</th>
      <th>最大半径</th>
      <th>平均灰度值</th>
      <th>肿瘤性质</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>184.60</td>
      <td>0.2654</td>
      <td>0.14710</td>
      <td>2019.0</td>
      <td>25.38</td>
      <td>17.33</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>158.80</td>
      <td>0.1860</td>
      <td>0.07017</td>
      <td>1956.0</td>
      <td>24.99</td>
      <td>23.41</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>152.50</td>
      <td>0.2430</td>
      <td>0.12790</td>
      <td>1709.0</td>
      <td>23.57</td>
      <td>25.53</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>98.87</td>
      <td>0.2575</td>
      <td>0.10520</td>
      <td>567.7</td>
      <td>14.91</td>
      <td>26.50</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>152.20</td>
      <td>0.1625</td>
      <td>0.10430</td>
      <td>1575.0</td>
      <td>22.54</td>
      <td>16.67</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>0代表肿瘤为恶性，1代表肿瘤为良性</p>
<p>2.划分特征变量和目标变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df.drop(columns=<span class="string">&#x27;肿瘤性质&#x27;</span>) </span><br><span class="line">y = df[<span class="string">&#x27;肿瘤性质&#x27;</span>]   </span><br></pre></td></tr></table></figure>

<h3 id="6-2-3-模型的搭建与使用"><a href="#6-2-3-模型的搭建与使用" class="headerlink" title="6.2.3 模型的搭建与使用"></a>6.2.3 模型的搭建与使用</h3><p>1.划分训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>2.模型搭建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB <span class="comment"># 高斯朴素贝叶斯适用于任何连续数值型的数据集</span></span><br><span class="line">nb_clf = GaussianNB()  <span class="comment"># 高斯朴素贝叶斯模型</span></span><br><span class="line">nb_clf.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>




<pre><code>GaussianNB()
</code></pre>
<p>3.模型预测与评估</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred = nb_clf.predict(X_test)</span><br><span class="line">y_pred[:<span class="number">100</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
       1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
       0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1], dtype=int64)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a =pd.DataFrame() <span class="comment"># 创建一个空DataFrame</span></span><br><span class="line">a[<span class="string">&#x27;预测试&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测试</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以看到前5项的预测准确率为80%，通过如下代码可以查看所有测试集数据的预测精准度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line">score</span><br></pre></td></tr></table></figure>




<pre><code>0.9473684210526315
</code></pre>
<p>朴素贝叶斯模型属于分类模型，所以也可以利用ROC曲线来评估其预测效果</p>
<p>朴素贝叶斯模型是一种非常典型的机器学习模型，它主要基于贝叶斯公式，在应用过程中会把数据集中的特征看成是相互独立的，而不需要考虑特征时间的关联关系，因此运算速度比较快。相比于其他经典的机器学习模型，朴素贝叶斯模型的泛化能力较弱，不过当样本及特征的数量增加时，其预测效果也是很不错的</p>
<h1 id="7-K近邻算法"><a href="#7-K近邻算法" class="headerlink" title="7 K近邻算法"></a>7 K近邻算法</h1><h2 id="7-1-K近邻算法的原理和代码实现"><a href="#7-1-K近邻算法的原理和代码实现" class="headerlink" title="7.1 K近邻算法的原理和代码实现"></a>7.1 K近邻算法的原理和代码实现</h2><p>又称为KNN算法</p>
<h3 id="7-1-1-K近邻算法的基本原理"><a href="#7-1-1-K近邻算法的基本原理" class="headerlink" title="7.1.1 K近邻算法的基本原理"></a>7.1.1 K近邻算法的基本原理</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(28).png" alt="下载 (28)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(29).png" alt="下载 (29)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(30).png" alt="下载 (30)"></p>
<h3 id="7-1-2-K近邻算法的计算步骤"><a href="#7-1-2-K近邻算法的计算步骤" class="headerlink" title="7.1.2 K近邻算法的计算步骤"></a>7.1.2 K近邻算法的计算步骤</h3><p>1.样本数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;葡萄酒.xlsx&#x27;</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>原始样本</th>
      <th>酒精含量(%)</th>
      <th>苹果酸含量(%)</th>
      <th>分类</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>样本1</td>
      <td>5</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>样本2</td>
      <td>6</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>样本3</td>
      <td>4</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>样本4</td>
      <td>8</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>样本5</td>
      <td>10</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>分类0表示葡萄酒A，分类1为葡萄酒B</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(31).png" alt="下载 (31)"></p>
<p>2.计算距离</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(32).png" alt="下载 (32)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(33).png" alt="下载 (33)"></p>
<p>3.根据K值判定类别</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(34).png" alt="下载 (34)"></p>
<p><strong>补充：数据标准化</strong></p>
<p>本小节使用的演示数据，不同特征变量的量纲级别相差不大，如果把“酒精含量”数据都放大为原来的10倍，“苹果酸含量”数据保持不变，那么两者的量纲级别就相差较大了，此时如果使用K近邻算法来搭建模型，那么“酒精含量”在模型中的重要性将远远超过“苹果酸含量”的重要性，这样会丧失“苹果酸含量”这一特征变量的作用，而且误差也会很大</p>
<p>所以，如果不同特征变量的<strong>量纲级别相差较大</strong>且在建模时<strong>相互影响</strong>，我们通常会对数据进行预处理，该手段称为<strong>数据标准化或数据归一化</strong>。数据标准化的常见方法有<strong>min-max标准化</strong>（也称离差标准化）和<strong>Z-score标准化</strong>（也称均值归一化），将在11.3节详细讲解</p>
<h3 id="7-1-3-K近邻算法的代码实现"><a href="#7-1-3-K近邻算法的代码实现" class="headerlink" title="7.1.3 K近邻算法的代码实现"></a>7.1.3 K近邻算法的代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;葡萄酒.xlsx&#x27;</span>)</span><br><span class="line"><span class="comment"># 特征变量和目标变量的切分</span></span><br><span class="line">X_train = df[[<span class="string">&#x27;酒精含量(%)&#x27;</span>,<span class="string">&#x27;苹果酸含量(%)&#x27;</span>]]</span><br><span class="line">y_train = df[<span class="string">&#x27;分类&#x27;</span>]  </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier <span class="keyword">as</span> KNN</span><br><span class="line">knn = KNN(n_neighbors=<span class="number">3</span>)  <span class="comment"># 近邻参数K为3，默认值为5</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>KNeighborsClassifier(n_neighbors=3)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型预测：预测单个样本</span></span><br><span class="line">X_test = [[<span class="number">7</span>, <span class="number">1</span>]]  <span class="comment"># X_test为测试集特征变量，即7%的酒精含量及1%的苹果酸含量</span></span><br><span class="line">answer = knn.predict(X_test)  </span><br><span class="line"><span class="built_in">print</span>(answer)</span><br></pre></td></tr></table></figure>

<pre><code>[0]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型预测：预测多个样本</span></span><br><span class="line">X_test = [[<span class="number">7</span>, <span class="number">1</span>], [<span class="number">8</span>, <span class="number">3</span>]]  <span class="comment"># 这里能帮助理解为什么要写成二维数组的样式</span></span><br><span class="line">answer = knn.predict(X_test)  </span><br><span class="line"><span class="built_in">print</span>(answer)</span><br></pre></td></tr></table></figure>

<pre><code>[0 1]
</code></pre>
<p><strong>补充知识点：K近邻算法回归模型</strong></p>
<p>上述代码是用K近邻算法中的K近邻算法分类模型（KNeighborsClassifier）进行分类分析，K近邻算法还可以做回归分析，对应的模型为K近邻算法回归模型（KNeighborsRegressor）。K近邻算法分类模型将离待预测样本点最近的K个训练样本点中出现次数最多的分类作为待预测样本点的分类，K近邻算法回归模型则将离待预测样本点最近的K个训练样本点的<strong>平均值</strong>作为待预测样本点的分类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line">y = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">model = KNeighborsRegressor(n_neighbors=<span class="number">2</span>)  <span class="comment"># K为2</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[2.5]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KNeighborsRegressor?</span><br></pre></td></tr></table></figure>

<h2 id="7-2-案例实战-手写数字识别模型"><a href="#7-2-案例实战-手写数字识别模型" class="headerlink" title="7.2 案例实战 - 手写数字识别模型"></a>7.2 案例实战 - 手写数字识别模型</h2><h3 id="7-2-1-案例背景"><a href="#7-2-1-案例背景" class="headerlink" title="7.2.1 案例背景"></a>7.2.1 案例背景</h3><h3 id="7-2-2-手写数字识别的原理"><a href="#7-2-2-手写数字识别的原理" class="headerlink" title="7.2.2 手写数字识别的原理"></a>7.2.2 手写数字识别的原理</h3><p>1.图像二值化</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(35).png" alt="下载 (35)"></p>
<p>2.二维数组转换为一维数组</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(36).png" alt="下载 (36)"></p>
<p>3.距离计算</p>
<p>手写数字图片处理后形成的1x1024的二维数组可以看成一个行向量，两张图片对应的行向量间的欧式距离可以反映两张图片的相似度。因此，我们可以利用K近邻算法模型计算新样本与原始训练集中各个样本的欧式距离，取新样本的K个近邻点，并以大多数近邻点所在的分类作为新样本的分类</p>
<p>例如，有一个样本手写数字4，其对应的1x1024行向量为：0000…010…111…0011…000</p>
<p>将另一个手写数字x转换成如下1x1024的行向量，假设其中只有中间一个数字不同：0000…110…111…0011…000,</p>
<p>那么手写数字x与样本手写数字4的距离为$|AB|&#x3D;[(0-0)^2+(0-0)^2+…(0-1)^2+(1-1)^2+(0-0)^2+…+(0-0)^2]^(0.5)&#x3D;1$</p>
<h3 id="7-2-3-手写数字识别的代码实现"><a href="#7-2-3-手写数字识别的代码实现" class="headerlink" title="7.2.3 手写数字识别的代码实现"></a>7.2.3 手写数字识别的代码实现</h3><p><strong>1.数据读取</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;手写字体识别.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>对应数字</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>...</th>
      <th>1014</th>
      <th>1015</th>
      <th>1016</th>
      <th>1017</th>
      <th>1018</th>
      <th>1019</th>
      <th>1020</th>
      <th>1021</th>
      <th>1022</th>
      <th>1023</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 1025 columns</p>
</div>



<p><strong>2.提取特征变量和目标变量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.提取特征变量和目标变</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;对应数字&#x27;</span>) </span><br><span class="line">y = df[<span class="string">&#x27;对应数字&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p><strong>3.划分训练集和测试集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.划分训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br></pre></td></tr></table></figure>

<p><strong>4.模型搭建</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.模型搭建</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier <span class="keyword">as</span> KNN</span><br><span class="line">knn = KNN(n_neighbors=<span class="number">5</span>) </span><br><span class="line">knn.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>KNeighborsClassifier()
</code></pre>
<p><strong>5.模型预测与评估</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5.模型预测 - 预测数据结果</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred[<span class="number">0</span>:<span class="number">100</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[5 3 7 8 9 2 1 4 5 8 9 5 9 3 3 2 3 7 9 1 0 0 7 6 6 7 0 9 6 9 1 8 6 9 2 5 2
 4 5 8 3 6 9 4 9 2 7 3 4 9 5 6 7 3 3 8 3 1 5 3 6 7 5 0 3 7 1 4 9 1 5 1 2 6
 9 1 9 5 5 9 2 8 8 4 4 9 4 3 9 8 0 3 4 3 6 8 5 2 9 0]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7</td>
      <td>7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8</td>
      <td>8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>9</td>
      <td>9</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测准确度评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line">score</span><br></pre></td></tr></table></figure>




<pre><code>0.979328165374677
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型自带的score()函数也可以进行打分</span></span><br><span class="line">score = knn.score(X_test, y_test)</span><br><span class="line">score</span><br></pre></td></tr></table></figure>




<pre><code>0.979328165374677
</code></pre>
<h2 id="7-3-图像识别原理详解"><a href="#7-3-图像识别原理详解" class="headerlink" title="7.3 图像识别原理详解"></a>7.3 图像识别原理详解</h2><p>1.图像大小调整及显示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;数字4.png&#x27;</span>)</span><br><span class="line">img = img.resize((<span class="number">32</span>,<span class="number">32</span>))  <span class="comment"># 调整像素</span></span><br><span class="line">img.show()</span><br><span class="line">img</span><br></pre></td></tr></table></figure>




<p><img src="output_221_0.png" alt="output_221_0"></p>
<p>2.图片灰度处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img = img.convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">img</span><br></pre></td></tr></table></figure>




<p><img src="output_223_0.png" alt="output_223_0"></p>
<p>3.图片二值化处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二值化处理</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img_new = img.point(<span class="keyword">lambda</span> x: <span class="number">0</span> <span class="keyword">if</span> x &gt; <span class="number">128</span> <span class="keyword">else</span> <span class="number">1</span>)<span class="comment"># 将色彩数值大于128的像素点赋值为0，反之赋值为1.</span></span><br><span class="line"><span class="comment"># 图像在进行灰度处理后，每一个像素点由一个取值范围为0~255的数字表示，其中0代表黑色，255代表白色</span></span><br><span class="line"><span class="comment"># ，所以这里以128为阈值进行划分，即原来偏白色的区域赋值为0，原来偏黑色的区域赋值为1</span></span><br><span class="line">arr = np.array(img_new) <span class="comment"># 使用array()函数将已经转换成数字01的32x32像素的图片转换为32x32的二维数组，并赋值给arr</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印arr中的每一行</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(arr.shape[<span class="number">0</span>]):<span class="comment"># arr.shape()获取的是数组的行数和列数，arr.shape[0]对应行数，arr.shape[1]则对应列数</span></span><br><span class="line">    <span class="built_in">print</span>(arr[i])</span><br></pre></td></tr></table></figure>

<pre><code>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0]
[0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
</code></pre>
<p>4.将二维数组转换成一维数组</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr_new = arr.reshape(<span class="number">1</span>, -<span class="number">1</span>) <span class="comment"># 将二维数组转换成一行</span></span><br><span class="line">arr_new</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 0, 0, ..., 0, 0, 0]], dtype=uint8)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(arr_new.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(1, 1024)
</code></pre>
<p>此时我们可以把这个处理过的图片“数字4”传入到我们上面训练好的knn模型中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">answer = knn.predict(arr_new) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;图片中的数字为：&#x27;</span> + <span class="built_in">str</span>(answer[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>图片中的数字为：4
</code></pre>
<p>用粗一点的笔手写一个数字，然后拍照，传到如下代码的23行位置，亲自试试模型的识别效果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主要分为三步，第一步训练模型，第二步处理图片，第三步导入模型并预测</span></span><br><span class="line"><span class="comment"># 1.训练模型</span></span><br><span class="line"><span class="comment"># 1.1 读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;手写字体识别.xlsx&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.2 提取特征变量和目标变量</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;对应数字&#x27;</span>) </span><br><span class="line">y = df[<span class="string">&#x27;对应数字&#x27;</span>]   </span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.3 划分训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.4 训练模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier <span class="keyword">as</span> KNN</span><br><span class="line">knn = KNN(n_neighbors=<span class="number">5</span>) </span><br><span class="line">knn.fit(X_train, y_train)  <span class="comment"># 自此手写字体识别模型便搭建好了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.处理图片</span></span><br><span class="line"><span class="comment"># 2.1 图片读取 &amp; 大小调整 &amp; 灰度处理</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;测试图片.png&#x27;</span>)  <span class="comment"># 这里传入手写的图片，注意写对文件路径</span></span><br><span class="line">img = img.resize((<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">img = img.convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.2 图片二值化处理 &amp; 二维数据转一维数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img_new = img.point(<span class="keyword">lambda</span> x: <span class="number">0</span> <span class="keyword">if</span> x &gt; <span class="number">128</span> <span class="keyword">else</span> <span class="number">1</span>)</span><br><span class="line">arr = np.array(img_new)</span><br><span class="line">arr_new = arr.reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.预测手写数字</span></span><br><span class="line">answer = knn.predict(arr_new) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;图片中的数字为：&#x27;</span> + <span class="built_in">str</span>(answer[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>图片中的数字为：1
</code></pre>
<h1 id="8-随机森林模型——集成学习模型"><a href="#8-随机森林模型——集成学习模型" class="headerlink" title="8 随机森林模型——集成学习模型"></a>8 随机森林模型——集成学习模型</h1><h2 id="8-1-随机森林模型的原理和代码实现"><a href="#8-1-随机森林模型的原理和代码实现" class="headerlink" title="8.1 随机森林模型的原理和代码实现"></a>8.1 随机森林模型的原理和代码实现</h2><h3 id="8-1-1-集成模型简介"><a href="#8-1-1-集成模型简介" class="headerlink" title="8.1.1 集成模型简介"></a>8.1.1 集成模型简介</h3><p>集成学习模型使用一系列弱学习器（也称为基础模型或基模型）进行学习，并将各个弱学习器的结果进行整合，从而获得比单个学习器更好的学习效果。集成学习模型的常见算法有Bagging算法和Boosting算法。Bagging算法的典型机器学习模型为随机森立，而Boosting算法的典型机器学习模型为AdaBoost，GBDT，XGBoost和LightGBM模型</p>
<p><strong>1.Bagging算法</strong></p>
<p>Bagging算法的原理类似投票，每个弱学习都有一票，最终根据所有弱学习器的投票，按照“少数服从多数”的原则产生最终的预测结果<br><img src="%E4%B8%8B%E8%BD%BD%20(37).png" alt="下载 (37)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(38).png" alt="下载 (38)"></p>
<p><strong>2.Boosting算法</strong></p>
<p>Boosting算法的本质是将弱学习提升为强学习器，Bagging算法对待所有弱学习器一视同仁；而Boosting算法则会对弱学习器区别对待，通俗来将就是“培养精英”和“重视错误”</p>
<p>“培养精英”就是每一轮训练后对预测结果较准确的弱学习器给予较大的权重，对表现不好的则降低权重，这样在最终预测时，“优秀模型”的权重是最大的，相当于它可以投出多票，而“一般模型”只能投出一票或者不能投票</p>
<p>“重视错误”就是在每一轮训练后改变训练集的权重或概率分布，通常提高在前一轮被弱学习器预测错误的样例的权重，降低前一轮被弱学习器预测正确的样例的权重，来<strong>提高弱学习器对预测错误的数据的重视程度</strong>，，从而提高模型的整体预测效果</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(39).png" alt="下载 (39)"></p>
<h3 id="8-1-2-随机森林模型的基本原理"><a href="#8-1-2-随机森林模型的基本原理" class="headerlink" title="8.1.2 随机森林模型的基本原理"></a>8.1.2 随机森林模型的基本原理</h3><p>随机森林（Random Forest）是一种经典的Bagging模型，其弱学习器为决策树模型，如下图所示，随机森林模型会在原始数据集中随机抽样，构成n 个不同的样本数据集，然后根据这些数据集搭建n个不同的决策树模型，最后根据这些决策树模型的平均值（针对回归模型）或者投票情况（针对分类模型）来获取最终结果</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(40).png" alt="下载 (40)"></p>
<p>为了保证模型的泛化能力（通用能力），随机森林模型在建立每棵树时，往往会遵循“数据随机”和“特征随机”这两个基本原则</p>
<p><strong>1.数据随机</strong></p>
<p>从所有数据中有放回地随机抽取数据作为其中一个决策树模型的训练模型。例如，有1000个原始数据，有放回地抽取1000次，构成一组新的数据，用于训练某一个决策树模型</p>
<p><strong>2.特征随机</strong></p>
<p>如果每个样本的特征维度为M，指定一个常数$k&lt;M$,随机从M个特征中选取k个特征，默认k&#x3D;根号M</p>
<p>与单独的决策树模型相比，随机森林模型由于集成了多个决策树，其预测结果会更准确，且不容易造成过拟合现象，泛化能力较强</p>
<h3 id="8-1-3-随机森林模型的代码实现"><a href="#8-1-3-随机森林模型的代码实现" class="headerlink" title="8.1.3 随机森林模型的代码实现"></a>8.1.3 随机森林模型的代码实现</h3><p>和决策树模型一样，随机森林模型既可以做分类分析，也可以做回归分析。</p>
<p>分别对应的模型为随机森林分类模型（RandomForestClassifier）及随机森林回归模型（RandomForestRegressor）。随机森林分类模型的基模型是分类决策树模型（详见5.1.2节），随机森林回归模型的基模型则是回归决策树模型（详见5.1.3节）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机森林分类模型简单代码演示如下所示：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">model = RandomForestClassifier(n_estimators=<span class="number">10</span>, random_state=<span class="number">123</span>) <span class="comment">#设置弱学习器的数量为10，有10个决策树模型作为弱学习器</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[0]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机森林回归模型简单代码演示如下所示：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line">y = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">model = RandomForestRegressor(n_estimators=<span class="number">10</span>, random_state=<span class="number">123</span>)</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[2.8]
</code></pre>
<h2 id="8-2-案例实战：股票涨跌预测模型"><a href="#8-2-案例实战：股票涨跌预测模型" class="headerlink" title="8.2 案例实战：股票涨跌预测模型"></a>8.2 案例实战：股票涨跌预测模型</h2><h3 id="8-2-1-股票基本数据获取-最好直接看tushare官方数据接口文档"><a href="#8-2-1-股票基本数据获取-最好直接看tushare官方数据接口文档" class="headerlink" title="8.2.1 股票基本数据获取(最好直接看tushare官方数据接口文档)"></a>8.2.1 股票基本数据获取(最好直接看tushare官方数据接口文档)</h3><p>注意：截至目前，老接口已经不能用了</p>
<p><strong>1.获取日线行情数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line">ts.set_token(<span class="string">&#x27;6a623020c3e0d5e4b7143b3e50cc56ba8e4b20c3d2ea12909c7620be&#x27;</span>)</span><br><span class="line"><span class="comment"># 以上方法只需要在第一次或者token失效后调用，完成调取tushare数据凭证的设置，正常情况下不需要重复设置</span></span><br><span class="line">pro = ts.pro_api() <span class="comment"># 初始化pro接口</span></span><br><span class="line">df = pro.daily(ts_code=<span class="string">&#x27;000002.SZ&#x27;</span>, start_date=<span class="string">&#x27;20180101&#x27;</span>, end_date=<span class="string">&#x27;20190131&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts_code</th>
      <th>trade_date</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>pre_close</th>
      <th>change</th>
      <th>pct_chg</th>
      <th>vol</th>
      <th>amount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>000002.SZ</td>
      <td>20190131</td>
      <td>27.39</td>
      <td>28.15</td>
      <td>27.00</td>
      <td>27.75</td>
      <td>27.21</td>
      <td>0.54</td>
      <td>1.9846</td>
      <td>411857.60</td>
      <td>1138512.393</td>
    </tr>
    <tr>
      <th>1</th>
      <td>000002.SZ</td>
      <td>20190130</td>
      <td>26.70</td>
      <td>27.82</td>
      <td>26.63</td>
      <td>27.21</td>
      <td>26.88</td>
      <td>0.33</td>
      <td>1.2277</td>
      <td>592303.18</td>
      <td>1615186.856</td>
    </tr>
    <tr>
      <th>2</th>
      <td>000002.SZ</td>
      <td>20190129</td>
      <td>25.91</td>
      <td>26.88</td>
      <td>25.87</td>
      <td>26.88</td>
      <td>26.06</td>
      <td>0.82</td>
      <td>3.1466</td>
      <td>368071.63</td>
      <td>974279.357</td>
    </tr>
    <tr>
      <th>3</th>
      <td>000002.SZ</td>
      <td>20190128</td>
      <td>26.20</td>
      <td>26.62</td>
      <td>25.86</td>
      <td>26.06</td>
      <td>26.10</td>
      <td>-0.04</td>
      <td>-0.1533</td>
      <td>308906.56</td>
      <td>810288.818</td>
    </tr>
    <tr>
      <th>4</th>
      <td>000002.SZ</td>
      <td>20190125</td>
      <td>25.51</td>
      <td>26.35</td>
      <td>25.49</td>
      <td>26.10</td>
      <td>25.41</td>
      <td>0.69</td>
      <td>2.7155</td>
      <td>451756.17</td>
      <td>1176479.676</td>
    </tr>
  </tbody>
</table>
</div>



<p><strong>2.获取分钟级别的数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df = ts.pro_bar(ts_code=<span class="string">&#x27;600000.SH&#x27;</span>,</span><br><span class="line">                    freq=<span class="string">&#x27;1min&#x27;</span>,  <span class="comment"># 目前分钟频度包括1分、5、15、30、60分数据</span></span><br><span class="line">                    start_date=<span class="string">&#x27;2020-01-07 09:00:00&#x27;</span>, </span><br><span class="line">                    end_date=<span class="string">&#x27;2020-01-08 17:00:00&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts_code</th>
      <th>trade_time</th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>vol</th>
      <th>amount</th>
      <th>trade_date</th>
      <th>pre_close</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>600000.SH</td>
      <td>2020-01-08 15:00:00</td>
      <td>12.32</td>
      <td>12.32</td>
      <td>12.32</td>
      <td>12.32</td>
      <td>1485837.0</td>
      <td>18305512.0</td>
      <td>20200108</td>
      <td>12.32</td>
    </tr>
    <tr>
      <th>1</th>
      <td>600000.SH</td>
      <td>2020-01-08 14:59:00</td>
      <td>12.32</td>
      <td>12.32</td>
      <td>12.32</td>
      <td>12.32</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>20200108</td>
      <td>12.32</td>
    </tr>
    <tr>
      <th>2</th>
      <td>600000.SH</td>
      <td>2020-01-08 14:58:00</td>
      <td>12.32</td>
      <td>12.32</td>
      <td>12.32</td>
      <td>12.32</td>
      <td>22000.0</td>
      <td>271040.0</td>
      <td>20200108</td>
      <td>12.32</td>
    </tr>
    <tr>
      <th>3</th>
      <td>600000.SH</td>
      <td>2020-01-08 14:57:00</td>
      <td>12.32</td>
      <td>12.32</td>
      <td>12.33</td>
      <td>12.32</td>
      <td>266196.0</td>
      <td>3280959.0</td>
      <td>20200108</td>
      <td>12.32</td>
    </tr>
    <tr>
      <th>4</th>
      <td>600000.SH</td>
      <td>2020-01-08 14:56:00</td>
      <td>12.32</td>
      <td>12.32</td>
      <td>12.33</td>
      <td>12.32</td>
      <td>265829.0</td>
      <td>3275157.0</td>
      <td>20200108</td>
      <td>12.32</td>
    </tr>
  </tbody>
</table>
</div>



<p><strong>3.获取指数信息</strong></p>
<p>查看指数基本信息，指数日线行情需要积分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pro.index_basic(market=<span class="string">&#x27;SSE&#x27;</span>)  <span class="comment"># 上交所</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts_code</th>
      <th>name</th>
      <th>market</th>
      <th>publisher</th>
      <th>category</th>
      <th>base_date</th>
      <th>base_point</th>
      <th>list_date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>000001.SH</td>
      <td>上证指数</td>
      <td>SSE</td>
      <td>中证公司</td>
      <td>综合指数</td>
      <td>19901219</td>
      <td>100.00</td>
      <td>19910715</td>
    </tr>
    <tr>
      <th>1</th>
      <td>000002.SH</td>
      <td>上证A指</td>
      <td>SSE</td>
      <td>中证公司</td>
      <td>综合指数</td>
      <td>19901219</td>
      <td>100.00</td>
      <td>19920221</td>
    </tr>
    <tr>
      <th>2</th>
      <td>000003.SH</td>
      <td>上证B指</td>
      <td>SSE</td>
      <td>中证公司</td>
      <td>综合指数</td>
      <td>19920221</td>
      <td>100.00</td>
      <td>19920221</td>
    </tr>
    <tr>
      <th>3</th>
      <td>000004.SH</td>
      <td>上证工业类指数</td>
      <td>SSE</td>
      <td>中证公司</td>
      <td>综合指数</td>
      <td>19930430</td>
      <td>1358.78</td>
      <td>19930503</td>
    </tr>
    <tr>
      <th>4</th>
      <td>000005.SH</td>
      <td>上证商业类指数</td>
      <td>SSE</td>
      <td>中证公司</td>
      <td>综合指数</td>
      <td>19930430</td>
      <td>1358.78</td>
      <td>19930503</td>
    </tr>
  </tbody>
</table>
</div>



<p><strong>通用行情接口</strong></p>
<p>这个比较杂，需要不断熟悉不断学习</p>
<p>ts_code（str	&#x2F;必选）：证券代码，不支持多值输入，多值输入获取结果会有重复记录</p>
<p>api（str	&#x2F;不必选）：pro版api对象，如果初始化了set_token，此参数可以不需要</p>
<p>start_date（str&#x2F;不必选）：开始日期 (日线格式：YYYYMMDD，提取分钟数据请用2019-09-01 09:00:00这种格式)</p>
<p>end_date（str&#x2F;不必选）：结束日期 (日线格式：YYYYMMDD)</p>
<p>asset（str&#x2F;必选）：资产类别：E股票 I沪深指数 C数字货币 FT期货 FD基金 O期权 CB可转债（v1.2.39），默认E</p>
<p>adj（str&#x2F;不必选）：复权类型(只针对股票)：None未复权 qfq前复权 hfq后复权 , 默认None，目前只支持日线复权，同时复权机制是根据设定的end_date参数动态复权，采用分红再投模式</p>
<p>freq（str&#x2F;必选）：数据频度 ：支持分钟(min)&#x2F;日(D)&#x2F;周(W)&#x2F;月(M)K线，其中1min表示1分钟（类推1&#x2F;5&#x2F;15&#x2F;30&#x2F;60分钟） ，默认D。对于分钟数据有600积分用户可以试用（请求2次）</p>
<p>ma（list&#x2F;不必选）：均线，支持任意合理int数值。注：均线是动态计算，要设置一定时间范围才能获得相应的均线，比如5日均线，开始和结束日期参数跨度必须要超过5日。目前只支持单一个股票提取均线，即需要输入ts_code参数。e.g: ma_5表示5日均价，ma_v_5表示5日均量</p>
<p>factors（list&#x2F;不必选）：股票因子（asset&#x3D;’E’有效）支持 tor换手率 vr量比</p>
<p>adjfactor（str&#x2F;不必选）：复权因子，在复权数据时，如果此参数为True，返回的数据中则带复权因子，默认为False。 该功能从1.2.33版本开始生效</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#取上证指数行情数据,权限不够，积分不够</span></span><br><span class="line"></span><br><span class="line">df = ts.pro_bar(ts_code=<span class="string">&#x27;000001.SH&#x27;</span>, asset=<span class="string">&#x27;I&#x27;</span>, start_date=<span class="string">&#x27;20180101&#x27;</span>, end_date=<span class="string">&#x27;20181011&#x27;</span>)</span><br><span class="line">df,head()</span><br></pre></td></tr></table></figure>

<pre><code>抱歉，您没有访问该接口的权限，权限的具体详情访问：https://tushare.pro/document/1?doc_id=108。
抱歉，您没有访问该接口的权限，权限的具体详情访问：https://tushare.pro/document/1?doc_id=108。
抱歉，您没有访问该接口的权限，权限的具体详情访问：https://tushare.pro/document/1?doc_id=108。



---------------------------------------------------------------------------

OSError                                   Traceback (most recent call last)

&lt;ipython-input-30-20166becde73&gt; in &lt;module&gt;
      1 #取上证指数行情数据
      2 
----&gt; 3 df = ts.pro_bar(ts_code=&#39;000001.SH&#39;, asset=&#39;I&#39;, start_date=&#39;20180101&#39;, end_date=&#39;20181011&#39;)
      4 df,head()


E:\anaconda  111\lib\site-packages\tushare\pro\data_pro.py in pro_bar(ts_code, api, start_date, end_date, freq, asset, exchange, adj, ma, factors, adjfactor, offset, limit, contract_type, retry_count)
    194         else:
    195             return data
--&gt; 196     raise IOError(&#39;ERROR.&#39;)
    197 
    198 


OSError: ERROR.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#均线</span></span><br><span class="line"></span><br><span class="line">df = ts.pro_bar(ts_code=<span class="string">&#x27;000001.SZ&#x27;</span>, start_date=<span class="string">&#x27;20180101&#x27;</span>, end_date=<span class="string">&#x27;20181011&#x27;</span>, ma=[<span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>])</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts_code</th>
      <th>trade_date</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>pre_close</th>
      <th>change</th>
      <th>pct_chg</th>
      <th>vol</th>
      <th>amount</th>
      <th>ma5</th>
      <th>ma_v_5</th>
      <th>ma20</th>
      <th>ma_v_20</th>
      <th>ma50</th>
      <th>ma_v_50</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>000001.SZ</td>
      <td>20181011</td>
      <td>10.05</td>
      <td>10.16</td>
      <td>9.70</td>
      <td>9.86</td>
      <td>10.45</td>
      <td>-0.59</td>
      <td>-5.6459</td>
      <td>1995143.83</td>
      <td>1994186.611</td>
      <td>10.474</td>
      <td>1570205.872</td>
      <td>10.2365</td>
      <td>1.068715e+06</td>
      <td>9.7594</td>
      <td>984673.7130</td>
    </tr>
    <tr>
      <th>1</th>
      <td>000001.SZ</td>
      <td>20181010</td>
      <td>10.54</td>
      <td>10.66</td>
      <td>10.38</td>
      <td>10.45</td>
      <td>10.56</td>
      <td>-0.11</td>
      <td>-1.0417</td>
      <td>995200.08</td>
      <td>1045666.180</td>
      <td>10.650</td>
      <td>1347249.772</td>
      <td>10.2460</td>
      <td>1.027931e+06</td>
      <td>9.7498</td>
      <td>957053.2926</td>
    </tr>
    <tr>
      <th>2</th>
      <td>000001.SZ</td>
      <td>20181009</td>
      <td>10.46</td>
      <td>10.70</td>
      <td>10.39</td>
      <td>10.56</td>
      <td>10.45</td>
      <td>0.11</td>
      <td>1.0526</td>
      <td>1064084.26</td>
      <td>1117946.550</td>
      <td>10.702</td>
      <td>1446312.442</td>
      <td>10.2450</td>
      <td>1.042988e+06</td>
      <td>9.7288</td>
      <td>965667.1742</td>
    </tr>
    <tr>
      <th>3</th>
      <td>000001.SZ</td>
      <td>20181008</td>
      <td>10.70</td>
      <td>10.79</td>
      <td>10.45</td>
      <td>10.45</td>
      <td>11.05</td>
      <td>-0.60</td>
      <td>-5.4299</td>
      <td>1686358.52</td>
      <td>1793455.283</td>
      <td>10.700</td>
      <td>1428898.530</td>
      <td>10.2265</td>
      <td>1.025368e+06</td>
      <td>9.7060</td>
      <td>980019.2542</td>
    </tr>
    <tr>
      <th>4</th>
      <td>000001.SZ</td>
      <td>20180928</td>
      <td>10.78</td>
      <td>11.27</td>
      <td>10.78</td>
      <td>11.05</td>
      <td>10.74</td>
      <td>0.31</td>
      <td>2.8864</td>
      <td>2110242.67</td>
      <td>2331358.288</td>
      <td>10.744</td>
      <td>1416530.508</td>
      <td>10.2105</td>
      <td>1.006594e+06</td>
      <td>9.6792</td>
      <td>981377.6318</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#换手率tor，量比vr</span></span><br><span class="line"></span><br><span class="line">df = ts.pro_bar(ts_code=<span class="string">&#x27;000001.SZ&#x27;</span>, start_date=<span class="string">&#x27;20180101&#x27;</span>, end_date=<span class="string">&#x27;20181011&#x27;</span>, factors=[<span class="string">&#x27;tor&#x27;</span>, <span class="string">&#x27;vr&#x27;</span>])</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>

<pre><code>抱歉，您没有访问该接口的权限，权限的具体详情访问：https://tushare.pro/document/1?doc_id=108。
抱歉，您没有访问该接口的权限，权限的具体详情访问：https://tushare.pro/document/1?doc_id=108。
抱歉，您没有访问该接口的权限，权限的具体详情访问：https://tushare.pro/document/1?doc_id=108。



---------------------------------------------------------------------------

OSError                                   Traceback (most recent call last)

&lt;ipython-input-32-df812ea09fe6&gt; in &lt;module&gt;
      1 #换手率tor，量比vr
      2 
----&gt; 3 df = ts.pro_bar(ts_code=&#39;000001.SZ&#39;, start_date=&#39;20180101&#39;, end_date=&#39;20181011&#39;, factors=[&#39;tor&#39;, &#39;vr&#39;])
      4 df.head()


E:\anaconda  111\lib\site-packages\tushare\pro\data_pro.py in pro_bar(ts_code, api, start_date, end_date, freq, asset, exchange, adj, ma, factors, adjfactor, offset, limit, contract_type, retry_count)
    194         else:
    195             return data
--&gt; 196     raise IOError(&#39;ERROR.&#39;)
    197 
    198 


OSError: ERROR.
</code></pre>
<h3 id="8-2-2-股票衍生变量生成"><a href="#8-2-2-股票衍生变量生成" class="headerlink" title="8.2.2 股票衍生变量生成"></a>8.2.2 股票衍生变量生成</h3><p>学习如何利用股票的基本数据获取一些衍生变量数据，如股票技术分析中常用的均线指标5日均线价格MA5与10日均线价格MA10、对抗强弱指标RSI、动量指标MOM、指数移动平均值EMA，异同移动平均线MACD等——这些在pro中属于技术面因子，需要积分，在聚宽环境中很全很全，建议在聚宽中编写策略</p>
<p>1.获取股票基本数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pro.daily(ts_code=<span class="string">&#x27;000002.SZ&#x27;</span>, start_date=<span class="string">&#x27;20150101&#x27;</span>, end_date=<span class="string">&#x27;20191231&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts_code</th>
      <th>trade_date</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>pre_close</th>
      <th>change</th>
      <th>pct_chg</th>
      <th>vol</th>
      <th>amount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>000002.SZ</td>
      <td>20191231</td>
      <td>31.35</td>
      <td>32.45</td>
      <td>31.32</td>
      <td>32.18</td>
      <td>31.57</td>
      <td>0.61</td>
      <td>1.9322</td>
      <td>663497.98</td>
      <td>2122966.722</td>
    </tr>
    <tr>
      <th>1</th>
      <td>000002.SZ</td>
      <td>20191230</td>
      <td>31.35</td>
      <td>31.79</td>
      <td>31.02</td>
      <td>31.57</td>
      <td>31.00</td>
      <td>0.57</td>
      <td>1.8387</td>
      <td>915751.42</td>
      <td>2870247.850</td>
    </tr>
    <tr>
      <th>2</th>
      <td>000002.SZ</td>
      <td>20191227</td>
      <td>31.23</td>
      <td>31.32</td>
      <td>30.81</td>
      <td>31.00</td>
      <td>31.12</td>
      <td>-0.12</td>
      <td>-0.3856</td>
      <td>703096.48</td>
      <td>2185106.849</td>
    </tr>
    <tr>
      <th>3</th>
      <td>000002.SZ</td>
      <td>20191226</td>
      <td>30.50</td>
      <td>31.30</td>
      <td>30.50</td>
      <td>31.12</td>
      <td>30.29</td>
      <td>0.83</td>
      <td>2.7402</td>
      <td>888790.74</td>
      <td>2758745.302</td>
    </tr>
    <tr>
      <th>4</th>
      <td>000002.SZ</td>
      <td>20191225</td>
      <td>30.40</td>
      <td>30.63</td>
      <td>30.18</td>
      <td>30.29</td>
      <td>30.38</td>
      <td>-0.09</td>
      <td>-0.2962</td>
      <td>685037.32</td>
      <td>2082008.206</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用set_index()函数将data列设置为行索引</span></span><br><span class="line">df = df.set_index(<span class="string">&#x27;trade_date&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts_code</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>pre_close</th>
      <th>change</th>
      <th>pct_chg</th>
      <th>vol</th>
      <th>amount</th>
    </tr>
    <tr>
      <th>trade_date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>20191231</th>
      <td>000002.SZ</td>
      <td>31.35</td>
      <td>32.45</td>
      <td>31.32</td>
      <td>32.18</td>
      <td>31.57</td>
      <td>0.61</td>
      <td>1.9322</td>
      <td>663497.98</td>
      <td>2122966.722</td>
    </tr>
    <tr>
      <th>20191230</th>
      <td>000002.SZ</td>
      <td>31.35</td>
      <td>31.79</td>
      <td>31.02</td>
      <td>31.57</td>
      <td>31.00</td>
      <td>0.57</td>
      <td>1.8387</td>
      <td>915751.42</td>
      <td>2870247.850</td>
    </tr>
    <tr>
      <th>20191227</th>
      <td>000002.SZ</td>
      <td>31.23</td>
      <td>31.32</td>
      <td>30.81</td>
      <td>31.00</td>
      <td>31.12</td>
      <td>-0.12</td>
      <td>-0.3856</td>
      <td>703096.48</td>
      <td>2185106.849</td>
    </tr>
    <tr>
      <th>20191226</th>
      <td>000002.SZ</td>
      <td>30.50</td>
      <td>31.30</td>
      <td>30.50</td>
      <td>31.12</td>
      <td>30.29</td>
      <td>0.83</td>
      <td>2.7402</td>
      <td>888790.74</td>
      <td>2758745.302</td>
    </tr>
    <tr>
      <th>20191225</th>
      <td>000002.SZ</td>
      <td>30.40</td>
      <td>30.63</td>
      <td>30.18</td>
      <td>30.29</td>
      <td>30.38</td>
      <td>-0.09</td>
      <td>-0.2962</td>
      <td>685037.32</td>
      <td>2082008.206</td>
    </tr>
  </tbody>
</table>
</div>



<p>2.生成简单衍生变量（pro接口已经给出这些数据了）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;close-open&#x27;</span>] = (df[<span class="string">&#x27;close&#x27;</span>] - df[<span class="string">&#x27;open&#x27;</span>])/df[<span class="string">&#x27;open&#x27;</span>]</span><br><span class="line">df[<span class="string">&#x27;high-low&#x27;</span>] = (df[<span class="string">&#x27;high&#x27;</span>] - df[<span class="string">&#x27;low&#x27;</span>])/df[<span class="string">&#x27;low&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># df[&#x27;pre_close&#x27;] = df[&#x27;close&#x27;].shift(1)  # 该列所有往下移一行形成昨日收盘价</span></span><br><span class="line"><span class="comment">#df[&#x27;price_change&#x27;] = df[&#x27;close&#x27;]-df[&#x27;pre_close&#x27;] # 当日股价变化</span></span><br><span class="line"><span class="comment"># df[&#x27;p_change&#x27;] = (df[&#x27;close&#x27;]-df[&#x27;pre_close&#x27;])/df[&#x27;pre_close&#x27;]*100 # 当日股价变化百分比</span></span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts_code</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>pre_close</th>
      <th>change</th>
      <th>pct_chg</th>
      <th>vol</th>
      <th>amount</th>
      <th>close-open</th>
      <th>high-low</th>
    </tr>
    <tr>
      <th>trade_date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>20191231</th>
      <td>000002.SZ</td>
      <td>31.35</td>
      <td>32.45</td>
      <td>31.32</td>
      <td>32.18</td>
      <td>31.57</td>
      <td>0.61</td>
      <td>1.9322</td>
      <td>663497.98</td>
      <td>2122966.722</td>
      <td>0.026475</td>
      <td>0.036079</td>
    </tr>
    <tr>
      <th>20191230</th>
      <td>000002.SZ</td>
      <td>31.35</td>
      <td>31.79</td>
      <td>31.02</td>
      <td>31.57</td>
      <td>31.00</td>
      <td>0.57</td>
      <td>1.8387</td>
      <td>915751.42</td>
      <td>2870247.850</td>
      <td>0.007018</td>
      <td>0.024823</td>
    </tr>
    <tr>
      <th>20191227</th>
      <td>000002.SZ</td>
      <td>31.23</td>
      <td>31.32</td>
      <td>30.81</td>
      <td>31.00</td>
      <td>31.12</td>
      <td>-0.12</td>
      <td>-0.3856</td>
      <td>703096.48</td>
      <td>2185106.849</td>
      <td>-0.007365</td>
      <td>0.016553</td>
    </tr>
    <tr>
      <th>20191226</th>
      <td>000002.SZ</td>
      <td>30.50</td>
      <td>31.30</td>
      <td>30.50</td>
      <td>31.12</td>
      <td>30.29</td>
      <td>0.83</td>
      <td>2.7402</td>
      <td>888790.74</td>
      <td>2758745.302</td>
      <td>0.020328</td>
      <td>0.026230</td>
    </tr>
    <tr>
      <th>20191225</th>
      <td>000002.SZ</td>
      <td>30.40</td>
      <td>30.63</td>
      <td>30.18</td>
      <td>30.29</td>
      <td>30.38</td>
      <td>-0.09</td>
      <td>-0.2962</td>
      <td>685037.32</td>
      <td>2082008.206</td>
      <td>-0.003618</td>
      <td>0.014911</td>
    </tr>
  </tbody>
</table>
</div>



<p>3.生成移动平均线MA值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;MA5&#x27;</span>] = df[<span class="string">&#x27;close&#x27;</span>].sort_index().rolling(<span class="number">5</span>).mean()</span><br><span class="line">df[<span class="string">&#x27;MA10&#x27;</span>] = df[<span class="string">&#x27;close&#x27;</span>].sort_index().rolling(<span class="number">10</span>).mean()</span><br><span class="line"></span><br><span class="line">df.head(<span class="number">15</span>)  <span class="comment"># head(15)表示展示前15行，因为要展示10行以上，才能看到MA10有值</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts_code</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>pre_close</th>
      <th>change</th>
      <th>pct_chg</th>
      <th>vol</th>
      <th>amount</th>
      <th>close-open</th>
      <th>high-low</th>
      <th>MA5</th>
      <th>MA10</th>
    </tr>
    <tr>
      <th>trade_date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>20191218</th>
      <td>000002.SZ</td>
      <td>30.50</td>
      <td>30.96</td>
      <td>30.20</td>
      <td>30.31</td>
      <td>30.45</td>
      <td>-0.14</td>
      <td>-0.4598</td>
      <td>907932.96</td>
      <td>2770950.971</td>
      <td>-0.006230</td>
      <td>0.025166</td>
      <td>29.272</td>
      <td>28.609</td>
    </tr>
    <tr>
      <th>20191217</th>
      <td>000002.SZ</td>
      <td>29.35</td>
      <td>31.34</td>
      <td>29.19</td>
      <td>30.45</td>
      <td>29.45</td>
      <td>1.00</td>
      <td>3.3956</td>
      <td>1596500.08</td>
      <td>4825522.418</td>
      <td>0.037479</td>
      <td>0.073655</td>
      <td>28.810</td>
      <td>28.356</td>
    </tr>
    <tr>
      <th>20191216</th>
      <td>000002.SZ</td>
      <td>28.25</td>
      <td>29.45</td>
      <td>28.15</td>
      <td>29.45</td>
      <td>28.15</td>
      <td>1.30</td>
      <td>4.6181</td>
      <td>1896062.95</td>
      <td>5422583.746</td>
      <td>0.042478</td>
      <td>0.046181</td>
      <td>28.300</td>
      <td>28.111</td>
    </tr>
    <tr>
      <th>20191213</th>
      <td>000002.SZ</td>
      <td>28.18</td>
      <td>28.34</td>
      <td>28.13</td>
      <td>28.15</td>
      <td>28.00</td>
      <td>0.15</td>
      <td>0.5357</td>
      <td>1431370.48</td>
      <td>4039464.921</td>
      <td>-0.001065</td>
      <td>0.007465</td>
      <td>28.044</td>
      <td>27.971</td>
    </tr>
    <tr>
      <th>20191212</th>
      <td>000002.SZ</td>
      <td>28.02</td>
      <td>28.17</td>
      <td>27.94</td>
      <td>28.00</td>
      <td>28.00</td>
      <td>0.00</td>
      <td>0.0000</td>
      <td>563827.86</td>
      <td>1580712.929</td>
      <td>-0.000714</td>
      <td>0.008232</td>
      <td>27.986</td>
      <td>27.926</td>
    </tr>
    <tr>
      <th>20191211</th>
      <td>000002.SZ</td>
      <td>27.99</td>
      <td>28.24</td>
      <td>27.95</td>
      <td>28.00</td>
      <td>27.90</td>
      <td>0.10</td>
      <td>0.3584</td>
      <td>985187.14</td>
      <td>2765688.862</td>
      <td>0.000357</td>
      <td>0.010376</td>
      <td>27.946</td>
      <td>27.906</td>
    </tr>
    <tr>
      <th>20191210</th>
      <td>000002.SZ</td>
      <td>28.08</td>
      <td>28.15</td>
      <td>27.73</td>
      <td>27.90</td>
      <td>28.17</td>
      <td>-0.27</td>
      <td>-0.9585</td>
      <td>685469.29</td>
      <td>1914187.737</td>
      <td>-0.006410</td>
      <td>0.015146</td>
      <td>27.902</td>
      <td>27.906</td>
    </tr>
    <tr>
      <th>20191209</th>
      <td>000002.SZ</td>
      <td>27.96</td>
      <td>28.35</td>
      <td>27.96</td>
      <td>28.17</td>
      <td>27.86</td>
      <td>0.31</td>
      <td>1.1127</td>
      <td>1160606.12</td>
      <td>3267608.704</td>
      <td>0.007511</td>
      <td>0.013948</td>
      <td>27.922</td>
      <td>27.951</td>
    </tr>
    <tr>
      <th>20191206</th>
      <td>000002.SZ</td>
      <td>27.86</td>
      <td>27.88</td>
      <td>27.72</td>
      <td>27.86</td>
      <td>27.80</td>
      <td>0.06</td>
      <td>0.2158</td>
      <td>358417.78</td>
      <td>997431.113</td>
      <td>0.000000</td>
      <td>0.005772</td>
      <td>27.898</td>
      <td>27.999</td>
    </tr>
    <tr>
      <th>20191205</th>
      <td>000002.SZ</td>
      <td>27.95</td>
      <td>27.95</td>
      <td>27.61</td>
      <td>27.80</td>
      <td>27.78</td>
      <td>0.02</td>
      <td>0.0720</td>
      <td>373263.56</td>
      <td>1036876.654</td>
      <td>-0.005367</td>
      <td>0.012314</td>
      <td>27.866</td>
      <td>27.914</td>
    </tr>
    <tr>
      <th>20191204</th>
      <td>000002.SZ</td>
      <td>27.88</td>
      <td>28.00</td>
      <td>27.65</td>
      <td>27.78</td>
      <td>28.00</td>
      <td>-0.22</td>
      <td>-0.7857</td>
      <td>343382.71</td>
      <td>954765.854</td>
      <td>-0.003587</td>
      <td>0.012658</td>
      <td>27.866</td>
      <td>27.809</td>
    </tr>
    <tr>
      <th>20191203</th>
      <td>000002.SZ</td>
      <td>27.97</td>
      <td>28.18</td>
      <td>27.73</td>
      <td>28.00</td>
      <td>28.05</td>
      <td>-0.05</td>
      <td>-0.1783</td>
      <td>596981.60</td>
      <td>1673918.787</td>
      <td>0.001073</td>
      <td>0.016228</td>
      <td>27.910</td>
      <td>27.696</td>
    </tr>
    <tr>
      <th>20191202</th>
      <td>000002.SZ</td>
      <td>27.87</td>
      <td>28.12</td>
      <td>27.86</td>
      <td>28.05</td>
      <td>27.70</td>
      <td>0.35</td>
      <td>1.2635</td>
      <td>1006843.93</td>
      <td>2823613.032</td>
      <td>0.006459</td>
      <td>0.009332</td>
      <td>27.980</td>
      <td>27.568</td>
    </tr>
    <tr>
      <th>20191129</th>
      <td>000002.SZ</td>
      <td>27.83</td>
      <td>27.85</td>
      <td>27.59</td>
      <td>27.70</td>
      <td>27.80</td>
      <td>-0.10</td>
      <td>-0.3597</td>
      <td>448064.84</td>
      <td>1242373.843</td>
      <td>-0.004671</td>
      <td>0.009424</td>
      <td>28.100</td>
      <td>27.405</td>
    </tr>
    <tr>
      <th>20191128</th>
      <td>000002.SZ</td>
      <td>28.01</td>
      <td>28.01</td>
      <td>27.62</td>
      <td>27.80</td>
      <td>28.00</td>
      <td>-0.20</td>
      <td>-0.7143</td>
      <td>439961.62</td>
      <td>1222840.122</td>
      <td>-0.007497</td>
      <td>0.014120</td>
      <td>27.962</td>
      <td>27.284</td>
    </tr>
  </tbody>
</table>
</div>



<p>MA5计算原理：</p>
<p>日期：股票收盘价</p>
<p>1：1.2&#x2F;2：1.4&#x2F;3：1.6&#x2F;4：1.8&#x2F;5：2.0&#x2F;6：2.2</p>
<p>5号的MA均值为(1.2+1.4+1.6+1.8+2.0)&#x2F;5&#x3D;1.6，而六号的MA5均值为(1.4+1.6+1.8+2.0+2.2)&#x2F;5&#x3D;1.8，以此类推。将一段时期内股价的移动平均值连成曲线，即为移动平均线</p>
<p>在计算MA5这样的数据时，因为最开始4天数据量不够，这4天对应的移动平均值为空值NaN，通常用dropna()函数删除空值，以免在后续计算中出现因空值造成的问题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除空值</span></span><br><span class="line">df.dropna(inplace=<span class="literal">True</span>)  <span class="comment"># 删除空值行，也可以写成df = df.dropna()</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts_code</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>pre_close</th>
      <th>change</th>
      <th>pct_chg</th>
      <th>vol</th>
      <th>amount</th>
      <th>close-open</th>
      <th>high-low</th>
      <th>MA5</th>
      <th>MA10</th>
    </tr>
    <tr>
      <th>trade_date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>20191218</th>
      <td>000002.SZ</td>
      <td>30.50</td>
      <td>30.96</td>
      <td>30.20</td>
      <td>30.31</td>
      <td>30.45</td>
      <td>-0.14</td>
      <td>-0.4598</td>
      <td>907932.96</td>
      <td>2770950.971</td>
      <td>-0.006230</td>
      <td>0.025166</td>
      <td>29.272</td>
      <td>28.609</td>
    </tr>
    <tr>
      <th>20191217</th>
      <td>000002.SZ</td>
      <td>29.35</td>
      <td>31.34</td>
      <td>29.19</td>
      <td>30.45</td>
      <td>29.45</td>
      <td>1.00</td>
      <td>3.3956</td>
      <td>1596500.08</td>
      <td>4825522.418</td>
      <td>0.037479</td>
      <td>0.073655</td>
      <td>28.810</td>
      <td>28.356</td>
    </tr>
    <tr>
      <th>20191216</th>
      <td>000002.SZ</td>
      <td>28.25</td>
      <td>29.45</td>
      <td>28.15</td>
      <td>29.45</td>
      <td>28.15</td>
      <td>1.30</td>
      <td>4.6181</td>
      <td>1896062.95</td>
      <td>5422583.746</td>
      <td>0.042478</td>
      <td>0.046181</td>
      <td>28.300</td>
      <td>28.111</td>
    </tr>
    <tr>
      <th>20191213</th>
      <td>000002.SZ</td>
      <td>28.18</td>
      <td>28.34</td>
      <td>28.13</td>
      <td>28.15</td>
      <td>28.00</td>
      <td>0.15</td>
      <td>0.5357</td>
      <td>1431370.48</td>
      <td>4039464.921</td>
      <td>-0.001065</td>
      <td>0.007465</td>
      <td>28.044</td>
      <td>27.971</td>
    </tr>
    <tr>
      <th>20191212</th>
      <td>000002.SZ</td>
      <td>28.02</td>
      <td>28.17</td>
      <td>27.94</td>
      <td>28.00</td>
      <td>28.00</td>
      <td>0.00</td>
      <td>0.0000</td>
      <td>563827.86</td>
      <td>1580712.929</td>
      <td>-0.000714</td>
      <td>0.008232</td>
      <td>27.986</td>
      <td>27.926</td>
    </tr>
  </tbody>
</table>
</div>



<p>4.股票衍生变量生成库TA-Lib的安装</p>
<p>（这个库安装在了python3.7版本下，所以这里用不了，但实际上这些指标聚宽上都有，这里只是学习了解一下即可）</p>
<p>5.~8.用TA-Lib库生成相对强弱指标RSI值&#x2F;MOM值&#x2F;EMA值&#x2F;MACD值</p>
<p>用pycharm中python3.7版本实现</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(41).png" alt="下载 (41)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(42).png" alt="下载 (42)"></p>
<p>RSI值能够反映短期内股价涨势相对于跌势的强弱，帮助我们更好判断股价的涨跌趋势。RSI值越大，涨势相对于跌势越强，反之则涨势相对于跌势越弱。<br><img src="%E4%B8%8B%E8%BD%BD%20(43).png" alt="下载 (43)"><br>一般N取值为6，12，24，代码参数timeperiod为12</p>
<p>通常情况下，RSI值位于20~80之间，超过80则为超买状态，低于20为超卖状态，等于50则为买卖双方力量均等。例如，如果连续6天股价都是上涨，则6日平均下跌价格为0，6日RSI值为100，表明此时股票买方处于非常强势的地位，但投资者也要警惕此时可能是超买状态，需要预防股价下跌的风险</p>
<p>MOM动量反映了一段时间内股价的涨跌速度，计算公式如下：<br><img src="%E4%B8%8B%E8%BD%BD%20(44).png" alt="下载 (44)"></p>
<p>EMA是以指数式递减加权的移动平均，并根据计算结果进行分析，用于判断股价未来走势的变动趋势。EMA和移动平均线指标MA有点相似，不过计算更复杂，只要知道EMA是一种趋势类指标即可<br><img src="%E4%B8%8B%E8%BD%BD%20(45).png" alt="下载 (45)"><br>对于EMA值而言，近期股价比之前更久远的股价更重要（计算中，近期股价的权重更大)，不像MA那样一视同仁。</p>
<p>此外，在计算MACD时，也会需要计算EMA值，其中N一般选12天和26天，因此α分别为2&#x2F;13和2&#x2F;27,对应的EMA12称为快的指数移动平均线，EMA26称为慢的指数移动平均线</p>
<p>MACD是股票市场上的常用指标，是基于EMA值的衍生变量，MACD是一种趋势类指标，其变化代表着市场趋势的变化，不同K线级别的MACD代表当前级别周期中的买卖趋势。上述代码中MACD，MACDsignal和MACDhist都是MACD值的相关指标<br><img src="%E4%B8%8B%E8%BD%BD%20(47).png" alt="下载 (47)"><br>MACD技术指标图是由两线一柱组成的，快速线为DIF值，慢速线为DEA值，柱形图为MACD值<br><img src="%E4%B8%8B%E8%BD%BD%20(48).png" alt="下载 (48)"></p>
<p>蓝色线DIF，粉色线DEA，柱MACD<br><img src="%E4%B8%8B%E8%BD%BD%20(49).png" alt="下载 (49)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(50).png" alt="下载 (50)"></p>
<h3 id="8-2-3-多因子模型搭建"><a href="#8-2-3-多因子模型搭建" class="headerlink" title="8.2.3 多因子模型搭建"></a>8.2.3 多因子模型搭建</h3><p>本案例中的模型是根据多个特征进行搭建的，在量化金融领域称为多因子模型。股票数据是时间序列数据，与之相关的一些数据处理工作和之前所讲的模型稍有不同</p>
<p>注意：由于talib库只能在python3.7环境下运行，而该环境下没有sklearn库，所以打算将处理好的数据保存为excel文件，再在jupyter notebook中调用</p>
<p><strong>1.引入之后需要用到的库</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts  <span class="comment"># 股票基本数据相关库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># 科学计算相关库</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  <span class="comment"># 科学计算相关库  </span></span><br><span class="line"><span class="comment"># import talib  # 股票衍生变量数据相关库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># 引入绘图相关库</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier  <span class="comment"># 引入分类决策树模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score  <span class="comment"># 引入准确度评分函数</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>) <span class="comment"># 忽略警告信息，警告非报错，不影响代码执行</span></span><br></pre></td></tr></table></figure>

<p><strong>2.股票数据处理与衍生变量生成</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(51).png" alt="下载 (51)"></p>
<p>此处将处理好的股票数据打包成excel文件，保存在本笔记同目录中</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(52).png" alt="下载 (52)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df =pd.read_excel(<span class="string">&#x27;股票数据.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>trade_date</th>
      <th>ts_code</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>pre_close</th>
      <th>change</th>
      <th>pct_chg</th>
      <th>vol</th>
      <th>...</th>
      <th>high-low</th>
      <th>MA5</th>
      <th>MA10</th>
      <th>RSI</th>
      <th>MOM</th>
      <th>EMA12</th>
      <th>EMA26</th>
      <th>MACD</th>
      <th>MACDsignal</th>
      <th>MACDhist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>20191114</td>
      <td>000002.SZ</td>
      <td>26.23</td>
      <td>26.47</td>
      <td>26.07</td>
      <td>26.39</td>
      <td>26.19</td>
      <td>0.20</td>
      <td>0.7637</td>
      <td>378340.59</td>
      <td>...</td>
      <td>0.015343</td>
      <td>26.396</td>
      <td>26.577</td>
      <td>26.694827</td>
      <td>-0.36</td>
      <td>27.116721</td>
      <td>28.154057</td>
      <td>-1.119303</td>
      <td>-1.194937</td>
      <td>0.075634</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20191113</td>
      <td>000002.SZ</td>
      <td>26.28</td>
      <td>26.30</td>
      <td>26.00</td>
      <td>26.19</td>
      <td>26.28</td>
      <td>-0.09</td>
      <td>-0.3425</td>
      <td>346952.18</td>
      <td>...</td>
      <td>0.011538</td>
      <td>26.432</td>
      <td>26.591</td>
      <td>25.231860</td>
      <td>-0.46</td>
      <td>26.974149</td>
      <td>28.008571</td>
      <td>-1.103780</td>
      <td>-1.176706</td>
      <td>0.072926</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20191112</td>
      <td>000002.SZ</td>
      <td>26.32</td>
      <td>26.53</td>
      <td>26.21</td>
      <td>26.28</td>
      <td>26.38</td>
      <td>-0.10</td>
      <td>-0.3791</td>
      <td>363338.48</td>
      <td>...</td>
      <td>0.012209</td>
      <td>26.524</td>
      <td>26.637</td>
      <td>27.190684</td>
      <td>-0.44</td>
      <td>26.867357</td>
      <td>27.880529</td>
      <td>-1.071859</td>
      <td>-1.155737</td>
      <td>0.083877</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20191111</td>
      <td>000002.SZ</td>
      <td>26.58</td>
      <td>26.59</td>
      <td>26.32</td>
      <td>26.38</td>
      <td>26.74</td>
      <td>-0.36</td>
      <td>-1.3463</td>
      <td>463259.32</td>
      <td>...</td>
      <td>0.010258</td>
      <td>26.644</td>
      <td>26.676</td>
      <td>29.431647</td>
      <td>-0.04</td>
      <td>26.792379</td>
      <td>27.769379</td>
      <td>-1.026658</td>
      <td>-1.129921</td>
      <td>0.103263</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20191108</td>
      <td>000002.SZ</td>
      <td>26.69</td>
      <td>26.81</td>
      <td>26.57</td>
      <td>26.74</td>
      <td>26.57</td>
      <td>0.17</td>
      <td>0.6398</td>
      <td>485246.82</td>
      <td>...</td>
      <td>0.009033</td>
      <td>26.740</td>
      <td>26.726</td>
      <td>37.041765</td>
      <td>0.25</td>
      <td>26.784321</td>
      <td>27.693129</td>
      <td>-0.950826</td>
      <td>-1.094102</td>
      <td>0.143276</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 22 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以看到索引变了，再调整一下</span></span><br><span class="line">df = df.set_index(<span class="string">&#x27;trade_date&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts_code</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>pre_close</th>
      <th>change</th>
      <th>pct_chg</th>
      <th>vol</th>
      <th>amount</th>
      <th>...</th>
      <th>high-low</th>
      <th>MA5</th>
      <th>MA10</th>
      <th>RSI</th>
      <th>MOM</th>
      <th>EMA12</th>
      <th>EMA26</th>
      <th>MACD</th>
      <th>MACDsignal</th>
      <th>MACDhist</th>
    </tr>
    <tr>
      <th>trade_date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>20191114</th>
      <td>000002.SZ</td>
      <td>26.23</td>
      <td>26.47</td>
      <td>26.07</td>
      <td>26.39</td>
      <td>26.19</td>
      <td>0.20</td>
      <td>0.7637</td>
      <td>378340.59</td>
      <td>996571.596</td>
      <td>...</td>
      <td>0.015343</td>
      <td>26.396</td>
      <td>26.577</td>
      <td>26.694827</td>
      <td>-0.36</td>
      <td>27.116721</td>
      <td>28.154057</td>
      <td>-1.119303</td>
      <td>-1.194937</td>
      <td>0.075634</td>
    </tr>
    <tr>
      <th>20191113</th>
      <td>000002.SZ</td>
      <td>26.28</td>
      <td>26.30</td>
      <td>26.00</td>
      <td>26.19</td>
      <td>26.28</td>
      <td>-0.09</td>
      <td>-0.3425</td>
      <td>346952.18</td>
      <td>906913.752</td>
      <td>...</td>
      <td>0.011538</td>
      <td>26.432</td>
      <td>26.591</td>
      <td>25.231860</td>
      <td>-0.46</td>
      <td>26.974149</td>
      <td>28.008571</td>
      <td>-1.103780</td>
      <td>-1.176706</td>
      <td>0.072926</td>
    </tr>
    <tr>
      <th>20191112</th>
      <td>000002.SZ</td>
      <td>26.32</td>
      <td>26.53</td>
      <td>26.21</td>
      <td>26.28</td>
      <td>26.38</td>
      <td>-0.10</td>
      <td>-0.3791</td>
      <td>363338.48</td>
      <td>957393.717</td>
      <td>...</td>
      <td>0.012209</td>
      <td>26.524</td>
      <td>26.637</td>
      <td>27.190684</td>
      <td>-0.44</td>
      <td>26.867357</td>
      <td>27.880529</td>
      <td>-1.071859</td>
      <td>-1.155737</td>
      <td>0.083877</td>
    </tr>
    <tr>
      <th>20191111</th>
      <td>000002.SZ</td>
      <td>26.58</td>
      <td>26.59</td>
      <td>26.32</td>
      <td>26.38</td>
      <td>26.74</td>
      <td>-0.36</td>
      <td>-1.3463</td>
      <td>463259.32</td>
      <td>1225718.598</td>
      <td>...</td>
      <td>0.010258</td>
      <td>26.644</td>
      <td>26.676</td>
      <td>29.431647</td>
      <td>-0.04</td>
      <td>26.792379</td>
      <td>27.769379</td>
      <td>-1.026658</td>
      <td>-1.129921</td>
      <td>0.103263</td>
    </tr>
    <tr>
      <th>20191108</th>
      <td>000002.SZ</td>
      <td>26.69</td>
      <td>26.81</td>
      <td>26.57</td>
      <td>26.74</td>
      <td>26.57</td>
      <td>0.17</td>
      <td>0.6398</td>
      <td>485246.82</td>
      <td>1296314.035</td>
      <td>...</td>
      <td>0.009033</td>
      <td>26.740</td>
      <td>26.726</td>
      <td>37.041765</td>
      <td>0.25</td>
      <td>26.784321</td>
      <td>27.693129</td>
      <td>-0.950826</td>
      <td>-1.094102</td>
      <td>0.143276</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>



<p><strong>3.特征变量和目标变量提取</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df[[<span class="string">&#x27;close&#x27;</span>, <span class="string">&#x27;vol&#x27;</span>, <span class="string">&#x27;close-open&#x27;</span>, <span class="string">&#x27;MA5&#x27;</span>, <span class="string">&#x27;MA10&#x27;</span>, <span class="string">&#x27;high-low&#x27;</span>, <span class="string">&#x27;RSI&#x27;</span>, <span class="string">&#x27;MOM&#x27;</span>, <span class="string">&#x27;EMA12&#x27;</span>, <span class="string">&#x27;MACD&#x27;</span>, <span class="string">&#x27;MACDsignal&#x27;</span>, <span class="string">&#x27;MACDhist&#x27;</span>]]</span><br><span class="line">y = np.where(df[<span class="string">&#x27;change&#x27;</span>].shift(-<span class="number">1</span>)&gt; <span class="number">0</span>, <span class="number">1</span>, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>首先应该强调的最核心的一点是：应该用当天的股价数据预测下一天的股价涨跌情况，所以目标变量y应该是下一天的股价涨跌情况</p>
<p>其中Numpy库中的where()函数的使用方法如下所示：<br>np.where(判断条件,满足条件的赋值,不满足条件的赋值)</p>
<p>其中df[‘change’].shift(-1)则是利用shift()函数将change（股价变化）这一列往上移动一行，这样就获得了每一行对应的下一天股价涨跌情况。</p>
<p>因此这里的判断条件就是下一天股价是否大于0，如果下一天股价涨了的我们则y赋值为数字1，下一天股价跌了的，则y赋值为数字-1。这个下一天的股价涨跌情况就是我们根据当天股票基本数据以及衍生变量预测的内容。</p>
<p><strong>3.训练集和测试集数据划分</strong></p>
<p>接下来，我们要将原始数据集进行分割，我们要注意到一点，训练集与测试集的划分要按照时间序列划分，而不是像之前利用train_test_split()函数进行划分。原因在于股票价格的变化趋势具有时间性，如果我们随机划分，则会破坏时间性特征，因为我们是根据当天数据来预测下一天的股价涨跌情况，而不是任意一天的股票数据来预测下一天的股价涨跌情况。<br>因此，我们将前90%的数据作为训练集，后10%的数据作为测试集，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_length = X.shape[<span class="number">0</span>]  <span class="comment"># shape属性获取X的行数和列数，shape[0]即表示行数 </span></span><br><span class="line">split = <span class="built_in">int</span>(X_length * <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">X_train, X_test = X[:split], X[split:]</span><br><span class="line">y_train, y_test = y[:split], y[split:]</span><br></pre></td></tr></table></figure>

<p><strong>4.模型搭建</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = RandomForestClassifier(max_depth=<span class="number">3</span>, n_estimators=<span class="number">10</span>, min_samples_leaf=<span class="number">10</span>, random_state=<span class="number">1</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>RandomForestClassifier(max_depth=3, min_samples_leaf=10, n_estimators=10,
                       random_state=1)
</code></pre>
<p>其中设置的参数：决策树最大深度max_depth设置为3，弱学习器个数n_estimators为10，叶子节点的最小样本数min_samples_lesf为10</p>
<h3 id="8-2-4-模型使用与评估"><a href="#8-2-4-模型使用与评估" class="headerlink" title="8.2.4 模型使用与评估"></a>8.2.4 模型使用与评估</h3><p><strong>1.预测下一天的涨跌情况</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure>

<pre><code>[ 1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1
  1 -1  1  1  1 -1  1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1
  1  1  1  1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1
 -1 -1  1  1  1 -1 -1 -1 -1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>-1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看预测概率</span></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)</span><br><span class="line">y_pred_proba[<span class="number">0</span>:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([[0.48342616, 0.51657384],
       [0.52112305, 0.47887695],
       [0.46987305, 0.53012695],
       [0.48293875, 0.51706125],
       [0.45513387, 0.54486613]])
</code></pre>
<p><strong>2.模型准确度评估</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.5523809523809524
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此外，我们还可以通过模型自带的score()函数记性打分，代码如下：</span></span><br><span class="line">model.score(X_test, y_test)</span><br></pre></td></tr></table></figure>




<pre><code>0.5523809523809524
</code></pre>
<p><strong>3.分析数据特征的重要性</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.feature_importances_</span><br></pre></td></tr></table></figure>




<pre><code>array([0.057474  , 0.21408127, 0.02888772, 0.05994007, 0.0428833 ,
       0.09075887, 0.05632758, 0.16353218, 0.09928015, 0.01865826,
       0.10250797, 0.06566863])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下代码可以更好的展示特征及其特征重要性：</span></span><br><span class="line">features = X.columns  </span><br><span class="line">importances = model.feature_importances_</span><br><span class="line">a = pd.DataFrame()</span><br><span class="line">a[<span class="string">&#x27;特征&#x27;</span>] = features</span><br><span class="line">a[<span class="string">&#x27;特征重要性&#x27;</span>] = importances</span><br><span class="line">a = a.sort_values(<span class="string">&#x27;特征重要性&#x27;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>特征</th>
      <th>特征重要性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>vol</td>
      <td>0.214081</td>
    </tr>
    <tr>
      <th>7</th>
      <td>MOM</td>
      <td>0.163532</td>
    </tr>
    <tr>
      <th>10</th>
      <td>MACDsignal</td>
      <td>0.102508</td>
    </tr>
    <tr>
      <th>8</th>
      <td>EMA12</td>
      <td>0.099280</td>
    </tr>
    <tr>
      <th>5</th>
      <td>high-low</td>
      <td>0.090759</td>
    </tr>
    <tr>
      <th>11</th>
      <td>MACDhist</td>
      <td>0.065669</td>
    </tr>
    <tr>
      <th>3</th>
      <td>MA5</td>
      <td>0.059940</td>
    </tr>
    <tr>
      <th>0</th>
      <td>close</td>
      <td>0.057474</td>
    </tr>
    <tr>
      <th>6</th>
      <td>RSI</td>
      <td>0.056328</td>
    </tr>
    <tr>
      <th>4</th>
      <td>MA10</td>
      <td>0.042883</td>
    </tr>
    <tr>
      <th>2</th>
      <td>close-open</td>
      <td>0.028888</td>
    </tr>
    <tr>
      <th>9</th>
      <td>MACD</td>
      <td>0.018658</td>
    </tr>
  </tbody>
</table>
</div>



<p>可见，成交量，动量指标MOM，MACDsignal等对下一天股价涨跌结果预测准确度影响较大</p>
<h3 id="8-2-5-参数调优"><a href="#8-2-5-参数调优" class="headerlink" title="8.2.5 参数调优"></a>8.2.5 参数调优</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV  <span class="comment"># 网格搜索合适的超参数</span></span><br><span class="line"><span class="comment"># 指定分类器中参数的范围</span></span><br><span class="line">parameters = &#123;<span class="string">&#x27;n_estimators&#x27;</span>:[<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>], <span class="string">&#x27;max_depth&#x27;</span>:[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="string">&#x27;min_samples_leaf&#x27;</span>:[<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]&#125;</span><br><span class="line">new_model = RandomForestClassifier(random_state=<span class="number">1</span>)  <span class="comment"># 构建分类器</span></span><br><span class="line">grid_search = GridSearchCV(new_model, parameters, cv=<span class="number">6</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)  <span class="comment"># cv=6表示交叉验证6次，scoring=&#x27;roc_auc&#x27;表示以ROC曲线的AUC评分作为模型评价准则, 默认为&#x27;accuracy&#x27;, 即按准确度评分</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grid_search.fit(X_train, y_train)  <span class="comment"># 传入数据</span></span><br><span class="line">grid_search.best_params_  <span class="comment"># 输出参数的最优值</span></span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;max_depth&#39;: 2, &#39;min_samples_leaf&#39;: 30, &#39;n_estimators&#39;: 20&#125;
</code></pre>
<p>用优化后的参数进行模型搭建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = RandomForestClassifier(max_depth=<span class="number">2</span>, n_estimators=<span class="number">20</span>, min_samples_leaf=<span class="number">30</span>, random_state=<span class="number">1</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>RandomForestClassifier(max_depth=2, min_samples_leaf=30, n_estimators=20,
                       random_state=1)
</code></pre>
<p>查看模型准确度（还那样哈哈哈）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.5523809523809524
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>-1</td>
    </tr>
  </tbody>
</table>
</div>



<p><img src="%E4%B8%8B%E8%BD%BD%20(53).png" alt="下载 (53)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(54).png" alt="下载 (54)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(55).png" alt="下载 (55)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(56).png" alt="下载 (56)"></p>
<h3 id="8-2-6-收益回测曲线绘制-知道就行了，实战中直接拿聚宽回测"><a href="#8-2-6-收益回测曲线绘制-知道就行了，实战中直接拿聚宽回测" class="headerlink" title="8.2.6 收益回测曲线绘制(知道就行了，实战中直接拿聚宽回测)"></a>8.2.6 收益回测曲线绘制(知道就行了，实战中直接拿聚宽回测)</h3><p>重点看收益回测曲线（净值曲线），也就是看搭建的模型获得的结果是否比不利用模型获得的结果更好</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_test[<span class="string">&#x27;prediction&#x27;</span>] = model.predict(X_test)</span><br><span class="line">X_test[<span class="string">&#x27;p_change&#x27;</span>] = (X_test[<span class="string">&#x27;close&#x27;</span>] - X_test[<span class="string">&#x27;close&#x27;</span>].shift(<span class="number">1</span>)) / X_test[<span class="string">&#x27;close&#x27;</span>].shift(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X_test[<span class="string">&#x27;origin&#x27;</span>] = (X_test[<span class="string">&#x27;p_change&#x27;</span>] + <span class="number">1</span>).cumprod()</span><br><span class="line">X_test[<span class="string">&#x27;strategy&#x27;</span>] = (X_test[<span class="string">&#x27;prediction&#x27;</span>].shift(<span class="number">1</span>) * X_test[<span class="string">&#x27;p_change&#x27;</span>] + <span class="number">1</span>).cumprod()</span><br><span class="line"></span><br><span class="line">X_test[[<span class="string">&#x27;strategy&#x27;</span>, <span class="string">&#x27;origin&#x27;</span>]].tail()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>strategy</th>
      <th>origin</th>
    </tr>
    <tr>
      <th>trade_date</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>20150122</th>
      <td>1.032030</td>
      <td>0.951647</td>
    </tr>
    <tr>
      <th>20150121</th>
      <td>0.991752</td>
      <td>0.914506</td>
    </tr>
    <tr>
      <th>20150120</th>
      <td>0.938555</td>
      <td>0.865452</td>
    </tr>
    <tr>
      <th>20150119</th>
      <td>0.952234</td>
      <td>0.852838</td>
    </tr>
    <tr>
      <th>20150116</th>
      <td>0.846604</td>
      <td>0.947442</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下代码将收益情况删除空值后可视化，并设置X轴刻度自动倾斜：</span></span><br><span class="line">X_test[[<span class="string">&#x27;strategy&#x27;</span>, <span class="string">&#x27;origin&#x27;</span>]].dropna().plot()  <span class="comment"># 我们读取的日期数据是20150101类型</span></span><br><span class="line">                                                <span class="comment"># 的，应该变为2015-01-01类型的再处理</span></span><br><span class="line">plt.gcf().autofmt_xdate()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_340_0.png" alt="output_340_0"></p>
<h1 id="9-AdaBoost与GBDT模型"><a href="#9-AdaBoost与GBDT模型" class="headerlink" title="9 AdaBoost与GBDT模型"></a>9 AdaBoost与GBDT模型</h1><h2 id="9-1-AdaBoost算法原理"><a href="#9-1-AdaBoost算法原理" class="headerlink" title="9.1 AdaBoost算法原理"></a>9.1 AdaBoost算法原理</h2><h3 id="9-1-1-AdaBoost算法的核心思想"><a href="#9-1-1-AdaBoost算法的核心思想" class="headerlink" title="9.1.1 AdaBoost算法的核心思想"></a>9.1.1 AdaBoost算法的核心思想</h3><p>AdaBoost算法是一种有效而实用的Boosting算法，它以一种<strong>高度自适应的方式</strong>按顺序训练弱学习器。针对分类问题，AdaBoost算法根据前一次的分类效果调整数据的权重，在上一个弱学习器中分类错误的样本的权重会在下一个弱学习器中增加，分类正确的样本的权重则相应减少，并且在每一轮迭代时会向模型加入一个新的弱学习器。不断重复调整权重和训练弱学习器，直到误分类数低于预设值或迭代次数达到指定的最大值，最终得到一个强学习器。<strong>其算法的核心思想是调整错误样本的权重，进而迭代升级</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(57).png" alt="下载 (57)"></p>
<h3 id="9-1-2-AdaBoost算法的数学原理概述"><a href="#9-1-2-AdaBoost算法的数学原理概述" class="headerlink" title="9.1.2 AdaBoost算法的数学原理概述"></a>9.1.2 AdaBoost算法的数学原理概述</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(58).png" alt="下载 (58)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(59).png" alt="下载 (59)"></p>
<p><strong>1，初始化各样本的权重（各权重相等）</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(60).png" alt="下载 (60)"></p>
<p><strong>2.计算误差率</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(61).png" alt="下载 (61)"></p>
<p><strong>3.调整弱学习器的权重</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(62).png" alt="下载 (62)"></p>
<p><strong>4.更新样本的权重</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(63).png" alt="下载 (63)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(64).png" alt="下载 (64)"></p>
<p><strong>5.反复迭代</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(65).png" alt="下载 (65)"></p>
<p><strong>补充：正则化项</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(66).png"></p>
<h3 id="9-1-3-AdaBoost算法的数学原理举例"><a href="#9-1-3-AdaBoost算法的数学原理举例" class="headerlink" title="9.1.3 AdaBoost算法的数学原理举例"></a>9.1.3 AdaBoost算法的数学原理举例</h3><p>略（到时候看ppt文档就行了）</p>
<h3 id="9-1-4-AdaBoost算法的简单代码实现"><a href="#9-1-4-AdaBoost算法的简单代码实现" class="headerlink" title="9.1.4 AdaBoost算法的简单代码实现"></a>9.1.4 AdaBoost算法的简单代码实现</h3><p>1.AdaBoost分类模型演示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">model = AdaBoostClassifier(random_state=<span class="number">123</span>)</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[0]
</code></pre>
<p>2.AdaBoost回归模型演示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostRegressor</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line">y = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">model = AdaBoostRegressor(random_state=<span class="number">123</span>)</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[3.]
</code></pre>
<h2 id="9-2-案例实战-AdaBoost信用卡精准营销模型"><a href="#9-2-案例实战-AdaBoost信用卡精准营销模型" class="headerlink" title="9.2 案例实战 - AdaBoost信用卡精准营销模型"></a>9.2 案例实战 - AdaBoost信用卡精准营销模型</h2><h3 id="9-2-1-案例背景"><a href="#9-2-1-案例背景" class="headerlink" title="9.2.1 案例背景"></a>9.2.1 案例背景</h3><p>本案例来搭建一个信用卡精准营销模型，该模型也可以应用于其他领域的精准营销，如信托公司信托产品的精准营销</p>
<h3 id="9-2-2-模型搭建"><a href="#9-2-2-模型搭建" class="headerlink" title="9.2.2 模型搭建"></a>9.2.2 模型搭建</h3><p><strong>1.读取数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;信用卡精准营销模型.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年龄</th>
      <th>月收入（元）</th>
      <th>月消费（元）</th>
      <th>性别</th>
      <th>月消费/月收入</th>
      <th>响应</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>30</td>
      <td>7275</td>
      <td>6062</td>
      <td>0</td>
      <td>0.833265</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>25</td>
      <td>17739</td>
      <td>13648</td>
      <td>0</td>
      <td>0.769378</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>29</td>
      <td>25736</td>
      <td>14311</td>
      <td>0</td>
      <td>0.556069</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23</td>
      <td>14162</td>
      <td>7596</td>
      <td>0</td>
      <td>0.536365</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>27</td>
      <td>15563</td>
      <td>12849</td>
      <td>0</td>
      <td>0.825612</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>目标变量是精准营销后客户是否响应（即客户在营销后是否办了信用卡），取值为1代表营销有效，取值为0代表营销失败。其中有400个客户响应，600个客户没有响应</p>
<p><strong>2.提取特征变量和目标变量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df.drop(columns=<span class="string">&#x27;响应&#x27;</span>) </span><br><span class="line">y = df[<span class="string">&#x27;响应&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p><strong>3.划分训练集和测试集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br></pre></td></tr></table></figure>

<p><strong>4.模型训练</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">clf = AdaBoostClassifier(random_state=<span class="number">123</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>AdaBoostClassifier(random_state=123)
</code></pre>
<h3 id="9-2-3-模型预测及评估"><a href="#9-2-3-模型预测及评估" class="headerlink" title="9.2.3 模型预测及评估"></a>9.2.3 模型预测及评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型搭建完毕后，通过如下代码预测测试集数据：</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure>

<pre><code>[1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0
 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1
 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1
 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1
 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1
 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过和之前章节类似的代码，我们可以将预测值和实际值进行对比：</span></span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看预测准确度</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.85
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看预测分类概率</span></span><br><span class="line">y_pred_proba = clf.predict_proba(X_test)</span><br><span class="line">y_pred_proba[<span class="number">0</span>:<span class="number">5</span>]  <span class="comment"># 查看前5项，第一列为分类为0的概率，第二列为分类为1的概率</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[0.19294615, 0.80705385],
       [0.41359387, 0.58640613],
       [0.42597039, 0.57402961],
       [0.66817389, 0.33182611],
       [0.32850159, 0.67149841]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制ROC曲线</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thres = roc_curve(y_test.values, y_pred_proba[:,<span class="number">1</span>])</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(fpr, tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_387_0.png" alt="output_387_0"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看AUC值</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">score = roc_auc_score(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.9559047909673483
</code></pre>
<p>可以看到预测效果很好</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看特征重要性</span></span><br><span class="line">clf.feature_importances_</span><br></pre></td></tr></table></figure>




<pre><code>array([0.18, 0.2 , 0.36, 0.02, 0.24])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过DataFrame的方式展示特征重要性</span></span><br><span class="line">features = X.columns  <span class="comment"># 获取特征名称</span></span><br><span class="line">importances = clf.feature_importances_  <span class="comment"># 获取特征重要性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过二维表格形式显示</span></span><br><span class="line">importances_df = pd.DataFrame()</span><br><span class="line">importances_df[<span class="string">&#x27;特征名称&#x27;</span>] = features</span><br><span class="line">importances_df[<span class="string">&#x27;特征重要性&#x27;</span>] = importances</span><br><span class="line">importances_df.sort_values(<span class="string">&#x27;特征重要性&#x27;</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>特征名称</th>
      <th>特征重要性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>月消费（元）</td>
      <td>0.36</td>
    </tr>
    <tr>
      <th>4</th>
      <td>月消费/月收入</td>
      <td>0.24</td>
    </tr>
    <tr>
      <th>1</th>
      <td>月收入（元）</td>
      <td>0.20</td>
    </tr>
    <tr>
      <th>0</th>
      <td>年龄</td>
      <td>0.18</td>
    </tr>
    <tr>
      <th>3</th>
      <td>性别</td>
      <td>0.02</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以看到，特征重要性最高的特征变量是“月消费”，其次是“月消费&#x2F;月收入”和“月收入”，“年龄”和“性别”的特征重要性排在最后</p>
<h3 id="9-2-4-模型参数介绍"><a href="#9-2-4-模型参数介绍" class="headerlink" title="9.2.4 模型参数介绍"></a>9.2.4 模型参数介绍</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分类模型，通过如下代码可以查看官方介绍</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">AdaBoostClassifier?</span><br></pre></td></tr></table></figure>

<p><img src="%E4%B8%8B%E8%BD%BD%20(67).png" alt="下载 (67)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回归模型，通过如下代码可以查看官方介绍</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostRegressor</span><br><span class="line">AdaBoostRegressor?</span><br></pre></td></tr></table></figure>

<p><img src="%E4%B8%8B%E8%BD%BD%20(68).png" alt="下载 (68)"></p>
<h2 id="9-3-GBDT算法原理"><a href="#9-3-GBDT算法原理" class="headerlink" title="9.3 GBDT算法原理"></a>9.3 GBDT算法原理</h2><h3 id="9-1-3-GBDT算法的核心思想"><a href="#9-1-3-GBDT算法的核心思想" class="headerlink" title="9.1.3 GBDT算法的核心思想"></a>9.1.3 GBDT算法的核心思想</h3><p>GBDT是梯度提升树的缩写，它与AdaBoost算法的区别在于：AdaBoost算法根据<strong>分类效果</strong>调整<strong>权重</strong>并不断迭代，最终生成强学习器；GDBT算法则将损失函数的负梯度作为残差的近似值，不断使用残差迭代和拟合回归树，最终生成强学习器。简单来说，AdaBoost算法是<strong>梯度权重</strong>，而GBDT算法则是<strong>拟合残差</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(69).png" alt="下载 (69)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(70).png" alt="下载 (70)"></p>
<p>A，C被划分到左节点，A的实际信用卡额度为8000元，而预测值为10000，因此，A的残差为8000-10000&#x3D;-2000，同理，C的残差为25000-20000&#x3D;5000.B，D被划分到右节点，B的残差为30000-35000&#x3D;-5000，D的残差为40000-35000&#x3D;5000</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(71).png" alt="下载 (71)"></p>
<p>在这棵树中，A，B被划分到左节点，A的实际残差为-2000，而预测的残差为-3000，那么此时A的新残差，即残差的残差为-2000-（-3000）&#x3D;1000，同理，B的新残差为-5000-（-5000）&#x3D;0.C，D被划分到右节点，C的新残差为5000-5000&#x3D;0，D的新残差为5000-5000&#x3D;0.继续用第二棵树产生的新残差去拟合第三棵树，并不断重复此步骤，使残差变小</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(72).png" alt="下载 (72)"></p>
<h3 id="9-3-2-GBDT算法的数学原理概述"><a href="#9-3-2-GBDT算法的数学原理概述" class="headerlink" title="9.3.2 GBDT算法的数学原理概述"></a>9.3.2 GBDT算法的数学原理概述</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(73).png" alt="下载 (73)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(74).png" alt="下载 (74)"></p>
<h3 id="9-3-3-GBDT算法的数学原理举例（略）"><a href="#9-3-3-GBDT算法的数学原理举例（略）" class="headerlink" title="9.3.3 GBDT算法的数学原理举例（略）"></a>9.3.3 GBDT算法的数学原理举例（略）</h3><h3 id="9-3-4-GBDT算法的简单代码实现"><a href="#9-3-4-GBDT算法的简单代码实现" class="headerlink" title="9.3.4 GBDT算法的简单代码实现"></a>9.3.4 GBDT算法的简单代码实现</h3><p>1.GBDT分类模型演示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">model = GradientBoostingClassifier(random_state=<span class="number">123</span>)</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[0]
</code></pre>
<p>2.GBDT回归模型演示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line">y = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">model = GradientBoostingRegressor(random_state=<span class="number">123</span>)</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[2.54908866]
</code></pre>
<h2 id="9-4-GBDT案例实战-产品定价模型"><a href="#9-4-GBDT案例实战-产品定价模型" class="headerlink" title="9.4 GBDT案例实战 - 产品定价模型"></a>9.4 GBDT案例实战 - 产品定价模型</h2><h3 id="9-4-1-案例背景"><a href="#9-4-1-案例背景" class="headerlink" title="9.4.1 案例背景"></a>9.4.1 案例背景</h3><p>根据图书页数，纸张，类别，内容，作者及读者等因素对图书进行定价，该产品定价模型也可以用于其他领域的产品定价，如金融产品的定价</p>
<h3 id="9-4-2-模型搭建"><a href="#9-4-2-模型搭建" class="headerlink" title="9.4.2 模型搭建"></a>9.4.2 模型搭建</h3><p>1.读取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;产品定价模型.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>页数</th>
      <th>类别</th>
      <th>彩印</th>
      <th>纸张</th>
      <th>价格</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>207</td>
      <td>技术类</td>
      <td>0</td>
      <td>双胶纸</td>
      <td>60</td>
    </tr>
    <tr>
      <th>1</th>
      <td>210</td>
      <td>技术类</td>
      <td>0</td>
      <td>双胶纸</td>
      <td>62</td>
    </tr>
    <tr>
      <th>2</th>
      <td>206</td>
      <td>技术类</td>
      <td>0</td>
      <td>双胶纸</td>
      <td>62</td>
    </tr>
    <tr>
      <th>3</th>
      <td>218</td>
      <td>技术类</td>
      <td>0</td>
      <td>双胶纸</td>
      <td>64</td>
    </tr>
    <tr>
      <th>4</th>
      <td>209</td>
      <td>技术类</td>
      <td>0</td>
      <td>双胶纸</td>
      <td>60</td>
    </tr>
  </tbody>
</table>
</div>



<p>类别包技术类，教辅类，办公类3种；纸张包含双胶纸，铜版纸，书写纸3种</p>
<p>用value_counts查看各个分类的数据量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;类别&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure>




<pre><code>技术类    336
教辅类    333
办公类    331
Name: 类别, dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;彩印&#x27;</span>].value_counts() <span class="comment"># 0代表黑白印刷，1为彩色印刷</span></span><br></pre></td></tr></table></figure>




<pre><code>0    648
1    352
Name: 彩印, dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;纸张&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure>




<pre><code>双胶纸    615
铜版纸    196
书写纸    189
Name: 纸张, dtype: int64
</code></pre>
<p>2.分类型文本变量处理</p>
<p>因为“类别”和“纸张”两列是分类型文本变量，所以可以用LabelEncoder（）函数进行数值化处理，便于后续进行拟合，关于LabelEncoder（）函数将在第11章详细地讲解</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">le = LabelEncoder()</span><br><span class="line">df[<span class="string">&#x27;类别&#x27;</span>] = le.fit_transform(df[<span class="string">&#x27;类别&#x27;</span>])  <span class="comment"># 处理类别</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将类别一列处理后，我们可以使用value_counts()方法查看转化效果：</span></span><br><span class="line">df[<span class="string">&#x27;类别&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure>




<pre><code>1    336
2    333
0    331
Name: 类别, dtype: int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面我们使用同样的方法处理“纸张”一列：</span></span><br><span class="line">le = LabelEncoder()</span><br><span class="line">df[<span class="string">&#x27;纸张&#x27;</span>] = le.fit_transform(df[<span class="string">&#x27;纸张&#x27;</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时的表格如下：</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>页数</th>
      <th>类别</th>
      <th>彩印</th>
      <th>纸张</th>
      <th>价格</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>207</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>60</td>
    </tr>
    <tr>
      <th>1</th>
      <td>210</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>62</td>
    </tr>
    <tr>
      <th>2</th>
      <td>206</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>62</td>
    </tr>
    <tr>
      <th>3</th>
      <td>218</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>64</td>
    </tr>
    <tr>
      <th>4</th>
      <td>209</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>60</td>
    </tr>
  </tbody>
</table>
</div>



<p>3.提取特征变量和目标变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df.drop(columns=<span class="string">&#x27;价格&#x27;</span>) </span><br><span class="line">y = df[<span class="string">&#x27;价格&#x27;</span>]  </span><br></pre></td></tr></table></figure>

<p>4.划分训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br></pre></td></tr></table></figure>

<p>5.模型训练及搭建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line">model = GradientBoostingRegressor(random_state=<span class="number">123</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>GradientBoostingRegressor(random_state=123)
</code></pre>
<h3 id="9-4-3-模型预测即评估"><a href="#9-4-3-模型预测即评估" class="headerlink" title="9.4.3 模型预测即评估"></a>9.4.3 模型预测即评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型搭建完毕后，通过如下代码预测测试集数据：</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred[<span class="number">0</span>:<span class="number">50</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[ 71.15004038  79.56199921  68.21751792  90.78788507  78.88479128
  42.28022702  39.27334177  60.74670841  53.59744659  77.65931771
  80.22295545  76.04437155  79.56199921  58.40372895  79.65245266
  44.27997693  53.18177447  35.31452467  92.1798291   58.40372895
  41.96644278  99.50466356  80.22295545  79.69648341  91.45061741
  42.93885741  42.86973046  75.71824996  48.55203652  62.94185778
  39.47077874  61.54190648  95.18389309  51.88118394  65.1293139
  50.17577837  39.54495179  83.63542315  56.24632221 102.1176112
  48.89080247  49.23639342  33.03502962  52.74862135  35.47220867
  35.00370671  53.9446399   74.62364353  35.31452467  53.9446399 ]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过和之前章节类似的代码，我们可以将预测值和实际值进行对比：</span></span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>71.150040</td>
      <td>75</td>
    </tr>
    <tr>
      <th>1</th>
      <td>79.561999</td>
      <td>84</td>
    </tr>
    <tr>
      <th>2</th>
      <td>68.217518</td>
      <td>68</td>
    </tr>
    <tr>
      <th>3</th>
      <td>90.787885</td>
      <td>90</td>
    </tr>
    <tr>
      <th>4</th>
      <td>78.884791</td>
      <td>85</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看预测评分 - 方法1：自带的score函数，本质就是R-squared值（也即统计学中常说的R^2)</span></span><br><span class="line">model.score(X_test, y_test)</span><br></pre></td></tr></table></figure>




<pre><code>0.8741691363311168
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看预测评分 - 方法2：r2_score()函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2 = r2_score(y_test, model.predict(X_test))</span><br><span class="line"><span class="built_in">print</span>(r2)</span><br></pre></td></tr></table></figure>

<pre><code>0.8741691363311168
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看特征重要性</span></span><br><span class="line">model.feature_importances_</span><br></pre></td></tr></table></figure>




<pre><code>array([0.49070203, 0.44718694, 0.04161545, 0.02049558])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过DataFrame的方式展示特征重要性</span></span><br><span class="line">features = X.columns  <span class="comment"># 获取特征名称</span></span><br><span class="line">importances = model.feature_importances_  <span class="comment"># 获取特征重要性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过二维表格形式显示</span></span><br><span class="line">importances_df = pd.DataFrame()</span><br><span class="line">importances_df[<span class="string">&#x27;特征名称&#x27;</span>] = features</span><br><span class="line">importances_df[<span class="string">&#x27;特征重要性&#x27;</span>] = importances</span><br><span class="line">importances_df.sort_values(<span class="string">&#x27;特征重要性&#x27;</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>特征名称</th>
      <th>特征重要性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>页数</td>
      <td>0.490702</td>
    </tr>
    <tr>
      <th>1</th>
      <td>类别</td>
      <td>0.447187</td>
    </tr>
    <tr>
      <th>2</th>
      <td>彩印</td>
      <td>0.041615</td>
    </tr>
    <tr>
      <th>3</th>
      <td>纸张</td>
      <td>0.020496</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以看到，页数和类别重要性较高</p>
<h3 id="9-4-4-模型参数介绍"><a href="#9-4-4-模型参数介绍" class="headerlink" title="9.4.4 模型参数介绍"></a>9.4.4 模型参数介绍</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回归模型，通过如下代码可以查看官方介绍</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line">GradientBoostingRegressor?</span><br></pre></td></tr></table></figure>

<p><img src="%E4%B8%8B%E8%BD%BD%20(75).png" alt="下载 (75)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(76).png" alt="下载 (76)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(77).png" alt="下载 (77)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分类模型，通过如下代码可以查看官方介绍</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">GradientBoostingClassifier?</span><br></pre></td></tr></table></figure>

<p>GBDT分类模型的常用参数和GDBT回归模型基本一致，唯一不同的是多了一个loss参数</p>
<p>loss参数：损失函数：取值范围为{‘deviance’,’exponential’}，其中’deviance’为对数损失函数，’exponential’为指数损失函数。默认为’deviance’</p>
<p><strong>补充：损失函数</strong></p>
<p>为了判断模型拟合的效果，我们会用损失函数（Loss Function）来衡量拟合程度，其实质就是根据实际值和预测值间的距离评估模型好坏。常用的损失函数$L(y,f(x))$见下表<br><img src="%E4%B8%8B%E8%BD%BD%20(78).png" alt="下载 (78)"><br>针对离散变量（即分类问题分析），常用0-1损失函数和指数损失函数；针对连续型变量（即回归问题分析)，常用L1范数损失函数和L2范数损失函数。损失函数越小，代表模型拟合的效果越好。下面对损失函数进行一些解释说明：</p>
<p>当y是离散型变量时，对应的问题都是分类问题：</p>
<p>1.实际值y和预测值f(x)都规定取±1</p>
<p>2.假设用1表示正类，-1表示反类，那么yf(x)&gt;0代表预测结果和真实结果一致，因而损失函数低；反之yf(x)&lt;0代表预测结果和真实结果不一致，因而损失函数高</p>
<p>3.以指数损失函数为例，如果实际值y和预测值f(x)一致，例如都为-1，那么此时损失函数为exp(-1x-1x-1)&#x3D;exp(-1)，小于两者不一致时的损失函数exp(1)。</p>
<p>当y时连续型变量时，对应的问题都是回归问题</p>
<p>1.实际值y和预测值f(x)可以取任实数</p>
<p>2.损失函数里面都有|y-f(x)|这一项，而且损失函数是它的增函数，所以当实际值y和预测值f(x)之间差别越大，损失函数越高，反之则越低</p>
<p>3.举个极端的例子，倘若预测值f(x)和实际值y完全一致，那么此时的损失函数为0，也是损失函数最低的情况</p>
<p>理解了损失函数，也就可以理解风险函数。风险函数可以认为是平均意义下的损失，又称为期望函数（expected loss）,其表达式如下：<br><img src="%E4%B8%8B%E8%BD%BD%20(79).png" alt="下载 (79)"><br>其中N为总样本数，yi是样本i的实际值，f(xi)是样本i的预测值</p>
<p>但损失函数也并非越小越好，若损失函数过小，容易出现过拟合。为了避免这个问题，可以在损失函数种加入正则化项或惩罚项，表达式如下：</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(80).png" alt="下载 (80)"></p>
<p>其中λ是正则化系数，J(f)是正则化项，了解即可</p>
<h1 id="10-机器学习神器：XGBoost与LightGBM算法"><a href="#10-机器学习神器：XGBoost与LightGBM算法" class="headerlink" title="10 机器学习神器：XGBoost与LightGBM算法"></a>10 机器学习神器：XGBoost与LightGBM算法</h1><p>这两种算法运行速度快，预测准确率高，且<strong>支持并行操作</strong>，极大地提升了机器学习的效率和效果，无论分类还是回归</p>
<h2 id="10-1-XGBoost算法原理"><a href="#10-1-XGBoost算法原理" class="headerlink" title="10.1 XGBoost算法原理"></a>10.1 XGBoost算法原理</h2><h3 id="10-1-1XGBoost算法的核心思想"><a href="#10-1-1XGBoost算法的核心思想" class="headerlink" title="10.1. 1XGBoost算法的核心思想"></a>10.1. 1XGBoost算法的核心思想</h3><p>最好方法是看文档：<a target="_blank" rel="noopener" href="https://xgboost.readthedocs.io/">https://xgboost.readthedocs.io</a></p>
<p>XGBoost算法在某种程度上可以说是GDBT算法的改良版，两者在本质上都是利用了Boosting算法种拟合残差的思想，下图所示为9.3.1小节讲解GDBT算法时提到的信用卡信用额度预测模型，其中初始决策树的预测结果不完全准确，会产生一些残差，因此会用新的决策树来拟合该残差，新的决策树又会产生新的残差，这时再构造新的决策树来拟合新的残差…如此迭代下去，直至符合预先设定的条件为止<br><img src="%E4%B8%8B%E8%BD%BD%20(81).png" alt="下载 (81)"><br>既然XGBoost算法的核心思想与GBDT算法一样，那么其优势又是什么呢？就是下一小节的内容</p>
<h3 id="10-1-2-XGBoost算法的数学原理概述"><a href="#10-1-2-XGBoost算法的数学原理概述" class="headerlink" title="10.1.2 XGBoost算法的数学原理概述"></a>10.1.2 XGBoost算法的数学原理概述</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(82).png" alt="下载 (82)"></p>
<h3 id="10-1-3-XGBoost算法的简单代码实现"><a href="#10-1-3-XGBoost算法的简单代码实现" class="headerlink" title="10.1.3 XGBoost算法的简单代码实现"></a>10.1.3 XGBoost算法的简单代码实现</h3><p>XGBoost模型既可以做分类分析，也可以做回归分析，分别对应的模型为XGBoost分类模型（XGBClassifier）及XGBoost回归模型（XGBRegressor）。</p>
<p>1.分类模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># XGBoost分类模型的引入方式：</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在Jupyter Notebook编辑器中，在引入该库后，可以通过如下代码获取官方讲解内容（需取消注释）：</span></span><br><span class="line">XGBClassifier?</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># XGBoost分类模型简单代码演示如下所示：</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]])  </span><br><span class="line"><span class="comment"># 2020年升级后必须是numpy或者DataFrame格式</span></span><br><span class="line"><span class="comment"># XGBoost分类模型的特征变量只支持array数组类型或DataFrame二维表格类型的数据</span></span><br><span class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">model = XGBClassifier()</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict(np.array([[<span class="number">5</span>, <span class="number">5</span>]])))</span><br></pre></td></tr></table></figure>

<pre><code>[0]
</code></pre>
<p>其中X是特征变量，其共有2个特征；y是目标变量；第4行代码使用array数组类型的数据做演示，因为XGBoost分类模型的特征变量不支持直接输入list列表类型的数据，可以传入array数组格式的数据或者DataFrame二维表格格式的数据；第7行引入模型；第8行通过fit()函数训练模型；最后1行通过predict()函数进行预测。</p>
<p>2.回归模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># XGBoost回归模型的引入方式：</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在Jupyter Notebook编辑器中，在引入该库后，可以通过如下代码获取官方讲解内容（需取消注释）：</span></span><br><span class="line">XGBRegressor?</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># XGBoost回归模型简单代码演示如下所示：</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">y = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">model = XGBRegressor()</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict(np.array([[<span class="number">5</span>, <span class="number">5</span>]])))</span><br></pre></td></tr></table></figure>

<pre><code>[3.0000014]
</code></pre>
<p>其中X是特征变量，其共有2个特征；y是目标变量；第5行引入模型；第6行通过fit()函数训练模型；最后1行通过predict()函数进行预测。</p>
<h2 id="10-2-XGBoost算法案例实战1：金融反欺诈模型"><a href="#10-2-XGBoost算法案例实战1：金融反欺诈模型" class="headerlink" title="10.2 XGBoost算法案例实战1：金融反欺诈模型"></a>10.2 XGBoost算法案例实战1：金融反欺诈模型</h2><h3 id="10-2-1-案例背景"><a href="#10-2-1-案例背景" class="headerlink" title="10.2.1 案例背景"></a>10.2.1 案例背景</h3><p>信用卡盗刷一般发生在持卡人信息被不法分子窃取后复制卡片进行消费或信用卡被他人冒领后激活并消费等情况下。一旦发生信用卡盗刷，持卡人和银行都会遭到一定的经济损失。因此，通过大数据技术搭建金融反欺诈模型对银行来说尤为重要</p>
<h3 id="10-2-2-模型搭建"><a href="#10-2-2-模型搭建" class="headerlink" title="10.2.2 模型搭建"></a>10.2.2 模型搭建</h3><p>1.读取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;信用卡交易数据.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>换设备次数</th>
      <th>支付失败次数</th>
      <th>换IP次数</th>
      <th>换IP国次数</th>
      <th>交易金额</th>
      <th>欺诈标签</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>11</td>
      <td>3</td>
      <td>5</td>
      <td>28836</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>6</td>
      <td>1</td>
      <td>4</td>
      <td>21966</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>18199</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5</td>
      <td>8</td>
      <td>2</td>
      <td>2</td>
      <td>24803</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7</td>
      <td>10</td>
      <td>5</td>
      <td>0</td>
      <td>26277</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>1代表欺诈，0代表正常交易，本次欺诈样本和非欺诈样本数量之比为4：6</p>
<p>2.提取特征变量和目标变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下代码将特征变量和目标变量单独提取出来，代码如下：</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;欺诈标签&#x27;</span>) </span><br><span class="line">y = df[<span class="string">&#x27;欺诈标签&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>3.划分训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取完特征变量后，通过如下代码将数据拆分为训练集及测试集：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br></pre></td></tr></table></figure>

<p>4.模型训练及搭建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分为训练集和测试集之后，就可以引入XGBoost分类器进行模型训练了，代码如下：</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line">clf = XGBClassifier(n_estimators=<span class="number">100</span>, learning_rate=<span class="number">0.05</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.05, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=None, num_parallel_tree=None,
              predictor=None, random_state=None, ...)
</code></pre>
<p>设置弱学习器的最大迭代次数，或者说弱学习器的个数n_estimators参数为100，以及弱学习器的权重缩减系数learning_rate为0.05.其余参数都使用默认值</p>
<h3 id="10-2-3-模型预测及评估"><a href="#10-2-3-模型预测及评估" class="headerlink" title="10.2.3 模型预测及评估"></a>10.2.3 模型预测及评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型搭建完毕后，通过如下代码预测测试集数据：</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">y_pred  <span class="comment"># 打印预测结果</span></span><br></pre></td></tr></table></figure>




<pre><code>array([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
       1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
       0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
       1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
       1, 1])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过和之前章节类似的代码，我们可以将预测值和实际值进行对比：</span></span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以看到此时前五项的预测准确度为60%，如果想看所有测试集数据的预测准确度，可以使用如下代码：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line">score</span><br></pre></td></tr></table></figure>




<pre><code>0.875
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们还可以通过XGBClassifier()自带的score()函数来查看模型预测的准确度评分，代码如下，获得的结果同样是0.875。</span></span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>




<pre><code>0.875
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># XGBClassifier分类器本质预测的并不是准确的0或1的分类，而是预测其属于某一分类的概率，可以通过predict_proba()函数查看预测属于各个分类的概率，代码如下：</span></span><br><span class="line">y_pred_proba = clf.predict_proba(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred_proba[<span class="number">0</span>:<span class="number">5</span>])  <span class="comment"># 查看前5个预测的概率</span></span><br></pre></td></tr></table></figure>

<pre><code>[[0.8265032  0.1734968 ]
 [0.02098632 0.9790137 ]
 [0.0084281  0.9915719 ]
 [0.8999369  0.1000631 ]
 [0.8290514  0.17094862]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时的y_pred_proba是个二维数组，其中第一列为分类为0（也即非欺诈）的概率，第二列为分类为1（也即欺诈）的概率，因此如果想查看欺诈（分类为1）的概率，可采用如下代码：</span></span><br><span class="line">y_pred_proba[:,<span class="number">1</span>]  <span class="comment"># 分类为1的概率</span></span><br></pre></td></tr></table></figure>




<pre><code>array([0.1734968 , 0.9790137 , 0.9915719 , 0.1000631 , 0.17094862,
       0.05910175, 0.9910392 , 0.09681219, 0.13342692, 0.15539762,
       0.83505297, 0.9910547 , 0.98652124, 0.9880756 , 0.9861544 ,
       0.13701914, 0.15566298, 0.24118835, 0.0716177 , 0.0971779 ,
       0.9872772 , 0.99113035, 0.99113035, 0.20501429, 0.12557584,
       0.12667589, 0.08110589, 0.10515223, 0.10336555, 0.98234504,
       0.11010769, 0.97932315, 0.2966702 , 0.98589283, 0.08400521,
       0.12474764, 0.9780611 , 0.9910547 , 0.839576  , 0.09878331,
       0.17097403, 0.9848075 , 0.17696406, 0.14638832, 0.11671831,
       0.09173213, 0.99113035, 0.08742005, 0.12493847, 0.09407753,
       0.9915719 , 0.15416545, 0.12085281, 0.09074759, 0.99113035,
       0.98910844, 0.07672593, 0.9886723 , 0.11257854, 0.08097712,
       0.11765514, 0.08194721, 0.9873984 , 0.13621715, 0.9903751 ,
       0.09413964, 0.09919545, 0.10474715, 0.06866676, 0.87525785,
       0.9888136 , 0.14974338, 0.11260021, 0.08011292, 0.18638378,
       0.1000631 , 0.07405062, 0.37492937, 0.9857265 , 0.14568247,
       0.09535199, 0.1639911 , 0.07690524, 0.20032772, 0.12399109,
       0.9886723 , 0.10535567, 0.11040998, 0.05972508, 0.9798737 ,
       0.14297816, 0.07451618, 0.10238884, 0.9870812 , 0.99168867,
       0.14201386, 0.10069738, 0.18759497, 0.13318208, 0.1685153 ,
       0.07376712, 0.14112188, 0.11628785, 0.4781888 , 0.09823318,
       0.1437491 , 0.98877805, 0.12747341, 0.12959816, 0.98555124,
       0.07324953, 0.11038433, 0.988771  , 0.10186808, 0.9888966 ,
       0.18757948, 0.13055463, 0.12775792, 0.9915719 , 0.10839844,
       0.21509002, 0.13022642, 0.12395982, 0.07925734, 0.09806271,
       0.97831273, 0.15548192, 0.0955901 , 0.9870221 , 0.9900383 ,
       0.12609024, 0.09053145, 0.07040057, 0.12883253, 0.15853025,
       0.125912  , 0.06919833, 0.99107367, 0.13062388, 0.10572928,
       0.9905813 , 0.07501796, 0.1280026 , 0.8297901 , 0.98045105,
       0.08528069, 0.12226298, 0.98722947, 0.11985523, 0.988342  ,
       0.06767909, 0.0841333 , 0.1600859 , 0.089141  , 0.9910547 ,
       0.1915876 , 0.9905813 , 0.8908516 , 0.9910547 , 0.10125441,
       0.98804003, 0.10338342, 0.8437707 , 0.97890466, 0.98632205,
       0.08181637, 0.08730008, 0.3435277 , 0.10980003, 0.10406717,
       0.15214603, 0.17876908, 0.08273794, 0.23869638, 0.08800983,
       0.08689297, 0.1847994 , 0.98582274, 0.06856089, 0.13558023,
       0.08308174, 0.9865254 , 0.20626086, 0.15845102, 0.8526961 ,
       0.08812908, 0.2054528 , 0.9863707 , 0.10820945, 0.12677956,
       0.9894076 , 0.2759298 , 0.19432847, 0.13334355, 0.08436833,
       0.12110002, 0.9886723 , 0.09794194, 0.9880642 , 0.9905813 ],
      dtype=float32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面我们利用4.3节相关代码绘制ROC曲线来评估模型预测的效果：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(fpr, tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_499_0.png" alt="output_499_0"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下代码求出模型的AUC值：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">score = roc_auc_score(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">score</span><br></pre></td></tr></table></figure>




<pre><code>0.8684772657918828
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们可以通过查看各个特征的特征重要性(feature importance)来得出信用卡欺诈行为判断中最重要的特征变量：</span></span><br><span class="line">clf.feature_importances_</span><br></pre></td></tr></table></figure>




<pre><code>array([0.40674362, 0.19018467, 0.04100984, 0.33347663, 0.02858528],
      dtype=float32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下5.2.2节特征重要性相关知识点进行整理，方便结果呈现，代码如下：</span></span><br><span class="line">features = X.columns  <span class="comment"># 获取特征名称</span></span><br><span class="line">importances = clf.feature_importances_  <span class="comment"># 获取特征重要性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过二维表格形式显示</span></span><br><span class="line">importances_df = pd.DataFrame()</span><br><span class="line">importances_df[<span class="string">&#x27;特征名称&#x27;</span>] = features</span><br><span class="line">importances_df[<span class="string">&#x27;特征重要性&#x27;</span>] = importances</span><br><span class="line">importances_df.sort_values(<span class="string">&#x27;特征重要性&#x27;</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>特征名称</th>
      <th>特征重要性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>换设备次数</td>
      <td>0.406744</td>
    </tr>
    <tr>
      <th>3</th>
      <td>换IP国次数</td>
      <td>0.333477</td>
    </tr>
    <tr>
      <th>1</th>
      <td>支付失败次数</td>
      <td>0.190185</td>
    </tr>
    <tr>
      <th>2</th>
      <td>换IP次数</td>
      <td>0.041010</td>
    </tr>
    <tr>
      <th>4</th>
      <td>交易金额</td>
      <td>0.028585</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="10-2-4-模型参数调优"><a href="#10-2-4-模型参数调优" class="headerlink" title="10.2.4 模型参数调优"></a>10.2.4 模型参数调优</h3><p>本小节要对前面搭建的XGBoost分类模型进行参数调优。这里选取以下参数进行调优：</p>
<p><strong>max_depth</strong>：弱学习器决策树的最大深度，默认取3</p>
<p><strong>n-estimators</strong>：弱学习器的个数，或者说弱学习器的最大迭代次数，默认取100</p>
<p><strong>learning_rate</strong>：学习率，又称为每个弱学习器的权重缩减系数，取值范围为(0,1]，取值较小意味着要达到一定的学习效果，需要更多迭代次数和更多弱学习器，默认值取0.1，通常用n_estimators和learning_rate一起决定算法的拟合效果，所以这两个参数要一起调优</p>
<hr>
<p>使用GridSearch网格搜索进行参数调优</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV  </span><br><span class="line">parameters = &#123;<span class="string">&#x27;max_depth&#x27;</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">50</span>, <span class="number">100</span>, <span class="number">150</span>], <span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]&#125;  <span class="comment"># 指定模型中参数的范围</span></span><br><span class="line">clf = XGBClassifier()  <span class="comment"># 构建模型</span></span><br><span class="line">grid_search = GridSearchCV(clf, parameters, scoring=<span class="string">&#x27;roc_auc&#x27;</span>, cv=<span class="number">5</span>) <span class="comment"># 以ROC曲线的AUC值作为模型评估标准，并设置cv参数为5，表示交叉验证5次  </span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面我们将数据传入网格搜索模型并输出参数最优值：</span></span><br><span class="line">grid_search.fit(X_train, y_train)  <span class="comment"># 传入数据</span></span><br><span class="line">grid_search.best_params_  <span class="comment"># 输出参数的最优值</span></span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;learning_rate&#39;: 0.05, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 100&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面我们根据新的参数建模，首先重新搭建XGBoost分类器，并将训练集数据传入其中：</span></span><br><span class="line">clf = XGBClassifier(max_depth=<span class="number">1</span>, n_estimators=<span class="number">100</span>, learning_rate=<span class="number">0.05</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.05, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=1, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=None, num_parallel_tree=None,
              predictor=None, random_state=None, ...)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为我们是通过ROC曲线的AUC评分作为模型评价准则来进行参数调优的，因此通过如下代码我们来查看新的AUC值：</span></span><br><span class="line">y_pred_proba = clf.predict_proba(X_test)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">score = roc_auc_score(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.8563218390804598
</code></pre>
<p>将获得的AUC评分打印出来为：0.856，比原来没有调参前的0.866还略微低了些，有的读者可能会奇怪为什么调参后的结果还不如未调参时的结果，通常来说参数调优出现这种情况的概率较小（本案例只有1000条数据），这里出现了也正好给大家解释下出现这种情况的原因。</p>
<p>出现这种情况的原因是因为<strong>交叉验证</strong>，我们来简单回顾下5.3节K折交叉验证的思路：它是将原来的测试数据分为K份（这里cv&#x3D;5，即5份），然后在这K份数据中，选K-1份作为训练数据，剩下的1份作为测试数据，训练K次，获得K个的ROC曲线下的AUC值，然后将K个AUC值取平均，取AUC值的均值为最大情况下的参数为模型的最优参数。注意这里AUC值的获取是基于训练集数据，只不过是将训练集数据中的1&#x2F;K作为测试集数据，这里的测试集数据并不是真正的测试集数据y_test，这也是为什么参数调优后结果反而不如不调优的结果的原因。实际应用中，通常不太会出现调参结果不如不调参的结果，出现这种情况某种程度也是因为数据量较小的原因（像本案例为1000条数据）。其他关于参数调优的其他一些注意点请参考5.3节，这里不再赘述。</p>
<p><strong>补充：XGBoost分类模型的常见超参数</strong><br><img src="%E4%B8%8B%E8%BD%BD%20(83).png" alt="下载 (83)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(84).png" alt="下载 (84)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(85).png" alt="下载 (85)"></p>
<p>至于XGBoost回归模型，大多数参数和上表中列举的XGBoost分类模型的参数一致，略微不同的是，objective参数在回归问题中取值为’reg:linear’,而在二分类问题中一般取值为’binary:logistic’，在多分类问题中一般取值为’multi:softmax’。不过当选择XGBoost回归模型时，该参数会自动切换，所以也无须手动调整</p>
<h2 id="10-3-XGBoost算法案例实战2-信用评分模型"><a href="#10-3-XGBoost算法案例实战2-信用评分模型" class="headerlink" title="10.3 XGBoost算法案例实战2 - 信用评分模型"></a>10.3 XGBoost算法案例实战2 - 信用评分模型</h2><p>本节用XGBoost回归模型搭建一个信用评分卡模型，为了凸显该算法的优越性，还将额外运用多元线性回归模型和GBDT回归模型进行模型搭建，并对比3种模型的效果</p>
<h3 id="10-3-1-案例背景"><a href="#10-3-1-案例背景" class="headerlink" title="10.3.1 案例背景"></a>10.3.1 案例背景</h3><p>为降低不良贷款率，保障自身资金安全，提高风险控制水平，银行等金融机构会根据客户的信用历史资料构建信用评分卡模型给客户评分。根据客户的信用得分，可以预估客户按时还款的可能性，并据此决定是否发放贷款及贷款的额度和利率</p>
<h3 id="10-3-2-多元线性回归模型"><a href="#10-3-2-多元线性回归模型" class="headerlink" title="10.3.2 多元线性回归模型"></a>10.3.2 多元线性回归模型</h3><p>1.读取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;信用评分卡模型.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>月收入</th>
      <th>年龄</th>
      <th>性别</th>
      <th>历史授信额度</th>
      <th>历史违约次数</th>
      <th>信用评分</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7783</td>
      <td>29</td>
      <td>0</td>
      <td>32274</td>
      <td>3</td>
      <td>73</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7836</td>
      <td>40</td>
      <td>1</td>
      <td>6681</td>
      <td>4</td>
      <td>72</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6398</td>
      <td>25</td>
      <td>0</td>
      <td>26038</td>
      <td>2</td>
      <td>74</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6483</td>
      <td>23</td>
      <td>1</td>
      <td>24584</td>
      <td>4</td>
      <td>65</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5167</td>
      <td>23</td>
      <td>1</td>
      <td>6710</td>
      <td>3</td>
      <td>73</td>
    </tr>
  </tbody>
</table>
</div>



<p>共1000条数据，目标变量是信用评分（取值范围是0~100）</p>
<p>2.提取特征变量和目标变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下代码将特征变量和目标变量单独提取出来，代码如下：</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;信用评分&#x27;</span>)</span><br><span class="line">Y = df[<span class="string">&#x27;信用评分&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>3.模型训练及搭建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从Scikit-Learn库中引入LinearRegression()模型进行模型训练，代码如下：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X,Y)</span><br></pre></td></tr></table></figure>




<pre><code>LinearRegression()
</code></pre>
<p>4.线性回归方程构造</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.线性回归方程构造</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;各系数为:&#x27;</span> + <span class="built_in">str</span>(model.coef_))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;常数项系数k0为:&#x27;</span> + <span class="built_in">str</span>(model.intercept_))</span><br></pre></td></tr></table></figure>

<pre><code>各系数为:[ 5.58658996e-04  1.62842002e-01  2.18430276e-01  6.69996665e-05
 -1.51063940e+00]
常数项系数k0为:67.1668660385318
</code></pre>
<p>5.模型评估</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用3.2节模型评估的方法对此多元线性回归模型进行评估，代码如下：</span></span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line">X2 = sm.add_constant(X)</span><br><span class="line">est = sm.OLS(Y, X2).fit()</span><br><span class="line">est.summary()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;frozen importlib._bootstrap&gt;:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject
</code></pre>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>信用评分</td>       <th>  R-squared:         </th> <td>   0.629</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.628</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   337.6</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 18 Jan 2023</td> <th>  Prob (F-statistic):</th> <td>2.32e-211</td>
</tr>
<tr>
  <th>Time:</th>                 <td>21:44:33</td>     <th>  Log-Likelihood:    </th> <td> -2969.8</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>   5952.</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   994</td>      <th>  BIC:               </th> <td>   5981.</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>  <td>   67.1669</td> <td>    1.121</td> <td>   59.906</td> <td> 0.000</td> <td>   64.967</td> <td>   69.367</td>
</tr>
<tr>
  <th>月收入</th>    <td>    0.0006</td> <td> 8.29e-05</td> <td>    6.735</td> <td> 0.000</td> <td>    0.000</td> <td>    0.001</td>
</tr>
<tr>
  <th>年龄</th>     <td>    0.1628</td> <td>    0.022</td> <td>    7.420</td> <td> 0.000</td> <td>    0.120</td> <td>    0.206</td>
</tr>
<tr>
  <th>性别</th>     <td>    0.2184</td> <td>    0.299</td> <td>    0.730</td> <td> 0.466</td> <td>   -0.369</td> <td>    0.806</td>
</tr>
<tr>
  <th>历史授信额度</th> <td>   6.7e-05</td> <td> 7.78e-06</td> <td>    8.609</td> <td> 0.000</td> <td> 5.17e-05</td> <td> 8.23e-05</td>
</tr>
<tr>
  <th>历史违约次数</th> <td>   -1.5106</td> <td>    0.140</td> <td>  -10.811</td> <td> 0.000</td> <td>   -1.785</td> <td>   -1.236</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>13.180</td> <th>  Durbin-Watson:     </th> <td>   1.996</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  12.534</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.236</td> <th>  Prob(JB):          </th> <td> 0.00190</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.721</td> <th>  Cond. No.          </th> <td>4.27e+05</td>
</tr>
</table><br><br>Warnings:<br>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br>[2] The condition number is large, 4.27e+05. This might indicate that there are<br>strong multicollinearity or other numerical problems.


<p><img src="%E4%B8%8B%E8%BD%BD%20(86).png" alt="下载 (86)"><br>这两个数不高，整体拟合效果一般，可能是因为数据量偏少</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(87).png" alt="下载 (87)"><br>再观察P值，可发现大部分特征变量的P值都很小（小于0，05)，的确是和目标变量“信用评分”显著相关的，而特征变量“性别”的P值达到了0.466，说明该特征变量与目标变量没有显著相关性。因此在多元线性回归模型中可以舍去“性别”这一特征变量</p>
<h3 id="10-3-3-GXDT回归模型"><a href="#10-3-3-GXDT回归模型" class="headerlink" title="10.3.3 GXDT回归模型"></a>10.3.3 GXDT回归模型</h3><p>1.读取数据并提取特征变量和目标变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里使用第九章讲过的GBDT回归模型同样来做一下回归分析，首先读取1000条信用卡客户的数据并划分特征变量和目标变量，这部分代码和上面线性回归的代码是一样的。</span></span><br><span class="line"><span class="comment"># 1.读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;信用评分卡模型.xlsx&#x27;</span>)</span><br><span class="line"><span class="comment"># 2.提取特征变量和目标变量</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;信用评分&#x27;</span>)</span><br><span class="line">y = df[<span class="string">&#x27;信用评分&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>2.划分训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下代码划分训练集和测试集数据：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br></pre></td></tr></table></figure>

<p>3.模型训练及搭建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分训练集和测试集完成后，就可以从Scikit-Learn库中引入GBDT模型进行模型训练了，代码如下：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line">model = GradientBoostingRegressor()  <span class="comment"># 使用默认参数</span></span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>GradientBoostingRegressor()
</code></pre>
<p>4.模型预测及评估</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型搭建完毕后，通过如下代码预测测试集数据：</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[70.77631652 71.40032104 73.73465155 84.52533945 71.09188294 84.9327599
 73.72232388 83.44560704 82.61221486 84.86927209]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过和之前章节类似的代码，我们可以将预测值和实际值进行对比：</span></span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>70.776317</td>
      <td>79</td>
    </tr>
    <tr>
      <th>1</th>
      <td>71.400321</td>
      <td>80</td>
    </tr>
    <tr>
      <th>2</th>
      <td>73.734652</td>
      <td>62</td>
    </tr>
    <tr>
      <th>3</th>
      <td>84.525339</td>
      <td>89</td>
    </tr>
    <tr>
      <th>4</th>
      <td>71.091883</td>
      <td>80</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为GradientBoostingRegressor()是一个回归模型，所以我们通过查看其R-squared值来评判模型的拟合效果：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2 = r2_score(y_test, model.predict(X_test))</span><br><span class="line"><span class="built_in">print</span>(r2)</span><br></pre></td></tr></table></figure>

<pre><code>0.6752289995786536
</code></pre>
<p>第1行代码从Scikit-Learn库中引入r2_score()函数；第2行代码将训练集的真实值和模型预测值传入r2_score()函数，得出R-squared评分为0.675，可以看到这个结果较线性回归模型获得的0.629是有所改善的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们还可以通过GradientBoostingRegressor()自带的score()函数来查看模型预测的效果：</span></span><br><span class="line">model.score(X_test, y_test)</span><br></pre></td></tr></table></figure>




<pre><code>0.6752289995786536
</code></pre>
<h3 id="10-3-4-XGBoost回归模型"><a href="#10-3-4-XGBoost回归模型" class="headerlink" title="10.3.4 XGBoost回归模型"></a>10.3.4 XGBoost回归模型</h3><p>1.读取数据、提取变量并划分训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如下所示，其中前3步读取数据，提取特征变量和目标变量，划分训练集和测试集都与GBDT模型相同，因此不再重复，直接从第四步模型开始讲解：</span></span><br><span class="line"><span class="comment"># 1.读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;信用评分卡模型.xlsx&#x27;</span>)</span><br><span class="line"><span class="comment"># 2.提取特征变量和目标变量</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;信用评分&#x27;</span>)</span><br><span class="line">y = df[<span class="string">&#x27;信用评分&#x27;</span>]</span><br><span class="line"><span class="comment"># 3.划分测试集和训练集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br></pre></td></tr></table></figure>

<p>2.模型训练及搭建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分训练集和测试集完成后，就可以从Scikit-Learn库中引入XGBRegressor()模型进行模型训练了，代码如下：</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line">model = XGBRegressor()  <span class="comment"># 使用默认参数</span></span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=None, num_parallel_tree=None,
             predictor=None, random_state=None, ...)
</code></pre>
<p>3.模型预测及评估</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型搭建完毕后，通过如下代码预测测试集数据：</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[74.62306  69.01495  76.393486 83.88998  71.5683   86.257324 76.0784
 81.38994  81.05504  83.24717 ]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过和之前章节类似的代码，我们可以将预测值和实际值进行对比：</span></span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>74.623062</td>
      <td>79</td>
    </tr>
    <tr>
      <th>1</th>
      <td>69.014954</td>
      <td>80</td>
    </tr>
    <tr>
      <th>2</th>
      <td>76.393486</td>
      <td>62</td>
    </tr>
    <tr>
      <th>3</th>
      <td>83.889977</td>
      <td>89</td>
    </tr>
    <tr>
      <th>4</th>
      <td>71.568298</td>
      <td>80</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为XGBRegressor()是一个回归模型，所以通过查看R-squared来评判模型的拟合效果：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2 = r2_score(y_test, model.predict(X_test))</span><br><span class="line"><span class="built_in">print</span>(r2)</span><br></pre></td></tr></table></figure>

<pre><code>0.5715437436791975
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们还可以通过XGBRegressor()自带的score()函数来查看模型预测的效果：</span></span><br><span class="line">model.score(X_test, y_test)</span><br></pre></td></tr></table></figure>




<pre><code>0.5715437436791975
</code></pre>
<p>4.查看特征重要性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过10.2.3节讲过的feature_importances_属性，我们来查看模型的特征重要性：</span></span><br><span class="line">features = X.columns  <span class="comment"># 获取特征名称</span></span><br><span class="line">importances = model.feature_importances_  <span class="comment"># 获取特征重要性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过二维表格形式显示</span></span><br><span class="line">importances_df = pd.DataFrame()</span><br><span class="line">importances_df[<span class="string">&#x27;特征名称&#x27;</span>] = features</span><br><span class="line">importances_df[<span class="string">&#x27;特征重要性&#x27;</span>] = importances</span><br><span class="line">importances_df.sort_values(<span class="string">&#x27;特征重要性&#x27;</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>特征名称</th>
      <th>特征重要性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>月收入</td>
      <td>0.324461</td>
    </tr>
    <tr>
      <th>4</th>
      <td>历史违约次数</td>
      <td>0.307467</td>
    </tr>
    <tr>
      <th>3</th>
      <td>历史授信额度</td>
      <td>0.202864</td>
    </tr>
    <tr>
      <th>1</th>
      <td>年龄</td>
      <td>0.098869</td>
    </tr>
    <tr>
      <th>2</th>
      <td>性别</td>
      <td>0.066339</td>
    </tr>
  </tbody>
</table>
</div>



<p>5.模型参数调优</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过和10.2.4节类似的代码，我们可以对XGBoost回归模型进行参数调优，代码如下：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV  </span><br><span class="line">parameters = &#123;<span class="string">&#x27;max_depth&#x27;</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">50</span>, <span class="number">100</span>, <span class="number">150</span>], <span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]&#125;  <span class="comment"># 指定模型中参数的范围</span></span><br><span class="line">clf = XGBRegressor()  <span class="comment"># 构建回归模型</span></span><br><span class="line">grid_search = GridSearchCV(model, parameters, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>) </span><br></pre></td></tr></table></figure>

<p>这里唯一需要注意的是最后一行代码中的scoring参数需要设置成’r2’，其表示的是R-squared值，因为是回归模型，所以参数调优时应该选择R-squared值来进行评判，而不是分类模型中常用的准确度’accuracy’或者ROC曲线对应的AUC值’roc_auc’。</p>
<p>通过如下代码获取最优参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grid_search.fit(X_train, y_train)  <span class="comment"># 传入数据</span></span><br><span class="line">grid_search.best_params_  <span class="comment"># 输出参数的最优值</span></span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 50&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在模型中设置参数，代码如下：</span></span><br><span class="line">model = XGBRegressor(max_depth=<span class="number">3</span>, n_estimators=<span class="number">50</span>, learning_rate=<span class="number">0.1</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.1, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=3, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=50, n_jobs=None, num_parallel_tree=None,
             predictor=None, random_state=None, ...)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时再通过r2_score()函数进行模型评估，代码如下（也可以用model.score(X_test, y_test)进行评分，效果一样）：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2 = r2_score(y_test, model.predict(X_test))</span><br><span class="line"><span class="built_in">print</span>(r2)</span><br></pre></td></tr></table></figure>

<pre><code>0.6884477645003766
</code></pre>
<p><strong>补充知识点2：对于XGBoost模型，有必要做很多数据预处理吗？</strong></p>
<p>在传统的机器模型中，我们往往需要做挺多的数据预处理，例如数据的归一化、缺失值及异常值的处理等（这些会在第十一章着重讲解），但是对于XGBoost模型而言，很多预处理都是不需要的，例如对于缺失值而言，XGBoost模型会自动处理，它会通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。</p>
<p>此外由于XGBoost是基于决策树模型，因此区别于线性回归等模型，像一些特征变换（例如离散化、归一化或者叫作标准化、取log、共线性问题处理等）都不太需要，这也是树模型的一个优点。如果有的读者还不太放心，可以自己尝试下做一下特征变换，例如数据归一化，会发现最终的结果都是一样的。这里给大家简单示范一下，通过如下代码对数据进行Z-score标准化或者叫作归一化(这部分内容也会在11.3节进行讲解)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">X_new = StandardScaler().fit_transform(X)</span><br><span class="line"></span><br><span class="line">X_new  <span class="comment"># 打印标准化后的数据</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[-0.88269208, -1.04890243, -1.01409939, -0.60873764,  0.63591822],
       [-0.86319167,  0.09630122,  0.98609664, -1.55243002,  1.27956013],
       [-1.39227834, -1.46534013, -1.01409939, -0.83867808, -0.0077237 ],
       ...,
       [ 1.44337605,  0.61684833,  0.98609664,  1.01172301, -0.0077237 ],
       [ 0.63723633, -0.21602705,  0.98609664, -0.32732239, -0.0077237 ],
       [ 1.57656755,  0.61684833, -1.01409939,  1.30047599, -0.0077237 ]])
</code></pre>
<p>利用标准化后的数据进行建模，看看是否有差别：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.划分测试集和训练集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.建模</span></span><br><span class="line"><span class="comment"># 划分训练集和测试集完成后，就可以从Scikit-Learn库中引入XGBRegressor()模型进行模型训练了，代码如下：</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line">model = XGBRegressor()  <span class="comment"># 使用默认参数</span></span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为XGBRegressor()是一个回归模型，所以通过查看R-squared来评判模型的拟合效果：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2 = r2_score(y_test, model.predict(X_test))</span><br><span class="line"><span class="built_in">print</span>(r2)</span><br></pre></td></tr></table></figure>

<pre><code>0.5716150813375576
</code></pre>
<p>此时再对这个X_new通过train_test_split()函数划分测试集和训练集，并进行模型的训练，最后通过r2_score()获得模型评分，会发现结果和没有归一化的数据的结果几乎一样，为0.571。这里也验证了树模型不需要进行特征的归一化或者说标准化，此外树模型对于共线性也不敏感。</p>
<p>通过上面这个演示，也可以得出这么一个读者经常会问到的一个疑问：需不需要进行某种数据预处理？以后如果还有这样的疑问，那么不妨就做一下该数据预处理，如果发现最终结果没有区别，那就能够明白对于该模型不需要做相关数据预处理。<br>当然绝大部分模型都无法自动完成的一步就是特征提取。很多自然语言处理的问题或者图象的问题，没有现成的特征，需要人工去提取这些特征。</p>
<p>综上来说，XGBoost的确比线性模型要省去很多特征工程的步骤，但是特征工程依然是非常必要的，这一结论同样适用于下面即将讲到的LightGBM模型。</p>
<h2 id="10-4-LightGBM算法原理"><a href="#10-4-LightGBM算法原理" class="headerlink" title="10.4 LightGBM算法原理"></a>10.4 LightGBM算法原理</h2><h3 id="10-4-1-LightGBM算法的核心思想"><a href="#10-4-1-LightGBM算法的核心思想" class="headerlink" title="10.4.1 LightGBM算法的核心思想"></a>10.4.1 LightGBM算法的核心思想</h3><p>该算法由微软研发，采用损失函数的负梯度作为当前决策树的残差近似值，去拟合新的决策树，与传统机器学习算法相比，LightGBM具有如下优势：训练效率更高，低内存使用，准确率更高，支持并行化学习，可以处理大规模数据</p>
<h3 id="10-4-2-LightGBM算法的数学原理概述"><a href="#10-4-2-LightGBM算法的数学原理概述" class="headerlink" title="10.4.2 LightGBM算法的数学原理概述"></a>10.4.2 LightGBM算法的数学原理概述</h3><p>最好的方法是看文档：<a target="_blank" rel="noopener" href="https://lightgbm.readthedocs.io/">https://lightgbm.readthedocs.io</a></p>
<p>下面讲解一下LightGBM算法数学原理的核心知识点</p>
<p><strong>1.基于leaf-wise的决策树生长策略</strong></p>
<p>大部分决策树算法使用的生长策略是level-wise生长策略，即同一层的叶子节点每次都一起分裂，如下图所示。但实际上一些叶子节点的分裂增益较低，这样分裂会增加不小的开销<br><img src="%E4%B8%8B%E8%BD%BD%20(88).png" alt="下载 (88)"></p>
<p>LightGBM算法使用的则是leaf-wise生长策略，每次在当前叶子节点中找出分裂增益最大的叶子节点进行分裂，而不是所有节点都进行分裂，如下图所示，这样可以提高精度。但是，keaf-wise策略在样本量较小是容易造成过拟合，LightGBM算法可以通过参数max_depth限制树的深度来防止过拟合<br><img src="%E4%B8%8B%E8%BD%BD%20(89).png" alt="下载 (89)"></p>
<p><strong>2.直方图算法</strong></p>
<p>直方图分为频数直方图和频率直方图，横坐标为相关数据，纵坐标为该数据出现的频数或频率，用hist()函数绘制如下所示的频数直方图<br><img src="%E4%B8%8B%E8%BD%BD%20(90).png" alt="下载 (90)"><br>直方图算法又称为histogram算法，简单来说，就是先对特征值进行<strong>装箱处理</strong>，将连续的浮点特征值离散化成k个整数，形成一个个箱体（bins），同时构造一个宽度为k的直方图，在遍历数据时，以离散化后的值作为索引在直方图中累计统计量（因此这里是频数直方图)。遍历一次数据后，直方图累计了需要的统计量，再根据直方图的离散值遍历寻找最优分割点。</p>
<p>对于连续特征来说，装箱处理就是特征工程中的离散化。例如， 【0，10） 区间的值都赋值为0，【10,20) 区间的值都赋值为1等。这样就可以把众多的数值划分到有限的分箱中。LightGBM算法中默认的分箱数（bins）为256</p>
<p>举例来说，现在有10000个客户，也就有10000个身高数据，将身高分箱为256份后（例如，身高180-180.2cm的所有客户都分箱为数字200），就变为256个数据，这时再统计每个数值对应的频数（身高180~180.2cm的所有客户为100人，那么数字200对应的频数就是100）.这样在节点分裂时，就不需要按照预排序算法对每个特征都计算10000遍，而只需要计算256遍（分箱数），大大加快了训练速度。</p>
<p>对于分类特征来说，则是将每一种取值放入一个分箱（bin），且当取值的个数大于最大分箱数时，会忽略那些很少出现的分类值。例如，10000个客户的国籍数据，便可以按国家名称进行分箱，如果超过最大分箱数（如256），那么很少出现的国家就会被忽略</p>
<p><strong>3.并行学习</strong></p>
<p>LightGBM算法支持<strong>特征并行</strong>和<strong>数据并行</strong>两种学习方式。传统的特征并行的主要思想是在并行化决策树中寻找最佳分割点，在数据量大时难以加速，同时需要对切分结果进行通信整合。而LightGBM算法在本地保存全部数据，这样就没有了机器间通信所需的开销。此外，传统的数据并行时构建本地直方图，然后进行整合，在全局直方图中寻找最佳切分点。LightGBM算法则使用分散规约（reduce scatter），将直方图合并的任务分给不同的机器，降低通信和计算的开销，并利用直方图做加速训练，进一步减少开销。</p>
<p>除了以上原理，LightGBM算法还包含一些重要的算法思想，如单边梯度采样GOSS算法和互斥特征绑定EFB算法。在GOSS算法中，梯度更大的样本点在计算信息增益时会发挥更重要的作用，当对样本进行下采样保留这些梯度较大的样本点，并随机去掉梯度小的样本点。EFB算法则将互斥特征绑在一起以减少特征维度。</p>
<h3 id="10-4-3-LightGBM算法的简单代码实现"><a href="#10-4-3-LightGBM算法的简单代码实现" class="headerlink" title="10.4.3 LightGBM算法的简单代码实现"></a>10.4.3 LightGBM算法的简单代码实现</h3><p>LightGBM模型既可以做分类分析，也可以做回归分析，分别对应的模型为LightGBM分类模型（LGBMClassifier）及LightGBM回归模型（LGBMRegressor）。</p>
<p>1.分类模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LightGBM分类模型简单代码演示如下所示：</span></span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"></span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">model = LGBMClassifier()</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[0]
</code></pre>
<p>其中X是特征变量，其共有2个特征；y是目标变量；第5行引入模型；第6行通过fit()函数训练模型；最后1行通过predict()函数进行预测。</p>
<p>2.回归模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在Jupyter Notebook编辑器中，在引入该库后，可以通过如下代码获取官方讲解内容：（需取消注释）</span></span><br><span class="line">LGBMRegressor?</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LightGBM回归模型简单代码演示如下所示：</span></span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMRegressor</span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line">y = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">model = LGBMRegressor()</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[3.]
</code></pre>
<h2 id="10-5-LightGBM算法案例实战1：客户违约预测模型"><a href="#10-5-LightGBM算法案例实战1：客户违约预测模型" class="headerlink" title="10.5 LightGBM算法案例实战1：客户违约预测模型"></a>10.5 LightGBM算法案例实战1：客户违约预测模型</h2><h3 id="10-5-1-案例背景"><a href="#10-5-1-案例背景" class="headerlink" title="10.5.1 案例背景"></a>10.5.1 案例背景</h3><p>银行等金融机构经常会根据客户的个人资料、财产等情况，来预测借款客户是否会违约，从而进行贷前审核，贷中管理，贷后违约处理等工作。金融处理的就是风险，需要在风险和收益间寻求到一个平衡点，现代金融某种程度上便是一个风险定价的过程，通过个人的海量数据，从而对其进行风险评估并进行合适的借款利率定价，这便是一个典型的风险定价过程，这也被称之为大数据风控。</p>
<h3 id="10-5-2-模型搭建"><a href="#10-5-2-模型搭建" class="headerlink" title="10.5.2 模型搭建"></a>10.5.2 模型搭建</h3><p><strong>1.读取数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;客户信息及违约表现.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>收入</th>
      <th>年龄</th>
      <th>性别</th>
      <th>历史授信额度</th>
      <th>历史违约次数</th>
      <th>是否违约</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>462087</td>
      <td>26</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>362324</td>
      <td>32</td>
      <td>0</td>
      <td>13583</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>332011</td>
      <td>52</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>252895</td>
      <td>39</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>352355</td>
      <td>50</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>读取1000条数据，目标变量为是否违约，若违约标记为1，否则标记为0</p>
<p><strong>2.提取特征变量和目标变量&#x2F;3.划分训练集和测试集&#x2F;4.模型训练和搭建</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.提取特征变量和目标变量</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;是否违约&#x27;</span>)</span><br><span class="line">Y = df[<span class="string">&#x27;是否违约&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.划分训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.模型训练及搭建</span></span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line">model = LGBMClassifier()</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>LGBMClassifier()
</code></pre>
<h3 id="10-5-3-模型预测及评估"><a href="#10-5-3-模型预测及评估" class="headerlink" title="10.5.3 模型预测及评估"></a>10.5.3 模型预测及评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测测试集数据</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure>

<pre><code>[1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0
 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1
 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0
 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1
 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0
 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测值和实际值对比</span></span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line">score</span><br></pre></td></tr></table></figure>




<pre><code>0.78
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看得分</span></span><br><span class="line">model.score(X_test, y_test)</span><br></pre></td></tr></table></figure>




<pre><code>0.78
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看预测属于各个分类的概率</span></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)</span><br><span class="line">y_pred_proba</span><br></pre></td></tr></table></figure>




<pre><code>array([[9.23160061e-04, 9.99076840e-01],
       [8.83139907e-01, 1.16860093e-01],
       [1.02049035e-02, 9.89795097e-01],
       [9.98549401e-01, 1.45059936e-03],
       [1.53030489e-01, 8.46969511e-01],
       [9.47957121e-01, 5.20428789e-02],
       [5.93375667e-04, 9.99406624e-01],
       [9.18125428e-01, 8.18745723e-02],
       [9.90917969e-01, 9.08203059e-03],
       [8.37315637e-01, 1.62684363e-01],
       [1.38299904e-02, 9.86170010e-01],
       [5.55705275e-04, 9.99444295e-01],
       [1.62601027e-01, 8.37398973e-01],
       [5.94276287e-01, 4.05723713e-01],
       [5.81701526e-03, 9.94182985e-01],
       [5.84880363e-01, 4.15119637e-01],
       [2.94447586e-01, 7.05552414e-01],
       [1.67458977e-01, 8.32541023e-01],
       [9.90972585e-01, 9.02741502e-03],
       [9.62402687e-01, 3.75973126e-02],
       [2.44305822e-03, 9.97556942e-01],
       [9.73598346e-01, 2.64016536e-02],
       [3.39364870e-03, 9.96606351e-01],
       [3.35544336e-01, 6.64455664e-01],
       [9.82397600e-01, 1.76024003e-02],
       [9.71460059e-01, 2.85399407e-02],
       [9.58518760e-01, 4.14812403e-02],
       [8.07022448e-03, 9.91929776e-01],
       [9.17226788e-01, 8.27732125e-02],
       [9.94194364e-01, 5.80563600e-03],
       [9.86284029e-01, 1.37159709e-02],
       [1.72645621e-02, 9.82735438e-01],
       [9.68614189e-01, 3.13858108e-02],
       [5.01087538e-03, 9.94989125e-01],
       [9.41226193e-01, 5.87738071e-02],
       [4.39429163e-02, 9.56057084e-01],
       [7.91124725e-01, 2.08875275e-01],
       [1.20421033e-02, 9.87957897e-01],
       [8.45566643e-02, 9.15443336e-01],
       [6.03680861e-01, 3.96319139e-01],
       [8.99893050e-01, 1.00106950e-01],
       [9.63688554e-01, 3.63114460e-02],
       [9.06662333e-01, 9.33376674e-02],
       [2.97418960e-03, 9.97025810e-01],
       [9.24191577e-01, 7.58084227e-02],
       [5.46229390e-01, 4.53770610e-01],
       [9.87438641e-01, 1.25613592e-02],
       [1.72400837e-01, 8.27599163e-01],
       [7.43895857e-01, 2.56104143e-01],
       [2.94236130e-03, 9.97057639e-01],
       [1.40762553e-02, 9.85923745e-01],
       [9.99511631e-01, 4.88368640e-04],
       [9.95614617e-01, 4.38538271e-03],
       [8.60804212e-01, 1.39195788e-01],
       [1.40370241e-03, 9.98596298e-01],
       [9.92192539e-01, 7.80746131e-03],
       [3.53639131e-01, 6.46360869e-01],
       [1.63511083e-03, 9.98364889e-01],
       [8.78219029e-01, 1.21780971e-01],
       [7.42675744e-03, 9.92573243e-01],
       [9.58363126e-01, 4.16368742e-02],
       [7.18876748e-01, 2.81123252e-01],
       [9.27168501e-03, 9.90728315e-01],
       [8.89133794e-01, 1.10866206e-01],
       [2.36389045e-03, 9.97636110e-01],
       [8.11413899e-01, 1.88586101e-01],
       [9.95723783e-01, 4.27621695e-03],
       [8.74791781e-01, 1.25208219e-01],
       [6.27761732e-01, 3.72238268e-01],
       [3.29925913e-01, 6.70074087e-01],
       [7.30765531e-01, 2.69234469e-01],
       [9.96324037e-01, 3.67596254e-03],
       [4.90342411e-01, 5.09657589e-01],
       [3.18188822e-03, 9.96818112e-01],
       [9.46898412e-01, 5.31015880e-02],
       [4.88269087e-01, 5.11730913e-01],
       [4.20516319e-01, 5.79483681e-01],
       [9.80219953e-01, 1.97800466e-02],
       [8.04544550e-01, 1.95455450e-01],
       [9.94869285e-01, 5.13071536e-03],
       [9.98352780e-01, 1.64722028e-03],
       [5.51345634e-03, 9.94486544e-01],
       [7.11952213e-01, 2.88047787e-01],
       [4.64253927e-01, 5.35746073e-01],
       [9.48686526e-01, 5.13134743e-02],
       [3.97192849e-01, 6.02807151e-01],
       [8.22145700e-01, 1.77854300e-01],
       [7.20608604e-01, 2.79391396e-01],
       [9.96941974e-01, 3.05802587e-03],
       [3.69200216e-02, 9.63079978e-01],
       [4.68416509e-01, 5.31583491e-01],
       [8.36147573e-01, 1.63852427e-01],
       [9.16274519e-01, 8.37254813e-02],
       [2.56634290e-03, 9.97433657e-01],
       [3.73536529e-03, 9.96264635e-01],
       [4.98355520e-01, 5.01644480e-01],
       [8.16305415e-01, 1.83694585e-01],
       [1.04309175e-01, 8.95690825e-01],
       [7.83779329e-01, 2.16220671e-01],
       [9.78083102e-01, 2.19168975e-02],
       [9.90653791e-01, 9.34620891e-03],
       [8.65661467e-01, 1.34338533e-01],
       [9.13831584e-01, 8.61684159e-02],
       [9.37101199e-01, 6.28988006e-02],
       [1.90620977e-01, 8.09379023e-01],
       [9.32239827e-01, 6.77601733e-02],
       [6.92969039e-04, 9.99307031e-01],
       [9.74408609e-01, 2.55913910e-02],
       [3.53732053e-01, 6.46267947e-01],
       [2.11247791e-01, 7.88752209e-01],
       [9.96303292e-01, 3.69670823e-03],
       [9.44406587e-01, 5.55934132e-02],
       [3.79106068e-02, 9.62089393e-01],
       [9.64609313e-01, 3.53906874e-02],
       [5.41232882e-03, 9.94587671e-01],
       [9.96059538e-01, 3.94046198e-03],
       [9.92143676e-01, 7.85632367e-03],
       [8.42660648e-01, 1.57339352e-01],
       [7.24307887e-03, 9.92756921e-01],
       [9.94312630e-01, 5.68737022e-03],
       [9.15130977e-01, 8.48690229e-02],
       [6.51393245e-01, 3.48606755e-01],
       [2.94340586e-02, 9.70565941e-01],
       [9.87183971e-01, 1.28160289e-02],
       [7.32453035e-01, 2.67546965e-01],
       [9.83352543e-01, 1.66474566e-02],
       [9.81747406e-01, 1.82525938e-02],
       [2.95226633e-02, 9.70477337e-01],
       [1.04715836e-03, 9.98952842e-01],
       [9.32904364e-01, 6.70956356e-02],
       [6.99978847e-01, 3.00021153e-01],
       [5.51184321e-01, 4.48815679e-01],
       [6.75122087e-01, 3.24877913e-01],
       [7.71181218e-01, 2.28818782e-01],
       [9.98413782e-01, 1.58621789e-03],
       [8.76237107e-01, 1.23762893e-01],
       [3.04481117e-01, 6.95518883e-01],
       [9.74363151e-01, 2.56368488e-02],
       [6.33134484e-01, 3.66865516e-01],
       [5.75854374e-01, 4.24145626e-01],
       [2.55537845e-02, 9.74446216e-01],
       [1.85756224e-03, 9.98142438e-01],
       [3.88831138e-02, 9.61116886e-01],
       [9.58731542e-01, 4.12684584e-02],
       [9.36469837e-01, 6.35301629e-02],
       [9.56176915e-01, 4.38230849e-02],
       [6.55053121e-01, 3.44946879e-01],
       [1.94187315e-01, 8.05812685e-01],
       [9.99337468e-01, 6.62531742e-04],
       [3.08437942e-03, 9.96915621e-01],
       [9.96566658e-01, 3.43334162e-03],
       [3.89773650e-01, 6.10226350e-01],
       [9.72082797e-01, 2.79172032e-02],
       [7.59648308e-01, 2.40351692e-01],
       [4.43021427e-04, 9.99556979e-01],
       [9.73066093e-01, 2.69339069e-02],
       [8.92475420e-01, 1.07524580e-01],
       [9.01226920e-01, 9.87730797e-02],
       [3.85327203e-03, 9.96146728e-01],
       [9.94614636e-01, 5.38536368e-03],
       [6.53230403e-04, 9.99346770e-01],
       [9.90987132e-01, 9.01286850e-03],
       [9.96317865e-01, 3.68213528e-03],
       [1.45412215e-01, 8.54587785e-01],
       [8.19346719e-04, 9.99180653e-01],
       [9.80095172e-01, 1.99048277e-02],
       [9.65256843e-01, 3.47431572e-02],
       [4.30306432e-03, 9.95696936e-01],
       [2.98781511e-03, 9.97012185e-01],
       [7.47533693e-01, 2.52466307e-01],
       [9.69449758e-01, 3.05502419e-02],
       [3.01177685e-01, 6.98822315e-01],
       [9.93546223e-01, 6.45377713e-03],
       [8.75033039e-01, 1.24966961e-01],
       [7.31268245e-01, 2.68731755e-01],
       [7.14534775e-01, 2.85465225e-01],
       [6.75215390e-01, 3.24784610e-01],
       [5.23341616e-01, 4.76658384e-01],
       [9.39475283e-01, 6.05247171e-02],
       [5.91353313e-04, 9.99408647e-01],
       [9.65066946e-01, 3.49330539e-02],
       [1.31939747e-02, 9.86806025e-01],
       [9.92408501e-01, 7.59149879e-03],
       [3.57838311e-02, 9.64216169e-01],
       [9.03043369e-01, 9.69566312e-02],
       [8.56581987e-01, 1.43418013e-01],
       [9.32189694e-01, 6.78103060e-02],
       [1.80107268e-03, 9.98198927e-01],
       [6.29906718e-03, 9.93700933e-01],
       [9.80944249e-01, 1.90557512e-02],
       [5.31286915e-03, 9.94687131e-01],
       [9.87155702e-01, 1.28442977e-02],
       [8.05893996e-01, 1.94106004e-01],
       [2.23174191e-01, 7.76825809e-01],
       [2.44490735e-03, 9.97555093e-01],
       [9.86554106e-01, 1.34458941e-02],
       [8.60442181e-03, 9.91395578e-01],
       [6.29328161e-01, 3.70671839e-01],
       [9.70922479e-04, 9.99029078e-01],
       [8.01000572e-04, 9.99198999e-01]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制ROC曲线</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(fpr, tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_604_0.png" alt="output_604_0"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># AUC值</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">score = roc_auc_score(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br><span class="line">score</span><br></pre></td></tr></table></figure>




<pre><code>0.8221950971416945
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征重要性</span></span><br><span class="line">model.feature_importances_</span><br></pre></td></tr></table></figure>




<pre><code>array([1175,  668,  118,  895,  126])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">features = X.columns  <span class="comment"># 获取特征名称</span></span><br><span class="line">importances = model.feature_importances_  <span class="comment"># 获取特征重要性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过二维表格形式显示</span></span><br><span class="line">importances_df = pd.DataFrame()</span><br><span class="line">importances_df[<span class="string">&#x27;特征名称&#x27;</span>] = features</span><br><span class="line">importances_df[<span class="string">&#x27;特征重要性&#x27;</span>] = importances</span><br><span class="line">importances_df.sort_values(<span class="string">&#x27;特征重要性&#x27;</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>特征名称</th>
      <th>特征重要性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>收入</td>
      <td>1175</td>
    </tr>
    <tr>
      <th>3</th>
      <td>历史授信额度</td>
      <td>895</td>
    </tr>
    <tr>
      <th>1</th>
      <td>年龄</td>
      <td>668</td>
    </tr>
    <tr>
      <th>4</th>
      <td>历史违约次数</td>
      <td>126</td>
    </tr>
    <tr>
      <th>2</th>
      <td>性别</td>
      <td>118</td>
    </tr>
  </tbody>
</table>
</div>



<p>注意：之前学过的机器学习模型的特征重要性为小数，而LightGBM模型的特征重要性均为整数</p>
<h3 id="10-5-4-模型参数调优"><a href="#10-5-4-模型参数调优" class="headerlink" title="10.5.4 模型参数调优"></a>10.5.4 模型参数调优</h3><p>LightGBM分类模型的参数有很多，这里选取以下参数进行调优：</p>
<p>num_leaves：决策树的最大叶子节点数，即决策树最多有多少个叶子节点，默认取31.因为LightGBM模型使用的是leaf-wise生长策略，所以在调节树的复杂度时常用的参数是num_leaves，而不是树的最大深度参数max_depth</p>
<p>n_estimators:弱学习器的个数，或者说是弱学习器的最大迭代次数，默认取100</p>
<p>learning_rate:学习率，又称为每个弱学习器的权重缩减系数，取值范围为（0，1]，默认取0.1.取值较小意味着要达到一定的误分类数或者学习效果，需要更多迭代次数和更多弱学习器。</p>
<p>了解上述参数的含义后，使用GridSearch网格搜索进行参数调优</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数调优</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV  <span class="comment"># 网格搜索合适的超参数</span></span><br><span class="line">parameters = &#123;<span class="string">&#x27;num_leaves&#x27;</span>: [<span class="number">10</span>, <span class="number">15</span>, <span class="number">31</span>], <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>], <span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]&#125;</span><br><span class="line">model = LGBMClassifier()  <span class="comment"># 构建分类器</span></span><br><span class="line">grid_search = GridSearchCV(model, parameters, scoring=<span class="string">&#x27;roc_auc&#x27;</span>, cv=<span class="number">5</span>)  <span class="comment"># cv=5表示交叉验证5次，scoring=&#x27;roc_auc&#x27;表示以ROC曲线的AUC评分作为模型评价准则</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出参数最优值</span></span><br><span class="line">grid_search.fit(X_train, y_train)  <span class="comment"># 传入数据</span></span><br><span class="line">grid_search.best_params_  <span class="comment"># 输出参数的最优值</span></span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;learning_rate&#39;: 0.2, &#39;n_estimators&#39;: 10, &#39;num_leaves&#39;: 10&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新搭建分类器</span></span><br><span class="line">model = LGBMClassifier(num_leaves=<span class="number">10</span>, n_estimators=<span class="number">10</span>,learning_rate=<span class="number">0.2</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>LGBMClassifier(learning_rate=0.2, n_estimators=10, num_leaves=10)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看ROC曲线</span></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(fpr, tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_614_0.png" alt="output_614_0"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看AUC值</span></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">score = roc_auc_score(y_test, y_pred_proba[:, <span class="number">1</span>])</span><br><span class="line">score</span><br></pre></td></tr></table></figure>




<pre><code>0.8712236801953005
</code></pre>
<p><strong>补充：LightGBM模型常见超参数</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(91).png" alt="下载 (91)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(92).png" alt="下载 (92)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(93).png" alt="下载 (93)"></p>
<h2 id="10-6-LghtGBM算法案例实战2：广告收益回归预测模型"><a href="#10-6-LghtGBM算法案例实战2：广告收益回归预测模型" class="headerlink" title="10.6 LghtGBM算法案例实战2：广告收益回归预测模型"></a>10.6 LghtGBM算法案例实战2：广告收益回归预测模型</h2><h3 id="10-6-1-案例背景"><a href="#10-6-1-案例背景" class="headerlink" title="10.6.1 案例背景"></a>10.6.1 案例背景</h3><p>投资商经常会通过多个不同渠道投放广告，以此来获得经济利益。在本案例中我们选取公司在电视、广播和报纸上的投入，来预测广告收益，这对公司策略的制定是有较重要的意义。</p>
<h3 id="10-6-2-模型搭建"><a href="#10-6-2-模型搭建" class="headerlink" title="10.6.2 模型搭建"></a>10.6.2 模型搭建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;广告收益数据.xlsx&#x27;</span>)</span><br><span class="line">df.head()  <span class="comment"># 目标变量是广告收益</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>电视</th>
      <th>广播</th>
      <th>报纸</th>
      <th>收益</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>230.1</td>
      <td>37.8</td>
      <td>69.2</td>
      <td>331.5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>44.5</td>
      <td>39.3</td>
      <td>45.1</td>
      <td>156.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17.2</td>
      <td>45.9</td>
      <td>69.3</td>
      <td>139.5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>151.5</td>
      <td>41.3</td>
      <td>58.5</td>
      <td>277.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>180.8</td>
      <td>10.8</td>
      <td>58.4</td>
      <td>193.5</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.提取特征变量和目标变量</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;收益&#x27;</span>) </span><br><span class="line">y = df[<span class="string">&#x27;收益&#x27;</span>]  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.划分训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 模型训练和搭建</span></span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMRegressor</span><br><span class="line">model = LGBMRegressor()</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>LGBMRegressor()
</code></pre>
<h3 id="10-6-3-模型预测及评估"><a href="#10-6-3-模型预测及评估" class="headerlink" title="10.6.3 模型预测及评估"></a>10.6.3 模型预测及评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测测试数据</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">y_pred[<span class="number">0</span>:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([192.6139063 , 295.11999665, 179.92649365, 293.45888909,
       166.86159398])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测值和实际值对比</span></span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>192.613906</td>
      <td>190.5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>295.119997</td>
      <td>292.5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>179.926494</td>
      <td>171.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>293.458889</td>
      <td>324.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>166.861594</td>
      <td>144.0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 手动输入数据进行预测</span></span><br><span class="line">X = [[<span class="number">71</span>, <span class="number">11</span>, <span class="number">2</span>]]</span><br><span class="line">model.predict(X)</span><br></pre></td></tr></table></figure>




<pre><code>array([140.00420258])
</code></pre>
<p>即电视广告费投入71万元、广播广告费投入11万元、报纸广告费投入2万元时，模型预测的广告收益为140万元</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看R-square</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2 = r2_score(y_test, model.predict(X_test))</span><br><span class="line">r2</span><br></pre></td></tr></table></figure>




<pre><code>0.9570203214455993
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看评分</span></span><br><span class="line">model.score(X_test, y_test)</span><br></pre></td></tr></table></figure>




<pre><code>0.9570203214455993
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征重要性</span></span><br><span class="line">model.feature_importances_</span><br></pre></td></tr></table></figure>




<pre><code>array([ 950, 1049,  963])
</code></pre>
<h3 id="10-6-4-模型参数调优"><a href="#10-6-4-模型参数调优" class="headerlink" title="10.6.4 模型参数调优"></a>10.6.4 模型参数调优</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数调优</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV  <span class="comment"># 网格搜索合适的超参数</span></span><br><span class="line">parameters = &#123;<span class="string">&#x27;num_leaves&#x27;</span>: [<span class="number">15</span>, <span class="number">31</span>, <span class="number">62</span>], <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">70</span>], <span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>]&#125;  <span class="comment"># 指定分类器中参数的范围</span></span><br><span class="line">model = LGBMRegressor()  <span class="comment"># 构建模型</span></span><br><span class="line">grid_search = GridSearchCV(model, parameters,scoring=<span class="string">&#x27;r2&#x27;</span>,cv=<span class="number">5</span>) <span class="comment"># cv=5表示交叉验证5次，scoring=&#x27;r2&#x27;表示以R-squared作为模型评价准则</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出参数最优值</span></span><br><span class="line">grid_search.fit(X_train, y_train)  <span class="comment"># 传入数据</span></span><br><span class="line">grid_search.best_params_  <span class="comment"># 输出参数的最优值</span></span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;learning_rate&#39;: 0.3, &#39;n_estimators&#39;: 50, &#39;num_leaves&#39;: 31&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新搭建LightGBM回归模型</span></span><br><span class="line">model = LGBMRegressor(num_leaves=<span class="number">31</span>, n_estimators=<span class="number">50</span>,learning_rate=<span class="number">0.3</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看得分</span></span><br><span class="line">model.score(X_test, y_test)</span><br></pre></td></tr></table></figure>




<pre><code>0.9558624845475153
</code></pre>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">数据科学</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/机器学习与深度学习//" class="article-tag-list-link color5">机器学习与深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/02/02/python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part2/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-Python大数据分析与机器学习商业案例实战-part1" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/02/02/Python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part1/">Python大数据分析与机器学习商业案例实战-part1</a>
    </h1>
  

        
        <a href="/2023/02/02/Python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part1/" class="archive-article-date">
  	<time datetime="2023-02-02T08:13:14.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-02-02</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-Python与数据科学"><a href="#1-Python与数据科学" class="headerlink" title="1. Python与数据科学"></a>1. Python与数据科学</h1><p>机器学习项目的构建步骤：</p>
<p>（1）问题定义</p>
<p>（2）数据的收集和预定义</p>
<p>（3）选择机器学习模型</p>
<p>（4）训练机器，确定参数</p>
<p>（5）超参数优化和性能确定</p>
<h1 id="2-数据分析三个库"><a href="#2-数据分析三个库" class="headerlink" title="2. 数据分析三个库"></a>2. 数据分析三个库</h1><h2 id="2-1-numpy"><a href="#2-1-numpy" class="headerlink" title="2.1 numpy"></a>2.1 numpy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">b = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(a))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(b))</span><br></pre></td></tr></table></figure>

<pre><code>[1, 2, 3, 4]
[1 2 3 4]
&lt;class &#39;list&#39;&gt;
&lt;class &#39;numpy.ndarray&#39;&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 接下来通过列表索引和数组索引来访问列表和数组中的元素，代码如下：</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">1</span>]) </span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>:<span class="number">2</span>]) </span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">0</span>:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<pre><code>2
2
[1, 2]
[1 2]
</code></pre>
<p>从上面结果可以看到列表和数组有着相同的索引机制，唯一的区别好像就是数组里面是通过空格分隔元素，而列表用的是逗号。</p>
<h3 id="2-1-1-Numpy数组与列表的区别"><a href="#2-1-1-Numpy数组与列表的区别" class="headerlink" title="2.1.1 Numpy数组与列表的区别"></a>2.1.1 Numpy数组与列表的区别</h3><p>从上面的分析得知Numpy数组和列表很类似，那么为什么Python又要创建一个Numpy库呢？其原因很多，这里主要讲两点：</p>
<p>1.数组可以比较方便的进行一些数学运算，而列表则比较麻烦；</p>
<p>2.数组可以支持多维的数据，而列表通常只能储存一维的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c = a * <span class="number">2</span></span><br><span class="line">d = b * <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br></pre></td></tr></table></figure>

<pre><code>[1, 2, 3, 4, 1, 2, 3, 4]
[2 4 6 8]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">e = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>]] <span class="comment"># 列表里的元素为小列表</span></span><br><span class="line">f = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>]]) <span class="comment"># 创建二维数组的一种方式</span></span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line"><span class="built_in">print</span>(f)</span><br></pre></td></tr></table></figure>

<pre><code>[[1, 2], [3, 4], [5, 6]]
[[1 2]
 [3 4]
 [5 6]]
</code></pre>
<p>可以看到列表虽然包含着三个小列表，但其还是一个一维的结构，而创建的二维数组则是一个三行两列的二维结构内容，这个也是之后学习pandas库的核心内容了，因为数据数据处理中经常用到二维数组，也即二维表格结构。</p>
<h3 id="2-1-2-创建数组的几种方式"><a href="#2-1-2-创建数组的几种方式" class="headerlink" title="2.1.2 创建数组的几种方式"></a>2.1.2 创建数组的几种方式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一维数组</span></span><br><span class="line">b = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="comment"># 创建二维数组</span></span><br><span class="line">f = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(f)</span><br></pre></td></tr></table></figure>

<pre><code>[1 2 3 4]
[[1 2]
 [3 4]
 [5 6]]
</code></pre>
<p>除此之外，还有一些常见的创建数组的方式，这里以一维数组为例，我们还可以采用np.arange()函数来产生一维数组，其中括号里可以选择1个或2个或3个参数，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个参数 参数值为终点，起点取默认值0，步长取默认值1</span></span><br><span class="line">x = np.arange(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 两个参数 第一个参数为起点，第二个参数为终点，步长取默认值1，左闭右开</span></span><br><span class="line">y = np.arange(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 三个参数 第一个参数为起点，第二个参数为终点，第三个参数为步长，左闭右开</span></span><br><span class="line">z = np.arange(<span class="number">5</span>, <span class="number">10</span>, <span class="number">0.5</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br></pre></td></tr></table></figure>

<pre><code>[0 1 2 3 4]
[5 6 7 8 9]
[5.  5.5 6.  6.5 7.  7.5 8.  8.5 9.  9.5]
</code></pre>
<p>我们还可以通过np.random模块来创建随机一维数组，比如可以通过np.random.randn(3)来创建一个服从正太分布（均值为0，方差为1的分布）的3个随机数一维数组，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)  <span class="comment"># 因为随机，所以每次运行的结果都会不太一样</span></span><br></pre></td></tr></table></figure>

<pre><code>[-0.44003854  0.15893829  0.18365733]
</code></pre>
<p>如果把np.random.randn(3)换成np.random.rand(3)，那生成的就是0-1之间的3个随机数，这个在之后2.3.1小节演示绘制散点图的时候会用到。</p>
<p>至于二维数组的创建与学习，可以利用一维数组中的np.arange()函数和reshape方法产生一个二维数组，比如将0到11个数转换成3行4列的二维数组，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>

<pre><code>[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]]
</code></pre>
<p>这里再简单提一种随机二维数组的创建，代码如下。其中np.random.randint()函数用来创建随机整数，括号里第一个元素0表示起始数，第二个元素10表示终止数，第三个元素(4, 4)则表示生成一个4行4列的二维数组</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randint(<span class="number">0</span>, <span class="number">10</span>, (<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>

<pre><code>[[1 2 8 9]
 [0 5 6 3]
 [7 7 7 2]
 [5 1 4 9]]
</code></pre>
<h2 id="2-2-pandas"><a href="#2-2-pandas" class="headerlink" title="2.2 pandas"></a>2.2 pandas</h2><h3 id="2-2-1-引言"><a href="#2-2-1-引言" class="headerlink" title="2.2.1 引言"></a>2.2.1 引言</h3><p>相较于Numpy来说，Pandas更善于处理二维数据。Pandas主要有两种数据结构：Series和DataFrame。Series类似于通过Numpy产生的一维数组，不同的是Series对象不仅包含数值，还包含一组索引，其创建方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">s1 = pd.Series([<span class="string">&#x27;丁一&#x27;</span>, <span class="string">&#x27;王二&#x27;</span>, <span class="string">&#x27;张三&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(s1)</span><br></pre></td></tr></table></figure>

<pre><code>0    丁一
1    王二
2    张三
dtype: object
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 它也是一个一维数据结构，并且对于每个元素都有一个行索引可以用来定位，比如可以通过s1[1]来定位到第二个元素“王二”。</span></span><br><span class="line"><span class="built_in">print</span>(s1[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>王二
</code></pre>
<p>Series单独使用相对较少，pandas主要采用DataFrame数据结构。DataFrame是一种二维表格数据结构，直观一点的话可以将其看作一个Excel表格。</p>
<h3 id="2-2-2-二维数据表格DataFrame的创建"><a href="#2-2-2-二维数据表格DataFrame的创建" class="headerlink" title="2.2.2 二维数据表格DataFrame的创建"></a>2.2.2 二维数据表格DataFrame的创建</h3><p>有三种DataFrame常见的创建方法：通过列表创建、通过字典创建及通过二维数组创建。</p>
<p>(1) 通过列表创建DataFrame</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">a = pd.DataFrame([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">a  <span class="comment"># 在Jupyter Nobebook中，代码框中最后一行代码可以只输入变量名称，即可自动打印，而无需通过print()函数</span></span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>


<p>可以看到通过pandas的DataFrame功能生成的二维数组更像我们在Excel中看到二维表格数据，它也有行索引和列索引，其中这里的索引序号都是从0开始的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们还可以自定义其列索引和行索引名称，代码如下：</span></span><br><span class="line">a = pd.DataFrame([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]], columns=[<span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;score&#x27;</span>], index=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>])</span><br><span class="line">a</span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>B</th>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <th>C</th>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过列表生成DataFrame还可以采用如下的方式，演示代码如下：</span></span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">date = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>]</span><br><span class="line">score = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line">a[<span class="string">&#x27;date&#x27;</span>] = date</span><br><span class="line">a[<span class="string">&#x27;score&#x27;</span>] = score</span><br><span class="line">a</span><br></pre></td></tr></table></figure>


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>


<p>(2) 通过字典创建DataFrame</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过Pandas创建二维数组 - 字典法</span></span><br><span class="line">b = pd.DataFrame(&#123;<span class="string">&#x27;a&#x27;</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], <span class="string">&#x27;b&#x27;</span>: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]&#125;, index=[<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;z&#x27;</span>])</span><br><span class="line">b  <span class="comment"># 在Jupyter Notebook编辑器中可以直接输入b进行查看</span></span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>a</th>
      <th>b</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>x</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>y</th>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <th>z</th>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想让字典键变成行索引，可以通过from_dict的方式来将字典转换成DataFrame，并同时设置orient参数为index，代码如下：</span></span><br><span class="line">c = pd.DataFrame.from_dict(&#123;<span class="string">&#x27;a&#x27;</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], <span class="string">&#x27;b&#x27;</span>: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]&#125;, orient=<span class="string">&quot;index&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(c)  <span class="comment"># 也可以直接输入c进行查看变量结果</span></span><br></pre></td></tr></table></figure>

<pre><code>   0  1  2
a  1  3  5
b  2  4  6
</code></pre>
<p>其中orient参数指定字典键对应的方向，默认值为columns，如果不设置成index的话，则还是默认字典键为列索引</p>
<p><strong>补充知识点：通过.T来对表格进行转置</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = pd.DataFrame(&#123;<span class="string">&#x27;a&#x27;</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], <span class="string">&#x27;b&#x27;</span>: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]&#125;)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(b.T)</span><br></pre></td></tr></table></figure>

<pre><code>   a  b
0  1  2
1  3  4
2  5  6
   0  1  2
a  1  3  5
b  2  4  6
</code></pre>
<p>(3) 通过二维数组创建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过Numpy产生的二维数组，也可以创建DataFrame，这里以2.1.3小节里提到的二维数组为例生成一个3行4列的DataFrame，代码如下：</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">d = pd.DataFrame(np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>), index=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], columns=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>])</span><br><span class="line">d</span><br></pre></td></tr></table></figure>


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8</td>
      <td>9</td>
      <td>10</td>
      <td>11</td>
    </tr>
  </tbody>
</table>




<p><strong>补充知识点：修改行索引或列索引名称</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = pd.DataFrame([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], columns=[<span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;score&#x27;</span>], index=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>])</span><br><span class="line">a</span><br></pre></td></tr></table></figure>


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>B</th>
      <td>3</td>
      <td>4</td>
    </tr>
  </tbody>
</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想对索引进行重命名的话，rename()函数的使用方法如下：</span></span><br><span class="line">a = a.rename(index=&#123;<span class="string">&#x27;A&#x27;</span>:<span class="string">&#x27;阿里&#x27;</span>, <span class="string">&#x27;B&#x27;</span>:<span class="string">&#x27;腾讯&#x27;</span>&#125;, columns=&#123;<span class="string">&#x27;date&#x27;</span>:<span class="string">&#x27;日期&#x27;</span>,<span class="string">&#x27;score&#x27;</span>:<span class="string">&#x27;分数&#x27;</span>&#125;)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>日期</th>
      <th>分数</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>阿里</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>腾讯</th>
      <td>3</td>
      <td>4</td>
    </tr>
  </tbody>
</table>


<p>补充知识点：这里通过rename之后并没有改变原表格结构，需要重新赋值给a才能改变原表格;或者在rename()中设置inplace参数为True，也能实现真正替换，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = pd.DataFrame([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], columns=[<span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;score&#x27;</span>], index=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>])</span><br><span class="line">a.rename(index=&#123;<span class="string">&#x27;A&#x27;</span>:<span class="string">&#x27;阿里&#x27;</span>, <span class="string">&#x27;B&#x27;</span>:<span class="string">&#x27;腾讯&#x27;</span>&#125;, columns=&#123;<span class="string">&#x27;date&#x27;</span>:<span class="string">&#x27;日期&#x27;</span>,<span class="string">&#x27;score&#x27;</span>:<span class="string">&#x27;分数&#x27;</span>&#125;, inplace=<span class="literal">True</span>)  <span class="comment"># 另一种方法</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>日期</th>
      <th>分数</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>阿里</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>腾讯</th>
      <td>3</td>
      <td>4</td>
    </tr>
  </tbody>
</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过.values属性，也可以查看此时的index值</span></span><br><span class="line"><span class="built_in">print</span>(a.index.values)</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;阿里&#39; &#39;腾讯&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想给行索引命名，也可以通过如下代码，相当于增加了以‘公司’为行索引的空行</span></span><br><span class="line">a.index.name = <span class="string">&#x27;公司&#x27;</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>日期</th>
      <th>分数</th>
    </tr>
    <tr>
      <th>公司</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>阿里</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>腾讯</th>
      <td>3</td>
      <td>4</td>
    </tr>
  </tbody>
</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想把行索引变成某列的内容，可以使用set_index()函数，代码如下：</span></span><br><span class="line">a = a.set_index(<span class="string">&#x27;日期&#x27;</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>分数</th>
    </tr>
    <tr>
      <th>日期</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
    </tr>
  </tbody>
</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果此时想把行索引换成数字索引，则可以使用reset_index()函数，代码如下：</span></span><br><span class="line">a = a.reset_index()</span><br><span class="line">a</span><br></pre></td></tr></table></figure>


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>日期</th>
      <th>分数</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>4</td>
    </tr>
  </tbody>
</table>


<h3 id="2-2-3-Excel等文件的读取和写入"><a href="#2-2-3-Excel等文件的读取和写入" class="headerlink" title="2.2.3 Excel等文件的读取和写入"></a>2.2.3 Excel等文件的读取和写入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入以下代码，用于读取Excel数据：</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_excel(<span class="string">r&#x27;C:\Users\王宇涵\Desktop\源代码汇总-2020-12-16\第2章 数据分析的基本武器\源代码汇总_Jupyter Notebook格式（推荐）\data.xlsx&#x27;</span>)  <span class="comment"># data为DataFrame结构</span></span><br><span class="line">data.head()  <span class="comment"># 通过head()可以查看前5行数据，如果写成head(10)则可以查看前10行数据</span></span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>score</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2018-09-03</td>
      <td>70</td>
      <td>23.55</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2018-09-04</td>
      <td>75</td>
      <td>24.43</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2018-09-05</td>
      <td>65</td>
      <td>23.41</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2018-09-06</td>
      <td>60</td>
      <td>22.81</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2018-09-07</td>
      <td>70</td>
      <td>23.21</td>
    </tr>
  </tbody>
</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 其中read_excel还可以设定参数，使用方式如下：</span></span><br><span class="line"><span class="comment"># pd.read_excel(&#x27;data.xlsx&#x27;, sheet_name=0, encoding=&#x27;utf-8&#x27;)</span></span><br></pre></td></tr></table></figure>

<p>注意：sheet_name用于指定要读取的工作表，可以是工作表的名称，也可以是数字（默认为0，即第一个工作表）</p>
<p>除了excel,pandas还可以读取CSV。CSV文件本质上是一个文本文件，它仅存储数据，不能向excel那样存储格式、公式、宏等信息，所以所占空间通常很小。CSV文件一般用逗号分割，既可以用excel打开，也可以用文本编辑器打开</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入以下代码，用于读取CSV文件：</span></span><br><span class="line">data = pd.read_csv(<span class="string">r&#x27;C:\Users\王宇涵\Desktop\源代码汇总-2020-12-16\第2章 数据分析的基本武器\源代码汇总_Jupyter Notebook格式（推荐）/data.csv&#x27;</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>score</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2018-09-03</td>
      <td>70</td>
      <td>23.55</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2018-09-04</td>
      <td>75</td>
      <td>24.43</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2018-09-05</td>
      <td>65</td>
      <td>23.41</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2018-09-06</td>
      <td>60</td>
      <td>22.81</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2018-09-07</td>
      <td>70</td>
      <td>23.21</td>
    </tr>
  </tbody>
</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># read_csv也可以指定参数，使用方式如下：</span></span><br><span class="line"><span class="comment"># data = pd.read_csv(&#x27;data.csv&#x27;, delimiter=&#x27;,&#x27;, encoding=&#x27;utf-8&#x27;)</span></span><br></pre></td></tr></table></figure>

<p>(2) 文件写入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先生成一个DataFrame</span></span><br><span class="line">data = pd.DataFrame([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]], columns=[<span class="string">&#x27;A列&#x27;</span>,<span class="string">&#x27;B列&#x27;</span>])</span><br><span class="line"><span class="comment"># 将DataFrame导入到Excel当中</span></span><br><span class="line">data.to_excel(<span class="string">&#x27;data_new2.xlsx&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>运行之后将在代码所在的文件夹生成一个名为data_new2的Excel文件</p>
<p><img src="%E4%B8%8B%E8%BD%BD.png"></p>
<p>在上表中，保存的Excel第一列还保留了索引信息，如果想将其删去，可以设置to_excel的参数index为False。to_excel的常见参数有如下一些：sheet_name：数据表名；index：True or False，默认为True保存索引信息，即输出文件的第一列为索引值，选择False的话则忽略索引信息；columns：选择所需要的列；encoding：编码方式。<br>例如要将数据表格导入到Excel文件中并忽略行索引信息，则代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.to_excel(<span class="string">&#x27;data_new2.xlsx&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p><img src="%E4%B8%8B%E8%BD%BD%20(1).png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过类似的方式，可以将数据写入到CSV文件当中，代码如下：</span></span><br><span class="line">data.to_csv(<span class="string">&#x27;data_new.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="%E4%B8%8B%E8%BD%BD%20(2).png"></p>
<p><strong>补充知识点：文件相对路径与绝对路径</strong></p>
<p><strong>相对路径</strong></p>
<p>文件相对路径，即代码所在的文件夹，例如上面案例中写的data.to_excel(‘data.xlsx’)就是在代码所在的文件夹生成Excel文件。</p>
<p><strong>绝对路径</strong></p>
<p>文件绝对路径，就是文件完整的路径名称，例如’E:\大数据分析\data.xlsx’就是绝对路径，不过因为在Python中反斜杠“\”经常有特殊含义，比如说“\n”表示换行，所以通常建议写绝对路径的时候写两个反斜杠取消可能存在的单个反斜杠的特殊含义，写成’E:\大数据分析\data.xlsx’。</p>
<p>除了用两个反斜杠来取消一个反斜杠的特殊意义外，还可以在文件路径的字符串前面加一个r，也可以取消单个反斜杠的特殊含义，代码如下：</p>
<p>data.to_excel(‘E:\大数据分析\data.xlsx’)  # 绝对路径推荐写法1，此时E盘要有一个名为“大数据分析”的文件夹</p>
<p>data.to_excel(r’E:\大数据分析\data.xlsx’)  # 绝对路径推荐写法2，此时E盘要有一个名为“大数据分析”的文件夹</p>
<h3 id="2-2-4-数据读取与筛选"><a href="#2-2-4-数据读取与筛选" class="headerlink" title="2.2.4 数据读取与筛选"></a>2.2.4 数据读取与筛选</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先创建一个三行三列的表格，行索引设定为r1、r2和r3，列索引设定为c1、c2和c3，以此为例来演示数据的读取与筛选，代码如下：</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.DataFrame([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]], index=[<span class="string">&#x27;r1&#x27;</span>, <span class="string">&#x27;r2&#x27;</span>, <span class="string">&#x27;r3&#x27;</span>], columns=[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c2&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>])</span><br><span class="line">data</span><br></pre></td></tr></table></figure>



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r1</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>7</td>
      <td>8</td>
      <td>9</td>
    </tr>
  </tbody>
</table>


<p>1.按照行列进行数据筛选</p>
<p>(1) 按照列来选取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过以下代码可以通过列来选取数据，这里先选取单列。</span></span><br><span class="line">a = data[<span class="string">&#x27;c1&#x27;</span>]</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>r1    1
r2    4
r3    7
Name: c1, dtype: int64
</code></pre>
<p>此时返回的结果里没有表头信息了，这是因为通过data[‘c1’]选取一列的时候返回的是一个一维序列结构的类，也可以通过如下代码返回一个二维的表格数据。注意，经常是[[..]]的形式来取列，要保证取出的是完整的表格形式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = data[[<span class="string">&#x27;c1&#x27;</span>]]</span><br><span class="line">b</span><br></pre></td></tr></table></figure>


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r1</th>
      <td>1</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>4</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>7</td>
    </tr>
  </tbody>
</table>


<p>若要选取多列，则需要在中括号[]中给个列表，比如要读取c1和c3列，则可以写为data[[‘c1’, ‘c3’]]。这里需要特别注意的是，必须是一个列表，而不能是data[‘c1’, ‘c3’]，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = data[[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>]]</span><br><span class="line">c</span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r1</th>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>6</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>7</td>
      <td>9</td>
    </tr>
  </tbody>
</table>


<p>(2) 按照行来选取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选取第2到3行的数据，注意序号从0开始，左闭右开</span></span><br><span class="line">a = data[<span class="number">1</span>:<span class="number">3</span>] </span><br><span class="line">a</span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>7</td>
      <td>8</td>
      <td>9</td>
    </tr>
  </tbody>
</table>


<p>而pandas推荐使用iloc方法来根据行的序号进行行选取，它是根据行序号选取的另一种方法，pandas觉得这样更加直观，不会像data[1:3]可能会引起混淆，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = data.iloc[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">b</span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>7</td>
      <td>8</td>
      <td>9</td>
    </tr>
  </tbody>
</table>


<p>而且如果要选取单行的话，就必须得用iloc了，比如选择倒数第一行，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = data.iloc[-<span class="number">1</span>]</span><br><span class="line">c</span><br></pre></td></tr></table></figure>




<pre><code>c1    7
c2    8
c3    9
Name: r3, dtype: int64
</code></pre>
<p>除了通过行的序号选取外，还可以通过loc方法根据行的名称来进行选取，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = data.loc[[<span class="string">&#x27;r2&#x27;</span>, <span class="string">&#x27;r3&#x27;</span>]]</span><br><span class="line">d</span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>7</td>
      <td>8</td>
      <td>9</td>
    </tr>
  </tbody>
</table>


<p>有的时候如果行数很多，可以通过head()方法来选取前5行，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">e = data.head()</span><br><span class="line">e</span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r1</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>7</td>
      <td>8</td>
      <td>9</td>
    </tr>
  </tbody>
</table>


<p>这里因为只创建了3行数据，所以通过data.head()会把全部数据都取到，如果只想取前两行的数据，可以写成data.head(2)。</p>
<p>(3) 按照区块来选取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想选取某几行的某几列，则可以通过如下代码来实现，比如获得c1和c3列的前二行。</span></span><br><span class="line">a = data[[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>]][<span class="number">0</span>:<span class="number">2</span>]  <span class="comment"># 也可写成data[0:2][[&#x27;c1&#x27;, &#x27;c3&#x27;]]</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r1</th>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>6</td>
    </tr>
  </tbody>
</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在实战中，通常采用iloc和列选取混合的方式来选取特定的区块或值，代码如下：（推荐使用）</span></span><br><span class="line">b = data.iloc[<span class="number">0</span>:<span class="number">2</span>][[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>]] </span><br><span class="line">b</span><br></pre></td></tr></table></figure>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r1</th>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>6</td>
    </tr>
  </tbody>
</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果要选取单个的值，那么该方法就更有优势，比如选取c3列第一行的信息，就不能写成data[&#x27;c3&#x27;][0]或data[0][&#x27;c3&#x27;]了。下面的写法则比较清晰，iloc[0]先选取第一行，然后再选取c3列。</span></span><br><span class="line">c = data.iloc[<span class="number">0</span>][<span class="string">&#x27;c3&#x27;</span>]</span><br><span class="line">c</span><br></pre></td></tr></table></figure>




<pre><code>3
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以通过iloc和loc方法来同时选择行和列，代码如下：</span></span><br><span class="line">d = data.loc[[<span class="string">&#x27;r1&#x27;</span>, <span class="string">&#x27;r2&#x27;</span>], [<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>]]  </span><br><span class="line">e = data.iloc[<span class="number">0</span>:<span class="number">2</span>, [<span class="number">0</span>, <span class="number">2</span>]]  </span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="built_in">print</span>(e)</span><br></pre></td></tr></table></figure>

<pre><code>    c1  c3
r1   1   3
r2   4   6
    c1  c3
r1   1   3
r2   4   6
</code></pre>
<p>2.按照特定条件筛选</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在方括号里还可以通过判断条件来过滤行，比如选取c1列数字大于1的行，代码如下：</span></span><br><span class="line">a = data[data[<span class="string">&#x27;c1&#x27;</span>] &gt; <span class="number">1</span>]</span><br><span class="line">a</span><br></pre></td></tr></table></figure>


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>7</td>
      <td>8</td>
      <td>9</td>
    </tr>
  </tbody>
</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果有多个筛选条件，则可以通过“&amp;”符号（表示“且”）或“|”（表示“或”）连接，比如这边筛选，c1列数字大于1且c2列数字小于8的行，代码如下，注意要记得加判断条件两旁的小括号。</span></span><br><span class="line">b = data[(data[<span class="string">&#x27;c1&#x27;</span>] &gt; <span class="number">1</span>) &amp; (data[<span class="string">&#x27;c2&#x27;</span>] &lt; <span class="number">8</span>)]</span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>



<p>3.数据整体情况查看</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过表格的shape属性，可以查看表格整体的行数和列数，在表格数据量较大的时候能快速了解表格的行数和列数。</span></span><br><span class="line">data.shape</span><br></pre></td></tr></table></figure>




<pre><code>(3, 3)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过表格的describe()函数可以快速的查看表格每一列的数量、平均值、标准差、最小值、25分位数、50分位数、75分位数、最大值等信息，代码如下：</span></span><br><span class="line">data.describe()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>3.0</td>
      <td>3.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>4.0</td>
      <td>5.0</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3.0</td>
      <td>3.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2.5</td>
      <td>3.5</td>
      <td>4.5</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>4.0</td>
      <td>5.0</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>5.5</td>
      <td>6.5</td>
      <td>7.5</td>
    </tr>
    <tr>
      <th>max</th>
      <td>7.0</td>
      <td>8.0</td>
      <td>9.0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过value_counts()函数则可以快速的查看某一列都有什么数据，以及该数据出现的频次，代码如下：</span></span><br><span class="line">data[<span class="string">&#x27;c1&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure>




<pre><code>7    1
1    1
4    1
Name: c1, dtype: int64
</code></pre>
<p>4.数据运算、排序与删除</p>
<p>(1) 数据运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从已有的列中，通过数据运算创造一个新的一列，代码如下：</span></span><br><span class="line">data[<span class="string">&#x27;c4&#x27;</span>] = data[<span class="string">&#x27;c3&#x27;</span>] - data[<span class="string">&#x27;c1&#x27;</span>]</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
      <th>c4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r1</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>7</td>
      <td>8</td>
      <td>9</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



<p>(2) 数据排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过sort_values()可以根据列对数据进行排序，比如要对c2列进行降序排序，代码如下：</span></span><br><span class="line">a = data.sort_values(by=<span class="string">&#x27;c2&#x27;</span>, ascending=<span class="literal">False</span>) </span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
      <th>c4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r3</th>
      <td>7</td>
      <td>8</td>
      <td>9</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r1</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 其实如果是按列筛选，我们也可以直接写成如下代码，不用写“by=”，效果一样：</span></span><br><span class="line">a = data.sort_values(<span class="string">&#x27;c2&#x27;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
      <th>c4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r3</th>
      <td>7</td>
      <td>8</td>
      <td>9</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r1</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此外，通过sort_index()可以根据&quot;行索引&quot;进行排序，如按行索引进行升序排列，代码如下：</span></span><br><span class="line">a = a.sort_index()</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
      <th>c4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r1</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>7</td>
      <td>8</td>
      <td>9</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



<p>(3) 数据删除</p>
<p>DateFrame.drop(index&#x3D;None,columns&#x3D;None,inplace&#x3D;False)</p>
<p>index用于指定要删除的行，columns用于指定要删除的列，inplace默认为false，表示该删除操作不改变原表格，而是返回一个执行删除操作后的新表格，如果设置inplace为True，则会直接在原表格中执行删除操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例如删除c1列的数据，代码如下：</span></span><br><span class="line">a = data.drop(columns=<span class="string">&#x27;c1&#x27;</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c2</th>
      <th>c3</th>
      <th>c4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r1</th>
      <td>2</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>5</td>
      <td>6</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>8</td>
      <td>9</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除多列的数据，比如c1和c3列，可以通过列表的方式将所需删除的列声明，代码如下：</span></span><br><span class="line">b = data.drop(columns=[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>])</span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c2</th>
      <th>c4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r1</th>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r2</th>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>r3</th>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果要删除行数据，比如删去第一行和第三行的数据，代码如下：</span></span><br><span class="line">c = data.drop(index=[<span class="string">&#x27;r1&#x27;</span>,<span class="string">&#x27;r3&#x27;</span>])</span><br><span class="line">c</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
      <th>c4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



<p>注意这里要输入行索引的名称而不是数字序号，不过如果行索引名称本来就是数字，那么可以输入对应数字。上面删除数据后又赋值给新的变量不会改变原来表格data的结构，如果想改变原来表格的结构，可以令inplace参数为True，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.drop(index=[<span class="string">&#x27;r1&#x27;</span>,<span class="string">&#x27;r3&#x27;</span>], inplace=<span class="literal">True</span>)</span><br><span class="line">data</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>c1</th>
      <th>c2</th>
      <th>c3</th>
      <th>c4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>r2</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="2-2-5-数据表拼接"><a href="#2-2-5-数据表拼接" class="headerlink" title="2.2.5 数据表拼接"></a>2.2.5 数据表拼接</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设有如下两个DataFrame表格，需要对它们进行合并：</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">&#x27;公司&#x27;</span>: [<span class="string">&#x27;万科&#x27;</span>, <span class="string">&#x27;阿里&#x27;</span>, <span class="string">&#x27;百度&#x27;</span>], <span class="string">&#x27;分数&#x27;</span>: [<span class="number">90</span>, <span class="number">95</span>, <span class="number">85</span>]&#125;)</span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">&#x27;公司&#x27;</span>: [<span class="string">&#x27;万科&#x27;</span>, <span class="string">&#x27;阿里&#x27;</span>, <span class="string">&#x27;京东&#x27;</span>], <span class="string">&#x27;股价&#x27;</span>: [<span class="number">20</span>, <span class="number">180</span>, <span class="number">30</span>]&#125;)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>公司</th>
      <th>分数</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>90</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>95</td>
    </tr>
    <tr>
      <th>2</th>
      <td>百度</td>
      <td>85</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>公司</th>
      <th>股价</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>180</td>
    </tr>
    <tr>
      <th>2</th>
      <td>京东</td>
      <td>30</td>
    </tr>
  </tbody>
</table>
</div>



<p>(1) merge()函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># merge()函数根据一个或多个键将不同表格中的行连接起来，示例如下：</span></span><br><span class="line">df3 = pd.merge(df1, df2) <span class="comment"># 交集</span></span><br><span class="line">df3</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>公司</th>
      <th>分数</th>
      <th>股价</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>90</td>
      <td>20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>95</td>
      <td>180</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以看到通过merge()函数直接选取相同的列名（“公司”这一列）进行合并，而且默认选取的是两种表共有的列内容（万科、阿里），有的时候如果相同的列名不止一个，可以通过on参数指定按照哪一列进行合并，代码如下：<br>df3 &#x3D; pd.merge(df1, df2, on&#x3D;’公司’)</p>
<p>默认的合并其实是取交集（inner连接），也即取两表共有的内容，如果想取并集（outer连接），也即选取两表所有的内容，可以设置how参数，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df3 = pd.merge(df1, df2, how=<span class="string">&#x27;outer&#x27;</span>)  <span class="comment"># 并集</span></span><br><span class="line">df3</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>公司</th>
      <th>分数</th>
      <th>股价</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>90.0</td>
      <td>20.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>95.0</td>
      <td>180.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>百度</td>
      <td>85.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>京东</td>
      <td>NaN</td>
      <td>30.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>如果想保留左表全部内容，而对右表不太在意的话，可以将how参数设置为left：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df3 = pd.merge(df1, df2, how=<span class="string">&#x27;left&#x27;</span>)  <span class="comment"># 左外连接</span></span><br><span class="line">df3</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>公司</th>
      <th>分数</th>
      <th>股价</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>90</td>
      <td>20.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>95</td>
      <td>180.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>百度</td>
      <td>85</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>



<p>同理，如果想保留右表全部内容，而对左表不太在意的话，可以将how参数设置为right。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想根据行索引进行合并，可以通过设置left_index和right_index参数，代码如下：</span></span><br><span class="line">df3 = pd.merge(df1, df2, left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>)</span><br><span class="line">df3</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>公司_x</th>
      <th>分数</th>
      <th>公司_y</th>
      <th>股价</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>90</td>
      <td>万科</td>
      <td>20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>95</td>
      <td>阿里</td>
      <td>180</td>
    </tr>
    <tr>
      <th>2</th>
      <td>百度</td>
      <td>85</td>
      <td>京东</td>
      <td>30</td>
    </tr>
  </tbody>
</table>
</div>



<p><strong>补充知识点：根据行索引合并的join()函数</strong></p>
<p>通过join()函数也可以根据行索引进行表格合并。join(）函数也是一种数据表拼接的常见函数，它是通过行索引进行合并，演示代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df3 = df1.join(df2, lsuffix=<span class="string">&#x27;_x&#x27;</span>, rsuffix=<span class="string">&#x27;_y&#x27;</span>)</span><br><span class="line">df3</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>公司_x</th>
      <th>分数</th>
      <th>公司_y</th>
      <th>股价</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>90</td>
      <td>万科</td>
      <td>20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>95</td>
      <td>阿里</td>
      <td>180</td>
    </tr>
    <tr>
      <th>2</th>
      <td>百度</td>
      <td>85</td>
      <td>京东</td>
      <td>30</td>
    </tr>
  </tbody>
</table>
</div>



<p>注意在通过join()函数进行拼接的时候，两张表格中不能有名字相同的列名，如果存在的话，则需要设置lsuffix参数（左表同名列的后缀，suffix的中文翻译就是后缀的意思，l表示left）和rsuffix参数（右表同名列的后缀，这里的r表示right），没有相同列名的话，则可以直接写df1.join(df2)，相对于merge()函数写法较为简洁一些。</p>
<p>实战中可以只记merge()函数的用法，这里讲解join()函数的目的是为了看到别人用join()函数的时候能够理解。该知识点在14.3.3小节进行数据表合并的时候便有应用。</p>
<p>(2) concat()函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认情况下，axis=0，按行方向进行连接。</span></span><br><span class="line">df3 = pd.concat([df1,df2], axis=<span class="number">0</span>) <span class="comment"># 全连接</span></span><br><span class="line">df3</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>公司</th>
      <th>分数</th>
      <th>股价</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>90.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>95.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>百度</td>
      <td>85.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>NaN</td>
      <td>20.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>NaN</td>
      <td>180.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>京东</td>
      <td>NaN</td>
      <td>30.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>(3) append()函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># append()函数可以说concat()函数的简化版，效果和pd.concat([df1,df2]) 类似，代码如下：</span></span><br><span class="line">df3 = df1.append(df2)</span><br><span class="line">df3</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>公司</th>
      <th>分数</th>
      <th>股价</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>90.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>95.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>百度</td>
      <td>85.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>NaN</td>
      <td>20.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>NaN</td>
      <td>180.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>京东</td>
      <td>NaN</td>
      <td>30.0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># append()函数还有个常用的功能，和列表.append()一样，可用来新增元素，代码如下：</span></span><br><span class="line">df3 = df1.append(&#123;<span class="string">&#x27;公司&#x27;</span>: <span class="string">&#x27;腾讯&#x27;</span>, <span class="string">&#x27;分数&#x27;</span>: <span class="string">&#x27;90&#x27;</span>&#125;, ignore_index=<span class="literal">True</span>)</span><br><span class="line">df3</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>公司</th>
      <th>分数</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>万科</td>
      <td>90</td>
    </tr>
    <tr>
      <th>1</th>
      <td>阿里</td>
      <td>95</td>
    </tr>
    <tr>
      <th>2</th>
      <td>百度</td>
      <td>85</td>
    </tr>
    <tr>
      <th>3</th>
      <td>腾讯</td>
      <td>90</td>
    </tr>
  </tbody>
</table>
</div>



<h2 id="2-3-Matplotlib库函数"><a href="#2-3-Matplotlib库函数" class="headerlink" title="2.3 Matplotlib库函数"></a>2.3 Matplotlib库函数</h2><h3 id="2-3-1-基本图形绘制"><a href="#2-3-1-基本图形绘制" class="headerlink" title="2.3.1 基本图形绘制"></a>2.3.1 基本图形绘制</h3><p>(1) 折线图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">y = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line">plt.plot(x, y)  <span class="comment"># 绘制折线图</span></span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</span><br><span class="line">pl.xticks(rotation=<span class="number">45</span>)</span><br><span class="line">plt.show()  <span class="comment"># 展示图形</span></span><br></pre></td></tr></table></figure>

<p><img src="output_143_0.png"></p>
<p>如果想让x和y之间有些数学关系，列表是不太容易进行数学运算的，这时候就可以通过2.1.2小节所讲的Numpy库引入一维数组进行数学运算，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x1 = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一条线：y = x + 1</span></span><br><span class="line">y1 = x1 + <span class="number">1</span></span><br><span class="line">plt.plot(x1, y1) <span class="comment"># 使用默认参数画图</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二条线：y = x*2</span></span><br><span class="line">y2 = x1*<span class="number">2</span></span><br><span class="line"><span class="comment"># color设置颜色，linewidth设置线宽，单位像素，linestyle默认为实线，“--”表示虚线</span></span><br><span class="line">plt.plot(x1, y2, color=<span class="string">&#x27;red&#x27;</span>, linewidth=<span class="number">3</span>, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_145_0.png"></p>
<p>(2) 柱状图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">y = [<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">plt.bar(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_147_0.png"></p>
<p>(3) 散点图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.random.rand(<span class="number">10</span>)</span><br><span class="line">y = np.random.rand(<span class="number">10</span>)</span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_149_0.png"></p>
<p>用np.random.rand(10)生成10个0~1之间的随机数</p>
<p>(4) 直方图</p>
<p>直方图分为频数直方图和频率直方图，横坐标为相关数据，纵坐标为该数据出现的频数或频率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成10000个服从正态分布的数据</span></span><br><span class="line">data = np.random.randn(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制直方图，bins为颗粒度，即直方图的长条形数目，edgecolor为长条形边框颜色</span></span><br><span class="line">plt.hist(data, bins=<span class="number">40</span>, edgecolor=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_152_0.png"></p>
<p>np.random.randn(10000)随机生成10000个服从标准正态分布的数据，其中x轴表示随机生成的数据，y轴表示该数据出现的次数，即频数。此外，若想绘制频率直方图，可以设置density参数为1</p>
<p><strong>补充知识点：在pandas库中的快捷绘图技巧</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这种写法只适合pandas中的DataFrame，不能直接用于Numpy的数组</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame(data)  <span class="comment"># 将绘制直方图中的data数组转换成DataFrame()格式</span></span><br><span class="line">df.hist(bins=<span class="number">40</span>, edgecolor=<span class="string">&#x27;black&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000002BB9DD80310&gt;]],
      dtype=object)
</code></pre>
<p><img src="output_155_1.png"></p>
<p>这种方法可以快速绘制和之前一样的直方图。因为这里的df只有一列，所以可以直接写df，如果有多列，那么就需要指明用哪一列绘制直方图，写成df[‘列名’].hist()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此外，除了写df.hist()外，还可以通过下面这种pandas库里的通用绘图代码绘图：</span></span><br><span class="line">df.plot(kind=<span class="string">&#x27;hist&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2bb9ee58760&gt;
</code></pre>
<p><img src="output_157_1.png"></p>
<p>这里是通过设置kind参数为hist来绘制直方图，通过这种通用绘图代码，pandas库除了可以便捷的绘制直方图外，它还可以通过设置kind参数快捷地绘制其他图形，演示代码如下，首先通过2.2.1节的知识点创建一个二维DataFrame表格df。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame([[<span class="number">8000</span>, <span class="number">6000</span>], [<span class="number">7000</span>, <span class="number">5000</span>], [<span class="number">6500</span>, <span class="number">4000</span>]], columns=[<span class="string">&#x27;人均收入&#x27;</span>, <span class="string">&#x27;人均支出&#x27;</span>], index=[<span class="string">&#x27;北京&#x27;</span>, <span class="string">&#x27;上海&#x27;</span>, <span class="string">&#x27;广州&#x27;</span>])</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>人均收入</th>
      <th>人均支出</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>北京</th>
      <td>8000</td>
      <td>6000</td>
    </tr>
    <tr>
      <th>上海</th>
      <td>7000</td>
      <td>5000</td>
    </tr>
    <tr>
      <th>广州</th>
      <td>6500</td>
      <td>4000</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时可以通过pandas同时绘制折线图或者柱状图，代码如下：</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决负号&#x27;-&#x27;显示为方块的问题</span></span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;人均收入&#x27;</span>].plot(kind=<span class="string">&#x27;line&#x27;</span>)  <span class="comment"># kind=line绘制折线图，不设置则默认折线图</span></span><br><span class="line">df[<span class="string">&#x27;人均收入&#x27;</span>].plot(kind=<span class="string">&#x27;bar&#x27;</span>)  <span class="comment"># kind=bar绘制柱状图</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2bb9eed2580&gt;
</code></pre>
<p><img src="output_160_1.png"></p>
<p>因为df有多列，所以要先通过df[‘列名’]的方式指明需要绘制的列数据，可以看到将折线图和柱状图绘制在一个坐标系中</p>
<p>此外，设置kind参数为pie，则可以绘制饼图，设置kind为box，可以绘制箱体图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;人均收入&#x27;</span>].plot(kind=<span class="string">&#x27;pie&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2bb9dcee4c0&gt;
</code></pre>
<p><img src="output_163_1.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;人均收入&#x27;</span>].plot(kind=<span class="string">&#x27;box&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2bb9f830a30&gt;
</code></pre>
<p><img src="output_164_1.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;人均收入&#x27;</span>].plot(kind=<span class="string">&#x27;area&#x27;</span>)  <span class="comment"># 绘制面积图</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2bb9dc4ed60&gt;
</code></pre>
<p><img src="output_165_1.png"></p>
<h3 id="2-3-2-数据可视化常见小技巧"><a href="#2-3-2-数据可视化常见小技巧" class="headerlink" title="2.3.2 数据可视化常见小技巧"></a>2.3.2 数据可视化常见小技巧</h3><p>(1) 添加文字说明</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过plt.title(name)给图画添加标题；通过plt.xlable()，plt.ylable()用于添加x轴和y轴标签。</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">y = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">&#x27;TITLE&#x27;</span>)  <span class="comment"># 添加标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>)  <span class="comment"># 添加X轴标签</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y&#x27;</span>)  <span class="comment"># 添加Y轴标签</span></span><br><span class="line">plt.show()  <span class="comment"># 显示图片</span></span><br></pre></td></tr></table></figure>


<p><img src="output_168_0.png"></p>
<p>(2) 添加图例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过plt.legend()来添加图例，添加前需要设置好lable（标签）参数，代码如下：</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一条线, 设定标签lable为y = x + 1</span></span><br><span class="line">x1 = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y1 = x1 + <span class="number">1</span></span><br><span class="line">plt.plot(x1, y1, label=<span class="string">&#x27;y = x + 1&#x27;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二条线, 设定标签lable为y = x*2</span></span><br><span class="line">y2 = x1*<span class="number">2</span></span><br><span class="line">plt.plot(x1, y2, color=<span class="string">&#x27;red&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&#x27;y = x*2&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>) <span class="comment"># 图例位置设置为左上角</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_170_0.png"></p>
<p>(3) 设置双坐标轴</p>
<p>上面的例子可以在一张图里画出两条线，但如果两条线的取值范围相差比较大，那么画出来的图效果便不太好，那么此时如何来画出两条y坐标轴呢？可以在画完第一个图之后，写如下一行代码即可设置双坐标轴。</p>
<p>plt.twinx()</p>
<p>需要注意的是如果设置了双坐标轴，那么添加图例的时候，每画一次图就得添加一次，而不能在最后统一添加。这里以y &#x3D; x和y &#x3D; x^2为例，演示下如何设置双坐标轴，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一条线, 设定标签lable为y = x</span></span><br><span class="line">x1 = np.array([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>])</span><br><span class="line">y1 = x1</span><br><span class="line">plt.plot(x1, y1, color=<span class="string">&#x27;red&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">&#x27;y = x&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)  <span class="comment"># 该图图例设置在左上角</span></span><br><span class="line"></span><br><span class="line">plt.twinx()  <span class="comment"># 设置双坐标轴</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二条线, 设定标签lable为y = x^2</span></span><br><span class="line">y2 = x1*x1</span><br><span class="line">plt.plot(x1, y2, label=<span class="string">&#x27;y = x^2&#x27;</span>) </span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)  <span class="comment"># 改图图例设置在右上角</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_173_0.png"></p>
<p>(4) 设置图片大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">y = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()  <span class="comment"># 显示图片</span></span><br></pre></td></tr></table></figure>


<p><img src="output_175_0.png"></p>
<p>(5) 设置X轴角度</p>
<p>如果x轴因为刻度内容较多，导致刻度太密，不便于阅读，可以通过设置刻度的角度来进行调节</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">y = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line">plt.plot(x, y)  <span class="comment"># 绘制折线图</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</span><br><span class="line">pl.xticks(rotation=<span class="number">45</span>)  <span class="comment"># 设置角度为45度</span></span><br><span class="line"></span><br><span class="line">plt.show()  <span class="comment"># 展示图形</span></span><br></pre></td></tr></table></figure>


<p><img src="output_177_0.png"></p>
<p>(6) 中文显示问题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决负号&#x27;-&#x27;显示为方块的问题</span></span><br><span class="line"></span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">y = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">&#x27;中文标题&#x27;</span>)  <span class="comment"># 添加标题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;中文X轴&#x27;</span>)  <span class="comment"># 添加X轴标签</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;中文Y轴&#x27;</span>)  <span class="comment"># 添加Y轴标签</span></span><br><span class="line">plt.show()  <span class="comment"># 显示图片</span></span><br></pre></td></tr></table></figure>


<p><img src="output_179_0.png"></p>
<p>(7) 绘制多图</p>
<p>如下图所示，有时我们需要在一张画布上输出多个图形，在Matplotlib库中有当前的图形（figure）以及当前轴（axes）概念，其对应的就是当前画布以及当前子图，在一张画布（figure）上可以绘制多个子图（axes）。绘制多图通常采用subplot()函数或subplots()函数，</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(3).png"></p>
<p>首先来讲解subplot()函数，如下图所示，它通常含有三个参数，子图的行数、列数以及第几个子图，例如subplot(221)表示的就是绘制2行2列的子图（共4个子图），并在第1个子图上进行绘图。</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(4).png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 演示代码如下：</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 绘制第一个子图：折线图</span></span><br><span class="line">ax1 = plt.subplot(<span class="number">221</span>)  </span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>])  <span class="comment"># 这里plt其实也可以换成ax1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制第二个子图：柱状图</span></span><br><span class="line">ax2 = plt.subplot(<span class="number">222</span>)  </span><br><span class="line">plt.bar([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制第三个子图：散点图</span></span><br><span class="line">ax3 = plt.subplot(<span class="number">223</span>)  </span><br><span class="line">plt.scatter([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制第四个子图：直方图</span></span><br><span class="line">ax4 = plt.subplot(<span class="number">224</span>)  </span><br><span class="line">plt.hist([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>




<pre><code>(array([3., 0., 0., 0., 0., 1., 0., 0., 0., 1.]),
 array([2. , 2.2, 2.4, 2.6, 2.8, 3. , 3.2, 3.4, 3.6, 3.8, 4. ]),
 &lt;a list of 10 Patch objects&gt;)
</code></pre>
<p><img src="output_185_1.png"></p>
<p>为了加强大家对画布（figure）和子图（axes）的理解，我们通过下面的代码来做一个简单演示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">8</span>, <span class="number">4</span>) <span class="comment"># 设置画布大小</span></span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>)  <span class="comment"># 第一张画布</span></span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)  <span class="comment"># 第一张画布的第一个子图</span></span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>])  <span class="comment"># 这里的plt可以换成ax1</span></span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)  <span class="comment"># 第一张画布的第二个子图</span></span><br><span class="line">plt.plot([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>], [<span class="number">4</span>, <span class="number">8</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">2</span>)  <span class="comment"># 第二张画布</span></span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure>




<pre><code>[&lt;matplotlib.lines.Line2D at 0x2bb9dc7b7f0&gt;]
</code></pre>
<p><img src="output_187_1.png"></p>
<p><img src="output_187_2.png"></p>
<p>在使用subplot()函数的时候，每次在新的子图上画图时，都得调用subplot()函数，例如第四个子图就得写成ax4 &#x3D; plt.subplot(224)，那有没有什么办法，一次性就生成多个子图呢？这时候就可以用到subplots()函数，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">8</span>)) <span class="comment"># 两行两列四张子图，1000x800像素</span></span><br><span class="line">ax1, ax2, ax3, ax4 = axes.flatten()  <span class="comment"># flatten()函数将子图展开，从而获得各张子图</span></span><br><span class="line">ax1.plot([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>])  <span class="comment"># 绘制第一个子图</span></span><br><span class="line">ax2.bar([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>])  <span class="comment"># 绘制第二个子图</span></span><br><span class="line">ax3.scatter([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>])  <span class="comment"># 绘制第三个子图</span></span><br><span class="line">ax4.hist([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])  <span class="comment"># 绘制第四个子图</span></span><br></pre></td></tr></table></figure>




<pre><code>(array([3., 0., 0., 0., 0., 1., 0., 0., 0., 1.]),
 array([2. , 2.2, 2.4, 2.6, 2.8, 3. , 3.2, 3.4, 3.6, 3.8, 4. ]),
 &lt;a list of 10 Patch objects&gt;)
</code></pre>
<p><img src="output_189_1.png"></p>
<p>此外，如果要在subplot()函数或者subplots()函数生成的子图中设置子图标题、X轴标签或Y轴标签，得通过set_title()函数、set_xlabel()函数、set_ylabel()函数进行设置，演示代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 用来正常显示中文标签</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">8</span>)) </span><br><span class="line">ax1, ax2, ax3, ax4 = axes.flatten()</span><br><span class="line">ax1.plot([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>])  <span class="comment"># 绘制第一个子图</span></span><br><span class="line">ax1.set_title(<span class="string">&#x27;子图1&#x27;</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">&#x27;日期&#x27;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;分数&#x27;</span>)</span><br><span class="line">ax2.bar([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>])  <span class="comment"># 绘制第二个子图</span></span><br><span class="line">ax3.scatter([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>])  <span class="comment"># 绘制第三个子图</span></span><br><span class="line">ax4.hist([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])  <span class="comment"># 绘制第四个子图</span></span><br></pre></td></tr></table></figure>




<pre><code>(array([3., 0., 0., 0., 0., 1., 0., 0., 0., 1.]),
 array([2. , 2.2, 2.4, 2.6, 2.8, 3. , 3.2, 3.4, 3.6, 3.8, 4. ]),
 &lt;a list of 10 Patch objects&gt;)
</code></pre>
<p><img src="output_191_1.png"></p>
<h2 id="2-4-案例实战：股票数据读取与K线图绘制"><a href="#2-4-案例实战：股票数据读取与K线图绘制" class="headerlink" title="2.4 案例实战：股票数据读取与K线图绘制"></a>2.4 案例实战：股票数据读取与K线图绘制</h2><h3 id="2-4-1-初步尝试：股票数据读取与可视化"><a href="#2-4-1-初步尝试：股票数据读取与可视化" class="headerlink" title="2.4.1 初步尝试：股票数据读取与可视化"></a>2.4.1 初步尝试：股票数据读取与可视化</h3><p>（1）股票数据读取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line">df = ts.get_k_data(<span class="string">&#x27;000002&#x27;</span>, start=<span class="string">&#x27;2009-01-01&#x27;</span>, end=<span class="string">&#x27;2019-01-01&#x27;</span>)<span class="comment"># 读取万科公司从2009-2019十年的股票日级别数据</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>

<pre><code>本接口即将停止更新，请尽快使用Pro版接口：https://tushare.pro/document/2
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>volume</th>
      <th>code</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2009-01-05</td>
      <td>-1.551</td>
      <td>-1.431</td>
      <td>-1.431</td>
      <td>-1.651</td>
      <td>936048.88</td>
      <td>000002</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2009-01-06</td>
      <td>-1.451</td>
      <td>-1.231</td>
      <td>-1.181</td>
      <td>-1.521</td>
      <td>1216831.18</td>
      <td>000002</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2009-01-07</td>
      <td>-1.201</td>
      <td>-1.271</td>
      <td>-1.071</td>
      <td>-1.271</td>
      <td>834829.31</td>
      <td>000002</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2009-01-08</td>
      <td>-1.381</td>
      <td>-1.231</td>
      <td>-1.131</td>
      <td>-1.451</td>
      <td>837661.70</td>
      <td>000002</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2009-01-09</td>
      <td>-1.231</td>
      <td>-1.241</td>
      <td>-1.121</td>
      <td>-1.321</td>
      <td>626815.66</td>
      <td>000002</td>
    </tr>
  </tbody>
</table>
</div>



<p>data为交易日期，open为开盘价，close为收盘价，high为最高价，low为最低价，volume为成交量，code为股票代码</p>
<p>将获取的数据写入excel工作簿</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.to_excel(<span class="string">&#x27;股价数据.xlsx&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>（2）绘制股价走势图</p>
<p>用set_index()函数将日期设置为行索引，以便直接使用pandas库绘图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.set_index(<span class="string">&#x27;date&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>volume</th>
      <th>code</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2009-01-05</th>
      <td>-1.551</td>
      <td>-1.431</td>
      <td>-1.431</td>
      <td>-1.651</td>
      <td>936048.88</td>
      <td>000002</td>
    </tr>
    <tr>
      <th>2009-01-06</th>
      <td>-1.451</td>
      <td>-1.231</td>
      <td>-1.181</td>
      <td>-1.521</td>
      <td>1216831.18</td>
      <td>000002</td>
    </tr>
    <tr>
      <th>2009-01-07</th>
      <td>-1.201</td>
      <td>-1.271</td>
      <td>-1.071</td>
      <td>-1.271</td>
      <td>834829.31</td>
      <td>000002</td>
    </tr>
    <tr>
      <th>2009-01-08</th>
      <td>-1.381</td>
      <td>-1.231</td>
      <td>-1.131</td>
      <td>-1.451</td>
      <td>837661.70</td>
      <td>000002</td>
    </tr>
    <tr>
      <th>2009-01-09</th>
      <td>-1.231</td>
      <td>-1.241</td>
      <td>-1.121</td>
      <td>-1.321</td>
      <td>626815.66</td>
      <td>000002</td>
    </tr>
  </tbody>
</table>
</div>



<p>通过2.3.1节补充知识点中pandas绘图的相关知识点来进行图形绘制，代码如下。因为在pandas库中plot()函数默认绘制的是折线图，所以直接写plot()即可，不需要传入kind参数。此外在金融领域，通常用收盘价作为当天价格来绘制股价走势图，因此这里选择的是close这一列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;close&#x27;</span>].plot()</span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2bba26c93a0&gt;
</code></pre>
<p><img src="output_203_1.png"></p>
<p>如果想给图片加一个标题，在pandas库中使用可以在plot()可以在里面传入一个title参数，代码如下，注意因为标题是中文内容，所以要写2.3.2节最后讲到的两行代码防止中文乱码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 用来正常显示中文标签</span></span><br><span class="line">df[<span class="string">&#x27;close&#x27;</span>].plot(title=<span class="string">&#x27;万科股价走势图&#x27;</span>)    </span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2bba29f36d0&gt;
</code></pre>
<p><img src="output_205_1.png"></p>
<p><strong>补充知识点：直接使用Matplotlib库画图的注意点</strong></p>
<p>上面使用的是pandas库中的plot()函数，pandas库其实是集成了Matplotlib库的一些功能，如果有的读者想直接用Matplotlib库进行股价走势画图，可以采用如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过Tushare库获取股价数据</span></span><br><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line">df = ts.get_k_data(<span class="string">&#x27;000002&#x27;</span>, start=<span class="string">&#x27;2009-01-01&#x27;</span>, end=<span class="string">&#x27;2019-01-01&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要注意的细节：调整日期格式使得横坐标显示清晰</span></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line">df[<span class="string">&#x27;date&#x27;</span>] = df[<span class="string">&#x27;date&#x27;</span>].apply(<span class="keyword">lambda</span> x:datetime.strptime(x,<span class="string">&#x27;%Y-%m-%d&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制折线图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(df[<span class="string">&#x27;date&#x27;</span>], df[<span class="string">&#x27;close&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>本接口即将停止更新，请尽快使用Pro版接口：https://tushare.pro/document/2
</code></pre>
<p><img src="output_208_1.png"></p>
<h3 id="2-4-2-进阶实战：股票K线图绘制（使用tushare-pro）"><a href="#2-4-2-进阶实战：股票K线图绘制（使用tushare-pro）" class="headerlink" title="2.4.2 进阶实战：股票K线图绘制（使用tushare pro）"></a>2.4.2 进阶实战：股票K线图绘制（使用tushare pro）</h3><p>（1）股票K线图基本知识</p>
<pre><code>一个实际中的股票K线图如下图所示（这个是“贵州茅台”股票的日线级别的K线图）：
</code></pre>
<p><img src="%E4%B8%8B%E8%BD%BD%20(5).png"></p>
<p>这些柱状图，通常称之为“K线图”，是由股票的四个价格来绘制的：开盘价（当天上午9点半开始交易时的价格）、收盘价（当天下午3点结束交易时的价格）、最高价（当天股价波动中的最高价）、最低价（当天股价波动中的最低价），简称“高、开、低、收”四个价格。<br>    如下图所示，根据这四个价格便可以绘制出红色和绿色的K线图，因为形似蜡烛，因此也常被称之为蜡烛图。K线图分为两种，如果当天的收盘价高于开盘价，也就是说当天的价格上涨，则称之为阳线，通常绘制成红色；反之如果当天的收盘价低于开盘价，也就是说当天的价格下跌，则称之为阴线，通常绘制成绿色。补充说一句，在美国，反而是红色代表跌，绿色代表涨。</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(6).png"></p>
<p>这里再解释下均线图，也就是那些折线图的绘制原理。均线分为5日均线（通常称之为MA5）、10日均线（通常称之为MA10）、20日均线（通常称之为MA20）等，其原理就是将股价的收盘价求均值，例如5日均线就是最近连续5个交易日收盘价之和的平均值，具体的计算公式如下，其中Close1为当天的收盘价，Close2为前一天的收盘价，其余依次类推。</p>
<p>MA5 &#x3D; (Close1 + Close2 + Close3 + Close4 + Close5)&#x2F;5</p>
<p>把每个5日均线的值连成一条平滑的曲线就是5日均线图了，同理10日均线图和20日均线图也是类似的原理，这些均线图也就是我们在这一小节最开始看到图中的那些折线图。<br>    了解了股票K线图的基本知识后，下面我们就来进行K线图的绘制工作。</p>
<p>（2）绘制股票K线图</p>
<p> 1.安装绘制K线图的相关库：mplfinance库</p>
<p>安装完mplfinance库便可以调用其中的plot()函数来绘制K线图或者说蜡烛图了，在正式绘制之前，我们还需要做一些前期的数据准备工作。</p>
<p>2.引入绘图相关库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line"><span class="keyword">import</span> mplfinance <span class="keyword">as</span> mpf</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>

<p>3.1 通过Tushare Pro获取股票基本数据</p>
<p>在这里我们使用的是Tushare Pro，需要到先到Tushare官（<a target="_blank" rel="noopener" href="https://tushare.pro/user/token%EF%BC%89">https://tushare.pro/user/token）</a> 注册账户以获取token。通过Tushare库获取股票代码为“000001.SZ”的股票“平安银行”在2020-01-01至2020-11-01的股价数据，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pro = ts.pro_api(<span class="string">&#x27;6a623020c3e0d5e4b7143b3e50cc56ba8e4b20c3d2ea12909c7620be&#x27;</span>)  </span><br><span class="line">df = pro.daily(ts_code=<span class="string">&#x27;000001.SZ&#x27;</span>, start_date=<span class="string">&#x27;20200101&#x27;</span>, end_date=<span class="string">&#x27;20201101&#x27;</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts_code</th>
      <th>trade_date</th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>pre_close</th>
      <th>change</th>
      <th>pct_chg</th>
      <th>vol</th>
      <th>amount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>000001.SZ</td>
      <td>20201030</td>
      <td>17.74</td>
      <td>18.36</td>
      <td>17.60</td>
      <td>17.75</td>
      <td>17.77</td>
      <td>-0.02</td>
      <td>-0.1125</td>
      <td>1007803.83</td>
      <td>1813064.343</td>
    </tr>
    <tr>
      <th>1</th>
      <td>000001.SZ</td>
      <td>20201029</td>
      <td>17.54</td>
      <td>17.93</td>
      <td>17.35</td>
      <td>17.77</td>
      <td>17.63</td>
      <td>0.14</td>
      <td>0.7941</td>
      <td>846603.62</td>
      <td>1498040.947</td>
    </tr>
    <tr>
      <th>2</th>
      <td>000001.SZ</td>
      <td>20201028</td>
      <td>17.76</td>
      <td>17.90</td>
      <td>17.29</td>
      <td>17.63</td>
      <td>17.76</td>
      <td>-0.13</td>
      <td>-0.7320</td>
      <td>1205823.86</td>
      <td>2125604.541</td>
    </tr>
    <tr>
      <th>3</th>
      <td>000001.SZ</td>
      <td>20201027</td>
      <td>18.00</td>
      <td>18.00</td>
      <td>17.50</td>
      <td>17.76</td>
      <td>17.70</td>
      <td>0.06</td>
      <td>0.3390</td>
      <td>1034865.04</td>
      <td>1839243.224</td>
    </tr>
    <tr>
      <th>4</th>
      <td>000001.SZ</td>
      <td>20201026</td>
      <td>18.20</td>
      <td>18.29</td>
      <td>17.45</td>
      <td>17.70</td>
      <td>18.13</td>
      <td>-0.43</td>
      <td>-2.3718</td>
      <td>1175598.65</td>
      <td>2085800.598</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>194</th>
      <td>000001.SZ</td>
      <td>20200108</td>
      <td>17.00</td>
      <td>17.05</td>
      <td>16.63</td>
      <td>16.66</td>
      <td>17.15</td>
      <td>-0.49</td>
      <td>-2.8571</td>
      <td>847824.12</td>
      <td>1423608.811</td>
    </tr>
    <tr>
      <th>195</th>
      <td>000001.SZ</td>
      <td>20200107</td>
      <td>17.13</td>
      <td>17.28</td>
      <td>16.95</td>
      <td>17.15</td>
      <td>17.07</td>
      <td>0.08</td>
      <td>0.4687</td>
      <td>728607.56</td>
      <td>1247047.135</td>
    </tr>
    <tr>
      <th>196</th>
      <td>000001.SZ</td>
      <td>20200106</td>
      <td>17.01</td>
      <td>17.34</td>
      <td>16.91</td>
      <td>17.07</td>
      <td>17.18</td>
      <td>-0.11</td>
      <td>-0.6403</td>
      <td>862083.50</td>
      <td>1477930.193</td>
    </tr>
    <tr>
      <th>197</th>
      <td>000001.SZ</td>
      <td>20200103</td>
      <td>16.94</td>
      <td>17.31</td>
      <td>16.92</td>
      <td>17.18</td>
      <td>16.87</td>
      <td>0.31</td>
      <td>1.8376</td>
      <td>1116194.81</td>
      <td>1914495.474</td>
    </tr>
    <tr>
      <th>198</th>
      <td>000001.SZ</td>
      <td>20200102</td>
      <td>16.65</td>
      <td>16.95</td>
      <td>16.55</td>
      <td>16.87</td>
      <td>16.45</td>
      <td>0.42</td>
      <td>2.5532</td>
      <td>1530231.87</td>
      <td>2571196.482</td>
    </tr>
  </tbody>
</table>
<p>199 rows × 11 columns</p>
</div>



<p>3.2 通过Tushare库获取股票基本数据</p>
<p>通过Tushare库获取股票代码为“000001”的股票“平安银行”在2020-01-01至2020-11-01的股价数据，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = ts.get_k_data(<span class="string">&#x27;000001&#x27;</span>,<span class="string">&#x27;2020-01-01&#x27;</span>, <span class="string">&#x27;2020-11-01&#x27;</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>

<pre><code>本接口即将停止更新，请尽快使用Pro版接口：https://tushare.pro/document/2
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>open</th>
      <th>close</th>
      <th>high</th>
      <th>low</th>
      <th>volume</th>
      <th>code</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2020-01-02</td>
      <td>16.024</td>
      <td>16.244</td>
      <td>16.324</td>
      <td>15.924</td>
      <td>1530231.0</td>
      <td>000001</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2020-01-03</td>
      <td>16.314</td>
      <td>16.554</td>
      <td>16.684</td>
      <td>16.294</td>
      <td>1116194.0</td>
      <td>000001</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2020-01-06</td>
      <td>16.384</td>
      <td>16.444</td>
      <td>16.714</td>
      <td>16.284</td>
      <td>862083.0</td>
      <td>000001</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2020-01-07</td>
      <td>16.504</td>
      <td>16.524</td>
      <td>16.654</td>
      <td>16.324</td>
      <td>728607.0</td>
      <td>000001</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2020-01-08</td>
      <td>16.374</td>
      <td>16.034</td>
      <td>16.424</td>
      <td>16.004</td>
      <td>847824.0</td>
      <td>000001</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>194</th>
      <td>2020-10-26</td>
      <td>17.792</td>
      <td>17.292</td>
      <td>17.882</td>
      <td>17.042</td>
      <td>1175599.0</td>
      <td>000001</td>
    </tr>
    <tr>
      <th>195</th>
      <td>2020-10-27</td>
      <td>17.592</td>
      <td>17.352</td>
      <td>17.592</td>
      <td>17.092</td>
      <td>1034865.0</td>
      <td>000001</td>
    </tr>
    <tr>
      <th>196</th>
      <td>2020-10-28</td>
      <td>17.352</td>
      <td>17.222</td>
      <td>17.492</td>
      <td>16.882</td>
      <td>1205824.0</td>
      <td>000001</td>
    </tr>
    <tr>
      <th>197</th>
      <td>2020-10-29</td>
      <td>17.132</td>
      <td>17.362</td>
      <td>17.522</td>
      <td>16.942</td>
      <td>846604.0</td>
      <td>000001</td>
    </tr>
    <tr>
      <th>198</th>
      <td>2020-10-30</td>
      <td>17.332</td>
      <td>17.342</td>
      <td>17.952</td>
      <td>17.192</td>
      <td>1007804.0</td>
      <td>000001</td>
    </tr>
  </tbody>
</table>
<p>199 rows × 7 columns</p>
</div>



<p>可以看出来使用Tushare库与使用Tushare Pro所得到的数据除了个别列名不同外（code, date与volume），其他都相似</p>
<p>4.1 Tushare Pro提取日期格式调整</p>
<p>在进行K线图绘制之前，得做一点数据准备工作。因为绘制的K线图需要按日期排列，所以我们需要将日期设置成索引，同时将原来文本类型的日期格式，调整一下，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#取所有行数据，后面取date列，open列等数据</span></span><br><span class="line">data = df.loc[:, [<span class="string">&#x27;trade_date&#x27;</span>, <span class="string">&#x27;open&#x27;</span>, <span class="string">&#x27;close&#x27;</span>, <span class="string">&#x27;high&#x27;</span>, <span class="string">&#x27;low&#x27;</span>, <span class="string">&#x27;vol&#x27;</span>]]</span><br><span class="line"><span class="comment">#更换列名，为后面函数变量做准备</span></span><br><span class="line">data = data.rename(columns=&#123;<span class="string">&#x27;trade_date&#x27;</span>: <span class="string">&#x27;Date&#x27;</span>, <span class="string">&#x27;open&#x27;</span>: <span class="string">&#x27;Open&#x27;</span>, <span class="string">&#x27;close&#x27;</span>: <span class="string">&#x27;Close&#x27;</span>, <span class="string">&#x27;high&#x27;</span>: <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;low&#x27;</span>: <span class="string">&#x27;Low&#x27;</span>, <span class="string">&#x27;vol&#x27;</span>: <span class="string">&#x27;Volume&#x27;</span>&#125;)  </span><br><span class="line"><span class="comment">#设置date列为索引，覆盖原来索引,这个时候索引还是 object 类型，就是字符串类型。</span></span><br><span class="line">data.set_index(<span class="string">&#x27;Date&#x27;</span> , inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#将object类型转化成 DateIndex 类型，pd.DatetimeIndex 是把某一列进行转换，同时把该列的数据设置为索引 index。</span></span><br><span class="line">data.index = pd.DatetimeIndex(data.index)</span><br><span class="line">data.index</span><br></pre></td></tr></table></figure>




<pre><code>DatetimeIndex([&#39;2020-11-03&#39;, &#39;2020-11-02&#39;, &#39;2020-10-30&#39;, &#39;2020-10-29&#39;,
               &#39;2020-10-28&#39;, &#39;2020-10-27&#39;, &#39;2020-10-26&#39;, &#39;2020-10-23&#39;,
               &#39;2020-10-22&#39;, &#39;2020-10-21&#39;,
               ...
               &#39;2020-01-15&#39;, &#39;2020-01-14&#39;, &#39;2020-01-13&#39;, &#39;2020-01-10&#39;,
               &#39;2020-01-09&#39;, &#39;2020-01-08&#39;, &#39;2020-01-07&#39;, &#39;2020-01-06&#39;,
               &#39;2020-01-03&#39;, &#39;2020-01-02&#39;],
              dtype=&#39;datetime64[ns]&#39;, name=&#39;Date&#39;, length=201, freq=None)
</code></pre>
<p>前2行代码先取所有行（：表示所有行）数据，然后再提取trade_date列，open列，close列，high列，low列和vol列的数据；</p>
<p>第3-4行代码将列名更换为更正式移动的名称，为后面函数变量做准备；</p>
<p>然后5-6行代码定义date列为索引，覆盖原先的索引，但是此时date还是字符串类型，我们需要在后面将它转换为时间戳索引；</p>
<p>第7-8行代码通过DatetimeIndex函数将字符类型转化成 DateIndex，同时设置索引。</p>
<p>（to_datetime函数在转化格式的功能上效果和DatetimeIndex函数一样，但是它仅转换格式，而DatetimeIndex函数还能设置为索引。）</p>
<p>转化成功。可以看到一开始的日期那列内容由文本类型的日期转换为数字格式的日期，且已经设置为index索引，这样就方便之后使用绘制K线图的plot()函数。但是此时时间依然按照从后向前的顺序排列，并不符合我们的习惯，所以我们还需要将日期按照从前向后的顺序升序排列，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = data.sort_index(ascending=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>绘制K线图</li>
</ol>
<p>转换好数据格式后，K线图的绘制就比较简单了，通过plot()函数便能够轻松的绘制K线图了，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mpf.plot(data, <span class="built_in">type</span>=<span class="string">&#x27;candle&#x27;</span>, mav=(<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>), volume=<span class="literal">True</span>, show_nontrading=<span class="literal">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p><img src="output_236_0.png"></p>
<p>这里需要强调的是参数data的类型，data必须是pandas.DataFrame数据类型，对所包含的列也有要求，必须包含’Open’, ‘High’, ‘Low’ 和 ‘Close’ 数据（注意:首字母是大写的），而且行索引必须是pandas.DatetimeIndex，行索引的名称必须是’Date‘(同理注意首字母大写)，此外还有一列是’Volume’，这一列不是必须的。</p>
<p>plot函数参数含义如下所示：</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(7).png"></p>
<ol start="6">
<li><p>图表美化</p>
<p> 我们还可以通过对一些格式的设置来美化K线图。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置字体</span></span><br><span class="line">mpl.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>] </span><br><span class="line"><span class="comment">#设置图片大小</span></span><br><span class="line">mpl.rcParams[<span class="string">&quot;figure.figsize&quot;</span>] = [<span class="number">6.4</span>, <span class="number">4.8</span>]</span><br><span class="line"><span class="comment">#解决保存图像时，负号无法显示问题</span></span><br><span class="line">mpl.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line"><span class="keyword">import</span> mplfinance <span class="keyword">as</span> mpf</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">pro = ts.pro_api(<span class="string">&#x27;6a623020c3e0d5e4b7143b3e50cc56ba8e4b20c3d2ea12909c7620be&#x27;</span>)  <span class="comment">#</span></span><br><span class="line"><span class="comment">#https://tushare.pro/user/token</span></span><br><span class="line">df = pro.daily(ts_code=<span class="string">&#x27;000001.SZ&#x27;</span>, start_date=<span class="string">&#x27;20200101&#x27;</span>, end_date=<span class="string">&#x27;20201103&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#df.sort_values(by=&#x27;trade_date&#x27;,ascending=False)</span></span><br><span class="line"><span class="comment">#取所有行数据，后面取date列，open列等数据</span></span><br><span class="line">data = df.loc[:, [<span class="string">&#x27;trade_date&#x27;</span>, <span class="string">&#x27;open&#x27;</span>, <span class="string">&#x27;close&#x27;</span>, <span class="string">&#x27;high&#x27;</span>, <span class="string">&#x27;low&#x27;</span>, <span class="string">&#x27;vol&#x27;</span>]]</span><br><span class="line">data = data.rename(columns=&#123;<span class="string">&#x27;trade_date&#x27;</span>: <span class="string">&#x27;Date&#x27;</span>, <span class="string">&#x27;open&#x27;</span>: <span class="string">&#x27;Open&#x27;</span>, <span class="string">&#x27;close&#x27;</span>: <span class="string">&#x27;Close&#x27;</span>, <span class="string">&#x27;high&#x27;</span>: <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;low&#x27;</span>: <span class="string">&#x27;Low&#x27;</span>, <span class="string">&#x27;vol&#x27;</span>: <span class="string">&#x27;Volume&#x27;</span>&#125;)  <span class="comment">#更换列名，为后面函数变量做准备</span></span><br><span class="line"><span class="comment">#设置date列为索引，覆盖原来索引,这个时候索引还是 object 类型，就是字符串类型。</span></span><br><span class="line">data.set_index(<span class="string">&#x27;Date&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#将object类型转化成 DateIndex 类型，pd.DatetimeIndex 是把某一列进行转换，同时把该列的数据设置为索引 index。</span></span><br><span class="line">data.index = pd.DatetimeIndex(data.index)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将时间顺序升序，符合时间序列</span></span><br><span class="line">data = data.sort_index(ascending=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#pd.set_option()就是pycharm输出控制显示的设置</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;expand_frame_repr&#x27;</span>, <span class="literal">False</span>)<span class="comment">#True就是可以换行显示。设置成False的时候不允许换行</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)<span class="comment"># 显示所有列</span></span><br><span class="line"><span class="comment">#pd.set_option(&#x27;display.max_rows&#x27;, None)# 显示所有行</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;colheader_justify&#x27;</span>, <span class="string">&#x27;centre&#x27;</span>)<span class="comment"># 显示居中</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mpl.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 指定默认字体</span></span><br><span class="line">mpl.rcParams[<span class="string">&quot;figure.figsize&quot;</span>] = [<span class="number">6.4</span>, <span class="number">4.8</span>]</span><br><span class="line">mpl.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决保存图像是负号&#x27;-&#x27;显示为方块的问题</span></span><br><span class="line"></span><br><span class="line">mpf.plot(data, <span class="built_in">type</span>=<span class="string">&#x27;candle&#x27;</span>, mav=(<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>), volume=<span class="literal">True</span>, show_nontrading=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>


<p><img src="output_240_0.png"></p>
<h1 id="3-线性回归模型"><a href="#3-线性回归模型" class="headerlink" title="3 线性回归模型"></a>3 线性回归模型</h1><h2 id="3-1-一元线性回归"><a href="#3-1-一元线性回归" class="headerlink" title="3.1 一元线性回归"></a>3.1 一元线性回归</h2><p><img src="%E4%B8%8B%E8%BD%BD%20(8).png"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(9).png"></p>
<h3 id="3-1-1-一元线性回归的数学原理"><a href="#3-1-1-一元线性回归的数学原理" class="headerlink" title="3.1.1 一元线性回归的数学原理"></a>3.1.1 一元线性回归的数学原理</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(10).png"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(11).png"></p>
<p>那么通过对残差平方和进行求导，然后令其导数为0便可以求得一元线性回归模型的系数a和截距b，这个便是一元线性回归的数学原理，学术上称其为最小二乘法</p>
<h3 id="3-1-2-一元线性回归的代码实现"><a href="#3-1-2-一元线性回归的代码实现" class="headerlink" title="3.1.2 一元线性回归的代码实现"></a>3.1.2 一元线性回归的代码实现</h3><p>1.绘制散点图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">X = [[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">4</span>], [<span class="number">5</span>]]</span><br><span class="line">Y = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>]</span><br><span class="line">plt.scatter(X, Y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_251_0.png"></p>
<p>需要注意的是，这里的自变量集合要写成二维结构的形式，即大列表里包含着小列表。这一点其实是符合之后要学习的多元回归的逻辑的，因为在多元回归中，一个因变量Y可能对应多个自变量X。例如，三元线性回归（即有三个特征变量）的自变量结合X就要写成类似如下形式：</p>
<p>X &#x3D; [[1,2,3],[2,4,5],[4,6,8],[5,7,9]]</p>
<p>2.引入Scikit-learn库搭建模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression  <span class="comment"># 从Scikit-Learn库引入线性回归相关模块LinearRegression</span></span><br><span class="line">regr = LinearRegression()  <span class="comment"># 构造一个初始的线性回归模型并命名为regr</span></span><br><span class="line">regr.fit(X,Y)  <span class="comment"># 利用fit()函数完成模型搭建，此时的regr就是一个搭建好的线性回归模型</span></span><br></pre></td></tr></table></figure>




<pre><code>LinearRegression()
</code></pre>
<p>3.模型预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型预测 - 预测一个数据</span></span><br><span class="line">y = regr.predict([[<span class="number">1.5</span>]])</span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>

<pre><code>[2.9]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型预测 - 预测多个数据</span></span><br><span class="line">y = regr.predict([[<span class="number">1.5</span>], [<span class="number">2.5</span>], [<span class="number">4.5</span>]])</span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>

<pre><code>[2.9 4.3 7.1]
</code></pre>
<p>4.模型可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X, Y)</span><br><span class="line">plt.plot(X, regr.predict(X))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_259_0.png"></p>
<p>此时的一元线性回归模型就是中间形成的一条直线，其原理为最小二乘法</p>
<p>5.线性回归方程构造</p>
<p>通过coef_和intercept_属性可以得到此时趋势线的系数和截距</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;系数a为:&#x27;</span> + <span class="built_in">str</span>(regr.coef_[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;截距b为:&#x27;</span> + <span class="built_in">str</span>(regr.intercept_))</span><br></pre></td></tr></table></figure>

<pre><code>系数a为:1.4000000000000004
截距b为:0.7999999999999989
</code></pre>
<p>因为通过regr.coef_获得的是一个列表，所以要选取其中的元素，又因为该元素为数字，所以进行字符串拼接时需要利用str()函数将其转换成字符串</p>
<p>因此得到的一元线性回归方程为y&#x3D;1.4x+0.8</p>
<h3 id="3-1-3-案例实战：工作年限与收入的线性回归模型"><a href="#3-1-3-案例实战：工作年限与收入的线性回归模型" class="headerlink" title="3.1.3 案例实战：工作年限与收入的线性回归模型"></a>3.1.3 案例实战：工作年限与收入的线性回归模型</h3><p>1.案例背景</p>
<p>通常来说，收入都会随着工作年限的增长而增长，而在不同的行业中收入的增长速度都会有所不同，本小节就是来通过一元线性回归模型来探寻工作年限对收入的影响，也即搭建收入预测模型，同时比较多个行业的收入预测模型来分析各个行业的特点。</p>
<p>2.读取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;IT行业收入表.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>工龄</th>
      <th>薪水</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>10808</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.1</td>
      <td>13611</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.2</td>
      <td>12306</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.3</td>
      <td>12151</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.3</td>
      <td>13057</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时的工龄为自变量，薪水为因变量，通过如下代码进行自变量、因变量选取</span></span><br><span class="line">X = df[[<span class="string">&#x27;工龄&#x27;</span>]]</span><br><span class="line">Y = df[<span class="string">&#x27;薪水&#x27;</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下代码可以将此时的散点图绘制出来：</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 用来正常显示中文标签</span></span><br><span class="line">plt.scatter(X,Y)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;工龄&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;薪水&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_270_0.png"></p>
<p>3.模型搭建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regr = LinearRegression()  <span class="comment"># 引入模型</span></span><br><span class="line">regr.fit(X,Y)  <span class="comment"># 训练模型</span></span><br></pre></td></tr></table></figure>




<pre><code>LinearRegression()
</code></pre>
<p>4.模型可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X,Y)</span><br><span class="line">plt.plot(X, regr.predict(X), color=<span class="string">&#x27;red&#x27;</span>)  <span class="comment"># color=&#x27;red&#x27;设置为红色</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;工龄&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;薪水&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_274_0.png"></p>
<p>5.线性回归方程构造</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;系数a为:&#x27;</span> + <span class="built_in">str</span>(regr.coef_[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;截距b为:&#x27;</span> + <span class="built_in">str</span>(regr.intercept_))</span><br></pre></td></tr></table></figure>

<pre><code>系数a为:2497.1513476046866
截距b为:10143.131966873787
</code></pre>
<p>所以此时的一元线性回归曲线方程为：y &#x3D; 2497*x + 10143</p>
<p><strong>补充知识点：模型优化 - 一元多次线性回归模型</strong></p>
<p>对于一元线性回归模型而言，其实它还有一个进阶版本，叫作一元多次线性回归模型，比较常见的有一元二次线性回归模型，其格式如下：</p>
<p><strong>y &#x3D; a<em>x^2 + b</em>x + c</strong></p>
<p>我们之所以还会研究一元多次线性回归模型，是因为有时真正契合的趋势线可能不是一条直线，而是一条曲线，比如下图根据一元二次线性回归模型形成的曲线更契合散点图背后的趋势。<br><img src="output_284_0.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过如下代码生成二次项数据：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures <span class="comment"># 引入用于增加一个多次项内容的模块PolynomialFeatures</span></span><br><span class="line">poly_reg = PolynomialFeatures(degree=<span class="number">2</span>) <span class="comment"># 设置最高次项为二次项，为生成二次项数据（x^2）做准备</span></span><br><span class="line">X_ = poly_reg.fit_transform(X)  <span class="comment"># 将代码原有的X转换为一个新的二维数组X_，该二维数组</span></span><br><span class="line">                                <span class="comment"># 包含新生成的二次项数据和原有一次项数据</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_</span><br></pre></td></tr></table></figure>




<pre><code>array([[1.000e+00, 0.000e+00, 0.000e+00],
       [1.000e+00, 1.000e-01, 1.000e-02],
       [1.000e+00, 2.000e-01, 4.000e-02],
       [1.000e+00, 3.000e-01, 9.000e-02],
       [1.000e+00, 3.000e-01, 9.000e-02],
       [1.000e+00, 3.000e-01, 9.000e-02],
       [1.000e+00, 4.000e-01, 1.600e-01],
       [1.000e+00, 4.000e-01, 1.600e-01],
       [1.000e+00, 4.000e-01, 1.600e-01],
       [1.000e+00, 5.000e-01, 2.500e-01],
       [1.000e+00, 5.000e-01, 2.500e-01],
       [1.000e+00, 8.000e-01, 6.400e-01],
       [1.000e+00, 8.000e-01, 6.400e-01],
       [1.000e+00, 1.000e+00, 1.000e+00],
       [1.000e+00, 1.000e+00, 1.000e+00],
       [1.000e+00, 1.000e+00, 1.000e+00],
       [1.000e+00, 1.100e+00, 1.210e+00],
       [1.000e+00, 1.100e+00, 1.210e+00],
       [1.000e+00, 1.200e+00, 1.440e+00],
       [1.000e+00, 1.200e+00, 1.440e+00],
       [1.000e+00, 1.200e+00, 1.440e+00],
       [1.000e+00, 1.400e+00, 1.960e+00],
       [1.000e+00, 1.400e+00, 1.960e+00],
       [1.000e+00, 1.500e+00, 2.250e+00],
       [1.000e+00, 1.500e+00, 2.250e+00],
       [1.000e+00, 1.600e+00, 2.560e+00],
       [1.000e+00, 1.600e+00, 2.560e+00],
       [1.000e+00, 1.800e+00, 3.240e+00],
       [1.000e+00, 1.800e+00, 3.240e+00],
       [1.000e+00, 2.200e+00, 4.840e+00],
       [1.000e+00, 2.200e+00, 4.840e+00],
       [1.000e+00, 2.200e+00, 4.840e+00],
       [1.000e+00, 2.300e+00, 5.290e+00],
       [1.000e+00, 2.400e+00, 5.760e+00],
       [1.000e+00, 2.400e+00, 5.760e+00],
       [1.000e+00, 2.400e+00, 5.760e+00],
       [1.000e+00, 2.500e+00, 6.250e+00],
       [1.000e+00, 2.800e+00, 7.840e+00],
       [1.000e+00, 2.900e+00, 8.410e+00],
       [1.000e+00, 3.000e+00, 9.000e+00],
       [1.000e+00, 3.000e+00, 9.000e+00],
       [1.000e+00, 3.200e+00, 1.024e+01],
       [1.000e+00, 3.300e+00, 1.089e+01],
       [1.000e+00, 3.300e+00, 1.089e+01],
       [1.000e+00, 3.500e+00, 1.225e+01],
       [1.000e+00, 3.600e+00, 1.296e+01],
       [1.000e+00, 3.600e+00, 1.296e+01],
       [1.000e+00, 3.800e+00, 1.444e+01],
       [1.000e+00, 4.100e+00, 1.681e+01],
       [1.000e+00, 4.400e+00, 1.936e+01],
       [1.000e+00, 4.400e+00, 1.936e+01],
       [1.000e+00, 4.800e+00, 2.304e+01],
       [1.000e+00, 4.800e+00, 2.304e+01],
       [1.000e+00, 4.800e+00, 2.304e+01],
       [1.000e+00, 4.900e+00, 2.401e+01],
       [1.000e+00, 4.900e+00, 2.401e+01],
       [1.000e+00, 5.000e+00, 2.500e+01],
       [1.000e+00, 5.100e+00, 2.601e+01],
       [1.000e+00, 5.400e+00, 2.916e+01],
       [1.000e+00, 5.500e+00, 3.025e+01],
       [1.000e+00, 5.500e+00, 3.025e+01],
       [1.000e+00, 5.500e+00, 3.025e+01],
       [1.000e+00, 5.600e+00, 3.136e+01],
       [1.000e+00, 5.700e+00, 3.249e+01],
       [1.000e+00, 5.700e+00, 3.249e+01],
       [1.000e+00, 6.100e+00, 3.721e+01],
       [1.000e+00, 6.200e+00, 3.844e+01],
       [1.000e+00, 6.300e+00, 3.969e+01],
       [1.000e+00, 6.300e+00, 3.969e+01],
       [1.000e+00, 6.400e+00, 4.096e+01],
       [1.000e+00, 6.400e+00, 4.096e+01],
       [1.000e+00, 6.500e+00, 4.225e+01],
       [1.000e+00, 6.500e+00, 4.225e+01],
       [1.000e+00, 6.500e+00, 4.225e+01],
       [1.000e+00, 6.600e+00, 4.356e+01],
       [1.000e+00, 6.800e+00, 4.624e+01],
       [1.000e+00, 6.800e+00, 4.624e+01],
       [1.000e+00, 6.800e+00, 4.624e+01],
       [1.000e+00, 6.800e+00, 4.624e+01],
       [1.000e+00, 6.900e+00, 4.761e+01],
       [1.000e+00, 7.100e+00, 5.041e+01],
       [1.000e+00, 7.100e+00, 5.041e+01],
       [1.000e+00, 7.100e+00, 5.041e+01],
       [1.000e+00, 7.100e+00, 5.041e+01],
       [1.000e+00, 7.100e+00, 5.041e+01],
       [1.000e+00, 7.200e+00, 5.184e+01],
       [1.000e+00, 7.300e+00, 5.329e+01],
       [1.000e+00, 7.400e+00, 5.476e+01],
       [1.000e+00, 7.400e+00, 5.476e+01],
       [1.000e+00, 7.400e+00, 5.476e+01],
       [1.000e+00, 7.400e+00, 5.476e+01],
       [1.000e+00, 7.500e+00, 5.625e+01],
       [1.000e+00, 7.500e+00, 5.625e+01],
       [1.000e+00, 7.500e+00, 5.625e+01],
       [1.000e+00, 7.800e+00, 6.084e+01],
       [1.000e+00, 7.800e+00, 6.084e+01],
       [1.000e+00, 7.800e+00, 6.084e+01],
       [1.000e+00, 7.900e+00, 6.241e+01],
       [1.000e+00, 7.900e+00, 6.241e+01],
       [1.000e+00, 8.000e+00, 6.400e+01]])
</code></pre>
<p>X_为二维数组，第一列为常数项，没有特殊含义，对分析结果不会产生影响；第二列数据为原有的一次项数据，第三列数据为新生成的二次项数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">regr = LinearRegression()</span><br><span class="line">regr.fit(X_, Y)</span><br></pre></td></tr></table></figure>




<pre><code>LinearRegression()
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">plt.scatter(X,Y)</span><br><span class="line">plt.plot(X, regr.predict(X_), color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_284_0.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印系数和常数项</span></span><br><span class="line"><span class="built_in">print</span>(regr.coef_)  <span class="comment"># 获取系数a, b </span></span><br><span class="line"><span class="built_in">print</span>(regr.intercept_)  <span class="comment"># 获取常数项c</span></span><br></pre></td></tr></table></figure>

<pre><code>[   0.         -743.68080444  400.80398224]
13988.159332096888
</code></pre>
<p>此时的系数项中为3个数，第一个0对应之前生成的X_常数项前面的系数，也对应之前说的X_的常数项不会产生影响；-743.68代表的X_一次项前面的系数，也即系数b；400.8代表的X_二次项前面的系数，也即系数a；而13988则代表常数项c，所以该一元二次线性回归方程为：</p>
<p><strong>y &#x3D; 400.8<em>x^2 - 743.68</em>x + 13988</strong></p>
<h3 id="3-1-4-案例教学1-加州房价预测（补充）"><a href="#3-1-4-案例教学1-加州房价预测（补充）" class="headerlink" title="3.1.4 案例教学1-加州房价预测（补充）"></a>3.1.4 案例教学1-加州房价预测（补充）</h3><p>本案例代码在colab上运行（colab牛逼）</p>
<p>本案例使用加州房价数据集，数据中包含17000个样本，其中包含每一个具体地区的经度、维度、房屋的平均年龄、房屋数量、家庭收入中位数等信息，这些信息都是加州地区房价的特征。数据集最后一列“房价中位数”是标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment">#导入Pandas，用于数据读取和处理</span></span><br><span class="line"><span class="comment"># 读入房价数据，示例代码中的文件地址为internet链接，读者也可以下载该文件到本机进行读取</span></span><br><span class="line"><span class="comment"># 如，当数据集和代码文件位于相同本地目录，路径名应为&quot;./house.csv&quot;，或直接放&quot;house.csv&quot;亦可</span></span><br><span class="line">df_housing = pd.read_csv(<span class="string">&quot;https://raw.githubusercontent.com/huangjia2019/house/master/house.csv&quot;</span>) </span><br><span class="line">df_housing.head <span class="comment">#显示加州房价数据</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;bound method NDFrame.head of        longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \</span><br><span class="line">0        -114.31     34.19                15.0       5612.0          1283.0   </span><br><span class="line">1        -114.47     34.40                19.0       7650.0          1901.0   </span><br><span class="line">2        -114.56     33.69                17.0        720.0           174.0   </span><br><span class="line">3        -114.57     33.64                14.0       1501.0           337.0   </span><br><span class="line">4        -114.57     33.57                20.0       1454.0           326.0   </span><br><span class="line">...          ...       ...                 ...          ...             ...   </span><br><span class="line">16995    -124.26     40.58                52.0       2217.0           394.0   </span><br><span class="line">16996    -124.27     40.69                36.0       2349.0           528.0   </span><br><span class="line">16997    -124.30     41.84                17.0       2677.0           531.0   </span><br><span class="line">16998    -124.30     41.80                19.0       2672.0           552.0   </span><br><span class="line">16999    -124.35     40.54                52.0       1820.0           300.0   </span><br><span class="line"></span><br><span class="line">       population  households  median_income  median_house_value  </span><br><span class="line">0          1015.0       472.0         1.4936             66900.0  </span><br><span class="line">1          1129.0       463.0         1.8200             80100.0  </span><br><span class="line">2           333.0       117.0         1.6509             85700.0  </span><br><span class="line">3           515.0       226.0         3.1917             73400.0  </span><br><span class="line">4           624.0       262.0         1.9250             65500.0  </span><br><span class="line">...           ...         ...            ...                 ...  </span><br><span class="line">16995       907.0       369.0         2.3571            111400.0  </span><br><span class="line">16996      1194.0       465.0         2.5179             79000.0  </span><br><span class="line">16997      1244.0       456.0         3.0313            103600.0  </span><br><span class="line">16998      1298.0       478.0         1.9797             85800.0  </span><br><span class="line">16999       806.0       270.0         3.0147             94600.0  </span><br><span class="line"></span><br><span class="line">[17000 rows x 9 columns]&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = df_housing.drop(<span class="string">&quot;median_house_value&quot;</span>,axis = <span class="number">1</span>) <span class="comment">#构建特征集X</span></span><br><span class="line">y = df_housing.median_house_value <span class="comment">#构建标签集y</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment">#导入数据集拆分工具</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, </span><br><span class="line">         test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>) <span class="comment">#以80%/20%的比例进行数据集的拆分</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression <span class="comment">#导入线性回归算法模型</span></span><br><span class="line">model = LinearRegression() <span class="comment">#使用线性回归算法</span></span><br><span class="line">model.fit(X_train, y_train) <span class="comment">#用训练集数据，训练机器，拟合函数，确定参数</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LinearRegression()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model.predict(X_test) <span class="comment">#预测测试集的Y值</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;房价的真值(测试集)&#x27;</span>,y_test)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;预测的房价(测试集)&#x27;</span>,y_pred)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">房价的真值(测试集) 3873     171400.0</span><br><span class="line">3625     189600.0</span><br><span class="line">3028     500001.0</span><br><span class="line">13814    229400.0</span><br><span class="line">15398    163400.0</span><br><span class="line">           ...   </span><br><span class="line">1363     212500.0</span><br><span class="line">7947     210500.0</span><br><span class="line">14574    142900.0</span><br><span class="line">10009    128300.0</span><br><span class="line">9149      84700.0</span><br><span class="line">Name: median_house_value, Length: 3400, dtype: float64</span><br><span class="line">预测的房价(测试集) [211157.06335418 218581.64298574 465317.31295564 ... 201751.2396963</span><br><span class="line"> 160873.51846959 138847.26913352]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;给预测评分：&quot;</span>, model.score(X_test, y_test)) <span class="comment">#评估预测结果,Sklearn线性回归模型的score属性给出的是R2分数，它是一个机器学习模型的评估指标</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">给预测评分： 0.6321014171579478</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#导入matplotlib画图库</span></span><br><span class="line"><span class="comment">#用散点图显示家庭收入中位数和房价中位数的分布</span></span><br><span class="line">plt.scatter(X_test.median_income, y_test,  color=<span class="string">&#x27;brown&#x27;</span>)</span><br><span class="line"><span class="comment">#画出回归函数(从特征到预测标签)</span></span><br><span class="line">plt.plot(X_test.median_income, y_pred, color=<span class="string">&#x27;green&#x27;</span>, linewidth=<span class="number">1</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Median Income&#x27;</span>) <span class="comment">#X轴-家庭收入中位数</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Median House Value&#x27;</span>) <span class="comment">#Y轴-房价中位数</span></span><br><span class="line">plt.show() <span class="comment">#显示房价分布和机器习得的函数图形</span></span><br></pre></td></tr></table></figure>

<p>![下载 (3787878)](Python大数据分析与机器学习商业案例实战-part1&#x2F;下载 (3787878).png)</p>
<h3 id="3-1-5-案例教学2-MNIST数据集"><a href="#3-1-5-案例教学2-MNIST数据集" class="headerlink" title="3.1.5 案例教学2-MNIST数据集"></a>3.1.5 案例教学2-MNIST数据集</h3><p>该数据集中包含了60000张训练图像和10000张测试图像，都是28px X 28px的手写数字灰度图像，此处要解决的问题是：将手写数字灰度图像分类为0，1，2，3，4，5，6，7，8，9共10个类别</p>
<p><strong>1.原始数据准备</strong></p>
<p><strong>2.数据预处理</strong></p>
<ul>
<li>可视化：</li>
<li>数据向量化</li>
<li>处理坏数据和缺失值</li>
<li>特征缩放：</li>
</ul>
<p>数据标准化：对数据特征分布的转换，目标是使其符合正态分布。在实践中，会去除特征的均值来转换数据，使其居中，然后除以特征的标准差来对其进行缩放</p>
<p>标准化的一种变体是将特征压缩到给定的最小值和最大值之间，通常为0~1，叫做归一化</p>
<p>规范化：将样本缩放为具有单位范数的过程，然后放入机器学习模型，这个过程消除了数据中的离群值</p>
<p><strong>3. 特征工程</strong></p>
<p><strong>4. 载入MNIST数据集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 导入NumPy数学工具箱</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># 导入Pandas数据处理工具箱</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist <span class="comment">#从Keras中导入mnist数据集</span></span><br><span class="line"><span class="comment">#读入训练集和测试集</span></span><br><span class="line">(X_train_image, y_train_lable), (X_test_image, y_test_lable) =  mnist.load_data() <span class="comment"># 训练集和测试集的特征都是图片，训练集和测试集的标签都是数字</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz</span><br><span class="line">11490434/11490434 [==============================] - 0s 0us/step</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&quot;特征集张量形状：&quot;</span>, X_train_image.shape) <span class="comment">#用shape方法显示张量的形状</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;第一个数据样本：\n&quot;</span>, X_train_image[<span class="number">0</span>]) <span class="comment">#注意Python的索引是从0开始的</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">特征集张量形状： (60000, 28, 28)</span><br><span class="line">第一个数据样本：</span><br><span class="line"> [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136</span><br><span class="line">  175  26 166 255 247 127   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253</span><br><span class="line">  225 172 253 242 195  64   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251</span><br><span class="line">   93  82  82  56  39   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119</span><br><span class="line">   25   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253</span><br><span class="line">  150  27   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252</span><br><span class="line">  253 187   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249</span><br><span class="line">  253 249  64   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253</span><br><span class="line">  253 207   2   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253</span><br><span class="line">  250 182   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201</span><br><span class="line">   78   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]</span><br><span class="line"> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0</span><br><span class="line">    0   0   0   0   0   0   0   0   0   0]]</span><br></pre></td></tr></table></figure>

<p>（凑合看，实际上可以看出一个5的形状）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&quot;第一个数据样本的标签：&quot;</span>, y_train_lable[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一个数据样本的标签： 5</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上面的数据集在输入机器学习模型之前还要做一些数据格式转换的工作</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.utils <span class="keyword">import</span> to_categorical <span class="comment"># 导入keras.utils工具箱的类别转换工具</span></span><br><span class="line">X_train = X_train_image.reshape(<span class="number">60000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>) <span class="comment"># 给标签增加一个维度</span></span><br><span class="line">X_test = X_test_image.reshape(<span class="number">10000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>) <span class="comment"># 给标签增加一个维度</span></span><br><span class="line">y_train = to_categorical(y_train_lable, <span class="number">10</span>) <span class="comment"># 特征转换为one-hot编码</span></span><br><span class="line">y_test = to_categorical(y_test_lable, <span class="number">10</span>) <span class="comment"># 特征转换为one-hot编码</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;数据集张量形状：&quot;</span>, X_train.shape) <span class="comment"># 特征集张量的形状</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;第一个数据标签：&quot;</span>,y_train[<span class="number">0</span>]) <span class="comment"># 显示标签集的第一个数据</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据集张量形状： (60000, 28, 28, 1)</span><br><span class="line">第一个数据标签： [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]</span><br></pre></td></tr></table></figure>

<p>解释为何要用新格式：</p>
<p>（1）Keras要求图像数据集导入卷积网络模型时为4阶张量，最后一阶代表颜色深度，灰度图像只有一个颜色通道，可以设置其为1。</p>
<p>（2）在机器学习分类问题中，标签[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]就代表着类别值为5。这是等会儿还要提到的one-hot编码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面引入卷积神经网络</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models <span class="comment"># 导入Keras模型, 和各种神经网络的层</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D</span><br><span class="line">model = models.Sequential() <span class="comment"># 用序贯方式建立模型</span></span><br><span class="line">model.add(Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, <span class="comment"># 添加Conv2D层</span></span><br><span class="line">                 input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>))) <span class="comment"># 指定输入数据样本张量的类型</span></span><br><span class="line">model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>))) <span class="comment"># 添加MaxPooling2D层</span></span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加Conv2D层</span></span><br><span class="line">model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>))) <span class="comment"># 添加MaxPooling2D层</span></span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>)) <span class="comment"># 添加Dropout层</span></span><br><span class="line">model.add(Flatten()) <span class="comment"># 展平</span></span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加全连接层</span></span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>)) <span class="comment"># 添加Dropout层</span></span><br><span class="line">model.add(Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)) <span class="comment"># Softmax分类激活，输出10维分类码</span></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;rmsprop&#x27;</span>, <span class="comment"># 指定优化器</span></span><br><span class="line">              loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, <span class="comment"># 指定损失函数</span></span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>]) <span class="comment"># 指定验证过程中的评估指标</span></span><br></pre></td></tr></table></figure>

<p>以上这段代码把数据集放入卷积神经网络进行处理。这个网络中包括两个Conv2D（二维卷积）层，两个MaxPooling2D（最大池化）层，两个Dropout层用于防止过拟合，还有Dense（全连接）层，最后通过Softmax分类器输出预测标签y’值，也就是所谓的分类值。这个y’值，是一个One-hot（一位有效编码）格式的10维向量。我们可以将y’与标签值y进行比较，以计算预测的准确率。</p>
<p>确定好机器学习模型算法后，就开始进行训练，训练机器以确定最佳的模型内部参数。</p>
<ul>
<li>内部参数：机器学习模型的具体参数值，y&#x3D;2x+1中的2和1就是模型的内参数。也叫做权重和偏置。神经网络也类似，每一个节点都有自己的权重，网络的每一层也有偏置。模型内参数在机器的训练过程中被确定，机器学习的过程就是把这些参数的最佳值找出来。</li>
<li>超参数：位于机器学习的外部，属于训练和调试过程中的参数。机器学习应该迭代（被训练）多少次？迭代时模型参数改变的速率（即学习率）时多大？正则化参数如何选择？这些都是超参数，他们需要在反复调试的过程中被最终确定。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, y_train, <span class="comment"># 指定训练特征集和训练标签集</span></span><br><span class="line">          validation_split = <span class="number">0.3</span>, <span class="comment"># 部分训练集数据拆分成验证集</span></span><br><span class="line">          epochs=<span class="number">5</span>, <span class="comment"># 训练轮次为5轮</span></span><br><span class="line">          batch_size=<span class="number">128</span>) <span class="comment"># 以128为批量进行训练</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/5</span><br><span class="line">329/329 [==============================] - 48s 139ms/step - loss: 1.0680 - accuracy: 0.8254 - val_loss: 0.1235 - val_accuracy: 0.9667</span><br><span class="line">Epoch 2/5</span><br><span class="line">329/329 [==============================] - 47s 141ms/step - loss: 0.1789 - accuracy: 0.9510 - val_loss: 0.1359 - val_accuracy: 0.9612</span><br><span class="line">Epoch 3/5</span><br><span class="line">329/329 [==============================] - 45s 137ms/step - loss: 0.1295 - accuracy: 0.9641 - val_loss: 0.0678 - val_accuracy: 0.9818</span><br><span class="line">Epoch 4/5</span><br><span class="line">329/329 [==============================] - 49s 150ms/step - loss: 0.1096 - accuracy: 0.9702 - val_loss: 0.0563 - val_accuracy: 0.9846</span><br><span class="line">Epoch 5/5</span><br><span class="line">329/329 [==============================] - 45s 136ms/step - loss: 0.0949 - accuracy: 0.9735 - val_loss: 0.0621 - val_accuracy: 0.9839</span><br><span class="line">&lt;keras.callbacks.History at 0x7f2bc973f5b0&gt;</span><br></pre></td></tr></table></figure>

<p>以上五轮中，准确率逐步提高</p>
<ul>
<li>accuracy: 代表<strong>训练集</strong>上的预测准确率，最后一轮达到0.9735</li>
<li>val_accuracy: 代表<strong>验证集</strong>上的预测准确率，最后一轮达到0.9839</li>
</ul>
<hr>
<p>机器学习重在评估，下面介绍两种重要的评估点：</p>
<ul>
<li><strong>损失函数</strong>，内部参数的评估方法，这些损失函数指出了当前模型针对训练集的预测误差，这个过程在调用fit()方法后就已经完成了</li>
<li><strong>验证</strong>，这个卷积神经网络的验证方式为metrics&#x3D;[‘accuracy’]，即<strong>分类的准确率</strong>作为验证指标</li>
</ul>
<p><strong>超参数优化和性能优化</strong></p>
<ol>
<li>训练集、验证集和测试集</li>
</ol>
<p>为了进行模型的评估，一般把数据集分成三部分：训练集、验证集和测试集。在训练集上训练模型，在验证集上评估模型。感觉已经找到最佳的模型内部参数和超参数之后，就在测试集上进行最终测试，以确定模型。</p>
<p>在训练的过程中，机器会自动调节模型内部参数，可能会出现过拟合；解决过拟合后，在继续优化的过程中，又需要反复地调整模型外部的超参数，这个过程是在<strong>训练集和验证集</strong>中共同完成的。当然，即使我们选择了对验证集效果最好的超参数，这个好结果也不一定真能泛化到最终的测试集；最后我们需要用一个完全不同的、前所未见的数据集对模型进行最终的评估和校正，他就是测试集。在最终验证之前，我们的模型一定不能读取任何与测试集有关的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score = model.evaluate(X_test, y_test) <span class="comment"># 在测试集上进行模型评估</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集预测准确率:&#x27;</span>, score[<span class="number">1</span>]) <span class="comment"># 打印测试集上的预测准确率</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">313/313 [==============================] - 4s 14ms/step - loss: 0.0479 - accuracy: 0.9874</span><br><span class="line">测试集预测准确率: 0.9873999953269958</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>K折验证</li>
</ol>
<p>如果有足够的数据可用，一般来说按照6：2：2的比例划分为训练集、验证集和测试集。但是如果数据不太够用。就需要使用K折检验</p>
<ol start="3">
<li><p>模型的优化和泛化</p>
</li>
<li><p>怎样查看预测结果</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pred = model.predict(X_test[<span class="number">0</span>].reshape(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)) <span class="comment"># 预测测试集第一个数据</span></span><br><span class="line"><span class="built_in">print</span>(pred[<span class="number">0</span>],<span class="string">&quot;转换一下格式得到：&quot;</span>,pred.argmax()) <span class="comment"># 把one-hot码转换为数字</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 导入绘图工具包</span></span><br><span class="line">plt.imshow(X_test[<span class="number">0</span>].reshape(<span class="number">28</span>, <span class="number">28</span>),cmap=<span class="string">&#x27;Greys&#x27;</span>) <span class="comment"># 输出这个图片</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/1 [==============================] - 0s 208ms/step</span><br><span class="line">[3.7988331e-15 7.4645612e-14 3.8871900e-12 7.8432694e-12 1.0137209e-12</span><br><span class="line"> 4.8863090e-15 5.3420574e-20 1.0000000e+00 5.5676674e-15 4.6507364e-11] 转换一下格式得到： 7</span><br><span class="line">&lt;matplotlib.image.AxesImage at 0x7f2bca0530d0&gt;</span><br></pre></td></tr></table></figure>

<p>![下载 (46)](Python大数据分析与机器学习商业案例实战-part1&#x2F;下载 (46).png)</p>
<h3 id="3-1-6-线性回归——预测网店的销售额"><a href="#3-1-6-线性回归——预测网店的销售额" class="headerlink" title="3.1.6 线性回归——预测网店的销售额"></a>3.1.6 线性回归——预测网店的销售额</h3><p>本案例代码在kaggle上运行（kaggle牛逼）</p>
<h4 id="1-数据的收集和预处理"><a href="#1-数据的收集和预处理" class="headerlink" title="1.数据的收集和预处理"></a>1.数据的收集和预处理</h4><h5 id="1-1收集网店销售额数据"><a href="#1-1收集网店销售额数据" class="headerlink" title="1.1收集网店销售额数据"></a>1.1收集网店销售额数据</h5><table>
<thead>
<tr>
<th align="right"></th>
<th align="right">wechat</th>
<th align="right">weibo</th>
<th align="right">others</th>
<th>sales</th>
</tr>
</thead>
<tbody><tr>
<td align="right">0</td>
<td align="right">304.4</td>
<td align="right">93.6</td>
<td align="right">294.4</td>
<td>9.7</td>
</tr>
<tr>
<td align="right">1</td>
<td align="right">1011.9</td>
<td align="right">34.4</td>
<td align="right">398.4</td>
<td>16.7</td>
</tr>
<tr>
<td align="right">2</td>
<td align="right">1091.1</td>
<td align="right">32.8</td>
<td align="right">295.2</td>
<td>17.3</td>
</tr>
<tr>
<td align="right">3</td>
<td align="right">85.5</td>
<td align="right">173.6</td>
<td align="right">403.2</td>
<td>7.0</td>
</tr>
<tr>
<td align="right">4</td>
<td align="right">1047.0</td>
<td align="right">302.4</td>
<td align="right">553.6</td>
<td>22.1</td>
</tr>
</tbody></table>
<p>前三列分别是微信、微博、其他类型广告投放金额，这三个是特征，商品销售额是标签</p>
<h5 id="1-2数据读取和可视化"><a href="#1-2数据读取和可视化" class="headerlink" title="1.2数据读取和可视化"></a>1.2数据读取和可视化</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#导入NumPy数学工具箱</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment">#导入Pandas数据处理工具箱</span></span><br><span class="line"><span class="comment">#读入数据并显示前面几行的内容，确保已经成功的读入数据</span></span><br><span class="line"><span class="comment">#示例代码是在Kaggle中数据集中读入文件，如果在本机中需要指定具体本地路径</span></span><br><span class="line"><span class="comment"># 如，当数据集和代码文件位于相同本地目录，路径名应为&#x27;./advertising.csv&#x27;，或直接放&#x27;advertising.csv&#x27;亦可</span></span><br><span class="line">df_ads = pd.read_csv(<span class="string">&#x27;../input/advertising/advertising.csv&#x27;</span>)</span><br><span class="line">df_ads.head()</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">wechat</th>
<th align="right">weibo</th>
<th align="right">others</th>
<th>sales</th>
</tr>
</thead>
<tbody><tr>
<td align="right">0</td>
<td align="right">304.4</td>
<td align="right">93.6</td>
<td align="right">294.4</td>
<td>9.7</td>
</tr>
<tr>
<td align="right">1</td>
<td align="right">1011.9</td>
<td align="right">34.4</td>
<td align="right">398.4</td>
<td>16.7</td>
</tr>
<tr>
<td align="right">2</td>
<td align="right">1091.1</td>
<td align="right">32.8</td>
<td align="right">295.2</td>
<td>17.3</td>
</tr>
<tr>
<td align="right">3</td>
<td align="right">85.5</td>
<td align="right">173.6</td>
<td align="right">403.2</td>
<td>7.0</td>
</tr>
<tr>
<td align="right">4</td>
<td align="right">1047.0</td>
<td align="right">302.4</td>
<td align="right">553.6</td>
<td>22.1</td>
</tr>
</tbody></table>
<h5 id="1-3数据的相关分析"><a href="#1-3数据的相关分析" class="headerlink" title="1.3数据的相关分析"></a>1.3数据的相关分析</h5><p>相关分析就是看两个变量的相关性（-1，1），正值表示正相关，负值表示负相关，数值越大相关性越强，下面通过热力图直观表现相关性强弱</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入数据可视化所需要的库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#Matplotlib – Python画图工具库</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment">#Seaborn – 统计学数据可视化工具库</span></span><br><span class="line"><span class="comment">#对所有的标签和特征两两显示其相关性的热力图(heatmap)</span></span><br><span class="line">sns.heatmap(df_ads.corr(), cmap=<span class="string">&quot;YlGnBu&quot;</span>, annot = <span class="literal">True</span>)</span><br><span class="line">plt.show() <span class="comment">#plt代表英文plot,就是画图的意思</span></span><br></pre></td></tr></table></figure>

<p>![下载 (47)](Python大数据分析与机器学习商业案例实战-part1&#x2F;下载 (47).png)</p>
<p>可以看到微信公众号投放金额和销售额相关性很强</p>
<h5 id="1-4数据的散点图"><a href="#1-4数据的散点图" class="headerlink" title="1.4数据的散点图"></a>1.4数据的散点图</h5><p>下面通过散点图两两一组展示商品销售额和各种广告投放金额之间的对应关系，来将重点聚焦</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#显示销量和各种广告投放量的散点图</span></span><br><span class="line">sns.pairplot(df_ads, </span><br><span class="line">             x_vars=[<span class="string">&#x27;wechat&#x27;</span>, <span class="string">&#x27;weibo&#x27;</span>, <span class="string">&#x27;others&#x27;</span>], </span><br><span class="line">             y_vars=<span class="string">&#x27;sales&#x27;</span>, </span><br><span class="line">             height=<span class="number">4</span>, aspect=<span class="number">1</span>, kind=<span class="string">&#x27;scatter&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>![下载 (48)](Python大数据分析与机器学习商业案例实战-part1&#x2F;下载 (48).png)</p>
<h5 id="1-5数据集清洗和规范化"><a href="#1-5数据集清洗和规范化" class="headerlink" title="1.5数据集清洗和规范化"></a>1.5数据集清洗和规范化</h5><p>我们观察到只有微信公众号和销售额的关系很直观，所以我们暂且将其余两个特征清洗掉，并将数据从一个1D张量变为2D张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = np.array(df_ads.wechat) <span class="comment">#构建特征集，只含有微信广告一个特征</span></span><br><span class="line">y = np.array(df_ads.sales) <span class="comment">#构建标签集，销售金额</span></span><br><span class="line">X = X.reshape((<span class="built_in">len</span>(X),<span class="number">1</span>)) <span class="comment">#通过reshape函数把向量转换为矩阵，len函数返回样本个数</span></span><br><span class="line">y = y.reshape((<span class="built_in">len</span>(y),<span class="number">1</span>)) <span class="comment">#通过reshape函数把向量转换为矩阵，len函数返回样本个数print (&quot;张量X的阶:&quot;,X.ndim)</span></span><br></pre></td></tr></table></figure>

<h5 id="1-6拆分数据集为训练集和测试集"><a href="#1-6拆分数据集为训练集和测试集" class="headerlink" title="1.6拆分数据集为训练集和测试集"></a>1.6拆分数据集为训练集和测试集</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将数据集进行80%(训练集)和20%(测试集)的分割</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, </span><br><span class="line">                                   test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h5 id="1-7把数据归一化"><a href="#1-7把数据归一化" class="headerlink" title="1.7把数据归一化"></a>1.7把数据归一化</h5><p>数据归一化后，数据分布不变，但是都落入一个小的特定区间内，比如0<del>1或者-1</del>+1，我们定义一个归一化函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scaler</span>(<span class="params">train, test</span>): <span class="comment">#定义归一化函数，进行数据压缩    </span></span><br><span class="line">    <span class="built_in">min</span> = train.<span class="built_in">min</span>(axis=<span class="number">0</span>) <span class="comment">#训练集最小值</span></span><br><span class="line">    <span class="built_in">max</span> = train.<span class="built_in">max</span>(axis=<span class="number">0</span>) <span class="comment">#训练集最大值</span></span><br><span class="line">    gap = <span class="built_in">max</span> - <span class="built_in">min</span> <span class="comment">#最大值和最小值的差</span></span><br><span class="line">    train -= <span class="built_in">min</span> <span class="comment">#所有数据减最小值</span></span><br><span class="line">    train /= gap <span class="comment">#所有数据除以大小值差</span></span><br><span class="line">    test -= <span class="built_in">min</span> <span class="comment">#把训练集最小值应用于测试集</span></span><br><span class="line">    test /= gap <span class="comment">#把训练集大小值差应用于测试集</span></span><br><span class="line">    <span class="keyword">return</span> train, test <span class="comment">#返回压缩后的数据</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：归一化函数中所有的数值均来自训练集，不能使用测试集中的数据信息进行特征缩放中间步骤中任何值的计算</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train,X_test = scaler(X_train,X_test) <span class="comment">#对特征归一化</span></span><br><span class="line">y_train,y_test = scaler(y_train,y_test) <span class="comment">#对标签也归一化</span></span><br></pre></td></tr></table></figure>

<p>下面的代码显示数据被压缩处理后的散点图，形状和之前完全一样，只是数值已经被限制在一个较小的区间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用之前已经导入的matplotlib.pyplot中的plot方法显示散点图</span></span><br><span class="line">plt.plot(X_train,y_train,<span class="string">&#x27;r.&#x27;</span>, label=<span class="string">&#x27;Training data&#x27;</span>) </span><br><span class="line">plt.xlabel(<span class="string">&#x27;Wechat Ads&#x27;</span>) <span class="comment"># x轴Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Sales&#x27;</span>) <span class="comment"># y轴Label</span></span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show() <span class="comment"># 显示绘图结果</span></span><br></pre></td></tr></table></figure>

<p>![下载 (49)](Python大数据分析与机器学习商业案例实战-part1&#x2F;下载 (49).png)</p>
<h4 id="2-选择机器学习模型"><a href="#2-选择机器学习模型" class="headerlink" title="2.选择机器学习模型"></a>2.选择机器学习模型</h4><h5 id="2-1-确定线性回归模型"><a href="#2-1-确定线性回归模型" class="headerlink" title="2.1 确定线性回归模型"></a>2.1 确定线性回归模型</h5><p>y&#x3D;wx+b</p>
<p>w代表权重，因为在多元变量中，一个特征对应的w参数值越大，就表示权重越大。参数b为偏置</p>
<h5 id="2-2-损失（误差）函数——L（w，b）"><a href="#2-2-损失（误差）函数——L（w，b）" class="headerlink" title="2.2 损失（误差）函数——L（w，b）"></a>2.2 损失（误差）函数——L（w，b）</h5><p>损失函数（loss function）用来计算平均损失的。</p>
<p>损失函数L是参数w和b的函数。计算当前假设函数所造成的损失的过程，就是前面提到过的模型内部参数的评估的过程。</p>
<p>机器学习中损失函数很多，主要包括：</p>
<ul>
<li>用于回归的损失函数：均方误差函数&#x2F;平方损失&#x2F;L2损失函数，平均绝对误差函数&#x2F;L1损失函数，平均偏差误差函数</li>
<li>用于分类的损失函数：交叉熵损失函数，多分类SVM损失函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">X, y, weight, bias</span>): <span class="comment"># 手工定义一个MSE均方误差函数</span></span><br><span class="line">    y_hat = weight*X + bias <span class="comment"># 这是假设函数,其中已经应用了Python的广播功能</span></span><br><span class="line">    loss = y_hat-y  <span class="comment"># 求出每一个y’和训练集中真实的y之间的差异 </span></span><br><span class="line">    cost = np.<span class="built_in">sum</span>(loss**<span class="number">2</span>)/(<span class="number">2</span>*<span class="built_in">len</span>(X)) <span class="comment"># 这是均方误差函数的代码实现</span></span><br><span class="line">    <span class="keyword">return</span> cost <span class="comment"># 返回当前模型的均方误差值</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&quot;当权重5，偏置3时，损失为：&quot;</span>,   <span class="comment"># 随便设置两个参数，看看其均方误差大小</span></span><br><span class="line">loss_function(X_train, y_train, weight=<span class="number">5</span>, bias=<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;当权重100，偏置1时，损失为：&quot;</span>, </span><br><span class="line">loss_function(X_train, y_train, weight=<span class="number">100</span>, bias=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">当权重5，偏置3时，损失为： 12.796390970780058</span><br><span class="line">当权重100，偏置1时，损失为： 1577.9592615030556</span><br></pre></td></tr></table></figure>

<h4 id="3-通过梯度下降找到最佳参数"><a href="#3-通过梯度下降找到最佳参数" class="headerlink" title="3. 通过梯度下降找到最佳参数"></a>3. 通过梯度下降找到最佳参数</h4><h5 id="3-1-训练机器要有正确的方向"><a href="#3-1-训练机器要有正确的方向" class="headerlink" title="3.1 训练机器要有正确的方向"></a>3.1 训练机器要有正确的方向</h5><p>有一种方法可以使猜测沿着正确的方向前进，因此总能找到比上一次猜测时误差更小的w和b组合。这种方法就是针对损失函数的<strong>梯度下降</strong></p>
<p><strong>梯度下降</strong>是整个机器学习的精髓</p>
<h5 id="3-2-凸函数确保有最小损失点"><a href="#3-2-凸函数确保有最小损失点" class="headerlink" title="3.2 凸函数确保有最小损失点"></a>3.2 凸函数确保有最小损失点</h5><p>凸函数存在着全局最小损失点，这种存在着底部最低点的函数为梯度下降奠定了基础。</p>
<h5 id="3-3-梯度下降的实现"><a href="#3-3-梯度下降的实现" class="headerlink" title="3.3 梯度下降的实现"></a>3.3 梯度下降的实现</h5><p>梯度下降的过程就是在程序中一点点变化参数w和b，使L，也就是损失值，逐渐趋近最低点（也称为机器学习中的最优解）</p>
<p>然后通过对损失函数求导就得到了梯度。梯度下降会沿着负梯度发方向走一步，以降低损失</p>
<h5 id="3-4-学习速率也很重要"><a href="#3-4-学习速率也很重要" class="headerlink" title="3.4 学习速率也很重要"></a>3.4 学习速率也很重要</h5><p>我们知道了权重w应该往哪个方向走，下一步就是走的速率有多快，这在机器学习中称为<strong>学习速率</strong>的确定，称为α。</p>
<p>学习速率乘以损失曲线求导之后的微分值，就是一次梯度变化的<strong>步长</strong>。它控制着当前梯度下降的节奏，或快或慢，w将在每一次迭代过程中被更新，优化。</p>
<p>下面给出梯度下降的完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, w, b, lr, <span class="built_in">iter</span></span>): <span class="comment"># 定义一个实现梯度下降的函数</span></span><br><span class="line">    l_history = np.zeros(<span class="built_in">iter</span>) <span class="comment"># 初始化记录梯度下降过程中损失的数组</span></span><br><span class="line">    w_history = np.zeros(<span class="built_in">iter</span>) <span class="comment"># 初始化记录梯度下降过程中权重的数组</span></span><br><span class="line">    b_history = np.zeros(<span class="built_in">iter</span>) <span class="comment"># 初始化记录梯度下降过程中偏置的数组</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">iter</span>): <span class="comment"># 进行梯度下降的迭代，就是下多少级台阶</span></span><br><span class="line">        y_hat  = w*X + b <span class="comment"># 这个是向量化运行实现的假设函数</span></span><br><span class="line">        loss = y_hat-y <span class="comment"># 这是中间过程,求得的是假设函数预测的y和真正的y值间的差值</span></span><br><span class="line">        derivative_w = X.T.dot(loss)/<span class="built_in">len</span>(X) <span class="comment"># 对权重求导, len(X)是样本总数</span></span><br><span class="line">        derivative_b = <span class="built_in">sum</span>(loss)*<span class="number">1</span>/<span class="built_in">len</span>(X) <span class="comment"># 对偏置求导</span></span><br><span class="line">        w = w - lr*derivative_w <span class="comment"># 结合下降速率alpha更新权重</span></span><br><span class="line">        b = b - lr*derivative_b <span class="comment"># 结合下降速率alpha更新偏置</span></span><br><span class="line">        l_history[i] = loss_function(X, y, w,b) <span class="comment"># 梯度下降过程中损失的历史</span></span><br><span class="line">        w_history[i] = w <span class="comment"># 梯度下降过程中权重的历史</span></span><br><span class="line">        b_history[i] = b <span class="comment"># 梯度下降过程中偏置的历史</span></span><br><span class="line">    <span class="keyword">return</span> l_history, w_history, b_history <span class="comment"># 返回梯度下降过程数据</span></span><br></pre></td></tr></table></figure>

<p>注意梯度下降的代码在程序中实现时，会被置入一个循环中，比如下降50次、100次甚至10000次，调试程序时，需要观察损失曲线是否已经开始收敛。具体迭代多少次合适，和学习速率一样，需要具体问题具体分析，还需要根据程序运行情况及时调整。</p>
<h4 id="4-实现一元线性回归模型并调试参数"><a href="#4-实现一元线性回归模型并调试参数" class="headerlink" title="4.实现一元线性回归模型并调试参数"></a>4.实现一元线性回归模型并调试参数</h4><h5 id="4-1-权重和偏置的初始值"><a href="#4-1-权重和偏置的初始值" class="headerlink" title="4.1 权重和偏置的初始值"></a>4.1 权重和偏置的初始值</h5><p>（随便设置）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先确定参数的初始值</span></span><br><span class="line">iterations = <span class="number">225</span>; <span class="comment"># 迭代250次</span></span><br><span class="line">alpha = <span class="number">0.5</span>; <span class="comment"># 此处初始学习速率设为0.5， 如果调整为1，你会看到不同的结果，0.5是通过不断调整的最优值</span></span><br><span class="line">weight = -<span class="number">5</span> <span class="comment"># 权重</span></span><br><span class="line">bias = <span class="number">3</span> <span class="comment"># 偏置</span></span><br><span class="line"><span class="comment"># 计算一下初始权重和偏置值所带来的损失</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;当前损失：&#x27;</span>,loss_function(X_train, y_train, weight, bias))</span><br></pre></td></tr></table></figure>

<p>画出当前回归函数的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制当前的函数模型</span></span><br><span class="line">plt.plot(X_train, y_train,<span class="string">&#x27;r.&#x27;</span>, label=<span class="string">&#x27;Training data&#x27;</span>) <span class="comment"># 显示训练集散点图</span></span><br><span class="line">line_X = np.linspace(X_train.<span class="built_in">min</span>(), X_train.<span class="built_in">max</span>(), <span class="number">500</span>) <span class="comment"># X值域</span></span><br><span class="line">line_y = [weight*xx + bias <span class="keyword">for</span> xx <span class="keyword">in</span> line_X] <span class="comment"># 假设函数y_hat</span></span><br><span class="line">plt.plot(line_X,line_y,<span class="string">&#x27;b--&#x27;</span>, label=<span class="string">&#x27;Current hypothesis&#x27;</span> ) <span class="comment">#显示当前拟合</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Wechat Ads&#x27;</span>) <span class="comment"># X轴Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Sales&#x27;</span>) <span class="comment"># y轴Label</span></span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show() <span class="comment"># 显示绘图</span></span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-03-14_19-46-01.png" alt="Snipaste_2023-03-14_19-46-01" style="zoom:50%;">

<h5 id="4-2-进行梯度下降"><a href="#4-2-进行梯度下降" class="headerlink" title="4.2 进行梯度下降"></a>4.2 进行梯度下降</h5><p>调用刚才已经定义好的梯度函数gradient_descent，并迭代100次：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据初始参数值，进行梯度下降，也就是开始训练机器，拟合函数</span></span><br><span class="line">loss_history, weight_history, bias_history = gradient_descent(</span><br><span class="line">             X_train, y_train, weight, bias, alpha, iterations)</span><br></pre></td></tr></table></figure>

<p>在训练机器的过程中，已经通过变量loss_history记录了每一次迭代的损失值，下面把损失大小和迭代次数的关系通过的关系通过函数图像显示出来，看看损失是不是如同预期的那样，随着梯度下降而逐渐减小并趋近最佳状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(loss_history,<span class="string">&#x27;g--&#x27;</span>,label=<span class="string">&#x27;Loss Curve&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Iterations&#x27;</span>) <span class="comment"># x轴Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>) <span class="comment"># y轴Label</span></span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show() <span class="comment"># 显示损失曲线</span></span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-03-14_19-52-47.png" alt="Snipaste_2023-03-14_19-52-47" style="zoom:50%;">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制当前的函数模型</span></span><br><span class="line">plt.plot(X_train, y_train,<span class="string">&#x27;r.&#x27;</span>, label=<span class="string">&#x27;Training data&#x27;</span>) <span class="comment"># 显示训练集散点图</span></span><br><span class="line">line_X = np.linspace(X_train.<span class="built_in">min</span>(), X_train.<span class="built_in">max</span>(), <span class="number">500</span>) <span class="comment"># X值域</span></span><br><span class="line"><span class="comment"># 关于weight_history[-1],这里的索引[-1]，就代表迭代500次后的最后一个W值</span></span><br><span class="line">line_y = [weight_history[-<span class="number">1</span>]*xx + bias_history[-<span class="number">1</span>] <span class="keyword">for</span> xx <span class="keyword">in</span> line_X] <span class="comment"># 假设函数</span></span><br><span class="line">plt.plot(line_X,line_y,<span class="string">&#x27;b--&#x27;</span>, label=<span class="string">&#x27;Current hypothesis&#x27;</span> ) <span class="comment"># 显示当前函数</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Wechat Ads&#x27;</span>) <span class="comment"># x轴Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Sales&#x27;</span>) <span class="comment"># y轴Label</span></span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show() <span class="comment"># 显示函数图像</span></span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-03-14_20-00-35.png" alt="Snipaste_2023-03-14_20-00-35" style="zoom:50%;">

<p>实例中以0.5的学习速率为例，迭代了200次，在此情况下，迭代200次之后的损失值，以及参数w和b的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;当前损失：&#x27;</span>,loss_function(X_train, y_train, </span><br><span class="line">                  weight_history[-<span class="number">1</span>], bias_history[-<span class="number">1</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;当前权重：&#x27;</span>,weight_history[-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;当前偏置：&#x27;</span>,bias_history[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">当前损失： 0.004656784743714472</span><br><span class="line">当前权重： 0.6581321194241853</span><br><span class="line">当前偏置： 0.17541401911198484</span><br></pre></td></tr></table></figure>

<h5 id="4-3-在测试集上进行测试"><a href="#4-3-在测试集上进行测试" class="headerlink" title="4.3 在测试集上进行测试"></a>4.3 在测试集上进行测试</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;测试集损失：&#x27;</span>,loss_function(X_test, y_test, </span><br><span class="line">                    weight_history[-<span class="number">1</span>], bias_history[-<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">测试集损失： 0.004556706461254487</span><br></pre></td></tr></table></figure>

<h2 id="3-2-线性回归模型的评估"><a href="#3-2-线性回归模型的评估" class="headerlink" title="3.2 线性回归模型的评估"></a>3.2 线性回归模型的评估</h2><p>搭建完模型后，还需要对模型进行评估，这里主要有三个值作为评价标准：R-squared(R^2),Adjusted R^2,P值，其中，前两者用来衡量线性拟合的优劣，P值用来衡量特征变量的显著值</p>
<h3 id="3-2-1-模型评估的编程实现"><a href="#3-2-1-模型评估的编程实现" class="headerlink" title="3.2.1 模型评估的编程实现"></a>3.2.1 模型评估的编程实现</h3><p>在实战中，R-squared和Adj.R-squared的取值范围为0~1，它们的值越接近1，则模型的<strong>拟合程度越高</strong></p>
<p>P值在本质上是个概率值，其取值范围也为0~1，P值越接近0，则特征变量的<strong>显著性越高</strong>，即该特征变量真的和目标变量具有相关性</p>
<p>代码汇总 ：不同行业工作年限与收入的线性回归模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">df = pandas.read_excel(<span class="string">&#x27;IT行业收入表.xlsx&#x27;</span>)</span><br><span class="line">X = df[[<span class="string">&#x27;工龄&#x27;</span>]]</span><br><span class="line">Y = df[<span class="string">&#x27;薪水&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.模型训练</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regr = LinearRegression()</span><br><span class="line">regr.fit(X,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.模型可视化</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>] </span><br><span class="line">plt.scatter(X,Y)</span><br><span class="line">plt.plot(X, regr.predict(X), color=<span class="string">&#x27;red&#x27;</span>)  <span class="comment"># color=&#x27;red&#x27;设置为红色</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;工龄&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;薪水&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.线性回归方程构造</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;系数a为:&#x27;</span> + <span class="built_in">str</span>(regr.coef_[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;截距b为:&#x27;</span> + <span class="built_in">str</span>(regr.intercept_))</span><br></pre></td></tr></table></figure>


<p><img src="output_292_0.png"></p>
<pre><code>系数a为:2497.1513476046866
截距b为:10143.131966873787
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm <span class="comment"># 引入用于评估线性回归模型的statsmodels库并简写为sm</span></span><br><span class="line">X2 = sm.add_constant(X)  <span class="comment"># 用add_constant()函数给原来的特征变量X添加常数项，并赋值给X2，这样才有</span></span><br><span class="line">                         <span class="comment"># y=ax+b中的常数项，即截距b，注意Scikit-Learn库不需要这一步</span></span><br><span class="line">est = sm.OLS(Y, X2).fit() <span class="comment"># 用OLS（）和fit（）函数对Y和X2进行线性回归方程搭建</span></span><br><span class="line">est.summary()  <span class="comment"># 在非Jupyter Notebook的编辑器中需要写成print(est.summary())</span></span><br></pre></td></tr></table></figure>




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>薪水</td>        <th>  R-squared:         </th> <td>   0.855</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.854</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   578.5</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Jan 2023</td> <th>  Prob (F-statistic):</th> <td>6.69e-43</td>
</tr>
<tr>
  <th>Time:</th>                 <td>10:40:12</td>     <th>  Log-Likelihood:    </th> <td> -930.83</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   1866.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   1871.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td> 1.014e+04</td> <td>  507.633</td> <td>   19.981</td> <td> 0.000</td> <td> 9135.751</td> <td> 1.12e+04</td>
</tr>
<tr>
  <th>工龄</th>    <td> 2497.1513</td> <td>  103.823</td> <td>   24.052</td> <td> 0.000</td> <td> 2291.118</td> <td> 2703.185</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.287</td> <th>  Durbin-Watson:     </th> <td>   0.555</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.867</td> <th>  Jarque-Bera (JB):  </th> <td>   0.463</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.007</td> <th>  Prob(JB):          </th> <td>   0.793</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.667</td> <th>  Cond. No.          </th> <td>    9.49</td>
</tr>
</table><br><br>Warnings:<br>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


<p><img src="%E4%B8%8B%E8%BD%BD%20(12).png"><br>coef就是常数项和特征变量（工龄)前的系数，即截距b和斜率系数a，可以看到和之前求得的结果是一致的</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(13).png"><br>这两个值说明模型的线性拟合程度比较高；这里的P值有两个，常数项和特征工程（工龄）的P值都约等于0，所以这两个变量都和目标变量（薪水)显著相关，即真的具有相关性，而不是由偶然因素导致的</p>
<p><strong>如果设置成一元二次方程，来看下模型评估效果</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">poly_reg = PolynomialFeatures(degree=<span class="number">2</span>)</span><br><span class="line">X_ = poly_reg.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line">X2 = sm.add_constant(X_)  <span class="comment"># 这里传入的是含有x^2的X_</span></span><br><span class="line">est = sm.OLS(Y, X2).fit()</span><br><span class="line">est.summary()  <span class="comment"># 在非Jupyter Notebook的编辑器中需要写成print(est.summary())</span></span><br></pre></td></tr></table></figure>




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>薪水</td>        <th>  R-squared:         </th> <td>   0.931</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.930</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   654.8</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Jan 2023</td> <th>  Prob (F-statistic):</th> <td>4.70e-57</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:19:47</td>     <th>  Log-Likelihood:    </th> <td> -893.72</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   1793.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   1801.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td> 1.399e+04</td> <td>  512.264</td> <td>   27.307</td> <td> 0.000</td> <td>  1.3e+04</td> <td>  1.5e+04</td>
</tr>
<tr>
  <th>x1</th>    <td> -743.6808</td> <td>  321.809</td> <td>   -2.311</td> <td> 0.023</td> <td>-1382.383</td> <td> -104.979</td>
</tr>
<tr>
  <th>x2</th>    <td>  400.8040</td> <td>   38.790</td> <td>   10.333</td> <td> 0.000</td> <td>  323.816</td> <td>  477.792</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 2.440</td> <th>  Durbin-Watson:     </th> <td>   1.137</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.295</td> <th>  Jarque-Bera (JB):  </th> <td>   2.083</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.352</td> <th>  Prob(JB):          </th> <td>   0.353</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.063</td> <th>  Cond. No.          </th> <td>    102.</td>
</tr>
</table><br><br>Warnings:<br>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.



<p>运行后输出的R-squared为0.931，与之前的0.855相比的确有所提升</p>
<p><strong>补充知识点：另一种获取R-squared值的代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">df = pandas.read_excel(<span class="string">&#x27;IT行业收入表.xlsx&#x27;</span>)</span><br><span class="line">X = df[[<span class="string">&#x27;工龄&#x27;</span>]]</span><br><span class="line">Y = df[<span class="string">&#x27;薪水&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.模型训练</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regr = LinearRegression()</span><br><span class="line">regr.fit(X,Y)</span><br></pre></td></tr></table></figure>




<pre><code>LinearRegression()
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2 = r2_score(Y, regr.predict(X))  <span class="comment"># Y为实际值，regr.predict(X)为预测值</span></span><br><span class="line"><span class="built_in">print</span>(r2)</span><br></pre></td></tr></table></figure>

<pre><code>0.8551365584870814
</code></pre>
<p>可以看到和之前通过statsmodels库评估的结果是一致的。</p>
<h3 id="3-2-2-模型评估的数学原理"><a href="#3-2-2-模型评估的数学原理" class="headerlink" title="3.2.2 模型评估的数学原理"></a>3.2.2 模型评估的数学原理</h3><p><strong>1. R-squared</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(14).png"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(15).png"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(16).png"></p>
<p><strong>补充知识点：过拟合与欠拟合</strong></p>
<p>过拟合即过度拟合，是指模型在训练样本中拟合程度过高，虽然它很好地贴合了训练数据集，但是丧失了泛化能力，不具有推广性，也就是说，如果换了训练集以外的数据就达不到较好的预测效果。与过拟合相对应的概念是欠拟合，欠拟合是指模型拟合程度不高，数据距离拟合曲线较远，或指模型没有很好地捕捉到数据特征，不能很好地拟合数据。</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(17).png"></p>
<p><strong>2. Adj.R-squared</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(18).png"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(19).png"></p>
<p><strong>3. P值</strong></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(20).png"></p>
<h2 id="3-3-多元线性回归"><a href="#3-3-多元线性回归" class="headerlink" title="3.3 多元线性回归"></a>3.3 多元线性回归</h2><h3 id="3-3-1-多元线性回归的数学原理和代码实现"><a href="#3-3-1-多元线性回归的数学原理和代码实现" class="headerlink" title="3.3.1 多元线性回归的数学原理和代码实现"></a>3.3.1 多元线性回归的数学原理和代码实现</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(21).png"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(22).png"></p>
<p>上述代码和一元回归代码的区别在于这里的X包含<strong>多个特征变量信息</strong>，利用多元线性回归可以构建更加丰富的实用的模型，如根据工龄、地域、行业等因素来预测薪水，根据房屋大小、所处位置、是否靠近地铁等因素来预测房价等。</p>
<h3 id="3-3-2-案例实战：客户价值预测模型"><a href="#3-3-2-案例实战：客户价值预测模型" class="headerlink" title="3.3.2 案例实战：客户价值预测模型"></a>3.3.2 案例实战：客户价值预测模型</h3><p>1.案例背景</p>
<p>这里以信用卡客户的客户价值来解释下客户价值预测的具体含义：客户价值预测就是指客户未来一段时间能带来多少利润，其利润的来源可能来自于信用卡的年费、取现手续费、分期手续费、境外交易手续费用等。而分析出客户的价值后，在进行营销、电话接听、催收、产品咨询等各项服务时，就可以针对高价值的客户进行区别于普通客户的服务，有助于进一步挖掘这些高价值客户的价值，并提高这些高价值客户的忠诚度。</p>
<p>2.读取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;客户价值数据表.xlsx&#x27;</span>)</span><br><span class="line">df.head()  <span class="comment"># 显示前5行数据</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>客户价值</th>
      <th>历史贷款金额</th>
      <th>贷款次数</th>
      <th>学历</th>
      <th>月收入</th>
      <th>性别</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1150</td>
      <td>6488</td>
      <td>2</td>
      <td>2</td>
      <td>9567</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1157</td>
      <td>5194</td>
      <td>4</td>
      <td>2</td>
      <td>10767</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1163</td>
      <td>7066</td>
      <td>3</td>
      <td>2</td>
      <td>9317</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>983</td>
      <td>3550</td>
      <td>3</td>
      <td>2</td>
      <td>10517</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1205</td>
      <td>7847</td>
      <td>3</td>
      <td>3</td>
      <td>11267</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>客户价值为1年里能给银行带来的收益；学历：2为高中学历，3为本科学习，4为研究生学历；性别：0为女，1为男</p>
<p>后五列为自变量，“客户价值”为因变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df[[<span class="string">&#x27;历史贷款金额&#x27;</span>, <span class="string">&#x27;贷款次数&#x27;</span>, <span class="string">&#x27;学历&#x27;</span>, <span class="string">&#x27;月收入&#x27;</span>, <span class="string">&#x27;性别&#x27;</span>]]</span><br><span class="line">Y = df[<span class="string">&#x27;客户价值&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>3.模型搭建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regr = LinearRegression()</span><br><span class="line">regr.fit(X,Y)</span><br></pre></td></tr></table></figure>




<pre><code>LinearRegression()
</code></pre>
<p>4.线性回归方程构造</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regr.coef_</span><br></pre></td></tr></table></figure>




<pre><code>array([5.71421731e-02, 9.61723492e+01, 1.13452022e+02, 5.61326459e-02,
       1.97874093e+00])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;各系数为:&#x27;</span> + <span class="built_in">str</span>(regr.coef_))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;常数项系数k0为:&#x27;</span> + <span class="built_in">str</span>(regr.intercept_))</span><br></pre></td></tr></table></figure>

<pre><code>各系数为:[5.71421731e-02 9.61723492e+01 1.13452022e+02 5.61326459e-02
 1.97874093e+00]
常数项系数k0为:-208.42004079925437
</code></pre>
<p>其中这里通过regr.coef_获得是一个系数列表，分别对应不同特征变量前面的系数，也即k1、k2、k3、k4及k5，所以此时的多元线性回归曲线方程为：</p>
<p>y &#x3D; -208 + 0.057<em>x1 + 96</em>x2 + 113<em>x3 + 0.056</em>x4 + 1.97*x5</p>
<p>5.模型评估</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm  <span class="comment"># 引入线性回归模型评估相关库</span></span><br><span class="line">X2 = sm.add_constant(X)</span><br><span class="line">est = sm.OLS(Y, X2).fit()</span><br><span class="line">est.summary()</span><br></pre></td></tr></table></figure>




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>客户价值</td>       <th>  R-squared:         </th> <td>   0.571</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.553</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   32.44</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Jan 2023</td> <th>  Prob (F-statistic):</th> <td>6.41e-21</td>
</tr>
<tr>
  <th>Time:</th>                 <td>18:47:44</td>     <th>  Log-Likelihood:    </th> <td> -843.50</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   128</td>      <th>  AIC:               </th> <td>   1699.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   122</td>      <th>  BIC:               </th> <td>   1716.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>  <td> -208.4200</td> <td>  163.810</td> <td>   -1.272</td> <td> 0.206</td> <td> -532.699</td> <td>  115.859</td>
</tr>
<tr>
  <th>历史贷款金额</th> <td>    0.0571</td> <td>    0.010</td> <td>    5.945</td> <td> 0.000</td> <td>    0.038</td> <td>    0.076</td>
</tr>
<tr>
  <th>贷款次数</th>   <td>   96.1723</td> <td>   25.962</td> <td>    3.704</td> <td> 0.000</td> <td>   44.778</td> <td>  147.567</td>
</tr>
<tr>
  <th>学历</th>     <td>  113.4520</td> <td>   37.909</td> <td>    2.993</td> <td> 0.003</td> <td>   38.406</td> <td>  188.498</td>
</tr>
<tr>
  <th>月收入</th>    <td>    0.0561</td> <td>    0.019</td> <td>    2.941</td> <td> 0.004</td> <td>    0.018</td> <td>    0.094</td>
</tr>
<tr>
  <th>性别</th>     <td>    1.9787</td> <td>   32.286</td> <td>    0.061</td> <td> 0.951</td> <td>  -61.934</td> <td>   65.891</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.597</td> <th>  Durbin-Watson:     </th> <td>   2.155</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.450</td> <th>  Jarque-Bera (JB):  </th> <td>   1.538</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.264</td> <th>  Prob(JB):          </th> <td>   0.464</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.900</td> <th>  Cond. No.          </th> <td>1.28e+05</td>
</tr>
</table><br><br>Warnings:<br>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br>[2] The condition number is large, 1.28e+05. This might indicate that there are<br>strong multicollinearity or other numerical problems.



<p><img src="%E4%B8%8B%E8%BD%BD%20(23).png"><br>两个R值整体的拟合效果不是特别好，可能是因为本案例的数据量偏少，不过在此数据量条件下也算可以接收的结果</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(24).png"><br>再看P值，可以发现大部分特征变量的P值都较小，的确与目标变量（客户价值)显著相关，而“性别”这一特征变量的P值达到了0，951，即与目标变量没有显著相关性，这个结论也符合经验认知，因此，在之后的建模中可以舍去“性别”这一特征变量</p>
<h1 id="4-逻辑回归模型"><a href="#4-逻辑回归模型" class="headerlink" title="4. 逻辑回归模型"></a>4. 逻辑回归模型</h1><h2 id="4-1-逻辑回归模型的算法原理"><a href="#4-1-逻辑回归模型的算法原理" class="headerlink" title="4.1 逻辑回归模型的算法原理"></a>4.1 逻辑回归模型的算法原理</h2><h3 id="4-1-1-逻辑回归模型的数学原理"><a href="#4-1-1-逻辑回归模型的数学原理" class="headerlink" title="4.1.1 逻辑回归模型的数学原理"></a>4.1.1 逻辑回归模型的数学原理</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(25).png"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(26).png" alt="下载 (26)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(27).png" alt="下载 (27)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 补充知识点：Sigmoid函数绘制</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">6</span>, <span class="number">6</span>)  <span class="comment"># 通过linspace()函数生成-6到6的等差数列，默认50个数</span></span><br><span class="line">y = <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-x))  <span class="comment"># Sigmoid函数计算公式，exp()函数表示指数函数</span></span><br><span class="line"></span><br><span class="line">plt.plot(x,y)  <span class="comment"># 画图</span></span><br><span class="line">plt.show()  <span class="comment"># 展示</span></span><br></pre></td></tr></table></figure>


<p><img src="output_344_0.png" alt="output_344_0"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(28).png" alt="下载 (28)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(29).png" alt="下载 (29)"></p>
<h3 id="4-1-2-逻辑回归模型的代码实现"><a href="#4-1-2-逻辑回归模型的代码实现" class="headerlink" title="4.1.2 逻辑回归模型的代码实现"></a>4.1.2 逻辑回归模型的代码实现</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(30).png" alt="下载 (30)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造数据</span></span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">5</span>, <span class="number">1</span>], [<span class="number">6</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">2</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="comment"># 从Scikit-Learn库中引入逻辑回归模型LogisticRegression</span></span><br><span class="line">model = LogisticRegression() <span class="comment"># 将逻辑回归模型赋给变量model，这里没有设置参数，即使用默认参数</span></span><br><span class="line">model.fit(X, y)  <span class="comment"># 用fit()函数进行模型的训练</span></span><br><span class="line"><span class="comment"># 如果运行时下面出现FutureWarning警告，不要在意，它只是在告诉你以后模型的官方默认参数会有所调整而已，不是报错</span></span><br></pre></td></tr></table></figure>




<pre><code>LogisticRegression()
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型预测 - 预测单个数据 ，Predict（）函数默认接收一个多维数据</span></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">2</span>,<span class="number">2</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[0]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型预测 - 预测多个数据1</span></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">1</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">2</span>], [<span class="number">5</span>, <span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[0 0 1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型预测 - 预测多个数据2</span></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">5</span>, <span class="number">1</span>], [<span class="number">6</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">2</span>]]))  <span class="comment"># 因为这里演示的多个数据和X是一样的，所以也可以直接写成model.predict(X)</span></span><br></pre></td></tr></table></figure>

<pre><code>[0 1 1 0 0]
</code></pre>
<p>可以看到其预测准确度为100%。</p>
<h3 id="4-1-3-逻辑回归模型的深入理解"><a href="#4-1-3-逻辑回归模型的深入理解" class="headerlink" title="4.1.3 逻辑回归模型的深入理解"></a>4.1.3 逻辑回归模型的深入理解</h3><p>逻辑回归模型的本质是概率预测，而不是直接预测具体类别（如属于0还是1）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测概率：左列是分类为0的概率，右列是分类为1的概率</span></span><br><span class="line">y_pred_proba = model.predict_proba(X)</span><br><span class="line">y_pred_proba  <span class="comment"># 直接打印</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[0.97344854, 0.02655146],
       [0.39071972, 0.60928028],
       [0.17991028, 0.82008972],
       [0.63167893, 0.36832107],
       [0.82424527, 0.17575473]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 另外一种打印概率的方式：通过DataFrame展示，更加好看些</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">a = pd.DataFrame(y_pred_proba, columns=[<span class="string">&#x27;分类为0的概率&#x27;</span>, <span class="string">&#x27;分类为1的概率&#x27;</span>])  <span class="comment"># 2.2.1 通过numpy数组创建DataFrame</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>分类为0的概率</th>
      <th>分类为1的概率</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.973449</td>
      <td>0.026551</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.390720</td>
      <td>0.609280</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.179910</td>
      <td>0.820090</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.631679</td>
      <td>0.368321</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.824245</td>
      <td>0.175755</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印系数和截距项</span></span><br><span class="line"><span class="built_in">print</span>(model.coef_)  <span class="comment"># 系数k1与k2</span></span><br><span class="line"><span class="built_in">print</span>(model.intercept_)  <span class="comment"># 截距项k0</span></span><br></pre></td></tr></table></figure>

<pre><code>[[1.00595248 0.02223835]]
[-4.60771284]
</code></pre>
<p><img src="%E4%B8%8B%E8%BD%BD%20(31).png" alt="下载 (31)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(32).png" alt="下载 (32)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想批量查看预测概率</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):  <span class="comment"># 这里共有5条数据，所以循环5次</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="number">1</span> / (<span class="number">1</span> + np.exp(-(np.dot(X[i], model.coef_.T) + model.intercept_))))</span><br></pre></td></tr></table></figure>

<pre><code>[0.02655146]
[0.60928028]
[0.82008972]
[0.36832107]
[0.17575473]
</code></pre>
<p><strong>补充知识点：多分类逻辑回归模型演示</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造数据，此时y有多个分类</span></span><br><span class="line">X = [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">5</span>, <span class="number">1</span>], [<span class="number">6</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">2</span>]]</span><br><span class="line">y = [-<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]  <span class="comment"># 这里有三个分类-1、0、1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X, y)  <span class="comment"># 如果运行时下面出现FutureWarning警告，不要在意，它只是在告诉你以后模型的官方默认参数会有所调整而已，不是报错</span></span><br></pre></td></tr></table></figure>




<pre><code>LogisticRegression()
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">0</span>, <span class="number">0</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[-1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(model.predict_proba([[<span class="number">0</span>, <span class="number">0</span>]])) <span class="comment"># 用predict_proba()函数获取各个分类的概率，从左至右分别是分类为-1，0，1的概率</span></span><br></pre></td></tr></table></figure>

<pre><code>[[0.88352311 0.02340026 0.09307662]]
</code></pre>
<h2 id="4-2-案例实战-股票客户流失预警模型"><a href="#4-2-案例实战-股票客户流失预警模型" class="headerlink" title="4.2 案例实战 - 股票客户流失预警模型"></a>4.2 案例实战 - 股票客户流失预警模型</h2><h3 id="4-2-1-案例背景"><a href="#4-2-1-案例背景" class="headerlink" title="4.2.1 案例背景"></a>4.2.1 案例背景</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(33).png" alt="下载 (33)"></p>
<h3 id="4-2-2-数据读取与变量划分"><a href="#4-2-2-数据读取与变量划分" class="headerlink" title="4.2.2 数据读取与变量划分"></a>4.2.2 数据读取与变量划分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;股票客户流失.xlsx&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>账户资金（元）</th>
      <th>最后一次交易距今时间（天）</th>
      <th>上月交易佣金（元）</th>
      <th>累计交易佣金（元）</th>
      <th>本券商使用时长（年）</th>
      <th>是否流失</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22686.5</td>
      <td>297</td>
      <td>149.25</td>
      <td>2029.85</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>190055.0</td>
      <td>42</td>
      <td>284.75</td>
      <td>3889.50</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>29733.5</td>
      <td>233</td>
      <td>269.25</td>
      <td>2108.15</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>185667.5</td>
      <td>44</td>
      <td>211.50</td>
      <td>3840.75</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>33648.5</td>
      <td>213</td>
      <td>353.50</td>
      <td>2151.65</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>0表示未流失，1表示流失</p>
<p>将“是否流失”作为目标变量，其他字段作为特征变量，通过一个客户的一些基本情况和交易记录来预测他是否会流失，或者说判断流失的概率大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.划分特征变量和目标变量</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;是否流失&#x27;</span>) </span><br><span class="line">y = df[<span class="string">&#x27;是否流失&#x27;</span>]   </span><br></pre></td></tr></table></figure>

<h3 id="4-2-3-模型的搭建与使用"><a href="#4-2-3-模型的搭建与使用" class="headerlink" title="4.2.3 模型的搭建与使用"></a>4.2.3 模型的搭建与使用</h3><ol>
<li>划分训练集和测试集</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.划分训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)  <span class="comment"># 设置random_state使得每次划分的数据一样</span></span><br><span class="line"></span><br><span class="line">X_train.head()  <span class="comment"># 显示训练集X_train的前5行，在别的代码编辑器里需要通过print()函数打印查看</span></span><br><span class="line"><span class="comment"># y_train.head()  # 显示训练集y_train的前5行，在别的代码编辑器里需要通过print()函数打印查看</span></span><br><span class="line"><span class="comment"># X_test.head()  # 显示测试集X_test的前5行，在别的代码编辑器里需要通过print()函数打印查看</span></span><br><span class="line"><span class="comment"># y_test.head()  # 显示测试集y_test的前5行，在别的代码编辑器里需要通过print()函数打印查看</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>账户资金（元）</th>
      <th>最后一次交易距今时间（天）</th>
      <th>上月交易佣金（元）</th>
      <th>累计交易佣金（元）</th>
      <th>本券商使用时长（年）</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1814</th>
      <td>43251.5</td>
      <td>192</td>
      <td>98.50</td>
      <td>2258.35</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5946</th>
      <td>304449.5</td>
      <td>22</td>
      <td>369.50</td>
      <td>5160.55</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3881</th>
      <td>441357.5</td>
      <td>9</td>
      <td>325.75</td>
      <td>6681.75</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2389</th>
      <td>587076.5</td>
      <td>2</td>
      <td>427.25</td>
      <td>8300.85</td>
      <td>5</td>
    </tr>
    <tr>
      <th>3676</th>
      <td>204027.5</td>
      <td>39</td>
      <td>352.00</td>
      <td>4044.75</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



<p>一些说明：用train_test_split()函数划分训练集和测试集，X_train,y_train为训练集的特征变量和目标变量数据，X_test,y_test则为测试集的特征变量和目标变量数据，函数的参数中，X和y便是之前划分的特征变量和目标变量，teat_size则是测试集数据所占的比例，这里选择20%</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.模型搭建</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>LogisticRegression()
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5.模型使用1 - 预测数据结果</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred[<span class="number">0</span>:<span class="number">100</span>])  <span class="comment"># 打印预测内容的前100个看看</span></span><br></pre></td></tr></table></figure>

<pre><code>[0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0
 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 放到一个DataFrame里进行查看比对</span></span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;预测值&#x27;</span>] = <span class="built_in">list</span>(y_pred)</span><br><span class="line">a[<span class="string">&#x27;实际值&#x27;</span>] = <span class="built_in">list</span>(y_test)</span><br><span class="line">a.head()  <span class="comment"># 可以看到此时前5个预测准确度为80%</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>预测值</th>
      <th>实际值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看全部的预测准确度</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)  <span class="comment"># 预测精确度将预测值（通过测试集计算出）和实际值（测试集中）</span></span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>

<pre><code>0.7977288857345636
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 另外一种查看模型预测准确度的方法</span></span><br><span class="line">model.score(X_test, y_test)</span><br></pre></td></tr></table></figure>




<pre><code>0.7977288857345636
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 6.模型使用2 - 预测概率，左列是不流失概率，右列是流失概率</span></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)  </span><br><span class="line">y_pred_proba[<span class="number">0</span>:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([[0.82041491, 0.17958509],
       [0.84029613, 0.15970387],
       [0.79819342, 0.20180658],
       [0.62989192, 0.37010808],
       [0.61636611, 0.38363389]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 另一种查看概率的方式</span></span><br><span class="line">a = pd.DataFrame(y_pred_proba, columns=[<span class="string">&#x27;不流失概率&#x27;</span>, <span class="string">&#x27;流失概率&#x27;</span>])</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>不流失概率</th>
      <th>流失概率</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.820415</td>
      <td>0.179585</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.840296</td>
      <td>0.159704</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.798193</td>
      <td>0.201807</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.629892</td>
      <td>0.370108</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.616366</td>
      <td>0.383634</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只查看流失的概率（也即y=1概率，即上面二维数组的第二列），：表示二维数组的所有行，“1”表示二维数组的第二列</span></span><br><span class="line">y_pred_proba[:,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([0.17958509, 0.15970387, 0.20180658, ..., 0.04220544, 0.09782449,
       0.63586739])
</code></pre>
<p><img src="%E4%B8%8B%E8%BD%BD%20(34).png" alt="下载 (34)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 7.查看各个特征变量的系数（额外知识点，供参考）</span></span><br><span class="line"><span class="built_in">print</span>(model.coef_)</span><br><span class="line"><span class="built_in">print</span>(model.intercept_) <span class="comment"># 和截距</span></span><br></pre></td></tr></table></figure>

<pre><code>[[ 2.41952469e-05  8.16881490e-03  1.04320950e-02 -2.54894468e-03
  -1.10120609e-04]]
[-1.43393291e-06]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过公式获取流失概率</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):  <span class="comment"># 这里演示前5条测试集数据的预测流失的概率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="number">1</span> / (<span class="number">1</span> + np.exp(-(np.dot(X_test.iloc[i], model.coef_.T) + model.intercept_))))</span><br></pre></td></tr></table></figure>

<pre><code>[0.17958509]
[0.15970387]
[0.20180658]
[0.37010808]
[0.38363389]
</code></pre>
<p><strong>代码汇总</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代码汇总</span></span><br><span class="line"><span class="comment"># 1.读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;股票客户流失.xlsx&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.划分特征变量和目标变量</span></span><br><span class="line">X = df.drop(columns=<span class="string">&#x27;是否流失&#x27;</span>) </span><br><span class="line">y = df[<span class="string">&#x27;是否流失&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.划分训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.模型搭建</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.模型使用1 - 预测数据结果</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred[<span class="number">0</span>:<span class="number">100</span>])  <span class="comment"># 打印预测内容的前100个看看</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看全部的预测准确度</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">score = accuracy_score(y_pred, y_test)</span><br><span class="line"><span class="built_in">print</span>(score)  <span class="comment"># 打印整体的预测准确度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.模型使用2 - 预测概率</span></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)  </span><br><span class="line"><span class="built_in">print</span>(y_pred_proba[<span class="number">0</span>:<span class="number">5</span>])  <span class="comment"># 打印前5个客户的分类概率</span></span><br></pre></td></tr></table></figure>

<pre><code>[0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0
 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1]
0.7977288857345636
[[0.82041491 0.17958509]
 [0.84029613 0.15970387]
 [0.79819342 0.20180658]
 [0.62989192 0.37010808]
 [0.61636611 0.38363389]]
</code></pre>
<h2 id="4-3-模型评估方法-ROC曲线与KS曲线（对于二分类模型）"><a href="#4-3-模型评估方法-ROC曲线与KS曲线（对于二分类模型）" class="headerlink" title="4.3 模型评估方法 - ROC曲线与KS曲线（对于二分类模型）"></a>4.3 模型评估方法 - ROC曲线与KS曲线（对于二分类模型）</h2><h3 id="4-3-1-分类模型的评估方法-ROC曲线"><a href="#4-3-1-分类模型的评估方法-ROC曲线" class="headerlink" title="4.3.1 分类模型的评估方法 - ROC曲线"></a>4.3.1 分类模型的评估方法 - ROC曲线</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(35).png" alt="下载 (35)"></p>
<p>混淆矩阵</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(36).png"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(37).png" alt="下载 (37)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(38).png" alt="下载 (38)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(39).png" alt="下载 (39)"></p>
<p>因此为了衡量一个模型的优劣，数据科学家根据不同阈值下的命中率和假警报率绘制了如下的曲线图，称之为ROC曲线：ROC曲线的横坐标为假警报率（FPR），其纵坐标为命中率（TPR），在某一个阈值条件下，我们希望命中率能尽可能的高，而假警报率尽可能的低<br><img src="%E4%B8%8B%E8%BD%BD%20(40).png" alt="下载 (40)"></p>
<p>如果把假警报率理解为<strong>代价</strong>的话，那么命中率就是<strong>收益</strong>，所以可以说在相同的阈值的情况下，我们希望<strong>假警报率尽可能小的情况下，命中率尽可能高</strong>，该思想反映在图形中就是这个曲线尽可能<strong>陡峭</strong>，曲线越靠近左上角。一个完美的模型是在不同的阈值下，假警报率都接近于0，而命中率接近于1</p>
<p><strong>补充知识点：混淆矩阵的Python实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">m = confusion_matrix(y_test, y_pred)  <span class="comment"># 传入预测值和真实值</span></span><br><span class="line"><span class="built_in">print</span>(m)</span><br></pre></td></tr></table></figure>

<pre><code>[[968  93]
 [192 156]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = pd.DataFrame(m, index=[<span class="string">&#x27;0（实际不流失）&#x27;</span>, <span class="string">&#x27;1（实际流失）&#x27;</span>], columns=[<span class="string">&#x27;0（预测不流失）&#x27;</span>, <span class="string">&#x27;1（预测流失）&#x27;</span>])</span><br><span class="line">a  <span class="comment"># 我们这里阈值为50%，也就是超过一半就被认为是对应的情形</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0（预测不流失）</th>
      <th>1（预测流失）</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0（实际不流失）</th>
      <td>968</td>
      <td>93</td>
    </tr>
    <tr>
      <th>1（实际流失）</th>
      <td>192</td>
      <td>156</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))  <span class="comment"># 传入预测值和真实值</span></span><br></pre></td></tr></table></figure>

<pre><code>              precision    recall  f1-score   support

           0       0.83      0.91      0.87      1061
           1       0.63      0.45      0.52       348

    accuracy                           0.80      1409
   macro avg       0.73      0.68      0.70      1409
weighted avg       0.78      0.80      0.79      1409
</code></pre>
<p>​    </p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(41).png" alt="下载 (41)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(42).png" alt="下载 (42)"></p>
<h3 id="4-3-2-案例实战：用ROC曲线评价客户流失预警模型"><a href="#4-3-2-案例实战：用ROC曲线评价客户流失预警模型" class="headerlink" title="4.3.2 案例实战：用ROC曲线评价客户流失预警模型"></a>4.3.2 案例实战：用ROC曲线评价客户流失预警模型</h3><p><img src="%E4%B8%8B%E8%BD%BD%20(43).png" alt="下载 (43)"></p>
<p>在python中，通过如下代码就可以求出不同阈值下的命中率（TPR）和假警报率（FPR）的值，从而绘制出ROC曲线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.计算ROC曲线需要的假警报率（fpr）、命中率（tpr）及阈值（thres）</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve <span class="comment"># 引入roc_curve()函数</span></span><br><span class="line">fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,<span class="number">1</span>])  <span class="comment">#在函数中传入测试集目标变量y_test及预测的流失概率y_pred_proba[:,1]</span></span><br><span class="line"><span class="comment"># 计算出不同阈值下的命中率和假警报率。因为roc_curve（）函数返回的是一个含有3个元素的元组</span></span><br><span class="line"><span class="comment"># 其中默认第一个元素为假警报率，第二个元素为命中率，第三个元素为阈值</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # 感兴趣的读者可以查看下roc_curve()函数返回的内容</span></span><br><span class="line"><span class="built_in">print</span>(roc_curve(y_test, y_pred_proba[:,<span class="number">1</span>]))</span><br><span class="line"><span class="built_in">type</span>(roc_curve(y_test, y_pred_proba[:,<span class="number">1</span>]))</span><br><span class="line"><span class="built_in">len</span>(roc_curve(y_test, y_pred_proba[:,<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>(array([0.        , 0.        , 0.        , 0.00188501, 0.00188501,
       0.00282752, 0.00282752, 0.00377003, 0.00377003, 0.00565504,
       0.00565504, 0.00659755, 0.00659755, 0.00754006, 0.00754006,
       0.00848256, 0.00848256, 0.01036758, 0.01036758, 0.01131008,
       0.01131008, 0.01225259, 0.01225259, 0.0131951 , 0.0131951 ,
       0.01508011, 0.01508011, 0.01602262, 0.01602262, 0.01696513,
       0.01696513, 0.01885014, 0.01885014, 0.02073516, 0.02073516,
       0.02356268, 0.02356268, 0.0273327 , 0.0273327 , 0.02827521,
       0.02827521, 0.02921772, 0.02921772, 0.03016023, 0.03016023,
       0.03110273, 0.03110273, 0.03204524, 0.03204524, 0.03298775,
       0.03298775, 0.03393025, 0.03393025, 0.03487276, 0.03487276,
       0.03581527, 0.03581527, 0.03675778, 0.03675778, 0.03770028,
       0.03770028, 0.03864279, 0.03864279, 0.0395853 , 0.0395853 ,
       0.04241282, 0.04241282, 0.04335533, 0.04335533, 0.04429783,
       0.04429783, 0.04712535, 0.04712535, 0.04901037, 0.04901037,
       0.04995287, 0.04995287, 0.05183789, 0.05183789, 0.05466541,
       0.05466541, 0.05749293, 0.05749293, 0.06314797, 0.06314797,
       0.06409048, 0.06409048, 0.06503299, 0.06503299, 0.066918  ,
       0.066918  , 0.06786051, 0.06786051, 0.07068803, 0.07068803,
       0.07163054, 0.07163054, 0.07257304, 0.07257304, 0.07917059,
       0.0801131 , 0.08105561, 0.08105561, 0.08294062, 0.08294062,
       0.08388313, 0.08388313, 0.08482564, 0.08482564, 0.08576814,
       0.08576814, 0.08765316, 0.08765316, 0.08859566, 0.08859566,
       0.09048068, 0.09048068, 0.0933082 , 0.0933082 , 0.09519321,
       0.09519321, 0.09990575, 0.09990575, 0.10179076, 0.10179076,
       0.10273327, 0.10273327, 0.10461828, 0.10461828, 0.10744581,
       0.10744581, 0.10838831, 0.10838831, 0.11215834, 0.11215834,
       0.11310085, 0.11310085, 0.11498586, 0.11498586, 0.11592837,
       0.11592837, 0.11875589, 0.11875589, 0.12535344, 0.12535344,
       0.12723845, 0.12723845, 0.13195099, 0.13195099, 0.13477851,
       0.13477851, 0.13854854, 0.13854854, 0.13949105, 0.13949105,
       0.14043355, 0.14043355, 0.14137606, 0.14137606, 0.14326107,
       0.14326107, 0.1470311 , 0.1470311 , 0.14985862, 0.14985862,
       0.15551367, 0.15551367, 0.15645617, 0.15645617, 0.1602262 ,
       0.1602262 , 0.16682375, 0.16682375, 0.16965127, 0.16965127,
       0.17059378, 0.17059378, 0.17530631, 0.17530631, 0.17719133,
       0.17719133, 0.17907634, 0.17907634, 0.18190386, 0.18190386,
       0.18567389, 0.18567389, 0.19132893, 0.19132893, 0.19509896,
       0.19509896, 0.19698398, 0.19698398, 0.19886899, 0.19886899,
       0.1998115 , 0.1998115 , 0.20358153, 0.20358153, 0.21771913,
       0.21771913, 0.22148916, 0.22148916, 0.22525919, 0.22525919,
       0.2271442 , 0.2271442 , 0.23468426, 0.23468426, 0.23751178,
       0.23751178, 0.23845429, 0.23845429, 0.24222432, 0.24222432,
       0.24316682, 0.24316682, 0.24787936, 0.24787936, 0.25070688,
       0.25070688, 0.2535344 , 0.25447691, 0.25541942, 0.25541942,
       0.26013195, 0.26013195, 0.27144204, 0.27144204, 0.28086711,
       0.28086711, 0.29594722, 0.29594722, 0.29783223, 0.29783223,
       0.30160226, 0.30160226, 0.30442978, 0.30442978, 0.30537229,
       0.30537229, 0.30819981, 0.30819981, 0.31196984, 0.31196984,
       0.31385485, 0.31385485, 0.32327992, 0.32327992, 0.32610745,
       0.32610745, 0.34495759, 0.34495759, 0.34590009, 0.34590009,
       0.34778511, 0.34778511, 0.35721018, 0.35721018, 0.36192271,
       0.36192271, 0.36286522, 0.36286522, 0.36663525, 0.36663525,
       0.37888784, 0.37888784, 0.3864279 , 0.3864279 , 0.39114043,
       0.39114043, 0.39208294, 0.39208294, 0.40433553, 0.40433553,
       0.40622055, 0.40622055, 0.4128181 , 0.4128181 , 0.42035815,
       0.42035815, 0.42884072, 0.42884072, 0.42978322, 0.42978322,
       0.43638077, 0.43638077, 0.43920829, 0.43920829, 0.44203582,
       0.44203582, 0.45051838, 0.45051838, 0.45240339, 0.45240339,
       0.46842601, 0.46842601, 0.47502356, 0.47502356, 0.49104618,
       0.49104618, 0.4929312 , 0.4929312 , 0.49670123, 0.49670123,
       0.50329877, 0.50329877, 0.50424128, 0.50424128, 0.5070688 ,
       0.5070688 , 0.50989632, 0.50989632, 0.51366635, 0.51366635,
       0.52309142, 0.52309142, 0.52497644, 0.52497644, 0.52874647,
       0.52874647, 0.53817154, 0.53817154, 0.54194156, 0.54194156,
       0.55890669, 0.55890669, 0.56361923, 0.56361923, 0.56550424,
       0.56550424, 0.56927427, 0.56927427, 0.57115928, 0.57115928,
       0.5730443 , 0.5730443 , 0.58058435, 0.58058435, 0.58152686,
       0.58152686, 0.60131951, 0.60131951, 0.61828464, 0.61828464,
       0.63242224, 0.63242224, 0.63901979, 0.63901979, 0.65786993,
       0.65786993, 0.68426013, 0.68426013, 0.69180019, 0.69180019,
       0.70499529, 0.70499529, 0.70593779, 0.70593779, 0.79170594,
       0.79170594, 0.79924599, 0.79924599, 0.81715363, 0.81715363,
       0.82092366, 0.82092366, 0.8397738 , 0.8397738 , 0.88407163,
       0.88407163, 0.88784166, 0.88784166, 0.91894439, 0.91894439,
       0.93213949, 0.93213949, 1.        ]), array([0.        , 0.00287356, 0.03448276, 0.03448276, 0.04022989,
       0.04022989, 0.04597701, 0.04597701, 0.06609195, 0.06609195,
       0.07758621, 0.07758621, 0.08908046, 0.08908046, 0.09195402,
       0.09195402, 0.11206897, 0.11206897, 0.12643678, 0.12643678,
       0.13505747, 0.13505747, 0.15804598, 0.15804598, 0.17241379,
       0.17241379, 0.17816092, 0.17816092, 0.18390805, 0.18390805,
       0.18678161, 0.18678161, 0.20689655, 0.20689655, 0.21264368,
       0.21264368, 0.2183908 , 0.2183908 , 0.22701149, 0.22701149,
       0.23275862, 0.23275862, 0.23850575, 0.23850575, 0.24425287,
       0.24425287, 0.27586207, 0.27586207, 0.2816092 , 0.2816092 ,
       0.29022989, 0.29022989, 0.29310345, 0.29310345, 0.29885057,
       0.29885057, 0.30747126, 0.30747126, 0.31034483, 0.31034483,
       0.31321839, 0.31321839, 0.31609195, 0.31609195, 0.32471264,
       0.32471264, 0.32758621, 0.32758621, 0.33045977, 0.33045977,
       0.33333333, 0.33333333, 0.33908046, 0.33908046, 0.34195402,
       0.34195402, 0.34482759, 0.34482759, 0.35057471, 0.35057471,
       0.3591954 , 0.3591954 , 0.36206897, 0.36206897, 0.37643678,
       0.37643678, 0.37931034, 0.37931034, 0.38505747, 0.38505747,
       0.38793103, 0.38793103, 0.3908046 , 0.3908046 , 0.39942529,
       0.39942529, 0.40517241, 0.40517241, 0.40804598, 0.40804598,
       0.41091954, 0.41091954, 0.41954023, 0.41954023, 0.42528736,
       0.42528736, 0.42816092, 0.42816092, 0.43390805, 0.43390805,
       0.44252874, 0.44252874, 0.45114943, 0.45114943, 0.45402299,
       0.45402299, 0.46551724, 0.46551724, 0.4683908 , 0.4683908 ,
       0.47413793, 0.47413793, 0.47701149, 0.47701149, 0.47988506,
       0.47988506, 0.48563218, 0.48563218, 0.48850575, 0.48850575,
       0.49137931, 0.49137931, 0.50862069, 0.50862069, 0.51436782,
       0.51436782, 0.51724138, 0.51724138, 0.52298851, 0.52298851,
       0.52586207, 0.52586207, 0.52873563, 0.52873563, 0.53448276,
       0.53448276, 0.54597701, 0.54597701, 0.54885057, 0.54885057,
       0.5545977 , 0.5545977 , 0.56321839, 0.56321839, 0.56896552,
       0.56896552, 0.57471264, 0.57471264, 0.57758621, 0.57758621,
       0.58045977, 0.58045977, 0.58333333, 0.58333333, 0.5862069 ,
       0.5862069 , 0.59195402, 0.59195402, 0.59770115, 0.59770115,
       0.60057471, 0.60057471, 0.60632184, 0.60632184, 0.6091954 ,
       0.6091954 , 0.61206897, 0.61206897, 0.61494253, 0.61494253,
       0.61781609, 0.61781609, 0.62643678, 0.62643678, 0.62931034,
       0.62931034, 0.63505747, 0.63505747, 0.64655172, 0.64655172,
       0.64942529, 0.64942529, 0.65517241, 0.65517241, 0.65804598,
       0.65804598, 0.66666667, 0.66666667, 0.67528736, 0.67528736,
       0.67816092, 0.67816092, 0.68678161, 0.68678161, 0.69252874,
       0.69252874, 0.69827586, 0.69827586, 0.70114943, 0.70114943,
       0.70689655, 0.70689655, 0.70977011, 0.70977011, 0.71264368,
       0.71264368, 0.71551724, 0.71551724, 0.72126437, 0.72126437,
       0.72413793, 0.72413793, 0.72701149, 0.72701149, 0.72988506,
       0.72988506, 0.73275862, 0.73275862, 0.73563218, 0.73563218,
       0.74137931, 0.74137931, 0.74425287, 0.74425287, 0.74712644,
       0.74712644, 0.75      , 0.75      , 0.75574713, 0.75574713,
       0.75862069, 0.75862069, 0.76436782, 0.76436782, 0.77011494,
       0.77011494, 0.77298851, 0.77298851, 0.77586207, 0.77586207,
       0.7816092 , 0.7816092 , 0.78448276, 0.78448276, 0.79310345,
       0.79310345, 0.79597701, 0.79597701, 0.79885057, 0.79885057,
       0.8045977 , 0.8045977 , 0.80747126, 0.80747126, 0.81896552,
       0.81896552, 0.82183908, 0.82183908, 0.83045977, 0.83045977,
       0.83333333, 0.83333333, 0.8362069 , 0.8362069 , 0.83908046,
       0.83908046, 0.84195402, 0.84195402, 0.84482759, 0.84482759,
       0.84770115, 0.84770115, 0.85057471, 0.85057471, 0.85344828,
       0.85344828, 0.85632184, 0.85632184, 0.8591954 , 0.8591954 ,
       0.86206897, 0.86206897, 0.86781609, 0.86781609, 0.87068966,
       0.87068966, 0.87356322, 0.87356322, 0.88218391, 0.88218391,
       0.88505747, 0.88505747, 0.88793103, 0.88793103, 0.8908046 ,
       0.8908046 , 0.89367816, 0.89367816, 0.89655172, 0.89655172,
       0.89942529, 0.89942529, 0.90229885, 0.90229885, 0.90517241,
       0.90517241, 0.90804598, 0.90804598, 0.91091954, 0.91091954,
       0.9137931 , 0.9137931 , 0.91954023, 0.91954023, 0.92241379,
       0.92241379, 0.92528736, 0.92528736, 0.93103448, 0.93103448,
       0.93390805, 0.93390805, 0.93678161, 0.93678161, 0.93965517,
       0.93965517, 0.94252874, 0.94252874, 0.9454023 , 0.9454023 ,
       0.94827586, 0.94827586, 0.95114943, 0.95114943, 0.95402299,
       0.95402299, 0.95689655, 0.95689655, 0.95977011, 0.95977011,
       0.96264368, 0.96264368, 0.96551724, 0.96551724, 0.9683908 ,
       0.9683908 , 0.97126437, 0.97126437, 0.97413793, 0.97413793,
       0.97701149, 0.97701149, 0.97988506, 0.97988506, 0.98275862,
       0.98275862, 0.98563218, 0.98563218, 0.98850575, 0.98850575,
       0.99137931, 0.99137931, 0.99425287, 0.99425287, 0.99712644,
       0.99712644, 1.        , 1.        ]), array([1.93036861, 0.93036861, 0.86734206, 0.86418723, 0.85730309,
       0.85649378, 0.84568164, 0.84543596, 0.82483126, 0.8201441 ,
       0.81311169, 0.81164465, 0.79891705, 0.79828598, 0.79744205,
       0.79673396, 0.79148484, 0.79018084, 0.78410715, 0.78325301,
       0.78068026, 0.77975555, 0.77314361, 0.77149969, 0.76317728,
       0.75771678, 0.75530964, 0.7467536 , 0.74633576, 0.74593898,
       0.73787043, 0.73150142, 0.7155645 , 0.7132533 , 0.71147411,
       0.71006504, 0.705358  , 0.70391142, 0.69934458, 0.6989776 ,
       0.69636937, 0.69577815, 0.69040906, 0.68975946, 0.68396617,
       0.68341116, 0.65824648, 0.65645995, 0.65437991, 0.64886034,
       0.64615326, 0.64511389, 0.64509061, 0.63984949, 0.63899737,
       0.63606738, 0.63405227, 0.63368792, 0.63038056, 0.62929326,
       0.62715999, 0.6256313 , 0.62344177, 0.61735308, 0.61333738,
       0.60597822, 0.60516151, 0.60438333, 0.60412377, 0.59867567,
       0.598105  , 0.59240035, 0.59157241, 0.58974186, 0.58832281,
       0.58633914, 0.58335363, 0.57618962, 0.57279834, 0.56287395,
       0.56209214, 0.55871588, 0.55851295, 0.55585014, 0.5524567 ,
       0.5519966 , 0.55173012, 0.54869136, 0.54496052, 0.54303347,
       0.5412379 , 0.54077989, 0.54068863, 0.53644826, 0.53378715,
       0.53207846, 0.53126791, 0.5289874 , 0.52850005, 0.52609366,
       0.52319534, 0.52295562, 0.51740022, 0.51454525, 0.5128666 ,
       0.51240449, 0.51220439, 0.51129218, 0.50843466, 0.50739239,
       0.50511755, 0.50351112, 0.49924286, 0.49774549, 0.49676048,
       0.48983178, 0.48542537, 0.48237841, 0.48127251, 0.47991475,
       0.4792829 , 0.47653343, 0.47493432, 0.46687942, 0.46466341,
       0.46235473, 0.45876252, 0.45762865, 0.45728654, 0.45396147,
       0.45327582, 0.45322971, 0.44540807, 0.44035387, 0.43909475,
       0.43851404, 0.43850714, 0.4374161 , 0.4339261 , 0.43365281,
       0.43253698, 0.42874795, 0.42838317, 0.4201639 , 0.41965098,
       0.41906404, 0.41688636, 0.40384019, 0.40300232, 0.40114515,
       0.40023483, 0.39694656, 0.39469319, 0.39339126, 0.39210513,
       0.39051892, 0.38930922, 0.38920281, 0.38850045, 0.38653204,
       0.38567056, 0.38363389, 0.38329059, 0.38001527, 0.37970083,
       0.37548454, 0.37365995, 0.37361821, 0.37010808, 0.36452856,
       0.3638944 , 0.35829263, 0.35772756, 0.35688172, 0.35595225,
       0.35580895, 0.35507775, 0.34766921, 0.34751604, 0.34609368,
       0.3459747 , 0.34360625, 0.34251243, 0.34066434, 0.34061995,
       0.33865711, 0.33811938, 0.33243296, 0.32872028, 0.32495288,
       0.32317745, 0.32231913, 0.32173408, 0.32145383, 0.32043845,
       0.31981616, 0.31849466, 0.31635419, 0.31353551, 0.30506185,
       0.3038763 , 0.30135394, 0.30036661, 0.29785788, 0.29672634,
       0.29611863, 0.29468651, 0.29032967, 0.28985051, 0.28895018,
       0.28851766, 0.28846186, 0.288309  , 0.2865539 , 0.28639019,
       0.28630836, 0.28567951, 0.28415397, 0.28391369, 0.28204003,
       0.28181318, 0.27923773, 0.27819659, 0.27780412, 0.27769018,
       0.2757513 , 0.27388533, 0.26977967, 0.26928634, 0.26393067,
       0.26370485, 0.25147294, 0.25074872, 0.24987825, 0.24946767,
       0.24641104, 0.24575234, 0.24385571, 0.24260233, 0.24246899,
       0.24191922, 0.23916651, 0.23650475, 0.23511477, 0.23304303,
       0.23200776, 0.23175645, 0.22536036, 0.2249086 , 0.22402926,
       0.2231088 , 0.21098824, 0.21093075, 0.21060639, 0.20817224,
       0.20748694, 0.20685561, 0.2011897 , 0.19955868, 0.19821109,
       0.19725543, 0.19692671, 0.19579434, 0.19358478, 0.19230412,
       0.18756879, 0.18719062, 0.18341986, 0.18229825, 0.17998326,
       0.17961746, 0.17958509, 0.17842096, 0.17468107, 0.1745642 ,
       0.17392123, 0.17388887, 0.17093441, 0.17053651, 0.1692642 ,
       0.16849918, 0.16624505, 0.16576882, 0.16534669, 0.16436001,
       0.16244558, 0.16243364, 0.16174278, 0.16097982, 0.16037339,
       0.16030797, 0.15847015, 0.15837338, 0.15814698, 0.15792851,
       0.15344008, 0.15271964, 0.15197113, 0.15181629, 0.14787347,
       0.14764305, 0.14717758, 0.14688051, 0.14592689, 0.1458334 ,
       0.14457912, 0.1442514 , 0.1435661 , 0.14292077, 0.14234737,
       0.1415144 , 0.13990107, 0.13965014, 0.13857806, 0.13823609,
       0.13514532, 0.13506902, 0.13446298, 0.13433325, 0.13401364,
       0.13349698, 0.12990337, 0.12984922, 0.12915526, 0.1290376 ,
       0.1252095 , 0.12518542, 0.12447726, 0.12412921, 0.12334499,
       0.12331518, 0.12213864, 0.1215982 , 0.12155747, 0.12122695,
       0.12074162, 0.1206066 , 0.11868242, 0.11864146, 0.11862816,
       0.11852828, 0.1120964 , 0.11146059, 0.10776697, 0.10720229,
       0.10381543, 0.10340206, 0.1019967 , 0.10191254, 0.09851539,
       0.09835161, 0.09051088, 0.09034801, 0.08813827, 0.0880036 ,
       0.08520469, 0.08516473, 0.08506201, 0.08502719, 0.06815525,
       0.06773585, 0.06617344, 0.06471183, 0.06018365, 0.05991696,
       0.05892893, 0.05884379, 0.05352678, 0.05333942, 0.04322809,
       0.04278959, 0.04171203, 0.04167299, 0.03494049, 0.03486069,
       0.03208817, 0.03201568, 0.023578  ]))





3
</code></pre>
<p>此时获得的fpr，tpr，thres为3个一维数组，通过如下代码可以将三者合并为一个二维数组</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.查看假警报率（fpr）、命中率（tpr）及阈值（thres）</span></span><br><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;阈值&#x27;</span>] = <span class="built_in">list</span>(thres)</span><br><span class="line">a[<span class="string">&#x27;假警报率&#x27;</span>] = <span class="built_in">list</span>(fpr)</span><br><span class="line">a[<span class="string">&#x27;命中率&#x27;</span>] = <span class="built_in">list</span>(tpr)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>阈值</th>
      <th>假警报率</th>
      <th>命中率</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.930369</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.930369</td>
      <td>0.000000</td>
      <td>0.002874</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.867342</td>
      <td>0.000000</td>
      <td>0.034483</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.864187</td>
      <td>0.001885</td>
      <td>0.034483</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.857303</td>
      <td>0.001885</td>
      <td>0.040230</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以看到，随着阈值的下降，命中率和假警报率都在上升，对于表格有几点说明：</p>
<p>1.表格第一行的阈值表示只有当一名客户被预测流失的概率≥193%，才判定其会流失，但因为概率不会超过100%，所以此时所有客户都不会被预测为流失，此时命中率和假警报率为0.可见这个阈值其实没有什么意义，这个阈值是roc_curve()函数的默认设置，官方文档中的意思是第一个阈值没有什么意义，其往往是设置为最大阈值（本案例中为0.9303）+1，以保证没有任何记录被选中，至于阈值是如何产生的，看后面的知识点扩展。</p>
<p>2.表格第二行数据表示只有当一个客户被预测流失的概率≥93.03%，才判定其会流失，这个条件还是比较苛刻的，此时被预测为流失的客户还是很少，命中率为0.0028</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.绘制ROC曲线</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 设置中文</span></span><br><span class="line">plt.plot(fpr, tpr)  <span class="comment"># 通过plot()函数绘制折线图</span></span><br><span class="line">plt.title(<span class="string">&#x27;ROC曲线&#x27;</span>)  <span class="comment"># 添加标题，注意如果要写中文，需要在之前添加一行代码：plt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;]</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;FPR&#x27;</span>)  <span class="comment"># 添加X轴标签</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;TPR&#x27;</span>)  <span class="comment"># 添加Y轴标</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_415_0.png" alt="output_415_0"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.求出模型的AUC值</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score  <span class="comment"># 传入roc_auc_score()函数</span></span><br><span class="line">score = roc_auc_score(y_test, y_pred_proba[:,<span class="number">1</span>]) <span class="comment"># 为函数传入测试集中目标变量y_test的值及预测的流失概率</span></span><br><span class="line">score</span><br></pre></td></tr></table></figure>




<pre><code>0.8103854528908967
</code></pre>
<p><strong>补充知识点：对阈值取值的理解</strong></p>
<p>阈值如何选取的？下表中第二个阈值0.930369是如何来的？<br><img src="%E4%B8%8B%E8%BD%BD%20(44).png" alt="下载 (44)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">max</span>(y_pred_proba[:,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>




<pre><code>0.9303686064618016
</code></pre>
<p>在测试样本中，对预测分类为1的概率进行排序，使用sort_values()函数，并设置降序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = pd.DataFrame(y_pred_proba, columns=[<span class="string">&#x27;分类为0概率&#x27;</span>, <span class="string">&#x27;分类为1概率&#x27;</span>])</span><br><span class="line">a = a.sort_values(<span class="string">&#x27;分类为1概率&#x27;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">a.head(<span class="number">15</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>分类为0概率</th>
      <th>分类为1概率</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>326</th>
      <td>0.069631</td>
      <td>0.930369</td>
    </tr>
    <tr>
      <th>366</th>
      <td>0.085373</td>
      <td>0.914627</td>
    </tr>
    <tr>
      <th>662</th>
      <td>0.092923</td>
      <td>0.907077</td>
    </tr>
    <tr>
      <th>1154</th>
      <td>0.105118</td>
      <td>0.894882</td>
    </tr>
    <tr>
      <th>1036</th>
      <td>0.105906</td>
      <td>0.894094</td>
    </tr>
    <tr>
      <th>1093</th>
      <td>0.111303</td>
      <td>0.888697</td>
    </tr>
    <tr>
      <th>1164</th>
      <td>0.115550</td>
      <td>0.884450</td>
    </tr>
    <tr>
      <th>891</th>
      <td>0.116594</td>
      <td>0.883406</td>
    </tr>
    <tr>
      <th>437</th>
      <td>0.123060</td>
      <td>0.876940</td>
    </tr>
    <tr>
      <th>1153</th>
      <td>0.127293</td>
      <td>0.872707</td>
    </tr>
    <tr>
      <th>749</th>
      <td>0.129633</td>
      <td>0.870367</td>
    </tr>
    <tr>
      <th>49</th>
      <td>0.132658</td>
      <td>0.867342</td>
    </tr>
    <tr>
      <th>681</th>
      <td>0.133410</td>
      <td>0.866590</td>
    </tr>
    <tr>
      <th>1327</th>
      <td>0.135813</td>
      <td>0.864187</td>
    </tr>
    <tr>
      <th>264</th>
      <td>0.136599</td>
      <td>0.863401</td>
    </tr>
  </tbody>
</table>
</div>



<p>上表的第一列为测试集样本的序号，后2列分别为分类为0和分类为1的概率。序号326的样本其分类为1的概率最高，为0.930369，这个概率就是之前提到的阈值。所有样本的分类就是根据这个阈值进行的，分类为1的概率小于该阈值的样本都被列为分类0，大于等于该阈值的样本都被列为分类1，因为只有序号326的样本满足分类为1的概率大于等于该阈值，所以只有该样本会被列为分类1（实际上该样本也的确为分类1），其余的样本都被列为分类0.</p>
<p>在举个例子，序号49的样本其分类为1的概率为0.867342，在之前的表格中，它是除1.930369外第二大的阈值。序号49的样本在本表格中排在第12位，因此大于等于该概率的共有12个样本，这12个样本会被列为分类1（实际上这12个样本也的确为分类1），其余样本则被列为分类0</p>
<h3 id="4-3-3-KS曲线的基本原理"><a href="#4-3-3-KS曲线的基本原理" class="headerlink" title="4.3.3 KS曲线的基本原理"></a>4.3.3 KS曲线的基本原理</h3><p>KS曲线和ROC曲线在本质上是相同的，区别于ROC曲线将假警报率（FPR）作为横坐标，将命中率（TPR）作为纵坐标，KS曲线将阈值作为横坐标，将命中率与假警报率之差作为纵坐标</p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(45).png" alt="下载 (45)"></p>
<p>ROC曲线对应的是AUC值，KS曲线对应的则是KS值</p>
<p>KS&#x3D;max(TPR-FPR)</p>
<p>KS值就是KS曲线的峰值，上图当阈值为40%时，KS值为55%，即该模型在阈值为40%时能尽可能地识别坏人，并尽可能地不误伤好人</p>
<p>一般情况下，我们希望模型有较大的KS值，因为较大的KS值说明模型有较强的区分能力，不同取值范围的KS值含义如下：</p>
<p>KS值&lt;0.2，一般认为模型的区分能力较弱</p>
<p>KS∈[0.2,0.3]，模型具有一定的区分能力（实战中达到该水平已经不错了）</p>
<p>KS∈[0.3,0.5]，模型具有较强的区分能力</p>
<p>但KS也不是越大越好，如果KS值大于0.75，往往表示模型有异常</p>
<h3 id="4-3-4-案例实战：用KS曲线评估客户流失预警模型"><a href="#4-3-4-案例实战：用KS曲线评估客户流失预警模型" class="headerlink" title="4.3.4 案例实战：用KS曲线评估客户流失预警模型"></a>4.3.4 案例实战：用KS曲线评估客户流失预警模型</h3><p>补充知识点：KS曲线绘制</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = pd.DataFrame()  <span class="comment"># 创建一个空DataFrame </span></span><br><span class="line">a[<span class="string">&#x27;阈值&#x27;</span>] = <span class="built_in">list</span>(thres)</span><br><span class="line">a[<span class="string">&#x27;假警报率&#x27;</span>] = <span class="built_in">list</span>(fpr)</span><br><span class="line">a[<span class="string">&#x27;命中率&#x27;</span>] = <span class="built_in">list</span>(tpr)</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>阈值</th>
      <th>假警报率</th>
      <th>命中率</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.930369</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.930369</td>
      <td>0.000000</td>
      <td>0.002874</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.867342</td>
      <td>0.000000</td>
      <td>0.034483</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.864187</td>
      <td>0.001885</td>
      <td>0.034483</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.857303</td>
      <td>0.001885</td>
      <td>0.040230</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(thres[<span class="number">1</span>:], tpr[<span class="number">1</span>:]) <span class="comment"># 阈值为横坐标，命中率为纵坐标</span></span><br><span class="line">plt.plot(thres[<span class="number">1</span>:], fpr[<span class="number">1</span>:]) <span class="comment"># 阈值为横坐标，假警报率为纵坐标</span></span><br><span class="line">plt.plot(thres[<span class="number">1</span>:], tpr[<span class="number">1</span>:] - fpr[<span class="number">1</span>:]) <span class="comment"># 阈值为横坐标，命中率-假警报率为横坐标</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;threshold&#x27;</span>) </span><br><span class="line">plt.legend([<span class="string">&#x27;tpr&#x27;</span>, <span class="string">&#x27;fpr&#x27;</span>, <span class="string">&#x27;tpr-fpr&#x27;</span>])</span><br><span class="line">plt.gca().invert_xaxis() <span class="comment"># 反转x轴，把阈值从大到小排序再绘制KS曲线，先用gac()函数获取坐标轴信息，再用invert_xaxis()函数反转x轴</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_431_0.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">max</span>(tpr - fpr)  <span class="comment"># 快速求出KS值</span></span><br></pre></td></tr></table></figure>




<pre><code>0.4744656418256471
</code></pre>
<p>获取KS值对应的阈值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># KS值对应的阈值</span></span><br><span class="line">a[<span class="string">&#x27;TPR-FPR&#x27;</span>] = a[<span class="string">&#x27;命中率&#x27;</span>] - a[<span class="string">&#x27;假警报率&#x27;</span>]</span><br><span class="line">a.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>阈值</th>
      <th>假警报率</th>
      <th>命中率</th>
      <th>TPR-FPR</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.930369</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.930369</td>
      <td>0.000000</td>
      <td>0.002874</td>
      <td>0.002874</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.867342</td>
      <td>0.000000</td>
      <td>0.034483</td>
      <td>0.034483</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.864187</td>
      <td>0.001885</td>
      <td>0.034483</td>
      <td>0.032598</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.857303</td>
      <td>0.001885</td>
      <td>0.040230</td>
      <td>0.038345</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 另外一种获取KS值的方式</span></span><br><span class="line"><span class="built_in">max</span>(a[<span class="string">&#x27;TPR-FPR&#x27;</span>])</span><br></pre></td></tr></table></figure>




<pre><code>0.4744656418256471
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取KS值对应的阈值等信息，按照特定条件筛选</span></span><br><span class="line">a[a[<span class="string">&#x27;TPR-FPR&#x27;</span>] == <span class="built_in">max</span>(a[<span class="string">&#x27;TPR-FPR&#x27;</span>])]</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>阈值</th>
      <th>假警报率</th>
      <th>命中率</th>
      <th>TPR-FPR</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>224</th>
      <td>0.27769</td>
      <td>0.255419</td>
      <td>0.729885</td>
      <td>0.474466</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">数据科学</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/机器学习与深度学习//" class="article-tag-list-link color5">机器学习与深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/02/02/Python%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98-part1/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-Numpy库详解" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/02/02/Numpy%E5%BA%93%E8%AF%A6%E8%A7%A3/">Numpy库详解</a>
    </h1>
  

        
        <a href="/2023/02/02/Numpy%E5%BA%93%E8%AF%A6%E8%A7%A3/" class="archive-article-date">
  	<time datetime="2023-02-02T06:19:27.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-02-02</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-np-array-函数生成写入数据的Numpy数组"><a href="#1-np-array-函数生成写入数据的Numpy数组" class="headerlink" title="1 np.array()函数生成写入数据的Numpy数组"></a>1 np.array()函数生成写入数据的Numpy数组</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ndarray是一个多维数组对象，由两部分构成，其一就是实际的数据，</span></span><br><span class="line"><span class="comment"># 其二时这些数据的元数据（数据维度，数据类型等）</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># 导入Numpy</span></span><br><span class="line">data1 = [<span class="number">6</span>, <span class="number">9.8</span>, <span class="number">3</span>, <span class="number">9</span>]</span><br><span class="line"><span class="comment"># 用np.array函&lt;span class=&quot;girk&quot;&gt;数生&lt;/span&gt;成Numpy数组，这是一种支持N维对象的且只能是同一数据类型的</span></span><br><span class="line">arr1 = np.array(data1)</span><br><span class="line">arr1</span><br></pre></td></tr></table></figure>




<pre><code>array([6. , 9.8, 3. , 9. ])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arr1.dtype <span class="comment"># 查看元数据精度</span></span><br></pre></td></tr></table></figure>




<pre><code>dtype(&#39;float64&#39;)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr1 = arr1.astype(np.int32)  <span class="comment"># 用astype()方法显式指定被转换数组的数据类型</span></span><br><span class="line">arr1</span><br></pre></td></tr></table></figure>




<pre><code>array([6, 9, 3, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data2 = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]] <span class="comment"># 高维数组</span></span><br><span class="line">arr2 = np.array(data2)</span><br><span class="line">arr2</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 2, 3, 4],
       [3, 4, 5, 6]])
</code></pre>
<h1 id="2-用np-arange-函数直接生成Numpy数组"><a href="#2-用np-arange-函数直接生成Numpy数组" class="headerlink" title="2 用np.arange()函数直接生成Numpy数组"></a>2 用np.arange()函数直接生成Numpy数组</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr3 = np.arange(<span class="number">0</span>,<span class="number">24</span>,<span class="number">2</span>)<span class="comment"># 用np.arange()函数可以直接生成Numpy数组；而range()表示一个迭代器，只能用在循环迭代中</span></span><br><span class="line">arr3</span><br></pre></td></tr></table></figure>




<pre><code>array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr3 = arr3 + <span class="number">1</span> <span class="comment"># 广播机制</span></span><br><span class="line">arr3</span><br></pre></td></tr></table></figure>




<pre><code>array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23])
</code></pre>
<h2 id="2-1-np-arange-与range-的区别"><a href="#2-1-np-arange-与range-的区别" class="headerlink" title="2.1 np.arange()与range()的区别"></a>2.1 np.arange()与range()的区别</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr4 = <span class="built_in">range</span>(<span class="number">10</span>) <span class="comment"># 系统并不能直接输出由range()函数生成的数据元素，但可以通过for 循环迭代出这些数据</span></span><br><span class="line"><span class="built_in">print</span>(arr4)</span><br></pre></td></tr></table></figure>

<pre><code>range(0, 10)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(arr3)) <span class="comment"># 类型是ndarray</span></span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;numpy.ndarray&#39;&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(arr4))<span class="comment"># 类型是range迭代器</span></span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;range&#39;&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">0.5</span>) <span class="comment"># np.arange()可以将步长设置为任意实数</span></span><br></pre></td></tr></table></figure>




<pre><code>array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ,
       6.5, 7. , 7.5, 8. , 8.5, 9. , 9.5])
</code></pre>
<h2 id="2-2-用np-linspace-函数在指定区间内生成指定个数的数组（比np-arange-方便）"><a href="#2-2-用np-linspace-函数在指定区间内生成指定个数的数组（比np-arange-方便）" class="headerlink" title="2.2 用np.linspace()函数在指定区间内生成指定个数的数组（比np.arange()方便）"></a>2.2 用np.linspace()函数在指定区间内生成指定个数的数组（比np.arange()方便）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = np.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">20</span>) <span class="comment"># 生成的数据区间是双闭的</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure>

<pre><code>[ 1.          1.47368421  1.94736842  2.42105263  2.89473684  3.36842105
  3.84210526  4.31578947  4.78947368  5.26315789  5.73684211  6.21052632
  6.68421053  7.15789474  7.63157895  8.10526316  8.57894737  9.05263158
  9.52631579 10.        ]
</code></pre>
<h1 id="3-用其他函数生成多维数组"><a href="#3-用其他函数生成多维数组" class="headerlink" title="3 用其他函数生成多维数组"></a>3 用其他函数生成多维数组</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zeros = np.zeros(shape = [<span class="number">3</span>,<span class="number">4</span>]) <span class="comment"># 生成尺寸为3*4的二维数组，元素均为0</span></span><br><span class="line">zeros</span><br></pre></td></tr></table></figure>




<pre><code>array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ones_ = np.ones(shape = [<span class="number">3</span>,<span class="number">4</span>], dtype = <span class="built_in">float</span>) <span class="comment"># 生成尺寸为3*4的二维数组，元素均为1，shape=是尺寸参数，dtype=是元素类型参数</span></span><br><span class="line">ones_</span><br></pre></td></tr></table></figure>




<pre><code>array([[1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array = np.array([[<span class="number">12</span>,<span class="number">2</span>,<span class="number">7.9</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>]])</span><br><span class="line">array</span><br></pre></td></tr></table></figure>




<pre><code>array([[12. ,  2. ,  7.9],
       [ 2. ,  3. ,  1. ]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array.dtype</span><br></pre></td></tr></table></figure>




<pre><code>dtype(&#39;float64&#39;)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b_zeros = np.zeros_like(array) <span class="comment"># 借壳上市，借用array的类型和尺寸</span></span><br><span class="line">b_zeros</span><br></pre></td></tr></table></figure>




<pre><code>array([[0., 0., 0.],
       [0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">0</span>,<span class="number">6</span>)  <span class="comment"># 创建一个一维数组，数组元素为0，1，...5</span></span><br><span class="line">arr = arr.reshape(<span class="number">2</span>,<span class="number">3</span>)    <span class="comment"># 将x的尺寸重构为两行三列</span></span><br><span class="line"><span class="built_in">print</span>(arr)</span><br></pre></td></tr></table></figure>

<pre><code>[[0 1 2]
 [3 4 5]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.ones_like(arr)   <span class="comment"># 生成尺寸信息为两行三列但全部元素为1的数组</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 1, 1],
       [1, 1, 1]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.eye(<span class="number">5</span>)   <span class="comment"># 生成单位阵</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceshi = np.full((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),<span class="number">25</span>,dtype=np.int32)   <span class="comment"># full以25为元素将数组填满</span></span><br><span class="line">ceshi</span><br></pre></td></tr></table></figure>




<pre><code>array([[[25, 25, 25, 25],
        [25, 25, 25, 25],
        [25, 25, 25, 25]],

       [[25, 25, 25, 25],
        [25, 25, 25, 25],
        [25, 25, 25, 25]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceshi.tolist()      <span class="comment"># 将ndarray数组类型转换为列表类型</span></span><br></pre></td></tr></table></figure>




<pre><code>[[[25, 25, 25, 25], [25, 25, 25, 25], [25, 25, 25, 25]],
 [[25, 25, 25, 25], [25, 25, 25, 25], [25, 25, 25, 25]]]
</code></pre>
<h1 id="4-N维数组的属性"><a href="#4-N维数组的属性" class="headerlink" title="4 N维数组的属性"></a>4 N维数组的属性</h1><h2 id="4-1-数组维度ndim"><a href="#4-1-数组维度ndim" class="headerlink" title="4.1 数组维度ndim"></a>4.1 数组维度ndim</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_array = np.arange(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">my_array.ndim           <span class="comment"># 用ndim表示数组的维度</span></span><br></pre></td></tr></table></figure>




<pre><code>1
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my_array</span><br></pre></td></tr></table></figure>




<pre><code>array([1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre>
<h2 id="4-2-数组的形状shape"><a href="#4-2-数组的形状shape" class="headerlink" title="4.2 数组的形状shape"></a>4.2 数组的形状shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my_array.shape   <span class="comment"># 一维数组也被称为1D张量，其形状信息就是其尺寸，输出结果为数组的长度</span></span><br></pre></td></tr></table></figure>




<pre><code>(9,)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = np.arange(<span class="number">15</span>)   <span class="comment"># 创建一个包含15个元素的一维数组</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = b.reshape(<span class="number">3</span>,<span class="number">5</span>)  <span class="comment"># 改变数组形状为3行5列，reshape()函数是很好的快速生成二维三维数组的方法</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.ndim   <span class="comment"># 查看数组维度信息</span></span><br></pre></td></tr></table></figure>




<pre><code>2
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.shape  <span class="comment"># 查看数组形状信息</span></span><br></pre></td></tr></table></figure>




<pre><code>(3, 5)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.size  <span class="comment"># 查看数组元素总个数</span></span><br></pre></td></tr></table></figure>




<pre><code>15
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">30</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>)  <span class="comment"># 重构数组为2通道3行5列（保证30=2*3*5即可）</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([[[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14]],

       [[15, 16, 17, 18, 19],
        [20, 21, 22, 23, 24],
        [25, 26, 27, 28, 29]]])
</code></pre>
<h2 id="4-3-数组对象的个数size，相当于shape中的m-n"><a href="#4-3-数组对象的个数size，相当于shape中的m-n" class="headerlink" title="4.3 数组对象的个数size，相当于shape中的m*n"></a>4.3 数组对象的个数size，相当于shape中的m*n</h2><h2 id="4-4-数组对象的元素类型dtype"><a href="#4-4-数组对象的元素类型dtype" class="headerlink" title="4.4 数组对象的元素类型dtype"></a>4.4 数组对象的元素类型dtype</h2><h2 id="4-5-数组对象中每个元素的大小itemsize，以字节为单位"><a href="#4-5-数组对象中每个元素的大小itemsize，以字节为单位" class="headerlink" title="4.5 数组对象中每个元素的大小itemsize，以字节为单位"></a>4.5 数组对象中每个元素的大小itemsize，以字节为单位</h2><h1 id="5-Numpy数组中的运算"><a href="#5-Numpy数组中的运算" class="headerlink" title="5 Numpy数组中的运算"></a>5 Numpy数组中的运算</h1><h2 id="5-1-向量运算"><a href="#5-1-向量运算" class="headerlink" title="5.1 向量运算"></a>5.1 向量运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">list1 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line">list2 = [<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>,<span class="number">19</span>,<span class="number">20</span>]</span><br><span class="line">list1_arr = np.array(list1)    <span class="comment"># 将list1转换成ndarray</span></span><br><span class="line">list2_arr = np.array(list2)    <span class="comment"># 将list2转换成ndarray</span></span><br><span class="line">list_sum = list1_arr + list2_arr  <span class="comment"># 求和</span></span><br><span class="line"><span class="built_in">print</span>(list_sum)</span><br></pre></td></tr></table></figure>

<pre><code>[12 14 16 18 20 22 24 26 28 30]
</code></pre>
<h2 id="5-2-算术运算"><a href="#5-2-算术运算" class="headerlink" title="5.2 算术运算"></a>5.2 算术运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">10</span>)</span><br><span class="line">b = np.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a + b   <span class="comment"># 数组加法，只要操作数组的形状（维度）一致，就可以对它们进行逐元素施加算术运算，下同</span></span><br></pre></td></tr></table></figure>




<pre><code>array([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a-b  <span class="comment"># 数组减法</span></span><br></pre></td></tr></table></figure>




<pre><code>array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a*b <span class="comment"># 数组乘法</span></span><br></pre></td></tr></table></figure>




<pre><code>array([ 0.,  2.,  6., 12., 20., 30., 42., 56., 72., 90.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a/b   <span class="comment"># 数组除法</span></span><br></pre></td></tr></table></figure>




<pre><code>array([0.        , 0.5       , 0.66666667, 0.75      , 0.8       ,
       0.83333333, 0.85714286, 0.875     , 0.88888889, 0.9       ])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a%b   <span class="comment"># 数组取余</span></span><br></pre></td></tr></table></figure>




<pre><code>array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a**<span class="number">2</span>   <span class="comment"># 数组元素平方</span></span><br></pre></td></tr></table></figure>




<pre><code>array([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81], dtype=int32)
</code></pre>
<h2 id="5-3-其余运算"><a href="#5-3-其余运算" class="headerlink" title="5.3 其余运算"></a>5.3 其余运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sum()求和,min()求最小值,max()求最大值,median()求中位数,mean()求平均数,average()求加权平均数,std()求标准差,var()求方差,sin(),cos(),tan().详见官方文档</span></span><br></pre></td></tr></table></figure>

<h2 id="5-4-逐元素运算与张量点乘运算"><a href="#5-4-逐元素运算与张量点乘运算" class="headerlink" title="5.4 逐元素运算与张量点乘运算"></a>5.4 逐元素运算与张量点乘运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># N维数组的“类矩阵”运算，元素对元素的运算,这种“元素对元素”的操作，要求两个操作对象的形状必须完全一致，否则无法计算</span></span><br><span class="line">a  = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 2],
       [3, 4]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = np.ones((<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>array([[1., 1.],
       [1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a + b</span><br></pre></td></tr></table></figure>




<pre><code>array([[2., 3.],
       [4., 5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 与以往数学经验相违背，二维数组（矩阵）的乘法也是基于“元素对元素”的</span></span><br><span class="line">a *b</span><br></pre></td></tr></table></figure>




<pre><code>array([[1., 2.],
       [3., 4.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.multiply(a,b)   <span class="comment"># 乘法的函数表达式为Multiply()</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[1., 2.],
       [3., 4.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)  <span class="comment"># 输出二维数组，三行三列</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = np.ones(shape = (<span class="number">3</span>,<span class="number">2</span>))</span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>array([[1., 1.],
       [1., 1.],
       [1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.dot(a,b)  <span class="comment"># dot()函数可以看作是数组（矩阵）相乘符号</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[ 3.,  3.],
       [12., 12.],
       [21., 21.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a @ b   <span class="comment"># @可以看作是数组相乘符号，与dot()等价</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[ 3.,  3.],
       [12., 12.],
       [21., 21.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.mat(a)  <span class="comment"># 将数组a转换成矩阵a</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>matrix([[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = np.mat(b)   <span class="comment"># 将数组b转换成矩阵b</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>matrix([[1., 1.],
        [1., 1.],
        [1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a * b    <span class="comment"># 此时矩阵a和矩阵b之间施加的是点乘运算</span></span><br></pre></td></tr></table></figure>




<pre><code>matrix([[ 3.,  3.],
        [12., 12.],
        [21., 21.]])
</code></pre>
<p> 对于矩阵matrix类，说几个较为常用的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.mat(np.random.random((<span class="number">3</span>,<span class="number">3</span>)))  <span class="comment"># 生成3x3的随机数矩阵</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>matrix([[0.42724265, 0.74533346, 0.07753052],
        [0.17421774, 0.68195858, 0.4268899 ],
        [0.96395622, 0.32820068, 0.89959132]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.I        <span class="comment"># 返回矩阵a的逆矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>matrix([[ 1.36969551, -1.86641782,  0.76763937],
        [ 0.73718785,  0.89583361, -0.48864065],
        [-1.73664645,  1.67312851,  0.46732492]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.T     <span class="comment"># 返回矩阵a的转置矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>matrix([[0.42724265, 0.17421774, 0.96395622],
        [0.74533346, 0.68195858, 0.32820068],
        [0.07753052, 0.4268899 , 0.89959132]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.A     <span class="comment"># 返回矩阵a对应的二维数组</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[0.42724265, 0.74533346, 0.07753052],
       [0.17421774, 0.68195858, 0.4268899 ],
       [0.96395622, 0.32820068, 0.89959132]])
</code></pre>
<p>总结：</p>
<p>（1）numpy二维数组之间相乘是元素对元素的相乘，不是矩阵相乘，要求参与运算的两个二维数组形状必须完全一致</p>
<p>（2）np.dot（a,b）与 a@b完全等价，表达将numpy数组a与numpy数组b进行矩阵相乘，得到的结果也是numpy数组类型</p>
<p>（3）np.mat(a)将numpy数组a转换为矩阵形式，矩阵和矩阵间用相乘符号表达的是矩阵相乘，得到的结果也是matrix矩阵类型</p>
<p>（4）a是matrix矩阵类型，a.I返回对应的逆矩阵，a.T返回对应的转置矩阵，a.A返回对应的二维数组</p>
<p>（5）向量点乘的三种方法：</p>
<ol>
<li><p>a*b    </p>
</li>
<li><p>np.multiply(a,b)</p>
</li>
<li><p>np.einsum(‘ij,ij-&gt;ij’,a,b)</p>
</li>
</ol>
<p>(6) 向量内积的两种方法：</p>
<ol>
<li><p>np.inner(a,b)</p>
</li>
<li><p>np.einsum(‘ij,ij-&gt;’,a,b)</p>
</li>
</ol>
<p>（7）抽取某个矩阵的对角线：</p>
<ol>
<li><p>np.diag(a)</p>
</li>
<li><p>np.einsum(‘ii-&gt;i’,a)</p>
</li>
</ol>
<p>（8）求矩阵的转置：</p>
<ol>
<li><p>np.transpose(a)</p>
</li>
<li><p>np.einsum(‘ij-&gt;ji’,a)</p>
</li>
</ol>
<h1 id="6-爱因斯坦求和约定"><a href="#6-爱因斯坦求和约定" class="headerlink" title="6 爱因斯坦求和约定"></a>6 爱因斯坦求和约定</h1><h2 id="6-1-Numpy中的einsum-方法"><a href="#6-1-Numpy中的einsum-方法" class="headerlink" title="6.1 Numpy中的einsum()方法"></a>6.1 Numpy中的einsum()方法</h2><p><img src="%E4%B8%8B%E8%BD%BD%20(2).png" alt="下载 (2)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">result = np.einsum(<span class="string">&#x27;ijk,ijl-&gt;kl&#x27;</span>,a,b)<span class="comment"># 输入向量a是一个三维矩阵（ijk），输入向量b也是一个三维矩阵（ijl），计算结果是一个二维张量（kl）</span></span><br></pre></td></tr></table></figure>


<pre><code>---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

&lt;ipython-input-55-3ce51b446b63&gt; in &lt;module&gt;
      1 import numpy as np
----&gt; 2 result = np.einsum(&#39;ijk,ijl-&gt;kl&#39;,a,b)# 输入向量a是一个三维矩阵（ijk），输入向量b也是一个三维矩阵（ijl），计算结果是一个二维张量（kl）


&lt;__array_function__ internals&gt; in einsum(*args, **kwargs)


E:\anaconda  111\lib\site-packages\numpy\core\einsumfunc.py in einsum(*operands, **kwargs)
   1354     # If no optimization, run pure einsum
   1355     if optimize_arg is False:
-&gt; 1356         return c_einsum(*operands, **kwargs)
   1357 
   1358     valid_einsum_kwargs = [&#39;out&#39;, &#39;dtype&#39;, &#39;order&#39;, &#39;casting&#39;]


ValueError: einstein sum subscripts string contains too many subscripts for operand 0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">arr = np.arange(<span class="number">10</span>)</span><br><span class="line">arr</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sum1 = np.einsum(<span class="string">&#x27;i-&gt;&#x27;</span>,arr)  <span class="comment"># 一个一维张量（箭头前面的字符长度为1），它的维度消失了（箭头后面的字符为空，其实是变成了一个标量），</span></span><br><span class="line">                             <span class="comment"># 如何变成标量的呢？是通过求和达成的“约简”</span></span><br><span class="line">sum1</span><br></pre></td></tr></table></figure>




<pre><code>45
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr2  = np.arange(<span class="number">20</span>).reshape(<span class="number">4</span>,<span class="number">5</span>)  <span class="comment"># 二维数组</span></span><br><span class="line">arr2</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14],
       [15, 16, 17, 18, 19]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sum_col = np.einsum(<span class="string">&#x27;ij-&gt;j&#x27;</span>,arr2)  <span class="comment"># 二维变一维，i(行)j（列）变为j（列），即行这个维度被消灭，通过求和的方式达到维度约简，实现了按列求和</span></span><br><span class="line">sum_col</span><br></pre></td></tr></table></figure>




<pre><code>array([30, 34, 38, 42, 46])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sum_row = np.einsum(<span class="string">&#x27;ab-&gt;a&#x27;</span>,arr2)  <span class="comment"># 按行求和，注意：在格式串中用什么字母并不重要(abc 或者 ijk)，重要的是他们的长度，以及它们是否在箭头之后出现</span></span><br><span class="line">sum_row</span><br></pre></td></tr></table></figure>




<pre><code>array([10, 35, 60, 85])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">            [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">            [<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>]])</span><br><span class="line">B = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">result = A @ B</span><br><span class="line">result</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 2,  3,  1],
       [ 4,  6,  2],
       [10, 15,  5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result2 = np.einsum(<span class="string">&#x27;ij,jk-&gt;ik&#x27;</span>,A,B) <span class="comment"># 也可以用这种方法实现矩阵乘法</span></span><br><span class="line">result2</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 2,  3,  1],
       [ 4,  6,  2],
       [10, 15,  5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">b = np.ones(shape = (<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">np.einsum(<span class="string">&#x27;ij,ij-&gt;ij&#x27;</span>,a,b)  <span class="comment"># 实现向量的点乘</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[1., 2.],
       [3., 4.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.einsum(<span class="string">&#x27;ij,ij-&gt;&#x27;</span>,a,b)   <span class="comment"># 实现向量的内积</span></span><br></pre></td></tr></table></figure>




<pre><code>10.0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">AA = np.array([[<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>],</span><br><span class="line">              [<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>],</span><br><span class="line">              [<span class="number">31</span>,<span class="number">32</span>,<span class="number">33</span>,<span class="number">34</span>],</span><br><span class="line">              [<span class="number">41</span>,<span class="number">42</span>,<span class="number">43</span>,<span class="number">44</span>]])</span><br><span class="line">np.einsum(<span class="string">&#x27;ii-&gt;i&#x27;</span>,AA)   <span class="comment"># 求某个矩阵的对角线</span></span><br></pre></td></tr></table></figure>




<pre><code>array([11, 22, 33, 44])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.einsum(<span class="string">&#x27;ii-&gt;&#x27;</span>,AA)   <span class="comment"># 求矩阵的迹</span></span><br></pre></td></tr></table></figure>




<pre><code>110
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.einsum(<span class="string">&#x27;ij-&gt;ji&#x27;</span>,AA)  <span class="comment"># 求矩阵的转置</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[11, 21, 31, 41],
       [12, 22, 32, 42],
       [13, 23, 33, 43],
       [14, 24, 34, 44]])
</code></pre>
<h1 id="7-Numpy中的“轴”方向"><a href="#7-Numpy中的“轴”方向" class="headerlink" title="7 Numpy中的“轴”方向"></a>7 Numpy中的“轴”方向</h1><h2 id="7-1-不给轴方向的“约减操作”"><a href="#7-1-不给轴方向的“约减操作”" class="headerlink" title="7.1 不给轴方向的“约减操作”"></a>7.1 不给轴方向的“约减操作”</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones((<span class="number">2</span>,<span class="number">3</span>))  <span class="comment"># 创建形状为2*3，元素值均为1的矩阵</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([[1., 1., 1.],
       [1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.<span class="built_in">sum</span>()   <span class="comment"># 约减操作，将矩阵元素求和变成一个元素</span></span><br></pre></td></tr></table></figure>




<pre><code>6.0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.mean()</span><br></pre></td></tr></table></figure>




<pre><code>1.0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.<span class="built_in">max</span>()</span><br></pre></td></tr></table></figure>




<pre><code>1.0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.<span class="built_in">min</span>()</span><br></pre></td></tr></table></figure>




<pre><code>1.0
</code></pre>
<h2 id="7-2-指定轴方向的约减操作"><a href="#7-2-指定轴方向的约减操作" class="headerlink" title="7.2 指定轴方向的约减操作"></a>7.2 指定轴方向的约减操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上述函数都有一个名为操作轴（axis）的参数，其默认值为None，即不指定约减方向,它将所有数据都约减为一个元素</span></span><br><span class="line"><span class="comment"># 若axis = 0,可简单地理解为从垂直方向进行约减，若axis = 1,从水平方向进行约减</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.<span class="built_in">sum</span>(axis = <span class="number">0</span>)  <span class="comment"># 垂直方向约减，也可用np.einsum()实现</span></span><br></pre></td></tr></table></figure>




<pre><code>array([2., 2., 2.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.<span class="built_in">sum</span>(<span class="number">1</span>)   <span class="comment"># 水平方向上约减，也可用np.einsum()实现</span></span><br></pre></td></tr></table></figure>




<pre><code>array([3., 3.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更加普适的解释：括号由外到内，对应从小到大的维数，对于一个三维数组[[[1,1,1],[2,2,3]],[[3,3,3],[4,4,4]]],有三层括号，其维度由外到内</span></span><br><span class="line"><span class="comment"># 分别为[0,1,2]</span></span><br><span class="line">a = np.array([[[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]],[[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>]]])</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([[[1, 1, 1],
        [2, 2, 2]],

       [[3, 3, 3],
        [4, 4, 4]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.<span class="built_in">sum</span>(axis = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>




<pre><code>array([[4, 4, 4],
       [6, 6, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.<span class="built_in">sum</span>(axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>array([[3, 3, 3],
       [7, 7, 7]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.<span class="built_in">sum</span>(axis = <span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 3,  6],
       [ 9, 12]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.ndim</span><br></pre></td></tr></table></figure>




<pre><code>3
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.<span class="built_in">sum</span>(axis = <span class="number">2</span>)</span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 3,  6],
       [ 9, 12]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.ndim</span><br></pre></td></tr></table></figure>




<pre><code>2
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.linspace(<span class="number">1</span>,<span class="number">9</span>,<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([[1., 2., 3.],
       [4., 5., 6.],
       [7., 8., 9.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(a.<span class="built_in">max</span>(<span class="number">0</span>),a.<span class="built_in">max</span>(<span class="number">1</span>),a.<span class="built_in">max</span>())</span><br></pre></td></tr></table></figure>

<pre><code>[7. 8. 9.] [3. 6. 9.] 9.0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(a.mean(<span class="number">0</span>),a.mean(<span class="number">1</span>),a.mean())</span><br></pre></td></tr></table></figure>

<pre><code>[4. 5. 6.] [2. 5. 8.] 5.0
</code></pre>
<h1 id="8-操作数组元素"><a href="#8-操作数组元素" class="headerlink" title="8 操作数组元素"></a>8 操作数组元素</h1><h2 id="8-1-通过索引访问数组元素"><a href="#8-1-通过索引访问数组元素" class="headerlink" title="8.1 通过索引访问数组元素"></a>8.1 通过索引访问数组元素</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">one_dim = np.linspace(-<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">12</span>)</span><br><span class="line"><span class="built_in">print</span>(one_dim)</span><br></pre></td></tr></table></figure>

<pre><code>[-0.5 -0.4 -0.3 -0.2 -0.1  0.   0.1  0.2  0.3  0.4  0.5  0.6]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">one_dim[<span class="number">0</span>]  <span class="comment"># 访问第一个元素</span></span><br></pre></td></tr></table></figure>




<pre><code>-0.5
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">one_dim[-<span class="number">1</span>]   <span class="comment"># 访问倒数第一个元素</span></span><br></pre></td></tr></table></figure>




<pre><code>0.6
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">one_dim[<span class="number">0</span>] = <span class="number">1</span>  <span class="comment"># 对第一个元素复制</span></span><br><span class="line"><span class="built_in">print</span>(one_dim)</span><br></pre></td></tr></table></figure>

<pre><code>[ 1.  -0.4 -0.3 -0.2 -0.1  0.   0.1  0.2  0.3  0.4  0.5  0.6]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">two_dim = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">                   [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line">                   [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">two_dim</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 2, 3],
       [4, 5, 6],
       [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">two_dim[<span class="number">0</span>][<span class="number">2</span>]   <span class="comment"># 类似Cpp的两个方括号索引</span></span><br></pre></td></tr></table></figure>




<pre><code>3
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">two_dim[<span class="number">0</span>,<span class="number">2</span>]  <span class="comment"># numpy中更为简单的索引方法</span></span><br></pre></td></tr></table></figure>




<pre><code>3
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">two_dim[<span class="number">0</span>,<span class="number">2</span>] = <span class="number">100</span></span><br><span class="line">two_dim</span><br></pre></td></tr></table></figure>




<pre><code>array([[  1,   2, 100],
       [  4,   5,   6],
       [  7,   8,   9]])
</code></pre>
<h2 id="8-2-Numpy中的切片访问"><a href="#8-2-Numpy中的切片访问" class="headerlink" title="8.2 Numpy中的切片访问"></a>8.2 Numpy中的切片访问</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切片操作的核心是从原始数据中，按照给定规则提取出一个新的数组，对原始数组没有任何影响</span></span><br><span class="line">a = np.arange(<span class="number">10</span>)</span><br><span class="line">s = <span class="built_in">slice</span>(<span class="number">0</span>,<span class="number">9</span>,<span class="number">2</span>)   <span class="comment">#创建切片对象，比较麻烦一般不用</span></span><br><span class="line">b = a[s]       <span class="comment"># 按照切片规则提取数据</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 2, 4, 6, 8])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a   <span class="comment"># 原始数组a的值并不受切片影响</span></span><br></pre></td></tr></table></figure>




<pre><code>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">0</span>:<span class="number">9</span>:<span class="number">2</span>]  <span class="comment"># 用这种更简单的</span></span><br></pre></td></tr></table></figure>




<pre><code>array([0, 2, 4, 6, 8])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">2</span>:]</span><br></pre></td></tr></table></figure>




<pre><code>array([2, 3, 4, 5, 6, 7, 8, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">2</span>:-<span class="number">2</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([2, 3, 4, 5, 6, 7])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[::-<span class="number">1</span>]  <span class="comment"># 完全逆序读取</span></span><br></pre></td></tr></table></figure>




<pre><code>array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">24</span>).reshape((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([[[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]],

       [[12, 13, 14, 15],
        [16, 17, 18, 19],
        [20, 21, 22, 23]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[:,<span class="number">1</span>,-<span class="number">3</span>]   <span class="comment"># 进行多维数组的切片</span></span><br></pre></td></tr></table></figure>




<pre><code>array([ 5, 17])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[:,<span class="number">1</span>:<span class="number">3</span>,:]</span><br></pre></td></tr></table></figure>




<pre><code>array([[[ 4,  5,  6,  7],
        [ 8,  9, 10, 11]],

       [[16, 17, 18, 19],
        [20, 21, 22, 23]]])
</code></pre>
<h2 id="8-3-二维数组的转置与展平"><a href="#8-3-二维数组的转置与展平" class="headerlink" title="8.3 二维数组的转置与展平"></a>8.3 二维数组的转置与展平</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以通过transpose()方法将二维数组转置</span></span><br><span class="line">two_dim = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">                   [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line">                   [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">two_dim.transpose()    <span class="comment"># 也可以使用大写的字母T来完成操作：two_dim.T</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 4, 7],
       [2, 5, 8],
       [3, 6, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">two_dim</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 2, 3],
       [4, 5, 6],
       [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用ravel()方法将多维数组降成一维数组</span></span><br><span class="line">two_dim.ravel()   <span class="comment"># 同样的，revel()返回的仅仅是原始数组的视图而已，原始数组本身并没有发生变化</span></span><br></pre></td></tr></table></figure>




<pre><code>array([1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过显示变形来完成数组的降维</span></span><br><span class="line">two_dim.shape = (<span class="number">1</span>,-<span class="number">1</span>)   <span class="comment"># -1表示列数由系统自动推导出来，这种做法在TensorFlow框架中很常用，是一个技巧</span></span><br><span class="line">two_dim</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 2, 3, 4, 5, 6, 7, 8, 9]])
</code></pre>
<h1 id="9-Numpy中的广播"><a href="#9-Numpy中的广播" class="headerlink" title="9 Numpy中的广播"></a>9 Numpy中的广播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy中，当两个数组的形状不相同时，可扩充较小数组中的元素来匹配较大数组的形状，即为广播机制</span></span><br></pre></td></tr></table></figure>

<img src="%E4%B8%8B%E8%BD%BD%20(3).png" alt="下载 (3)" style="zoom:50%;">


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">3</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.shape</span><br></pre></td></tr></table></figure>




<pre><code>(3,)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a + <span class="number">5</span></span><br></pre></td></tr></table></figure>




<pre><code>array([5, 6, 7])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = np.arange(<span class="number">3</span>)</span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a + b</span><br></pre></td></tr></table></figure>




<pre><code>array([[1., 2., 3.],
       [1., 2., 3.],
       [1., 2., 3.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c = np.arange(<span class="number">3</span>).reshape((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">d = np.arange(<span class="number">3</span>)</span><br><span class="line">c + d</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 1, 2],
       [1, 2, 3],
       [2, 3, 4]])
</code></pre>
<h1 id="10-Numpy数组的高级索引"><a href="#10-Numpy数组的高级索引" class="headerlink" title="10 Numpy数组的高级索引"></a>10 Numpy数组的高级索引</h1><h2 id="10-1-花式索引"><a href="#10-1-花式索引" class="headerlink" title="10.1 花式索引"></a>10.1 花式索引</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 花式索引/整数索引</span></span><br><span class="line">normal_array = np.array([<span class="number">34</span>,<span class="number">45</span>,<span class="number">56</span>,<span class="number">69</span>,<span class="number">9</span>,<span class="number">11</span>,<span class="number">22</span>,<span class="number">71</span>,<span class="number">82</span>,<span class="number">10</span>,<span class="number">123</span>])</span><br><span class="line">normal_array</span><br></pre></td></tr></table></figure>




<pre><code>array([ 34,  45,  56,  69,   9,  11,  22,  71,  82,  10, 123])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fancy_index_array = normal_array[[<span class="number">0</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">7</span>]]  <span class="comment"># 注意花式索引的方法</span></span><br><span class="line">fancy_index_array</span><br></pre></td></tr></table></figure>




<pre><code>array([34, 82, 71, 71])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分解来看</span></span><br><span class="line">index = [<span class="number">0</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">7</span>]</span><br><span class="line">normal_array[index]</span><br></pre></td></tr></table></figure>




<pre><code>array([34, 82, 71, 71])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">two_dim_array = np.arange(<span class="number">20</span>).reshape(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">two_dim_array</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14],
       [15, 16, 17, 18, 19]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">two_dim_array[[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]]  <span class="comment"># 二维数组中的花式索引特指“行”索引，即会输出第0行数据，第2行数据，第1行数据，第0行数据</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  1,  2,  3,  4],
       [10, 11, 12, 13, 14],
       [ 5,  6,  7,  8,  9],
       [ 0,  1,  2,  3,  4]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 所有行的数据都涉及，但列的访问范围由col_index限定</span></span><br><span class="line">col_index = [<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">2</span>]</span><br><span class="line">two_dim_array[:,col_index]</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  2,  4,  2],
       [ 5,  7,  9,  7],
       [10, 12, 14, 12],
       [15, 17, 19, 17]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 访问二维数组two_dim_array的第二行的第0列、第3列和第1列</span></span><br><span class="line">two_dim_array[<span class="number">2</span>,[<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>]]</span><br></pre></td></tr></table></figure>




<pre><code>array([10, 13, 11])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在二维数组中设置两个花式索引，一个对应行坐标，一个对应列坐标，系统会自动两两配对，构成一个二维数组坐标，然后一一获取坐标点位置所指引的数据</span></span><br><span class="line">row_index = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>]</span><br><span class="line">col_index = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">two_dim_array[row_index,col_index]</span><br></pre></td></tr></table></figure>




<pre><code>array([ 0,  6, 15, 10])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">two_dim_array</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14],
       [15, 16, 17, 18, 19]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">row_index = np.array([<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>])  <span class="comment"># 定义花式访问的行</span></span><br><span class="line">col_mask = np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],dtype = <span class="built_in">bool</span>)   <span class="comment"># 定义列访问掩码</span></span><br><span class="line">two_dim_array[row_index[:,np.newaxis],col_mask]</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  2,  4],
       [10, 12, 14],
       [ 5,  7,  9]])
</code></pre>
<h2 id="10-2-布尔索引"><a href="#10-2-布尔索引" class="headerlink" title="10.2 布尔索引"></a>10.2 布尔索引</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">10</span>).reshape(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 1, 2, 3, 4],
       [5, 6, 7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[a&gt;<span class="number">5</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([6, 7, 8, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a&gt;<span class="number">5</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[False, False, False, False, False],
       [False,  True,  True,  True,  True]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[a&lt;<span class="number">5</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 1, 2, 3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[a&gt;=<span class="number">3</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([3, 4, 5, 6, 7, 8, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[a%<span class="number">2</span>==<span class="number">0</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 2, 4, 6, 8])
</code></pre>
<h1 id="11-数组的堆叠操作"><a href="#11-数组的堆叠操作" class="headerlink" title="11 数组的堆叠操作"></a>11 数组的堆叠操作</h1><h2 id="11-1-水平方向堆叠hstack"><a href="#11-1-水平方向堆叠hstack" class="headerlink" title="11.1 水平方向堆叠hstack()"></a>11.1 水平方向堆叠hstack()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr1 = np.zeros(shape = (<span class="number">2</span>,<span class="number">2</span>),dtype = np.<span class="built_in">int</span>)</span><br><span class="line">arr1</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 0],
       [0, 0]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr2 = np.ones(shape = (<span class="number">2</span>,<span class="number">3</span>),dtype = np.<span class="built_in">int</span>)</span><br><span class="line">arr2</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 1, 1],
       [1, 1, 1]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.hstack((arr1,arr2))</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 0, 1, 1, 1],
       [0, 0, 1, 1, 1]])
</code></pre>
<h2 id="11-2-垂直方向堆叠vstack"><a href="#11-2-垂直方向堆叠vstack" class="headerlink" title="11.2 垂直方向堆叠vstack()"></a>11.2 垂直方向堆叠vstack()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr2 = np.ones(shape = (<span class="number">2</span>,<span class="number">3</span>),dtype = np.<span class="built_in">int</span>)</span><br><span class="line">arr3 = np.zeros(shape = (<span class="number">3</span>,<span class="number">3</span>),dtype = np.<span class="built_in">int</span>)</span><br><span class="line">np.vstack((arr2,arr3))</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 1, 1],
       [1, 1, 1],
       [0, 0, 0],
       [0, 0, 0],
       [0, 0, 0]])
</code></pre>
<h2 id="11-3-深度方向的堆叠hstack"><a href="#11-3-深度方向的堆叠hstack" class="headerlink" title="11.3 深度方向的堆叠hstack()"></a>11.3 深度方向的堆叠hstack()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">red = np.arange(<span class="number">0</span>,<span class="number">9</span>)</span><br><span class="line">red</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 1, 2, 3, 4, 5, 6, 7, 8])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">green = np.arange(<span class="number">9</span>,<span class="number">18</span>)</span><br><span class="line">green</span><br></pre></td></tr></table></figure>




<pre><code>array([ 9, 10, 11, 12, 13, 14, 15, 16, 17])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blue = np.arange(<span class="number">18</span>,<span class="number">27</span>)</span><br><span class="line">blue</span><br></pre></td></tr></table></figure>




<pre><code>array([18, 19, 20, 21, 22, 23, 24, 25, 26])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.dstack((red,green,blue))</span><br></pre></td></tr></table></figure>




<pre><code>array([[[ 0,  9, 18],
        [ 1, 10, 19],
        [ 2, 11, 20],
        [ 3, 12, 21],
        [ 4, 13, 22],
        [ 5, 14, 23],
        [ 6, 15, 24],
        [ 7, 16, 25],
        [ 8, 17, 26]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">red2 = np.arange(<span class="number">0</span>,<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">red2</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">green2 = np.arange(<span class="number">9</span>,<span class="number">18</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">green2</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 9, 10, 11],
       [12, 13, 14],
       [15, 16, 17]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blue2 = blue = np.arange(<span class="number">18</span>,<span class="number">27</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">blue2</span><br></pre></td></tr></table></figure>




<pre><code>array([[18, 19, 20],
       [21, 22, 23],
       [24, 25, 26]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.dstack((red2,green2,blue2))</span><br></pre></td></tr></table></figure>




<pre><code>array([[[ 0,  9, 18],
        [ 1, 10, 19],
        [ 2, 11, 20]],

       [[ 3, 12, 21],
        [ 4, 13, 22],
        [ 5, 14, 23]],

       [[ 6, 15, 24],
        [ 7, 16, 25],
        [ 8, 17, 26]]])
</code></pre>
<img src="%E4%B8%8B%E8%BD%BD%20(4).png" alt="下载 (4)" style="zoom:50%;">

<h2 id="11-4-列堆叠与行堆叠"><a href="#11-4-列堆叠与行堆叠" class="headerlink" title="11.4 列堆叠与行堆叠"></a>11.4 列堆叠与行堆叠</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">one = np.arange(<span class="number">3</span>)</span><br><span class="line">one</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">two = <span class="number">2</span>*one</span><br><span class="line">two</span><br></pre></td></tr></table></figure>




<pre><code>array([0, 2, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.column_stack((one,two))</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 0],
       [1, 2],
       [2, 4]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ones = np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">ones</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">twices = <span class="number">2</span>*ones</span><br><span class="line">twices</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  2,  4],
       [ 6,  8, 10],
       [12, 14, 16]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.column_stack((ones,twices))  <span class="comment"># 对于二维数组，np.column_stack()的效果等价于np.hstack()</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  1,  2,  0,  2,  4],
       [ 3,  4,  5,  6,  8, 10],
       [ 6,  7,  8, 12, 14, 16]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.row_stack((one,two))</span><br></pre></td></tr></table></figure>




<pre><code>array([[0, 1, 2],
       [0, 2, 4]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.row_stack((ones,twices))  <span class="comment"># 对于二维数组，row_stack()的效果也等价于vstack()</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  1,  2],
       [ 3,  4,  5],
       [ 6,  7,  8],
       [ 0,  2,  4],
       [ 6,  8, 10],
       [12, 14, 16]])
</code></pre>
<h2 id="11-5-数组的分割操作"><a href="#11-5-数组的分割操作" class="headerlink" title="11.5 数组的分割操作"></a>11.5 数组的分割操作</h2><p><img src="%E4%B8%8B%E8%BD%BD%20(5).png" alt="下载 (5)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array1 = np.arange(<span class="number">16.0</span>).reshape(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">array1</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0.,  1.,  2.,  3.],
       [ 4.,  5.,  6.,  7.],
       [ 8.,  9., 10., 11.],
       [12., 13., 14., 15.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.hsplit(array1,<span class="number">2</span>)   <span class="comment"># 水平分割为两个部分</span></span><br></pre></td></tr></table></figure>




<pre><code>[array([[ 0.,  1.],
        [ 4.,  5.],
        [ 8.,  9.],
        [12., 13.]]),
 array([[ 2.,  3.],
        [ 6.,  7.],
        [10., 11.],
        [14., 15.]])]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">list_arr = np.hsplit(array1,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">type</span>(list_arr)</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<pre><code>list
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(list_arr)</span><br></pre></td></tr></table></figure>




<pre><code>2
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list_arr[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0.,  1.],
       [ 4.,  5.],
       [ 8.,  9.],
       [12., 13.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list_arr[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 2.,  3.],
       [ 6.,  7.],
       [10., 11.],
       [14., 15.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array2 = np.arange(<span class="number">16.0</span>).reshape(<span class="number">2</span>,<span class="number">8</span>)</span><br><span class="line">array2</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
       [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">split_arrays = np.hsplit(array2,[<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>])  <span class="comment"># [2,4,6]提供了三个分割位置，即第二列、第4列和第6列</span></span><br><span class="line">split_arrays</span><br></pre></td></tr></table></figure>




<pre><code>[array([[0., 1.],
        [8., 9.]]),
 array([[ 2.,  3.],
        [10., 11.]]),
 array([[ 4.,  5.],
        [12., 13.]]),
 array([[ 6.,  7.],
        [14., 15.]])]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.vsplit(array1,<span class="number">2</span>)  <span class="comment"># 垂直分为两部分</span></span><br></pre></td></tr></table></figure>




<pre><code>[array([[0., 1., 2., 3.],
        [4., 5., 6., 7.]]),
 array([[ 8.,  9., 10., 11.],
        [12., 13., 14., 15.]])]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array3 = np.arange(<span class="number">16.0</span>).reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">array3</span><br></pre></td></tr></table></figure>




<pre><code>array([[[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.]],

       [[ 8.,  9., 10., 11.],
        [12., 13., 14., 15.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.dsplit(array3,<span class="number">2</span>)   <span class="comment"># 将array3在深度方向等分为两部分</span></span><br></pre></td></tr></table></figure>




<pre><code>[array([[[ 0.,  1.],
         [ 4.,  5.]],
 
        [[ 8.,  9.],
         [12., 13.]]]),
 array([[[ 2.,  3.],
         [ 6.,  7.]],
 
        [[10., 11.],
         [14., 15.]]])]
</code></pre>
<img src="%E4%B8%8B%E8%BD%BD%20(6).png" alt="下载 (6)" style="zoom:50%;">

<h1 id="12-Numpy中的随机数模块"><a href="#12-Numpy中的随机数模块" class="headerlink" title="12 Numpy中的随机数模块"></a>12 Numpy中的随机数模块</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">rdm = np.random.RandomState(<span class="number">1</span>)   <span class="comment"># 定义随机种子</span></span><br><span class="line">np.random.seed(<span class="number">19680101</span>)    <span class="comment"># 定义随机种子</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成2*3的二维随机数组，随机数服从均匀分布，有几个参数就生成几维数组</span></span><br><span class="line">rand = np.random.rand(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;rand(d0,d1,...,dn):生成均匀分布的随机数\n&quot;</span>,rand)</span><br><span class="line"></span><br><span class="line">randn = np.random.randn(<span class="number">2</span>,<span class="number">3</span>)  <span class="comment"># 生成2*3的二维随机数组，随机数服从标准正态分布</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;randn(d0,d1,...,dn):生成标准正态分布的随机数\n&quot;</span>,randn)</span><br><span class="line"></span><br><span class="line">randint = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,(<span class="number">2</span>,<span class="number">3</span>))<span class="comment"># 生成2*3的1~10内的随机整数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;randint(low,high,size,dtype):生成随机整数\n&quot;</span>,randint)</span><br><span class="line"></span><br><span class="line">random = np.random.random((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;random(size):在[0,1)内生成随机数\n&quot;</span>,random)</span><br></pre></td></tr></table></figure>

<pre><code>rand(d0,d1,...,dn):生成均匀分布的随机数
 [[0.23675601 0.59353868 0.74897519]
 [0.66065819 0.8813292  0.93499822]]
randn(d0,d1,...,dn):生成标准正态分布的随机数
 [[-0.40220931 -0.43350804 -0.60667041]
 [ 0.84595394 -0.25406136 -0.97673417]]
randint(low,high,size,dtype):生成随机整数
 [[8 5 6]
 [8 6 4]]
random(size):在[0,1)内生成随机数
 [[0.23670515 0.4502083  0.20860286]
 [0.65182319 0.23997355 0.63095928]]
</code></pre>
<p>np.random的随机数函数（1）<br><img src="%E4%B8%8B%E8%BD%BD%20(7).png" alt="下载 (7)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.random.rand(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)  <span class="comment"># 三个维度</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([[[0.45018732, 0.03203868, 0.48380041, 0.22348965, 0.19974034],
        [0.9106697 , 0.97114598, 0.62637963, 0.80588029, 0.20492673],
        [0.30044863, 0.68303841, 0.08844603, 0.84622634, 0.12640675],
        [0.74493356, 0.72512066, 0.5833894 , 0.96415413, 0.88104409]],

       [[0.25648255, 0.00573882, 0.35458981, 0.18683969, 0.95193353],
        [0.15069594, 0.1697872 , 0.90220896, 0.51430006, 0.11109016],
        [0.22026206, 0.98818953, 0.89121858, 0.85166736, 0.54725869],
        [0.88330433, 0.61168909, 0.3554996 , 0.37289857, 0.01174385]],

       [[0.01909594, 0.14273833, 0.31214296, 0.97788404, 0.03571599],
        [0.50363779, 0.7153643 , 0.9875985 , 0.11002479, 0.73161782],
        [0.61692702, 0.78488445, 0.55927087, 0.95784523, 0.1650421 ],
        [0.72556161, 0.54232251, 0.91349007, 0.54510843, 0.38270292]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sn = np.random.randn(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">sn</span><br></pre></td></tr></table></figure>




<pre><code>array([[[ 2.35112544e-01, -1.33645961e+00,  6.39968791e-02,
          5.46855959e-01, -1.27491307e+00],
        [-1.12949169e-01, -1.18755814e+00,  2.02744631e+00,
         -2.52613616e-01, -4.19948531e-01],
        [ 4.15038930e-01,  1.06204009e+00, -1.06418845e+00,
         -2.13814085e-02, -6.52749039e-01],
        [ 1.74671262e-01,  9.94953131e-01,  8.35436977e-01,
          3.03890112e-02, -9.28925645e-01]],

       [[-1.34635010e+00,  5.55766689e-01,  1.06153201e+00,
          9.63539329e-01, -7.03270518e-01],
        [-2.27847748e-02, -7.63917529e-01,  6.82983240e-01,
          2.83723657e+00,  1.13891691e+00],
        [-6.23252999e-01, -3.75653087e+00, -2.20701715e-02,
         -1.22816342e+00,  4.49782510e-03],
        [ 6.34179436e-02,  1.16689748e+00,  9.12107336e-01,
          9.47848329e-01, -3.85522315e-01]],

       [[-1.25345947e+00, -2.95515840e-01,  1.37847546e+00,
          1.37281043e+00, -7.99348852e-01],
        [-4.01980222e-01, -6.61491015e-01,  4.94478176e-01,
          3.56184780e-01,  4.61888598e-02],
        [ 2.16639290e-01, -1.70358153e-01,  7.76149257e-01,
         -3.46552616e-01, -1.18954912e-01],
        [-6.70773646e-01,  2.66651406e-01,  7.46138375e-01,
          2.17222730e-03, -4.08765642e-01]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = np.random.randint(<span class="number">100</span>,<span class="number">200</span>,(<span class="number">3</span>,<span class="number">4</span>))<span class="comment"># 100-200之间的，形状是3*4的随机数数组</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>array([[180, 105, 182, 167],
       [174, 136, 109, 119],
       [194, 116, 134, 166]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">10</span>)   <span class="comment"># 设置种子，如果下次再调用种子10，生成的随机数是相同的，即什么的种子唯一对应什么样的数据</span></span><br><span class="line">np.random.randint(<span class="number">100</span>,<span class="number">200</span>,(<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure>




<pre><code>array([[109, 115, 164, 128],
       [189, 193, 129, 108],
       [173, 100, 140, 136]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line">np.random.randint(<span class="number">100</span>,<span class="number">200</span>,(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<pre><code>array([[109, 115, 164, 128],
       [189, 193, 129, 108],
       [173, 100, 140, 136]])
</code></pre>
<p>np.random的随机数函数（2）<br><img src="%E4%B8%8B%E8%BD%BD%20(8).png" alt="下载 (8)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = np.random.uniform(<span class="number">0</span>,<span class="number">10</span>,(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">u</span><br></pre></td></tr></table></figure>




<pre><code>array([[6.83818429, 8.16601835, 3.36071584, 8.90816531],
       [1.98121813, 0.30616654, 8.7761494 , 7.27435514],
       [5.40880931, 1.31458152, 4.13667374, 7.78728808]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n = np.random.normal(<span class="number">10</span>,<span class="number">5</span>,(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">n</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 3.72879043, 11.65789277,  8.45699757,  9.23132257],
       [13.39465597,  0.43222893,  5.28770972,  1.2722851 ],
       [ 4.89527367,  9.18965487,  6.7101455 , 12.11056531]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="13-数据的CSV文件存取"><a href="#13-数据的CSV文件存取" class="headerlink" title="13 数据的CSV文件存取"></a>13 数据的CSV文件存取</h1><p>CSV (逗号分隔值)，是一种常见的文件格式，用来存储批量数据<br><img src="%E4%B8%8B%E8%BD%BD%20(9).png" alt="下载 (9)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">100</span>).reshape(<span class="number">5</span>,<span class="number">20</span>)</span><br><span class="line">np.savetxt(<span class="string">&#x27;a.csv&#x27;</span>,a,fmt=<span class="string">&#x27;%d&#x27;</span>,delimiter=<span class="string">&#x27;,&#x27;</span>)  <span class="comment"># 整数格式，向CSV文件中写入数据</span></span><br></pre></td></tr></table></figure>

<p><img src="%E4%B8%8B%E8%BD%BD%20(10).png" alt="下载 (10)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.savetxt(<span class="string">&#x27;a.csv&#x27;</span>,a,fmt=<span class="string">&#x27;%.1f&#x27;</span>,delimiter=<span class="string">&#x27;,&#x27;</span>)  <span class="comment"># 浮点数格式</span></span><br></pre></td></tr></table></figure>

<p><img src="%E4%B8%8B%E8%BD%BD%20(11).png" alt="下载 (11)"></p>
<p><img src="%E4%B8%8B%E8%BD%BD%20(12).png" alt="下载 (12)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = np.loadtxt(<span class="string">&#x27;a.csv&#x27;</span>,delimiter=<span class="string">&#x27;,&#x27;</span>)  <span class="comment"># 将CSV文件中的数据读入到ndarry数组类型中</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
        13., 14., 15., 16., 17., 18., 19.],
       [20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30., 31., 32.,
        33., 34., 35., 36., 37., 38., 39.],
       [40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52.,
        53., 54., 55., 56., 57., 58., 59.],
       [60., 61., 62., 63., 64., 65., 66., 67., 68., 69., 70., 71., 72.,
        73., 74., 75., 76., 77., 78., 79.],
       [80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90., 91., 92.,
        93., 94., 95., 96., 97., 98., 99.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = np.loadtxt(<span class="string">&#x27;a.csv&#x27;</span>,dtype=np.<span class="built_in">int</span>,delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,
        16, 17, 18, 19],
       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39],
       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,
        56, 57, 58, 59],
       [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75,
        76, 77, 78, 79],
       [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95,
        96, 97, 98, 99]])
</code></pre>
<p><img src="%E4%B8%8B%E8%BD%BD%20(13).png" alt="下载 (13)"></p>
<h1 id="14-多维数组的存取"><a href="#14-多维数组的存取" class="headerlink" title="14 多维数组的存取"></a>14 多维数组的存取</h1><p><img src="%E4%B8%8B%E8%BD%BD%20(14).png" alt="下载 (14)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.arange(<span class="number">100</span>).reshape(<span class="number">5</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">a.tofile(<span class="string">&quot;b.dat&quot;</span>,sep= <span class="string">&quot;,&quot;</span>,<span class="built_in">format</span>=<span class="string">&#x27;%d&#x27;</span>)  <span class="comment"># 不包含任何维度信息，只是一维罗列)</span></span><br></pre></td></tr></table></figure>

<p><img src="%E4%B8%8B%E8%BD%BD%20(16).png" alt="下载 (16)"></p>
<h1 id="15-Numpy统计函数"><a href="#15-Numpy统计函数" class="headerlink" title="15 Numpy统计函数"></a>15 Numpy统计函数</h1><p><img src="%E4%B8%8B%E8%BD%BD%20(17).png" alt="下载 (17)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">15</span>).reshape(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.<span class="built_in">sum</span>(a)</span><br></pre></td></tr></table></figure>




<pre><code>105
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.mean(a,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>array([ 2.,  7., 12.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.mean(a,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>




<pre><code>array([5., 6., 7., 8., 9.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.average(a,axis=<span class="number">0</span>,weights=[<span class="number">10</span>,<span class="number">5</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>




<pre><code>array([2.1875, 3.1875, 4.1875, 5.1875, 6.1875])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.std(a)</span><br></pre></td></tr></table></figure>




<pre><code>4.320493798938574
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.var(a)</span><br></pre></td></tr></table></figure>




<pre><code>18.666666666666668
</code></pre>
<h1 id="16-Numpy的梯度函数"><a href="#16-Numpy的梯度函数" class="headerlink" title="16 Numpy的梯度函数"></a>16 Numpy的梯度函数</h1><p><img src="%E4%B8%8B%E8%BD%BD%20(18).png" alt="下载 (18)"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randint(<span class="number">0</span>,<span class="number">20</span>,(<span class="number">5</span>))</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>array([12,  5,  4,  7, 18])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.gradient(a)</span><br></pre></td></tr></table></figure>




<pre><code>array([-7., -4.,  1.,  7., 11.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = np.random.randint(<span class="number">0</span>,<span class="number">50</span>,(<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line">c</span><br></pre></td></tr></table></figure>




<pre><code>array([[44, 34, 40, 15, 13],
       [24, 15,  6, 21, 42],
       [22, 11, 48, 12, 28]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.gradient(c)</span><br></pre></td></tr></table></figure>




<pre><code>[array([[-20. , -19. , -34. ,   6. ,  29. ],
        [-11. , -11.5,   4. ,  -1.5,   7.5],
        [ -2. ,  -4. ,  42. ,  -9. , -14. ]]),
 array([[-10. ,  -2. ,  -9.5, -13.5,  -2. ],
        [ -9. ,  -9. ,   3. ,  18. ,  21. ],
        [-11. ,  13. ,   0.5, -10. ,  16. ]])]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">数据科学</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/python//" class="article-tag-list-link color2">python</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/02/02/Numpy%E5%BA%93%E8%AF%A6%E8%A7%A3/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-13张动图快速理解马尔科夫链、PCA、贝叶斯！" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/01/31/13%E5%BC%A0%E5%8A%A8%E5%9B%BE%E5%BF%AB%E9%80%9F%E7%90%86%E8%A7%A3%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE%E3%80%81PCA%E3%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%81/">13张动图快速理解马尔科夫链、PCA、贝叶斯！</a>
    </h1>
  

        
        <a href="/2023/01/31/13%E5%BC%A0%E5%8A%A8%E5%9B%BE%E5%BF%AB%E9%80%9F%E7%90%86%E8%A7%A3%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE%E3%80%81PCA%E3%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%81/" class="archive-article-date">
  	<time datetime="2023-01-31T04:59:17.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-01-31</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>马尔科夫链、主成分分析以及条件概率等概念，是计算机学生必学的知识点，然而理论的抽象性往往让学生很难深入地去体会和理解。而本文，将这些抽象的理论概念，<strong>用可视化的方式来解释</strong>，还可调节相应参数来改变结果，使这些抽象概念变得生动而立体！</p>
</blockquote>
<p>人类对视觉信息的记忆要远远大于文字信息。使用图表等形式的可视化，可以让抽象、难懂的概念一目了然；在此基础之上，添加可控的参数调节器，将更有助于对概念的深入学习与理解。</p>
<h1 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h1><p>马尔科夫链是指数学中具有马尔科夫性质的离散事件随机过程。在其每一步中，系统根据概率分布可以从一个状态变到另一个状态，也可以保持当前状态。状态的改变叫做转移，与不同的状态改变相关的概率叫做转移概率。</p>
<p>这概念是不是看着有点晕？没关系，我们来看下面这张图：</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131130417.gif"></p>
<p>​											2种状态的马尔科夫链</p>
<p>在状态空间中有两种状态，A和B。共有4种可能的转换。如果我们在A，接下来可以过渡到B或留在A。如果我们在B，可以过渡到A或者留在B。在这张图中，从任意状态到任意状态的转移概率是0.5。</p>
<p>当然，真正的建模工作者不会总是就画一张马尔科夫链图。<strong>相反，他们会使用“转移矩阵”来计算转移概率。</strong>状态空间中的每个状态都会出现在表格中的一列或者一行中。矩阵中的每个单元格都告诉你从行状态转换到列状态的概率。因此，在矩阵中，单元格做的工作和图中的箭头所示是一样。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131130744.gif"></p>
<p>如果状态空间添加了一个状态，我们将添加一行和一列，向每个现有的列和行添加一个单元格。<strong>这意味着当我们向马尔可夫链添加状态时，单元格的数量会呈二次方增长。</strong>因此，转换矩阵就起到了很大的作用（除非你想把法尔科夫链图画的跟丛林一样）。</p>
<p>马尔科夫链的一个作用是用计算机模拟现实世界中的现象。例如，可以用来检测一个新建的水坝溢流的频率（取决于连续下雨的天数）。为建立这个模型，可以从下面的雨天（R）和晴天（S）开始：</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131130819.gif"></p>
<p>表述这种模拟天气的方法就是：“有一半的天数是下雨天。所以模拟中的每一天都有50%的概率是下雨的。”这个规则在模拟中所产生的序列如下：</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131130846.gif"></p>
<p>你注意到上面的序列和原来的不太一样了吗?第二个序列似乎具有跳跃性，而第一个(真实数据)似乎具有“粘性”。在真实的数据中，如果某一天是晴天，那么第二天也很可能是晴天。</p>
<p>可以通过两个状态的马尔可夫链来消除这种“粘性”。当马尔科夫链处于状态“R”时，它保持在该状态的概率是0.9，状态改变的概率是0.1。同样，“S”状态保持不变的概率是0.9，过渡到“R”状态的概率是0.1。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131130910.gif"></p>
<p>在许多需要对大规模的现象做研究的工作人员手中，马尔科夫链的作用可以变得非常强大。例如，谷歌用于确定搜索结果顺序的算法，称为PageRank，就是一种马尔可夫链。</p>
<h1 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h1><p>主成分分析，是一种统计方法。<strong>通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分。</strong>PCA是最重要的降维方法之一,在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。</p>
<ul>
<li><strong>2D示例</strong></li>
</ul>
<p>首先，只考虑两个维度的数据集，比如高度和重量。这个数据集可以绘制成平面上的点。但如果想要整理出变量，PCA会找到一个新的坐标系，其中每个点都有一个新的(x,y)值。坐标轴实际上没有任何物理意义。它们是高度和重量的组合，被称为“主分量”。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131131015.gif"></p>
<p>​					拖动原始数据集中的点，可以看到PC坐标系统正在调整</p>
<p>PCA对于降维很有用。下面，我们将数据绘制成两条直线:一条由x值组成，另一条由y值组成。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131131046.gif"></p>
<p>但是，如果我们只打算沿一个维度查看数据，那么将该维度作为具有最大变化的主成分可能会更好。通过减少PC2，不会造成太大损失，因为它对数据集的变化贡献最小。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131131109.gif"></p>
<ul>
<li><strong>3D示例</strong></li>
</ul>
<p>看透一个数据云是非常困难的，因此，在3D空间中，PCA显得更为重要。在下面的示例中，原始数据以3D的形式绘制，但可以通过不同的视角，将其投射到2D空间。确定好角度之后，点击“显示PCA”按钮，即可呈现2D的结果。在本例中，PCA变换确保水平轴PC1的变化量最大，垂直轴PC2的变化量次之，第三轴PC3的变化量最少。显然，PC3是丢弃的。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131131136.gif"></p>
<ul>
<li><strong>应用：吃喝在英国</strong></li>
</ul>
<p>如果数据集不仅仅是三维的，而是17个维度的呢？！如下表所示：</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131131230.jpg"></p>
<p>表中是英国每个地区平均每人每周17种食物的消费量，单位为克。这张表显示了不同食物类型之间存在的一些有趣的差异，但总体差异并不显著。让我们看看PCA是否可以通过降维来强地区家之间的差异。</p>
<p>下图是第一个主成分的数据图。我们可以看到一些有关北爱尔兰的情况已经发生了变化。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131131259.png"></p>
<p>现在，看看第一和第二主成分，可以看到北爱尔兰是一个主要的异常值。一旦回过头来看看表格中的数据，这就显得很有道理了:北爱尔兰人吃的新鲜土豆要很多，吃的新鲜水果、奶酪、鱼和酒精饮料较少。这是一个很好的迹象，我们所看到的结构反映了现实世界地理的一个重要事实北爱尔兰是四个国家中唯一一个不在大不列颠岛上的。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131131321.jpg"></p>
<h1 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h1><p><strong>条件概率是指一个事件在另外一个事件已经发生条件下的发生概率。</strong>一个落下来的球可能落在红色的架子上(称之为A事件)，或者落在蓝色架子上(称之为B事件)，或者两者兼而有之。</p>
<p>那么给定一个球，它击中了红色架子（A事件），而后击中蓝色架子（B事件）的概率会是多少呢？可以通过给定A的条件概率，即P（B | A）来回答这个问题。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131131403.png"></p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131131421.gif"></p>
<p>将抽象、难懂的计算机概念，以可视化的形式展现出来，可以帮助学生、研究者更好的理解；甚至可以帮助教师们提高教学质量。</p>
<p>无论如何，希望读者们能从本文中得到或多或少的帮助。</p>
<p>当然还有一些其他的抽象概念的可视化，读者们可访问下方链接地址查看：</p>
<p><a target="_blank" rel="noopener" href="http://setosa.io/ev/">http://setosa.io/ev/</a></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">常用算法</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/机器学习与深度学习//" class="article-tag-list-link color5">机器学习与深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/01/31/13%E5%BC%A0%E5%8A%A8%E5%9B%BE%E5%BF%AB%E9%80%9F%E7%90%86%E8%A7%A3%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE%E3%80%81PCA%E3%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%81/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-数据仓库技术落地现状和趋势总结" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/01/31/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%8A%80%E6%9C%AF%E8%90%BD%E5%9C%B0%E7%8E%B0%E7%8A%B6%E5%92%8C%E8%B6%8B%E5%8A%BF%E6%80%BB%E7%BB%93/">数据仓库技术落地现状和趋势总结</a>
    </h1>
  

        
        <a href="/2023/01/31/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%8A%80%E6%9C%AF%E8%90%BD%E5%9C%B0%E7%8E%B0%E7%8A%B6%E5%92%8C%E8%B6%8B%E5%8A%BF%E6%80%BB%E7%BB%93/" class="archive-article-date">
  	<time datetime="2023-01-31T04:43:14.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-01-31</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>数据仓库是大数据技术的核心模型，其发展历程也见证了数据智能从关系型向非关系型、从结构化到非结构化、从分布式到中心化、从零散化到标准化、从显式分析到智能分析的演化走向。如今，业界流行的各种新兴概念，包括数据中台、数据湖、流批一体等，都是基于数据仓库提出的优化方向。那么，<strong>数据仓库目前在业界的发展具备什么特点？如何从数据仓库的优化来理解这些新兴概念？</strong></p>
<p>本文转载自数据智能专家访谈 01期</p>
</blockquote>
<h1 id="01-标准化"><a href="#01-标准化" class="headerlink" title="01 标准化"></a>01 标准化</h1><h2 id="1-数据治理"><a href="#1-数据治理" class="headerlink" title="1. 数据治理"></a>1. 数据治理</h2><p>数据仓库的标准化主要指的是数据治理。数据治理是数仓落地应用的核心问题，在近年受到越来越多的关注，其根本上是为了解决数据仓库烟囱式开发带来的资源浪费等问题。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131124627.png"></p>
<p>例如上图所示，企业发展初期，因为业务模式不稳定，多个业务线都有独立研发的技术栈，到后期就会出现标准不统一、重复计算、模型依赖关系混乱的问题。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131124657.png"></p>
<p>而经过标准统一、底层逻辑屏蔽和不同粒度的汇总，各个业务线的技术栈得以统一，就能大大简化模型计算链路、降低成本、提高速度。</p>
<p>也可以从数据建模的角度来理解这个问题。数据建模是数仓建设的核心环节之一，包括自上而下（范式建模，Inmon模式）和自下而上（维度建模，Kimball模式）两种建模方式。</p>
<p>范式建模主要在电信、金融、政务、工业等传统行业用的比较多，而互联网由于业务变化很快，初期需要更加灵活的分布式决策结构，因此维度建模会更加合适，但也因此基于Kimball模式建模的后期会出现数据孤岛问题和治理需求。不过，专家表示，维度建模在前期若有指导思想或方法论的话，或许能提前避免这个问题。</p>
<p>在业界，数仓治理最核心的工作是改善数据质量，数据的完整性、一致性等指标都会影响最终的数据决策的好坏，这对于整个行业都是一个挑战。因为数据质量仅仅通过简单的唯一性校验、波动性校验等手段是很难排查出业务波动的根本原因的。</p>
<p>而除了人为制定规则以外，如今也有不少企业正在尝试引入AI算法对质量监控进行预测，目前技术上尚未成熟，但AI的潜力值得期待。</p>
<p>DataOPs是近期比较热的概念，很大程度上也是围绕数据质量的工作，但其本质和数据治理相差不远，关注数据生产的标准化、流程化、自动化、智能化，也就是将越来越多大数据技术环节中的人工工作自动化，也会开始结合AI技术，涉及开发和运维过程。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131124703.png"></p>
<p>上图展示了一个数仓开发的工作流程，包括模型检索、模型创建、ETL开发、作业发布、监控报警等全流程的标准化、自动化都是DataOPs关心的问题。</p>
<p>提到标准化和数据治理，又不得不提到数据中台。目前业界对于数据中台都有不同的解释，有专家对DataFun表示，数据仓库和数据中台其实是相对的概念。</p>
<p>实际上，中小企业通常只限于搭建数据仓库，很多中小企业声称的数据中台其实也是局限于某个业务的数据仓库，不是真正的数据中台。</p>
<p>真正的数据中台主要在大企业，其中每个部门都拥有一个数仓体系，那么每个部门相当于一个小型公司。而在业务发展中后期会出现数据一致性、数据标准等方面的困难，因而就产生数据治理和建设中台的需求。这也意味着，数据治理通常只在大企业中有足够的需求。</p>
<h2 id="2-流批一体"><a href="#2-流批一体" class="headerlink" title="2. 流批一体"></a>2. 流批一体</h2><p>数据治理在标准化方面的一大特点是将并行的业务线进行合并。实际上，这种合并统一的趋势不仅存在于业务逻辑和数据层面，其根本上还存在于存储、计算、处理等底层逻辑中。存储、计算、处理的两种基本的开发模式是离线计算和实时计算，因此数仓标准化的另一个方向是流批一体。</p>
<p>流批一体除了解决多条技术栈之间的标准不统一之外，还有一大好处就在于成本层面。在发展到一定阶段的时候，离线数仓通常已经无法满足业务需求了。而实时数仓对于下游的成本比较高，普惠性不足。</p>
<p>根据一些分析结果，批计算的成本和数据体量大致呈线性关系，而流计算的成本却随着数据体量的增长而呈指数级增长，背后原因包括随机IO、存算不分离、写放大等。因而实时计算一般不直接面向业务，更多面向算法或数据工具。</p>
<p>另外，流批一体能够实现状态复用，很多时候这是有必要的，因为离线计算在取数的时候，经常会遇到数据有效期不足的问题，而复用实时计算的结果就能很好地解决这个问题。</p>
<p>总体而言，流批一体架构的好处是解决流批不统一带来的数据不一致、开发成本、使用成本、运维成本问题。</p>
<p>人们一般默认流批一体的解决方案是Kappa架构，采用Kafka和Flink也就是消息队列和实时计算引擎的组合。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131124813.png"></p>
<p>但Kappa架构严重依赖消息队列的顺序处理，而在顺序存储上进行OLAP分析是比较困难的。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131124840.png"></p>
<p>因此在业界，许多企业开始探索通过数据湖方案实现流批一体，比如Iceberg支持读写分离，又支持并发读、增量读、小文件合并，还可以支持秒级到分钟级的延迟，因此可以实现近实时数据接入。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131124901.png"></p>
<p>同时，Iceberg底层依赖列式存储，用于替换Kafka后就可以对OLAP分析进行基本的优化，在中间层直接用Flink执行批式计算或流式计算。最后，再结合Alluxio的缓存能力，就可以对近实时的数据湖架构进一步加速。</p>
<p>尽管如此，目前在业界，无论是流批一体还是数据湖，其技术发展都还存在很大挑战。据专家反馈，业界的方案基本都局限于部分场景，距离通用方案还很遥远。</p>
<p>流批一体的含义包含了多个层次，首先是存储流批一体，其次是计算流批一体，最后是处理结果的流批一体，也就是让同一段代码在分别做批处理和流处理时，得到的结果是一致的，而这通常也是最难的。</p>
<h1 id="02-实时处理"><a href="#02-实时处理" class="headerlink" title="02 实时处理"></a>02 实时处理</h1><h2 id="1-实时查询"><a href="#1-实时查询" class="headerlink" title="1. 实时查询"></a>1. 实时查询</h2><p>流批一体技术的需求自然来源于实时计算的发展。如今越来越多的服务面向ToC用户，实时性需求越来越强，这些业务包括了风控事件处理，搜广推的实时特征计算，以及指标监控等等，实时数仓的开发也愈发受到企业的重视和投入。</p>
<p>目前而言，业界最关心的数据仓库核心性能指标是查询的实时性。性能指标设置对于业务成长非常重要，背后的考虑因素有两个，一个是性能本身会导致数据产出的延迟，另一个是性能差一般也代表着资源消耗大。</p>
<p>提高数据处理实时性的解决方案类型主要有两种，包括：数据和业务逻辑优化（主要指数据治理）、底层计算引擎优化。</p>
<p>其中，底层计算引擎的优化也是大企业比较常用的方法，常用的选型包括Spark、Flink、Blink等。</p>
<p>但严格来说，对于大企业而言，一般不存在选型的概念。专家表示，因为大企业一般都有成熟的大数据平台，里面包括了采集、模型设计等模块，经过优化和协同，这些组件都已经封装成了一套完整的体系。</p>
<p>但对于中小企业来说，他们一般很难抉择如何做具体的选型，一般都是考虑模仿大企业的架构，或者直接购买大企业的平台产品。</p>
<h2 id="2-流式ETL"><a href="#2-流式ETL" class="headerlink" title="2. 流式ETL"></a>2. 流式ETL</h2><p>除了查询以外，数仓中另一个消耗资源较大的流程是ETL。在业界，数仓比较常用的ETL模式是增量ETL和全量ETL。</p>
<p>数仓ETL通常面临的核心挑战是高效实施，也就是如何用最低资源产出最多成果，另一个是数据质量。</p>
<p>除了增量ETL、全量ETL之外，还有一种ETL的模式是流式ETL，自然也是源于实时计算的业务需求，据专家介绍，目前在业界的成熟度还比较低。</p>
<h1 id="03-模块化"><a href="#03-模块化" class="headerlink" title="03 模块化"></a>03 模块化</h1><h2 id="1-存算分离"><a href="#1-存算分离" class="headerlink" title="1. 存算分离"></a>1. 存算分离</h2><p>模块化与标准化是相辅相成的，有标准化则必有模块化。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131125037.png"></p>
<p>以数据治理为例，不同的业务线得以统一依赖的一个基本规律是，每个业务技术栈发展到中后期的时候，基本都能划分成标准化流程集合的不同组合，本质上与中台或微服务背后的思想是一致的。</p>
<p>流批一体也类似，由于目前的大数据架构都实现了存算分离，因此也可以分成计算和存储两大类。</p>
<p>相对于计算的流批一体，存储的流批一体已经比较成熟，比如Hive表流批查询一体，既可以查到离线的数据，也可以查到实时分钟级的数据。</p>
<p>而计算流批一体仍有较大挑战性，目前行业内也是局限于局部应用，还无法全场景应用。</p>
<p>计算流批一体有两种实现方式，其一是基于离线计算的流批一体方案，比如支撑阿里双十一活动的数仓架构，其二是基于流式计算的流批一体方案，比如支撑字节信息流广告或抖音视频推荐的数仓架构。</p>
<p>相比而言，基于流式计算的流批一体方案更好实现。主要是因为流式计算的体系尚未固化，还有很大的可改造和优化空间。而离线数仓体系已经比较固化，一旦涉及改造，成本就非常高了，包括人力成本、改造成本等等。</p>
<p>计算和存储是实现所有业务逻辑的核心，也是性能优化的根本。计算方面关心的性能指标主要包括查询速度、计算成本、高可用等，存储方面主要包括存储成本、数据读取等。</p>
<p>当然，这里提到存储方向比较成熟的主要考虑因素是成本。相对于计算成本，技术的进步使得存储成本要低得多。</p>
<p>但另一方面，计算和存储也是相对的概念，存储属于空间换时间（比如预计算），计算属于时间换空间（比如高压缩），在实际业务中，通常两者之间需要互相权衡和协同才可以兼顾性能和成本。</p>
<h2 id="2-数据建模"><a href="#2-数据建模" class="headerlink" title="2. 数据建模"></a>2. 数据建模</h2><p>在模块化方面，数据建模遇到的问题在于还不能很好地实现模块化。其主要挑战之一是数据域划分与业务域划分的良好匹配，由于数据域和业务域会有不同步的变化过程，也会用不同的建模方式，因此快速的适应和匹配就成为难题。</p>
<p>而业务域的合理数据划分本身也是一个难题，很多企业一般都局限于数据源数据的划分，比如财务域、用户等，而没有直接针对业务进行数据划分，比如营销域、直播域、风控域等，缺少成熟的方法论指导。</p>
<h2 id="3-OLAP"><a href="#3-OLAP" class="headerlink" title="3. OLAP"></a>3. OLAP</h2><p>在模块化方面，OLAP的一个比较重要的发展趋势是自助分析，也就是屏蔽底层逻辑的产品化趋势。</p>
<p>自助分析针对实时和离线场景的计算语义区别，可以通过参数化的方式，使系统可以自动判断场景是实时的还是离线的。甚至可以是选型透明的，可以基于不同的性能需求自动转换选型的加载。</p>
<p>模块化的一大好处是实现技术的快速迭代，以及快速地适应业务需求的变化，即敏捷OLAP，这也是OLAP在业界面临的一大挑战。</p>
<h1 id="04-整体衡量"><a href="#04-整体衡量" class="headerlink" title="04 整体衡量"></a>04 整体衡量</h1><p>以上我们相继讨论过实时查询、计算成本等指标，但实际上，这些都不能作为判断一个数仓好坏的标准。</p>
<p>专家表示，对数据模型的优劣判断（比如数据的业务覆盖率、数据的业务使用率等），目前行业内还缺乏统一的、成熟的衡量指标。而数据模型是数仓的核心，其优劣判断关系到数仓整体能力的判断，重要性很高。</p>
<h1 id="05-总结"><a href="#05-总结" class="headerlink" title="05 总结"></a>05 总结</h1><p>通过以上分析可知，除了标准化、模块化、实时处理、整体衡量等发展特点以外，数据仓库也还面临许多整体层面的挑战，包括解决方案通用性、整体衡量标准等。</p>
<p>目前业界正在投入数据编织、DataOPs等方向，使得数据仓库在标准化、模块化等方面走的更远，并实现更强的通用性，从而实至名归地演化成数据中台、数据湖等架构，适应数据智能业务不断增长的规模化、多样化、产品化需求。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247523245&idx=1&sn=41a903dbd4a290ed3ceb2ec194698cff&scene=21#wechat_redirect">腾讯全场景实时数仓建设实践</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247511422&idx=1&sn=b5aa158ce673a045553435f78bd2342f&scene=21#wechat_redirect">菜鸟实时数仓2.0进阶之路</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247561076&idx=1&sn=e97752f39ea4f6b96198444980c6e902&scene=21#wechat_redirect">汤楚熙：美团实时数仓架构演进与建设实践</a></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">数据仓库</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/项目实战//" class="article-tag-list-link color5">项目实战</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/01/31/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%8A%80%E6%9C%AF%E8%90%BD%E5%9C%B0%E7%8E%B0%E7%8A%B6%E5%92%8C%E8%B6%8B%E5%8A%BF%E6%80%BB%E7%BB%93/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-推荐系统难点解读" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/01/31/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%9A%BE%E7%82%B9%E8%A7%A3%E8%AF%BB/">推荐系统难点解读</a>
    </h1>
  

        
        <a href="/2023/01/31/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%9A%BE%E7%82%B9%E8%A7%A3%E8%AF%BB/" class="archive-article-date">
  	<time datetime="2023-01-31T02:32:30.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-01-31</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>本文为推荐系统在落地过程当中的一些难点问题，转载自“数据智能专家访谈 06期”</p>
</blockquote>
<h1 id="01-推荐系统的技术架构"><a href="#01-推荐系统的技术架构" class="headerlink" title="01 推荐系统的技术架构"></a>01 推荐系统的技术架构</h1><p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131103409.png"></p>
<p>推荐系统模块一般如上图所示，先通过召回模块，将候选集召回，然后经过粗排、精排、重排等排序方式，将排序靠前的候选集推送给用户。</p>
<h1 id="02-数据源"><a href="#02-数据源" class="headerlink" title="02 数据源"></a>02 数据源</h1><h2 id="1-埋点"><a href="#1-埋点" class="headerlink" title="1. 埋点"></a>1. 埋点</h2><p>埋点不难，用埋点的数据构造样本比较难，特别是实时社交比如直播。</p>
<p><strong>理想的样本：</strong></p>
<ul>
<li>用用户id把用户所有行为串起来；</li>
<li>可以回溯用户过去看直播间的行为，比如评论、打赏等。</li>
</ul>
<p><strong>实际上面临的困难：</strong></p>
<ul>
<li>埋点的数据不准确，是脏数据；</li>
<li>埋点数据排查困难：前端代码工程复杂，很容易出问题。但是前端的同学的主要工作也不是数据上报，所以数据出了问题，也不会实时排查，非常容易导致数据脏；</li>
<li>回溯模型也很复杂；</li>
<li>非常耗资源。</li>
</ul>
<p>综上，将用户对应的行为，拼成样本，需要花费很多精力。</p>
<h2 id="2-用户画像"><a href="#2-用户画像" class="headerlink" title="2. 用户画像"></a>2. 用户画像</h2><p>包括用户的基础画像和兴趣画像。兴趣画像来源于两个部分：用户的离线画像、用户的实时画像。其中，离线画像又分为长期离线画像、中期离线画像、短期离线画像。</p>
<h2 id="3-内容结构化"><a href="#3-内容结构化" class="headerlink" title="3. 内容结构化"></a>3. 内容结构化</h2><p>根据内容信息的不同，内容结构化方式不同，比如电商领域，内容为商品，商品的结构化信息包括分类、品牌、价格、规格等。</p>
<p><strong>多模态要是应用到推荐系统来，是个难点。</strong></p>
<ul>
<li>耗费资源：图片转化为向量，信息量太大，计算起来太耗资源。</li>
<li>现有的电商算法大多基于行为做描述，而多模态从内容上对商品做描述，怎么结合到一起，需要考虑。</li>
</ul>
<p>所以目前多模态性价比不高，讨论较多，但是用的较少。</p>
<h1 id="03-特征"><a href="#03-特征" class="headerlink" title="03 特征"></a>03 特征</h1><p>特征工程，将结构化的信息转换成模型支持的数据格式。</p>
<p>特征选取的优劣，会最终影响到用户体验。所以特征工程及特征组合的自动化，一直是推动实用化推荐系统技术演进最主要的方向之一。</p>
<h2 id="1-特征内容"><a href="#1-特征内容" class="headerlink" title="1. 特征内容"></a>1. 特征内容</h2><p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131122439.png"></p>
<h2 id="2-特征生成"><a href="#2-特征生成" class="headerlink" title="2. 特征生成"></a>2. 特征生成</h2><p><strong>（1）特征生成过程有什么难点？</strong></p>
<ul>
<li>样本拼接：在特征生成过程中，样本拼接也比较难。</li>
<li>一些脏数据的识别。如2.1埋点所说，埋点的数据很容易出问题，数据清洗和处理非常耗精力。</li>
</ul>
<p><strong>（2）有什么热门的特征提取方式？</strong></p>
<p>用Embedding（可以理解为稠密向量）进行特征交叉。</p>
<p><strong>（3）特征工程的趋势：</strong></p>
<ul>
<li>序列特征：用户历史的行为、浏览行为、点击行为，过去看的直播间、视频，前提就是比较基础的特征做好了</li>
<li>上下文特征</li>
<li>Embedding</li>
</ul>
<p><strong>（4）用在召回的特征提取，和用在排序的特征提取，有什么不同？</strong></p>
<p>① 特征有差别：</p>
<ul>
<li>召回模型大多是双塔模型，用户、商品用双塔模型召回，没有交叉特征</li>
<li>精排需要交叉特征，比如用户和物品的交叉、属性的交叉等</li>
</ul>
<p>② 样本有差异：</p>
<ul>
<li>召回面向全量</li>
<li>精排面对的是召回后的候选集</li>
</ul>
<p>③ 做召回的时候，要考虑精准性和效率，精排要用到所有考虑到的特征。所以召回特征是精排特征的子集。</p>
<p><strong>（5）特征抽取：</strong></p>
<ul>
<li>特征需要结合业务场景去抽取特征，每个场景涉及的都不一样。要涉及到对推荐场景有一个很深的认知，才能抽到好的特征。因为每个场景输入的维度不同。</li>
<li>推荐涉及人货场三个方面的特征。有了基础特征之后，就做特征交叉，人货场中任意两三者去做交叉。</li>
<li>目前专家所在的大厂某业务，是一个大模型，所有的行业的输入都是同源的。专家认为这是不合理的，所以他认为趋势是，分行业去挖掘特征，每个行业做小的特征，而不是所有行业用一套特征。</li>
</ul>
<h1 id="04-召回"><a href="#04-召回" class="headerlink" title="04 召回"></a>04 召回</h1><p>从全量信息集合中触发尽可能多的正确结果，并将结果返回给排序模块。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131122629.png"></p>
<p>数据决定模型的上限。召回决定了推荐的上限，因为精排的候选集是召回出来的。</p>
<h2 id="1-召回的要点"><a href="#1-召回的要点" class="headerlink" title="1. 召回的要点"></a>1. 召回的要点</h2><ul>
<li>处理数据量大</li>
<li>速度要够快</li>
<li>模型不能太复杂</li>
<li>使用较少特征</li>
</ul>
<h2 id="2-召回的难点"><a href="#2-召回的难点" class="headerlink" title="2. 召回的难点"></a>2. 召回的难点</h2><ul>
<li>召回怎么样和后链路做一个耦合的学习。会有一种情况，排序很适合之前召回算法的商品，排序的非常好，换了召回算法，出一批新的商品，排序算法就不一定排的很好。</li>
<li>评估离线指标和线上指标的一致性：这是基础工作，因为离线评估指标涨了，线上不一定也涨了。指标主要看Hit rate。</li>
</ul>
<p><em>注：Hit rate,在top-K推荐中，HR是一种常用的衡量召回率的指标.分母是所有的测试集合，分子是每个用户top-K推荐列表中属于测试集合的个数的总和。</em></p>
<p><em>举例：三个用户在测试集中的商品个数分别是10，12，8，模型得到的top-10推荐列表中，分别有6个，5个，4个在测试集中，那么此时HR的值是 (6+5+4)&#x2F;(10+12+8) &#x3D; 0.5。</em></p>
<h2 id="3-哪一种召回方式用的多？"><a href="#3-哪一种召回方式用的多？" class="headerlink" title="3. 哪一种召回方式用的多？"></a>3. 哪一种召回方式用的多？</h2><ul>
<li>召回的方式特别多，而且每种类型不一样，差异特别大，不同的召回方式数据集差异也比较大。</li>
<li>双塔用的最多，双塔包含很多种双塔模型，是成熟期了。</li>
<li>图神经网络不能用双塔模型。</li>
<li>一般有几十种召回算法同时在用，多路一起召回，包括双塔、ebadding、专家策略、知识图谱召回（用的少，其他厂用的多）、图上的传统召回、知识召回、表示召回、匹配召回几种都用。</li>
<li>专家策略效果也可以，只是可能没有那么多，而且每一步都有策略，不像双塔训练好就行。</li>
</ul>
<h2 id="4-召回的趋势和新算法有哪些？"><a href="#4-召回的趋势和新算法有哪些？" class="headerlink" title="4. 召回的趋势和新算法有哪些？"></a>4. 召回的趋势和新算法有哪些？</h2><ul>
<li>图神经网络召回；很有前景的值得探索的方向，信息在图中的传播性，所以对于推荐的冷启动以及数据稀疏场景应该特别有用。</li>
<li>知识图谱召回：知识图谱有一个独有的优势和价值，那就是对于推荐结果的可解释性。</li>
<li>因果推断。</li>
</ul>
<h2 id="5-因果推断算不算召回的新算法？召回是怎么用因果推断的。"><a href="#5-因果推断算不算召回的新算法？召回是怎么用因果推断的。" class="headerlink" title="5. 因果推断算不算召回的新算法？召回是怎么用因果推断的。"></a>5. 因果推断算不算召回的新算法？召回是怎么用因果推断的。</h2><ul>
<li>因果推断实现方式：在深度学习加一些embedding，对因果关系做一些建模。</li>
<li>因果推断是一个理念，在召回中容易给热门内容打高分，形成马太效应，因果推断的理念指排除掉因为马太效应出现，而是因为相关性被召回。这是一个比较大的领域，最近研究的人比较多，是一个热点。</li>
<li>在精排里试效果一般。</li>
</ul>
<h2 id="6-在做召回时，主要考虑的因素和性能指标有哪些？"><a href="#6-在做召回时，主要考虑的因素和性能指标有哪些？" class="headerlink" title="6. 在做召回时，主要考虑的因素和性能指标有哪些？"></a>6. 在做召回时，主要考虑的因素和性能指标有哪些？</h2><ul>
<li>每一路召回算法，在后面精排曝光的占比。</li>
<li>快、相应速度快，能在全量物品库找用户喜欢的东西。</li>
<li>每一路召回算法的点击率也是看的。毕竟所有的优化都是为了线上提效，所以一般看线上的指标。点击率是最明显指标的指标。</li>
<li>有时候也看用户转化（是否电话联系）。</li>
<li>算法是否上线，也要结合线上的指标看。</li>
</ul>
<h1 id="05-排序"><a href="#05-排序" class="headerlink" title="05 排序"></a>05 排序</h1><p>根据提前设定的目标，对信息进行打分，使评分高的信息优先展示给用户。排序环节是推荐系统最关键，也是最具有技术含量的部分，目前大多数推荐技术其实都聚焦在这块。</p>
<h2 id="1-排序算法"><a href="#1-排序算法" class="headerlink" title="1. 排序算法"></a>1. 排序算法</h2><p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131123116.png"></p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131123138.png"></p>
<p><strong>（1）粗排</strong></p>
<ul>
<li>粗排输出的结果要给精排用，粗排的打分商品多，比如粗排打两万个，精排打top5k个，粗排打分空间更大。</li>
<li>粗排的样本选取和精排不能用同样的样本。粗排是所有商品，精排是在粗排的结果中选择样本。</li>
<li>粗排的要求是高效输出，一般也是双塔，因为要快，不能用太复杂的模型，都是基于一些简单的策略，截断topN，给到精排。</li>
</ul>
<p><strong>（2）精排</strong></p>
<ul>
<li>精排更集中于top商品，用有曝光的样本去训练。优中选优。</li>
<li>难点是在特征工程做的很好的情况下，设计模型结构，得到更好的结果。</li>
<li>大部分公司用的阿里巴巴提供的din，一个开源的算法包。</li>
</ul>
<h2 id="2-多目标优化"><a href="#2-多目标优化" class="headerlink" title="2. 多目标优化"></a>2. 多目标优化</h2><p>多目标优化最关键的有两个问题。第一个问题是多个优化目标的模型结构问题；第二个问题是不同优化目标的重要性如何界定的问题。</p>
<p>如何设定不同目标权重，能够尽量减少相互之间的负面影响，就非常重要。这块貌似目前并没有特别简单实用的方案，<strong>很多实际做法做起来还是根据经验拍一些权重参数上线AB测试</strong>，费时费力。</p>
<p>而如何用模型自动寻找最优权重参数组合就是一个非常有价值的方向，<strong>目前最常用的方式是采用帕累托最优的方案来进行权重组合寻优。</strong></p>
<p>（1）精排的多目标优化用的比较多，比如总的目标是成交gmv，就会分成点击率和转化率两个目标。</p>
<p>（2）多目标训练的好处：</p>
<ul>
<li>点击率和转化率如果分开训练，打分就会有延时，消耗的计算资源也会更大。</li>
<li>还有一个好处，多个目标可以相互借鉴，特别是数据量稀疏的情况下。</li>
</ul>
<p>（3）至于多目标优化，也有一些自动化的方式调权重，但是一般是人工拍。拍很多组权重，不同组权重的模型，在同一份测试集上出效果。</p>
<p>（4）多目标优化一般用PLE(Progressive Layered Extraction)，腾讯CGC出的模型，一直没有被超越。新出的目标关系之间的建模，db-mtl，esmm等，都不如PLE。</p>
<p>（5）经典的还有Mmoe。</p>
<h2 id="3-多模态融合"><a href="#3-多模态融合" class="headerlink" title="3. 多模态融合"></a>3. 多模态融合</h2><p>在对专家的访谈中，发现业界对多模态的定义有两种：</p>
<ul>
<li>推荐的内容同时有多种形式，比如文字、图片、视频等。</li>
<li>推荐的内容同时有多种业务线，比如新房、二手房、租房等。</li>
</ul>
<p>多模态，比较让人头疼。</p>
<p>比如首页推荐，内容包括帖子、视频，排序的时候怎么排，很难用统一的模型，因为帖子、视频分属于不同的业务线，很多特征在这条业务线上有，其他业务线上没有。所以没有好的统一的召回模型和统一的精排模型，只能偏人工策略。</p>
<p>某大厂采用的方法是，先算首页有多少个坑位，基于流量价值和用户喜欢哪条业务线，人工给权重，分给每个业务线多少个坑位，再将业务线中的商品按照推荐算出来的排序填充坑位。</p>
<h2 id="4-重排序"><a href="#4-重排序" class="headerlink" title="4. 重排序"></a>4. 重排序</h2><p>根据用户最终的使用体验及运营需求，进行排序结果的重新排序。</p>
<p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131123340.png"></p>
<p>（1）难点</p>
<p><strong>① 如何保证用户满意度最高的同时，也保证创造者能够得到流量。</strong></p>
<ul>
<li>对创造者进行新手扶持，冷启动阶段会给高一些的权重。</li>
<li>但是更加考虑用户的满意度。所以如果创造者内容质量低，也有可能不给流量。</li>
</ul>
<p><strong>② 用户产品和商业产品的平衡：</strong></p>
<ul>
<li>有商业的产品，要保证收入。所以用户产品排完序之后，要把商业产品排到前面去。</li>
<li>具体商业产品的排序，要多种权重，不断权衡，尝试商业产品排序掉一点，商业价值不掉。</li>
<li>如果商业价值不变，但是整个模型数据提高，就可以上线。</li>
</ul>
<h1 id="06-其他步骤和需要注意的"><a href="#06-其他步骤和需要注意的" class="headerlink" title="06 其他步骤和需要注意的"></a>06 其他步骤和需要注意的</h1><h2 id="1-推荐系统的冷启动"><a href="#1-推荐系统的冷启动" class="headerlink" title="1. 推荐系统的冷启动"></a>1. 推荐系统的冷启动</h2><p><img src="%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230131123437.png"></p>
<p>冷启动的解决办法太多了。</p>
<p>① 类协同过滤的，找和新用户基础属性相似，又行为丰富的人，推荐这样的人喜欢的商品给新用户。</p>
<p>② 通过ml的方法，因为冷启动行为少，先利用之前的数据，训练好一个模型，直接赋给冷启动的用户，这样用少量数据，模型也可以快速收敛。</p>
<p>③ 通过业务的规则去做。什么场景下，给用户推荐什么内容。</p>
<p>④ 图神经网络。对物品冷启动有很好效果。</p>
<p>⑤ 用户的冷启动，可以收集跨域信息，比如去其他业务线包括二手房、商业地产收集用户信息。</p>
<p>⑥ 还可以通过提示是否喜欢一些标签，来获取用户数据。</p>
<p>⑦ 冷启动的基本原则是老物品给新用户，新物品给老用户。</p>
<ul>
<li>老物品给新用户，指基于流行度，选择热门商品，给新用户，做新用户兴趣的探索。</li>
<li>老用户给新物品，老用户已经有行为，新物品与老用户行为过的已有物品有关系，就推给老用户，度过新物品的冷启动周期。</li>
</ul>
<h2 id="2-推荐系统的评估指标"><a href="#2-推荐系统的评估指标" class="headerlink" title="2. 推荐系统的评估指标"></a>2. 推荐系统的评估指标</h2><p>离线评估指标：不同环节，不同指标。</p>
<ul>
<li>召回和粗排中，使用hitrate。</li>
<li>精排中：是auc，NDCG。</li>
</ul>
<p>业务场景中：</p>
<p>① 推荐系统评估使用工具：ab测试平台。</p>
<p>② 使用指标：CTR、CVR、人均使用时长、信息相关性等；</p>
<p>③ 也会关注留电率，但是这种线下数据太稀疏了，所以还是主要看CTR、CVR。</p>
<p>④ 平台在不同阶段关注的不同：</p>
<ul>
<li>平台在前期追求点击率；</li>
<li>相对长的时间段，关注用户观看时长；</li>
<li>更长期：关注用户的留存率。</li>
</ul>
<h1 id="07-推荐系统的应用"><a href="#07-推荐系统的应用" class="headerlink" title="07 推荐系统的应用"></a>07 推荐系统的应用</h1><p>推荐系统在不同的界面（比如首页、购买成功页、商品详情页）等，推荐系统的算法逻辑差异比较大。</p>
<ul>
<li>在商品列表页的推荐，主要是根据历史行为推荐；</li>
<li>在商品详情页的推荐，主要是根据当前商品推荐。</li>
</ul>
<h1 id="08-专家对整体推荐系统的观点"><a href="#08-专家对整体推荐系统的观点" class="headerlink" title="08 专家对整体推荐系统的观点"></a>08 专家对整体推荐系统的观点</h1><h2 id="1-推荐系统在业务上的难点"><a href="#1-推荐系统在业务上的难点" class="headerlink" title="1. 推荐系统在业务上的难点"></a>1. 推荐系统在业务上的难点</h2><p>不同的公司目标不一样，选用什么样的数据模型来完成公司的战略意图。比如公司现在想要用户的真实互动和分享，推荐系统应该把分享多的内容推荐给用户，但是这样会导致诱导分享的内容更容易被推荐。</p>
<p>如何判断优质内容，从而更好地把优质内容推荐给用户，是推荐系统在业务上的难点。</p>
<h2 id="2-精确性和惊喜性的平衡"><a href="#2-精确性和惊喜性的平衡" class="headerlink" title="2. 精确性和惊喜性的平衡"></a>2. 精确性和惊喜性的平衡</h2><p>推荐系统的精准性现在很容易做，基于上述全链路的算法，再配合好的特征，那么能得到一些好的商品。但是会面临问题：<strong>推荐商品很单一</strong>。比如点了很多连衣裙相关的，会不断推连衣裙。推荐系统具有滞后性，只会推用户已经行为过这些东西。虽然用户可能也会点击，但是<strong>这样对于一个推荐系统，是不够优秀的</strong>。</p>
<p>如果用户有多种兴趣：连衣裙、小吃等，会有打散策略，给她推多种兴趣的商品，这样问题还小一点。</p>
<p>如果用户兴趣单一，推荐系统就会只推她喜欢的那个兴趣，就是推荐系统不够好。</p>
<p><strong>好的推荐系统，要为用户提供惊喜性、发现性，要推荐用户恰好想要的。精准性和发现性需要兼顾，做一个平衡。</strong></p>
<h2 id="3-没有数据是最难的。不断会有新场景出来，新场景的数据不足。"><a href="#3-没有数据是最难的。不断会有新场景出来，新场景的数据不足。" class="headerlink" title="3. 没有数据是最难的。不断会有新场景出来，新场景的数据不足。"></a>3. 没有数据是最难的。不断会有新场景出来，新场景的数据不足。</h2><h2 id="4-推荐系统给新的内容生产者的流量"><a href="#4-推荐系统给新的内容生产者的流量" class="headerlink" title="4. 推荐系统给新的内容生产者的流量"></a>4. 推荐系统给新的内容生产者的流量</h2><p>没有看到一些很好的保量的算法。现在用的多的是pid，比例微分积分。投放速度快，就限制一些，投放速度慢，就减慢一些。</p>
<h2 id="5-推荐对业务的价值"><a href="#5-推荐对业务的价值" class="headerlink" title="5. 推荐对业务的价值"></a>5. 推荐对业务的价值</h2><p>怎么让推荐对整个业务起到作用，让整体业务增长。</p>
<h2 id="6-推荐所需的环境和条件"><a href="#6-推荐所需的环境和条件" class="headerlink" title="6. 推荐所需的环境和条件"></a>6. 推荐所需的环境和条件</h2><p>数据、abtest、线上工程，都能具有非常好的鲁棒性。</p>
<h2 id="7-产学研的结合"><a href="#7-产学研的结合" class="headerlink" title="7. 产学研的结合"></a>7. 产学研的结合</h2><p>希望datafun等机构可以请学界的人来，与业界进行交流。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">推荐系统</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/项目实战//" class="article-tag-list-link color5">项目实战</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/01/31/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%9A%BE%E7%82%B9%E8%A7%A3%E8%AF%BB/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/2/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/">Next &amp;raquo;</a>
    </nav>
  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2023 John Doe
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">数据科学</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">因子投资</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">随笔</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">推荐系统</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">数据仓库</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">常用算法</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">聚宽</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">贝叶斯</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">NLP基础</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">考试</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">java</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Linux</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://www.csdn.net/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>CSDN</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.zhihu.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>知乎</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.huaweicloud.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>华为云</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.aliyun.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>阿里云</a>
            </li>
          
            <li class="search-li">
              <a href="https://leetcode.cn/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>力扣</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.joinquant.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>聚宽</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">王宇涵//本科：哈尔滨工程大学//研究生：大连理工大学//专业：计算机技术//方向：量化交易与深度学习//热爱大数据平台开发与数仓开发，会分享一些技术文章和读书笔记</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>