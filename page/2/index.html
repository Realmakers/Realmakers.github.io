<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://example.com">
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div> 
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/123.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/categories">分类</a></li>
	        
			</ul>
		</nav>
		<nav>
			总文章数 58
		</nav>		
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/Realmakers" title="github"><i class="icon-github"></i></a>
		        
					<a class="qq" target="_blank" href="/3558084726" title="qq"><i class="icon-qq"></i></a>
		        
					<a class="mail" target="_blank" href="mailto: 17745182605@163.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>



    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/img/123.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/Realmakers" title="github"><i class="icon-github"></i></a>
			        
						<a class="qq" target="_blank" href="/3558084726" title="qq"><i class="icon-qq"></i></a>
			        
						<a class="mail" target="_blank" href="mailto: 17745182605@163.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 50%">
				
				
					<li style="width: 50%"><a href="/">主页</a></li>
		        
					<li style="width: 50%"><a href="/categories">分类</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            
  
    <article id="post-《大数据之路-阿里巴巴大数据实践》读书笔记与感悟" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/12/%E3%80%8A%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E8%B7%AF-%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E8%B7%B5%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%84%9F%E6%82%9F/">《大数据之路-阿里巴巴大数据实践》读书笔记与感悟</a>
    </h1>
  

        
        <a href="/2023/12/12/%E3%80%8A%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E8%B7%AF-%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E8%B7%B5%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%84%9F%E6%82%9F/" class="archive-article-date">
  	<time datetime="2023-12-12T11:42:24.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-12-12</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>大数据，源于技术，终于业务。数据仓库，因业务而生，因决策而发展</p>
</blockquote>
<h1 id="第2篇-数据模型篇"><a href="#第2篇-数据模型篇" class="headerlink" title="第2篇 数据模型篇"></a>第2篇 数据模型篇</h1><h2 id="第八章-大数据领域建模综述"><a href="#第八章-大数据领域建模综述" class="headerlink" title="第八章 大数据领域建模综述"></a>第八章 大数据领域建模综述</h2><h3 id="8-1-为什么需要数据建模？"><a href="#8-1-为什么需要数据建模？" class="headerlink" title="8.1 为什么需要数据建模？"></a>8.1 为什么需要数据建模？</h3><p>目标：将数据进行有序、有结构地分类组织和存储。</p>
<p>数据模型就是数据组织和存储方法，它强调从业务、数据存取和使用角度合理存储数据。以便在性能、成本、效率和质量之间取得最佳平衡。</p>
<h3 id="8-2-关系数据库系统和数据仓库"><a href="#8-2-关系数据库系统和数据仓库" class="headerlink" title="8.2 关系数据库系统和数据仓库"></a>8.2 关系数据库系统和数据仓库</h3><h4 id="8-2-1-从OLTP和OLAP系统的区别看模型方法论的选择"><a href="#8-2-1-从OLTP和OLAP系统的区别看模型方法论的选择" class="headerlink" title="8.2.1 从OLTP和OLAP系统的区别看模型方法论的选择"></a>8.2.1 从OLTP和OLAP系统的区别看模型方法论的选择</h4><ul>
<li>OLTP 系统通常面向的主要数据操作是随机读写，主要采用满足 3NF 的实体关系模型存储数据，从而在事务处理中解决数据的冗余和一 致性问题</li>
<li>OLAP 系统面向的主要数据操作是批量读写，事务处理中 的一致性不是OLAP 所关注的，其主要关注数据的整合，以及在一次性的复杂大数据查询和处理中的性能，因此它需要采用一些不同的数据建模方法</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>数据库</th>
<th>数据仓库</th>
</tr>
</thead>
<tbody><tr>
<td>处理方式</td>
<td>联机事务处理OLTP</td>
<td>联机分析处理OLAP</td>
</tr>
<tr>
<td>读特性</td>
<td>每次查询返回少量记录</td>
<td>对大量数据进行汇总查询</td>
</tr>
<tr>
<td>写特性</td>
<td>随机、低延时写入</td>
<td>批量导入</td>
</tr>
<tr>
<td>数据存储</td>
<td>业务数据</td>
<td>历史数据</td>
</tr>
<tr>
<td>设计理念</td>
<td>面向事务设计，为了捕获数据，避免冗余</td>
<td>面向主题设计，为了分析数据，引入冗余</td>
</tr>
<tr>
<td>数据量</td>
<td>GB</td>
<td>TB、PB</td>
</tr>
</tbody></table>
<h3 id="8-4-典型的数据仓库建模方法论"><a href="#8-4-典型的数据仓库建模方法论" class="headerlink" title="8.4 典型的数据仓库建模方法论"></a>8.4 典型的数据仓库建模方法论</h3><h4 id="8-4-1-ER模型"><a href="#8-4-1-ER模型" class="headerlink" title="8.4.1 ER模型"></a>8.4.1 ER模型</h4><p>数据仓库之父Bill Inmon提出的建模方法是从全企业的高度，用实体关系（Entity Relationship，ER）模型来描述企业业务，并用规范化的方式表示出来，在关系型数据库中常用，<strong>在范式理论上符合3NF</strong>。范式就是指在设计关系型数据库时，需要遵从的不同的规范。关系型数据库的范式一共有<strong>六种</strong>，分别是<strong>第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）</strong>。遵循的范式级别越高，数据冗余性就越低。这种模型并不适合直接用于分析统计。一个符合第三范式的关系必须具有以下三个条件 :</p>
<ul>
<li>每个属性值唯一，不具有多义性 ;</li>
<li>每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;</li>
<li>每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。</li>
</ul>
<h4 id="8-4-2-维度模型"><a href="#8-4-2-维度模型" class="headerlink" title="8.4.2 维度模型"></a>8.4.2 维度模型</h4><p>源于Ralph Kimall的《数据仓库工具箱》这本书中的理论。维度模型将复杂的业务通过<strong>事实</strong>和<strong>维度</strong>两个概念进行呈现。<strong>事实</strong>通常对应<strong>业务过程</strong>，而<strong>维度</strong>通常对应<strong>业务过程发生时所处的环境</strong>。虽然存在数据冗余但是方便查询分析。典型代表如星形模型，雪花模型，星座模型。</p>
<p>根据《数据仓库工具箱》中的总结，维度建模分四步走：</p>
<p>（1）<strong>选择业务过程</strong>：在整个业务流程中选择我们需要建模的业务（一般根据运营提供的需求）</p>
<p>（2）<strong>声明粒度</strong>：在同一事实表中，必须具有相同的粒度（一个用户有一个身份证号，多个手机号，那么用户粒度和身份证粒度相同，比两者更细的粒度就是手机号粒度）。对于有明确需求，就建立针对需求的粒度，需求不明就建立原子（最细）粒度。</p>
<p>（3）<strong>确认维度</strong>：确保维度表中不能出现重复数据，应使维度主键唯一</p>
<p>（4）<strong>确认事实</strong>：同一事实表中的所有度量必须具有相同的粒度。最实用的事实就是数值类型和可加类事实</p>
<h4 id="8-4-3-Data-Vault模型"><a href="#8-4-3-Data-Vault模型" class="headerlink" title="8.4.3 Data Vault模型"></a>8.4.3 Data Vault模型</h4><p>它强调建立一个可审计的基础数据层，也就是强调数据的历史 性、可追溯性和原子性，而不要求对数据进行过度的一致性处理和整合；同时它基于主题概念将企业数据进行结构化组织，并引入了更进一步的范式处理来优化模型，以应对下游、系统变更的扩展性。</p>
<h4 id="8-4-4-Anchor模型"><a href="#8-4-4-Anchor模型" class="headerlink" title="8.4.4 Anchor模型"></a>8.4.4 Anchor模型</h4><p>Anchor 对 Data Vault 模型做了进一步规范化处理， Lars.Ronnback 的初衷是设计一个高度可扩展的模型，其核心思想是所有的扩展只是添加而不是修改，因此将模型规范到 6NF ，基本变成了k-v 结构化模型。</p>
<h3 id="8-5-阿里巴巴数据模型实践综述"><a href="#8-5-阿里巴巴数据模型实践综述" class="headerlink" title="8.5 阿里巴巴数据模型实践综述"></a>8.5 阿里巴巴数据模型实践综述</h3><ul>
<li><p>第一个阶段：构建在 Oracle 上，数据完全以满足报表需求为目的</p>
</li>
<li><p>第二个阶段：引入了当时 MPP 架构体系的 Greenplum，ODL（操作数据层）+BDL（基础数据层）+IDL（接口数据层）+ADL （应用数据层）；BDL希望引入 ER 模型，加强 数据的整合，构建一致的基础数据模型，但构建 ER 模型时遇到了比较大的困难和挑战，互联网业务的快速发展、人员的快速变化、业务知识功底的不够全面，导致 ER 模型设计迟迟不能产出。至此，我们也得到了一个经验：在不太成熟、快速变化的业务面前，构建 ER 模型的风险非常大，不太适合去构建 ER 模型。</p>
</li>
<li><p>第三个阶段：迎来了以Hadoop 为代表的分布式存储计算平台的快速发展，同时阿里巴 巴集团自主研发的分布式计算平台 MaxCompute 也在紧锣密鼓地进行着。以 Kimball 的维度建模为核心理念的模型方法论，构建了阿里巴巴集团的公共层模型数据架构体系。</p>
</li>
<li><p>数据公共层建设的目的是着力解决数据存储和计算的共享问题。数据每年以近 2.5 倍的速度在增长，数据的增长远远超过业务的增长。</p>
</li>
<li><p>统一化的集团数据整合及管理的方法体系“OneData”：一致性的指标定义体系、模型设计方法体系以及配套工具。</p>
</li>
</ul>
<h2 id="第九章-阿里巴巴数据整合及管理体系"><a href="#第九章-阿里巴巴数据整合及管理体系" class="headerlink" title="第九章 阿里巴巴数据整合及管理体系"></a>第九章 阿里巴巴数据整合及管理体系</h2><h3 id="9-1-概述"><a href="#9-1-概述" class="headerlink" title="9.1 概述"></a>9.1 概述</h3><p>核心：从业务架构设计（如何快速上手工作）到模型设计，从数据研发到数据服务，做到数据可管理、可追溯、可规避重复建设。</p>
<h4 id="9-1-1-定位及价值"><a href="#9-1-1-定位及价值" class="headerlink" title="9.1.1 定位及价值"></a>9.1.1 定位及价值</h4><p>建设统一的、规范化的数据接入层（ ODS ）和数据中间层（ DWD 和 DWS ），通过数据服务和数据产品，完成服务于阿里巴巴的大数据系 统建设，即数据公共层建设。</p>
<p>业务板块：根据业务属性划分板块，板块之间的指标或业务重叠性较小。</p>
<p>规范定义：一套数据规范命名体系，用在模型设计中</p>
<p>模型设计：以维度建模理论为基础，基于维度建模总线架构，构建一致性的维度和事实(进行规范定义)。</p>
<h3 id="9-2-规范意义"><a href="#9-2-规范意义" class="headerlink" title="9.2 规范意义"></a>9.2 规范意义</h3><p>规范定义指以维度建模作为理论基础，构建总线矩阵，划分和定义 数据域、业务过程、维度、度量／原子指标、修饰类型、修饰词、时间周期、派生指标。</p>
<h4 id="9-2-1-名词解释"><a href="#9-2-1-名词解释" class="headerlink" title="9.2.1 名词解释"></a>9.2.1 名词解释</h4><ul>
<li>数据域（主题域）：面向业务分析，将业务过程或者维度进行抽象的集合。业务过程可以概括为一个个不可拆分的行为事件，在业务过程之下，可以定义指标；维度是指度量的环境，如买家下单事件，买家是维度 。 为保障整个体系的生命力 ， 数据域是需要抽象提炼，并且长期维护和更新的 ， 但不轻易变动。常见主题域：用户、渠道、营销、流量、交易、财务、商品。</li>
<li>业务过程：指企业的业务活动事件，如下单、支付、退款都是业务过程。 请注意，业务过程 是一个不可拆分的行为事件 ， 通俗地讲 ，业务过程就是企业活动中的事件</li>
<li>时间周期：用来明确数据统计的时间范用或者时间点，如最近 30天、自然周、截至当日等</li>
<li>修饰类型：是对修饰词的一种抽象划分 。 修饰类型从属于某个业务域，如日志域的访问终端类型涵盖无线端、 PC 端等修饰词</li>
<li>修饰词：指除了统计维度以外指标的业务场景限定抽象 。 修饰词隶属于一种修饰类型，如 在日志域的访问终端类型下 ， 有修饰词 PC 端、无线端等</li>
<li>度量&#x2F;原子指标：原子指标和度量含义相同，基于某一业务事件行为下的度量，是业务定义中不可 再拆分的指标，具有明确业务含义的名词 ，如支付金额</li>
<li>维度：维度是度量的环境，用来反映业务的一类属性 ，这类属性的集合构成一个维度 ，也可以称为实体对象。维度属于一个数据域，如地理维度（其中包括国家、地区、 省以及城市等级别的内容)、时间维度(其中包括年、季、月、周、日等级别的内容）</li>
<li>维度属性：维度属性隶属于一个维度 ，如地理维度里面的国家名称、国家 ID、省份名称等都属于维度属性</li>
<li>派生指标：派生指标&#x3D;一个原子指标+多个修饰词(可选)+时间周期+粒度。 可以理解为对原子指标业务统计范围的圈定。 如原子指标：支付金额，最近 1 天海外买家支付金额则为派生指标(最近1天为时间周期 ， 海外为修饰词 ， 买家作为维度，而不作为修饰词)</li>
</ul>
<h4 id="9-2-2-指标体系"><a href="#9-2-2-指标体系" class="headerlink" title="9.2.2 指标体系"></a>9.2.2 指标体系</h4><p>一、基本原则</p>
<ol>
<li>组成体系之间的关系</li>
</ol>
<ul>
<li>派生指标由原子指标、时间周期修饰词、若干其他修饰词组合得到</li>
<li>原子指标、修饰类型及修饰词，直接归属在业务过程下，其中修饰词继承修饰类型的数据域</li>
<li>派生指标可以选择多个修饰词，修饰词之间的关系为”或”或者”且”，由派生指标具体语义决定</li>
<li>派生指标唯一归属一个原子指标，继承原子指标的数据域，与修饰词的数据域无关</li>
<li>原子指标有确定的英文字段名、数据类型和算法说明；派生指标要继承原子指标的英文名、数据类型和算法要求</li>
</ul>
<ol start="2">
<li>命名规定</li>
</ol>
<ul>
<li>命名所用术语。指标命名尽量使用英文简写，其次是英文。太长也可以考虑汉语拼音首字母</li>
<li>业务过程。英文名：用英文或英文的缩写或者中文拼音简写</li>
<li>原子指标。英文名：动作+度量</li>
<li>修饰词。只有时间周期才会有英文名</li>
<li>派生指标。英文名：原子指标英文名+时间周期修饰词(3位,例如_1d)+序号(4位，例如_001)</li>
</ul>
<ol start="3">
<li>算法</li>
</ol>
<ul>
<li>算法概述一一算法对应的用户容易理解的阐述。</li>
<li>举例一一通过具体例子帮助理解指标算法。</li>
<li>SQL 算法说明一一对于派生指标给出SQL的写法或者伪代码。</li>
</ul>
<p>二、操作细则</p>
<p>派生指标可以分为三类：事务型指标、存量型指标和复合型指标。</p>
<ul>
<li>事务型指标：是指对业务活动进行衡量的指标。例如新发商品数、 重发商品数、新增注册会员数、订单支付金额，这类指标需维护 原子指标及修饰词，在此基础上创建派生指标。</li>
<li>存量型指标：是指对实体对象(如商品、会员)某些状态的统计。 例如商品总数、注册会员总数，这类指标需维护原子指标及修饰词，在此基础上创建派生指标，对应的时间周期 一般为“历史截至当前某个时间”。</li>
<li>复合型指标：是在事务型指标和存量型指标的基础上复合而成的。例如浏览 UV-下单买家数转化率 ， 有些需要 创 建新原子指标， 有些则可以在事务型或存量型原子指标的基础上增加修饰词得到派生指标。</li>
</ul>
<h3 id="9-3-模型设计"><a href="#9-3-模型设计" class="headerlink" title="9.3 模型设计"></a>9.3 模型设计</h3><h4 id="9-3-1-理论指导"><a href="#9-3-1-理论指导" class="headerlink" title="9.3.1 理论指导"></a>9.3.1 理论指导</h4><p>数据模型的维度设计主要以维度建模理论为基础，基于维度数据模型总线架构，构建一致性的维度和事实。</p>
<h4 id="9-3-2-模型层次"><a href="#9-3-2-模型层次" class="headerlink" title="9.3.2 模型层次"></a>9.3.2 模型层次</h4><ul>
<li><p>操作数据层（ODS）：把操作系统数据几乎无处理地存放在数据仓库系统中。</p>
</li>
<li><p>公共维度模型层（CDM）：存放明细事实数据、维表数据及公共指标汇总数据 ，其中明细事实数据、维表数据一般根据 ODS 层数据加工生成 ；公共指标汇总数据一般根据维表数据和明细事实数据加工生成。</p>
</li>
<li><p>CDM 层又细分为 DWD 层和 DWS 层，分别是明细数据层和汇总数据层，采用维度模型方法作为理论基础 ，更多地采用一些维度退化手法， 将维度退化至事实表中，减少事实表和维表的关联 ，提高明细数据表的易用性；同时在汇总数据层， 加强指标的维度退化， 采取更多的宽表化手段构建公共指标数据层，提升公共指标的复用性，减少重复加工。其主要功能如下。</p>
</li>
<li><ul>
<li>组合相关和相似数据：采用明细宽表，复用关联计算，减少数据扫描。</li>
<li>公共指标统一加工：基于 OneData体系构建命名规范、口径一致 和算法统一 的统计指标，为上层数据产品、应用和服务提供公共指标建立逻辑汇总宽表。</li>
<li>建立一致性维度:建立一致的数据分析维表，降低数据计算口径、算法不统一的风险。</li>
</ul>
</li>
<li><p>应用数据层（ADS）：存放数据产品个性化的统计指标数据，根据 CDM 层与 ODS 层加工生成 。</p>
</li>
</ul>
<h4 id="9-3-3-基本原则"><a href="#9-3-3-基本原则" class="headerlink" title="9.3.3 基本原则"></a>9.3.3 基本原则</h4><ul>
<li>高内聚和低耦合</li>
</ul>
<p>一个逻辑或者物理模型由哪些记录和字段组成，应该遵循最基本的软件设计方法论的高内聚和低耦合原则。主要从数据业务特性和访问特性两个角度来考虑：将业务相近或者相关、粒度相同的数据设计为一个逻辑或者物理模型；将高概率同时访问的数据放一起，将低概率同时访问的数据分开存储。</p>
<ul>
<li>核心模型与扩展模型分离</li>
</ul>
<p>建立核心模型与扩展模型体系，核心模型包括的字段支持常用的核心业务，扩展模型包括的字段支持个性化或少量应用的需要 ，不能让扩展模型的宇段过度侵入核心模型，以免破坏核心模型的架构简洁性与可维护性。</p>
<ul>
<li>公共处理逻辑下沉及单一</li>
</ul>
<p>越是底层公用的处理逻辑越应该在数据调度依赖的底层进行封装与实现，不要让公用的处理逻辑暴露给应用层实现，不要让公共逻辑多处同时存在。</p>
<ul>
<li>成本与性能平衡</li>
</ul>
<p>适当的数据冗余可换取查询和刷新性能，不宜过度冗余与数据复制。</p>
<ul>
<li>数据可回滚</li>
</ul>
<p>处理逻辑不变，在不同时间多次运行数据结果确定不变。</p>
<ul>
<li>一致性</li>
</ul>
<p>具有相同含义的字段在不同表中的命名必须相同，必须使用规范定义中的名称。</p>
<ul>
<li>命名清晰、可理解</li>
</ul>
<p>表命名需清晰、一致，表名需易于消费者理解和使用。</p>
<h3 id="9-4-模型实施"><a href="#9-4-模型实施" class="headerlink" title="9.4 模型实施"></a>9.4 模型实施</h3><h4 id="9-4-1-业界常用模型实施过程"><a href="#9-4-1-业界常用模型实施过程" class="headerlink" title="9.4.1 业界常用模型实施过程"></a>9.4.1 业界常用模型实施过程</h4><p>构建维度模型一般要经历四个阶段：</p>
<ul>
<li>第一个阶段是高层模型设计时期 ，定义业务过程维度模型的范围，提供每种星形模式的技术和功能描述；直接产出目标是创建高层维度模型图，它是对业务过程中的维表和事实表的图形描述。确定维表创建初始属性列表，为每个事实表创建提议度量；</li>
<li>第二个阶段是详细模型设计时期，对每个星形模型添加属性和度量信息；确定每个维表的属性和每个事实表的度量，并确定信息来源的位置、定义，确定属性和度量如何填入模型的初步业务规则。</li>
<li>第三个阶段是进行模型的审查、再设计和验证，本阶段主要召集相关人员进行模型的审查和验证，根据审查结果对详细维度进行再设计。</li>
<li>第四个阶段是产生详细设计文档，提交 ETL 设计和开发，最后，完成模型详细设计文档，提交 ETL 开发人员，进入 ETL 设计和开发阶段，由 ETL 人员完成物理模型的设计和开发。</li>
</ul>
<h4 id="9-4-2-OneData实施过程"><a href="#9-4-2-OneData实施过程" class="headerlink" title="9.4.2 OneData实施过程"></a>9.4.2 OneData实施过程</h4><ol>
<li>指导方针</li>
</ol>
<ul>
<li>首先，在建设大数据数据仓库时，要进行充分的业务调研和需求分析。这是数据仓库建设的基石，业务调研和需求分析做得是否充分直接决定了数据仓库建设是否成功。</li>
<li>其次，进行数据总体架构设计，主要是根据数据域对数据进行划分；按照维度建模理论，构建总线矩阵、抽象出业务过程和维度。</li>
<li>再次，对报表需求进行抽象整理出相关指标体系， 使用 OneData 工具完成指标规范定义和模型设计。</li>
<li>最后，就是代码研发和运维。</li>
</ul>
<ol start="2">
<li>实施工作流</li>
</ol>
<p>（1）数据调研</p>
<ol>
<li>业务调研：需要了解各个业务领域、业务线的业务有什么共同点和不同点 ，以及各个业务线可以细分为哪几个业务模块，每个业务模块具体的业务流程又是怎样的。业务调研是否充分，将会直接决定数据仓库 建设是否成功</li>
<li>需求调研：需求调研的途径有两种：一是根据与分析师、业务运营人员的沟通 （邮件、 IM ）获知需求；二是对报表系统中现有的报表进行研究分析；</li>
</ol>
<p>（2）架构设计</p>
<ol>
<li>数据域划分</li>
</ol>
<p>数据域是指面向业务分析，将业务过程或者维度进行抽象的集合。 业务过程可以概括为 一 个个不可拆分的行为事件，如下单、支付、退款。数据域需要抽象提炼，并且长期维护和更新，但不轻易变动。</p>
<ol start="2">
<li>构建总线矩阵</li>
</ol>
<p>在进行充分的业务调研和需求调研后，就要构建总线矩阵了。需要 做两件事情：明确每个数据域下有哪些业务过程；业务过程与哪些维度相关，并定义每个数据域下的业务过程和维度。</p>
<ol start="3">
<li>规范定义</li>
</ol>
<p>规范定义主要定义指标体系，包括原子指标、修饰词、时间周期和 派生指标。</p>
<ol start="4">
<li>模型设计</li>
</ol>
<p>模型设计主要包括维度及属性的规范定义，维表、明细事实表和汇 总事实表的模型设计。</p>
<ol start="5">
<li>总结</li>
</ol>
<p>OneData 的实施过程是一个高度迭代和动态的过程， 一般采用螺旋式实施方法。在总体架构设计完成之后，开始根据数据域进行迭代式模型设计和评审。在架构设计、规范定义和模型设计等模型实施过程中， 都会引入评审机制，以确保模型实施过程的正确性。</p>
<h2 id="第十章-维度设计"><a href="#第十章-维度设计" class="headerlink" title="第十章 维度设计"></a>第十章 维度设计</h2><h3 id="10-1-维度设计基础"><a href="#10-1-维度设计基础" class="headerlink" title="10.1 维度设计基础"></a>10.1 维度设计基础</h3><h4 id="10-1-1-维度的基本概念"><a href="#10-1-1-维度的基本概念" class="headerlink" title="10.1.1 维度的基本概念"></a>10.1.1 维度的基本概念</h4><ul>
<li>维度建模中，将度量称为“事实”，将环境描述为“维度”，维度是用于分析事实所需要的多样环境。例如，在分析交易过程时，可以通过买家、卖家、商品和时间等维度描述交易发生的环境。</li>
<li>维度所包含的表示维度的列，称为维度属性。维度属性是查询约束条件、分组和报表标签生成的基本来源，是数据易用性的关键。</li>
<li>维度使用主键标识其唯一性，主键也是确保与之相连的任何事实表 之间存在引用完整性的基础。</li>
</ul>
<h4 id="10-1-2-维度的基本设计方法"><a href="#10-1-2-维度的基本设计方法" class="headerlink" title="10.1.2 维度的基本设计方法"></a>10.1.2 维度的基本设计方法</h4><ol>
<li>选择维度或新建维度。须保证维度的唯一性。</li>
<li>确定主维表。 一般是ODS表，直接与业务系统同步。</li>
<li>确定相关维表。确定哪些表和主维表存在关联关系，并选择其中的某些表用于生成维度属性。</li>
<li>确定维度属性。第一阶段从主维表中选择维度属性或生成新的维度属性；第二阶段是从相关维表中选择维度属性或生成新的维度属性。</li>
</ol>
<p>确认维度属性的几点提示：</p>
<ol>
<li>尽可能生成丰富的维度属性</li>
<li>尽可能多地给出包括一些富有意义的文字性描述</li>
<li>区分数值型属性和事实</li>
</ol>
<ul>
<li>如果通常用于查询约束条件或分组统计，则是作为维度属性；如果通常 用于参与度量的计算，则是作为事实。比如商品价格，可以用于查询约 束条件或统计价格区间的商品数量，此时是作为维度属性使用的；也可 以用于统计某类目下商品的平均价格，此时是作为事实使用的。另外， 如果数值型字段是离散值，则作为维度属性存在的可能性较大；如果数 值型宇段是连续值，则作为度量存在的可能性较大，但并不绝对，需要 同时参考宇段的具体用途。</li>
</ul>
<ol start="4">
<li>尽量沉淀出通用的维度属性</li>
</ol>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">面试</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/12/12/%E3%80%8A%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E8%B7%AF-%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E8%B7%B5%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%84%9F%E6%82%9F/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-clickhouse框架学习笔记" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/01/clickhouse%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">clickhouse框架学习笔记</a>
    </h1>
  

        
        <a href="/2023/12/01/clickhouse%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="archive-article-date">
  	<time datetime="2023-12-01T07:10:08.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-12-01</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>万事不决问官方，clickhouse官方文档地址：<a target="_blank" rel="noopener" href="https://clickhouse.com/docs/zh/introduction-clickhouse">ClickHouse Docs</a></p>
</blockquote>
<h1 id="第一章-Clickhouse入门"><a href="#第一章-Clickhouse入门" class="headerlink" title="第一章 Clickhouse入门"></a>第一章 Clickhouse入门</h1><p>ClickHouse 是俄罗斯的 Yandex 于 2016 年开源的列式存储数据库（DBMS），使用 C++语言编写，主要用于在线分析处理查询（OLAP），能够使用 SQL 查询实时生成分析数据报告。<strong>ck查询很快，并且不需要依赖HDFS存储，YARN资源等hadoop生态。</strong></p>
<h2 id="1-1-clickhouse的特点"><a href="#1-1-clickhouse的特点" class="headerlink" title="1.1 clickhouse的特点"></a>1.1 clickhouse的特点</h2><h3 id="1-1-1-列式存储"><a href="#1-1-1-列式存储" class="headerlink" title="1.1.1 列式存储"></a>1.1.1 列式存储</h3><p><strong>列式储存的好处：</strong></p>
<p>①对于列的聚合，计数，求和等统计操作原因优于行式存储。 </p>
<p>②由于某一列的数据类型都是相同的，针对于数据存储更容易进行数据压缩，每一列选择更优的数据压缩算法，大大提高了数据的压缩比重。</p>
<p>③由于数据压缩比更好，一方面节省了磁盘空间，另一方面对于 cache 也有了更大的发挥空间。</p>
<h3 id="1-1-2-DBMS的功能"><a href="#1-1-2-DBMS的功能" class="headerlink" title="1.1.2 DBMS的功能"></a>1.1.2 DBMS的功能</h3><p>几乎覆盖了标准 SQL 的大部分语法，包括 DDL 和 DML，以及配套的各种函数，用户管理及权限管理，数据的备份与恢复。</p>
<h3 id="1-1-3-多样化引擎"><a href="#1-1-3-多样化引擎" class="headerlink" title="1.1.3 多样化引擎"></a>1.1.3 多样化引擎</h3><p>ClickHouse 和 MySQL 类似，把表级的存储引擎插件化，根据表的不同需求可以设定不同的存储引擎。目前包括合并树、日志、接口和其他四大类 20 多种引擎。</p>
<h3 id="1-1-4-高吞吐写入能力"><a href="#1-1-4-高吞吐写入能力" class="headerlink" title="1.1.4 高吞吐写入能力"></a>1.1.4 高吞吐写入能力</h3><p>ClickHouse 采用类 <strong>LSM Tree</strong>的结构，数据写入后定期在后台 Compaction。通过类 LSM tree的结构，ClickHouse 在数据导入时全部是顺序 append 写，写入后数据段不可更改，在后台compaction 时也是多个段 merge sort 后顺序写回磁盘。<strong>顺序写</strong>的特性，充分利用了磁盘的吞吐能力，即便在 HDD 上也有着优异的写入性能。</p>
<p>官方公开 benchmark 测试显示能够达到 50MB-200MB&#x2F;s 的写入吞吐能力，按照每行100Byte 估算，大约相当于 50W-200W 条&#x2F;s 的写入速度。</p>
<h3 id="1-1-5-数据分区与线程级并行"><a href="#1-1-5-数据分区与线程级并行" class="headerlink" title="1.1.5 数据分区与线程级并行"></a>1.1.5 数据分区与线程级并行</h3><p>ClickHouse 将数据划分为多个 partition，每个 partition 再进一步划分为多个 index granularity(索引粒度)，然后通过多个 CPU核心分别处理其中的一部分来实现并行数据处理。在这种设计下，<strong>单条 Query 就能利用整机所有 CPU</strong>。极致的并行处理能力，极大的降低了查询延时。</p>
<p>所以，ClickHouse 即使对于大量数据的查询也能够化整为零平行处理。但是有一个弊端就是对于单条查询使用多 cpu，就不利于同时并发多条查询。<strong>所以对于高 qps（query per second每秒查询次数） 的查询业务，ClickHouse 并不是强项。</strong></p>
<p><strong>clickhouse不适合做初始的存储，适合存储已经处理过的大量的字段特别多的宽表</strong></p>
<h3 id="1-1-6-性能查询"><a href="#1-1-6-性能查询" class="headerlink" title="1.1.6 性能查询"></a>1.1.6 性能查询</h3><p>结论: ClickHouse 像很多 OLAP 数据库一样，单表查询速度快于关联查询，而且 ClickHouse的两者差距更为明显。</p>
<h1 id="第二章-clickhouse安装"><a href="#第二章-clickhouse安装" class="headerlink" title="第二章 clickhouse安装"></a>第二章 clickhouse安装</h1><h2 id="2-1-准备工作"><a href="#2-1-准备工作" class="headerlink" title="2.1 准备工作"></a>2.1 准备工作</h2><h3 id="2-1-1-确定防火墙处于关闭状态"><a href="#2-1-1-确定防火墙处于关闭状态" class="headerlink" title="2.1.1 确定防火墙处于关闭状态"></a>2.1.1 确定防火墙处于关闭状态</h3><h3 id="2-1-2-CentOS-取消打开文件数限制"><a href="#2-1-2-CentOS-取消打开文件数限制" class="headerlink" title="2.1.2  CentOS 取消打开文件数限制"></a>2.1.2  <strong>CentOS</strong> <strong>取消打开文件数限制</strong></h3><p>（1）在 hadoop102 的 &#x2F;etc&#x2F;security&#x2F;limits.conf 文件的末尾加入以下内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# vim /etc/security/limits.conf</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2024-05-19_15-45-15.png" alt="Snipaste_2024-05-19_15-45-15" style="zoom:43%;">

<p>*：表示所有用户所有用户组</p>
<p>soft：当前生效的</p>
<p>hard：最大、上限</p>
<p>nofile：打开的文件数</p>
<p>noproc：打开的进程数</p>
<p>（2）在 hadoop102 的&#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;20-nproc.conf 文件的末尾加入以下内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# vim /etc/security/limits.d/20-nproc.conf</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2024-05-19_15-51-55.png" alt="Snipaste_2024-05-19_15-51-55" style="zoom:43%;">

<p>（3）执行同步操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# xsync /etc/security/limits.conf</span><br><span class="line">[root@hadoop102 ~]# xsync /etc/security/limits.d/20-nproc.conf</span><br></pre></td></tr></table></figure>

<h3 id="2-1-3-安装依赖"><a href="#2-1-3-安装依赖" class="headerlink" title="2.1.3 安装依赖"></a>2.1.3 安装依赖</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# yum install -y libtool</span><br><span class="line">[root@hadoop102 ~]# yum install -y *unixODBC*</span><br></pre></td></tr></table></figure>

<p>在 hadoop103、hadoop104 上执行以上操作</p>
<h3 id="2-1-4-CentOS-取消-SELINUX"><a href="#2-1-4-CentOS-取消-SELINUX" class="headerlink" title="2.1.4  CentOS 取消 SELINUX"></a>2.1.4  <strong>CentOS</strong> <strong>取消</strong> <strong>SELINUX</strong></h3><p>（1）修改&#x2F;etc&#x2F;selinux&#x2F;config 中的 SELINUX&#x3D;disabled</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# vim /etc/selinux/config</span><br></pre></td></tr></table></figure>

<p>SELINUX&#x3D;disabled</p>
<p>（2）执行同步操作</p>
<p>（3）重启三台服务器</p>
<h3 id="2-1-5-启动Zookeeper"><a href="#2-1-5-启动Zookeeper" class="headerlink" title="2.1.5 启动Zookeeper"></a>2.1.5 启动Zookeeper</h3><p>注意在启动ck服务前保证三台服务器节点的zookeeper服务都要启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse-server]# zk.sh start</span><br><span class="line">---------- zookeeper hadoop102 启动 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">---------- zookeeper hadoop103 启动 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">---------- zookeeper hadoop104 启动 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure>

<h2 id="2-2-单机安装"><a href="#2-2-单机安装" class="headerlink" title="2.2 单机安装"></a>2.2 单机安装</h2><p>官网：<a target="_blank" rel="noopener" href="https://clickhouse.tech/">https://clickhouse.tech/</a></p>
<p>下载地址：<a target="_blank" rel="noopener" href="http://repo.red-soft.biz/repos/clickhouse/stable/el7/">http://repo.red-soft.biz/repos/clickhouse/stable/el7/</a></p>
<p>（1）在hadoop102的&#x2F;opt&#x2F;software下创建 clickhouse目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# pwd</span><br><span class="line">/opt/software</span><br><span class="line">[root@hadoop102 software]# mkdir clickhouse</span><br></pre></td></tr></table></figure>

<p>（2）将以下四个安装文件上传到hadoop102的software&#x2F;clickhouse目录下</p>
<p><img src="Snipaste_2024-05-19_22-27-08.png" alt="Snipaste_2024-05-19_22-27-08"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]# ll</span><br><span class="line">总用量 938164</span><br><span class="line">-rw-r--r-- 1 root root     78074 5月  19 22:29 clickhouse-client-21.7.3.14-2.noarch.rpm</span><br><span class="line">-rw-r--r-- 1 root root 174283244 5月  19 22:29 clickhouse-common-static-21.7.3.14-2.x86_64.rpm</span><br><span class="line">-rw-r--r-- 1 root root 786208040 5月  19 22:29 clickhouse-common-static-dbg-21.7.3.14-2.x86_64.rpm</span><br><span class="line">-rw-r--r-- 1 root root    101969 5月  19 22:29 clickhouse-server-21.7.3.14-2.noarch.rpm</span><br></pre></td></tr></table></figure>

<p>（3）将安装文件同步到hadoop103，104</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# xsync clickhouse/</span><br></pre></td></tr></table></figure>

<p>（4）分别在这三台机器上安装这4个rpm文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]# rpm -ivh *.rpm</span><br><span class="line">[root@hadoop103 clickhouse]# rpm -ivh *.rpm</span><br><span class="line">[root@hadoop104 clickhouse]# rpm -ivh *.rpm</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果安装失败，强制安装</span><br><span class="line">rpm -ivh *.rpm --force --nodeps</span><br></pre></td></tr></table></figure>

<p>设置了用户名和密码</p>
<p>default 123456</p>
<p>确认已安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]#  rpm -qa|grep clickhouse</span><br><span class="line">clickhouse-client-21.7.3.14-2.noarch</span><br><span class="line">clickhouse-server-21.7.3.14-2.noarch</span><br><span class="line">clickhouse-common-static-dbg-21.7.3.14-2.x86_64</span><br><span class="line">clickhouse-common-static-21.7.3.14-2.x86_64</span><br></pre></td></tr></table></figure>

<p>（5）修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]# vim /etc/clickhouse-server/config.xml</span><br></pre></td></tr></table></figure>

<p>把 <listen_host>::</listen_host> 的注释打开，这样的话才能让 ClickHouse 被除本机以外的服务器访问。</p>
<p>分发配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]# xsync /etc/clickhouse-server/config.xml</span><br></pre></td></tr></table></figure>

<p>在这个文件中，有 ClickHouse 的一些默认路径配置，比较重要的</p>
<p>数据文件路径：<path></path>&#x2F;var&#x2F;lib&#x2F;clickhouse&#x2F;</p>
<p><strong>前面说到ck不需要依赖hadoop生态，其将数据存储在本地磁盘中</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]# cd /var/lib/clickhouse/</span><br><span class="line">[root@hadoop102 clickhouse]# ls</span><br><span class="line">access  data              flags           metadata          preprocessed_configs  store  user_files</span><br><span class="line">cores   dictionaries_lib  format_schemas  metadata_dropped  status                tmp</span><br><span class="line">[root@hadoop102 clickhouse]# cd data/</span><br><span class="line">[root@hadoop102 data]# ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse   6 5月  19 23:08 default</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse 113 5月  20 14:45 system</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse  36 5月  21 10:56 test</span><br></pre></td></tr></table></figure>

<p>日志文件路径：<log>&#x2F;var&#x2F;log&#x2F;clickhouse-server&#x2F;clickhouse-server.log</log></p>
<p>报错日志文件路径：<errorlog>&#x2F;var&#x2F;log&#x2F;clickhouse-server&#x2F;clickhouse-server.err.log&lt;&#x2F;error log&gt;</errorlog></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ClickHouse各文件目录：</span><br><span class="line">    bin/    ===&gt;  /usr/bin/ </span><br><span class="line">    conf/   ===&gt;  /etc/clickhouse-server/</span><br><span class="line">    lib/    ===&gt;  /var/lib/clickhouse </span><br><span class="line">    log/    ===&gt;  /var/log/clickhouse-server</span><br></pre></td></tr></table></figure>

<p>（6）启动Server</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]# clickhouse start</span><br><span class="line"> chown --recursive clickhouse &#x27;/var/run/clickhouse-server/&#x27;</span><br><span class="line">Will run su -s /bin/sh &#x27;clickhouse&#x27; -c &#x27;/usr/bin/clickhouse-server --config-file /etc/clickhouse-server/config.xml --pid-file /var/run/clickhouse-server/clickhouse-server.pid --daemon&#x27;</span><br><span class="line">Waiting for server to start</span><br><span class="line">Waiting for server to start</span><br><span class="line">Server started</span><br><span class="line">[root@hadoop102 clickhouse]# clickhouse status</span><br><span class="line">/var/run/clickhouse-server/clickhouse-server.pid file exists and contains pid = 6745.</span><br><span class="line">The process with pid = 6745 is running.</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">clickhouse自带的服务器端启停命令</span></span><br><span class="line">clickhouse start</span><br><span class="line">clickhouse stop</span><br><span class="line">cliskhouse status</span><br><span class="line">clickhouse restart</span><br></pre></td></tr></table></figure>

<p>（7）三台机器上关闭开机自启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]# systemctl disable clickhouse-server</span><br><span class="line">[root@hadoop103 clickhouse]# systemctl disable clickhouse-server</span><br><span class="line">[root@hadoop104 clickhouse]# systemctl disable clickhouse-server</span><br></pre></td></tr></table></figure>

<p>（8）使用client连接server</p>
<p>（-m :可以在命令窗口输入多行命令）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]# clickhouse-client -m</span><br><span class="line">ClickHouse client version 21.7.3.14 (official build).</span><br><span class="line">Connecting to localhost:9000 as user default.</span><br><span class="line">Connected to ClickHouse server version 21.7.3 revision 54449.</span><br><span class="line"></span><br><span class="line">hadoop102 :) </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hadoop102 :) show databases;</span><br><span class="line"></span><br><span class="line">SHOW DATABASES</span><br><span class="line"></span><br><span class="line">Query id: 01c3aacb-41ff-485a-bfda-1451c9bf3529</span><br><span class="line"></span><br><span class="line">┌─name────┐</span><br><span class="line">│ default │</span><br><span class="line">│ system  │</span><br><span class="line">└─────────┘</span><br><span class="line"></span><br><span class="line">2 rows in set. Elapsed: 0.015 sec. </span><br><span class="line"></span><br><span class="line">hadoop102 :) quit;</span><br><span class="line">Bye.</span><br></pre></td></tr></table></figure>

<p>也可以不使用交互式，直接写查询语句</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]# clickhouse-client --query &quot;show databases;&quot;;</span><br><span class="line">default</span><br><span class="line">system</span><br></pre></td></tr></table></figure>

<h1 id="第三章-数据类型"><a href="#第三章-数据类型" class="headerlink" title="第三章 数据类型"></a>第三章 数据类型</h1><h2 id="3-1-整形"><a href="#3-1-整形" class="headerlink" title="3.1 整形"></a>3.1 整形</h2><p>固定长度的整型，包括有符号整型或无符号整型。clickhouse中没有byte、short、int、long、bigint等整数类型，统一都叫int，范围根据后面的8，16，32，64控制</p>
<p>整型范围（-2n-1~2n-1-1）：</p>
<p><strong>Int8</strong> - [-128 : 127]</p>
<p><strong>Int16</strong> - [-32768 : 32767]</p>
<p><strong>Int32</strong> - [-2147483648 : 2147483647]</p>
<p><strong>Int64</strong> - [-9223372036854775808 : 9223372036854775807]</p>
<p>无符号整型范围（0~2n-1）：</p>
<p><strong>UInt8</strong> - [0 : 255]</p>
<p><strong>UInt16</strong> - [0 : 65535]</p>
<p><strong>UInt32</strong> - [0 : 4294967295]</p>
<p><strong>UInt64</strong> - [0 : 18446744073709551615]</p>
<p><strong>使用场景： 个数、数量、也可以存储型</strong> <strong>id</strong>。</p>
<h2 id="3-2-浮点型"><a href="#3-2-浮点型" class="headerlink" title="3.2 浮点型"></a>3.2 浮点型</h2><p><strong>Float32</strong> - float</p>
<p><strong>Float64</strong> – double</p>
<p>建议尽可能以整数形式存储数据。例如，将固定精度的数字转换为整数值，如时间用毫秒为单位表示，因为浮点型进行计算时可能引起四舍五入的误差。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hadoop102 :) select 1.0-0.9;</span><br><span class="line"></span><br><span class="line">SELECT 1. - 0.9</span><br><span class="line"></span><br><span class="line">Query id: ed0f0f55-724a-4815-8128-1afd39cb7357</span><br><span class="line"></span><br><span class="line">┌──────minus(1., 0.9)─┐</span><br><span class="line">│ 0.09999999999999998 │</span><br><span class="line">└─────────────────────┘</span><br><span class="line"></span><br><span class="line">1 rows in set. Elapsed: 0.019 sec.</span><br></pre></td></tr></table></figure>

<p><strong>使用场景：一般数据值比较小，不涉及大量的统计计算，精度要求不高的时候。比如保存商品的重量。</strong></p>
<p>存钱这种数据的时候，一般使用decimal</p>
<h2 id="3-3-布尔型"><a href="#3-3-布尔型" class="headerlink" title="3.3 布尔型"></a>3.3 布尔型</h2><p>没有单独的类型来存储布尔值，可以使用<strong>UInt8</strong>类型，取值限制为0或1</p>
<h2 id="3-4-Decimal"><a href="#3-4-Decimal" class="headerlink" title="3.4 Decimal"></a>3.4 Decimal</h2><p>有符号的浮点数，可在加、减和乘法运算过程中保持精度。对于除法，最低有效数字会被丢弃（不舍入）。</p>
<p>有三种声明：</p>
<p>➢ <strong>Decimal32(s)<strong>，相当于 Decimal(9-s,s)，有效位数为 1~9；</strong>整数+小数一共9位，小数部分s位</strong></p>
<p>➢ <strong>Decimal64(s)<strong>，相当于 Decimal(18-s,s)，有效位数为 1~18；</strong>整数+小数一共18位，小数部分s位</strong></p>
<p>➢ <strong>Decimal128(s)<strong>，相当于 Decimal(38-s,s)，有效位数为 1~38；</strong>整数+小数一共38位，小数部分s位</strong></p>
<p>s 标识小数位</p>
<p>使用场景： 一般金额字段、汇率、利率等字段为了保证小数点精度，都使用 Decimal进行存储。</p>
<h2 id="3-5-字符串"><a href="#3-5-字符串" class="headerlink" title="3.5 字符串"></a>3.5 字符串</h2><p>1）<strong>String</strong></p>
<p>字符串可以任意长度的。它可以包含任意的字节集，包含空字节。</p>
<p>2）<strong>FixedString(N)</strong></p>
<p>固定长度 N 的字符串，N 必须是严格的正自然数。当服务端读取长度小于 N 的字符串时候，通过在字符串末尾添加空字节来达到 N 字节长度。 当服务端读取长度大于 N 的字符串时候，将返回错误消息。</p>
<p>与 String 相比，极少会使用 FixedString，因为使用起来不是很方便。</p>
<p>使用场景：名称、文字描述、字符型编码。 固定长度的可以保存一些定长的内容，比如一些编码，性别等但是考虑到一定的变化风险，带来收益不够明显，所以定长字符串使用意义有限。</p>
<h2 id="3-6-枚举类型（谨慎用）"><a href="#3-6-枚举类型（谨慎用）" class="headerlink" title="3.6 枚举类型（谨慎用）"></a>3.6 枚举类型（谨慎用）</h2><p>包括 Enum8 和 Enum16 类型。Enum 保存** ‘string’&#x3D; integer **的对应关系。</p>
<p><strong>Enum8</strong> 用 ‘String’&#x3D; Int8 对描述。</p>
<p><strong>Enum16</strong> 用 ‘String’&#x3D; Int16 对描述。</p>
<p>用法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">--创建一个数据库</span><br><span class="line">create database test;</span><br><span class="line">--枚举数据类型</span><br><span class="line">--创建一张表</span><br><span class="line">create table t_enum</span><br><span class="line">(</span><br><span class="line">    x Enum8(&#x27;hello&#x27; = 1, &#x27;world&#x27; = 2)</span><br><span class="line">)</span><br><span class="line">ENGINE = TinyLog;--TinyLog代表表引擎，表引擎在建表的时候必须使用</span><br><span class="line">--插入数据</span><br><span class="line">--这个 x 列只能存储类型定义中列出的值：&#x27;hello&#x27;或&#x27;world&#x27;</span><br><span class="line">insert into t_enum values (&#x27;hello&#x27;),(&#x27;world&#x27;),(&#x27;hello&#x27;);</span><br><span class="line">--查询表</span><br><span class="line">select * from t_enum;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2024-05-21_10-48-25.png" alt="Snipaste_2024-05-21_10-48-25" style="zoom:43%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--插入枚举值也可以</span><br><span class="line">insert into t_enum (x)</span><br><span class="line">values (1),(2),(1);</span><br><span class="line">select * from t_enum;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2024-05-21_10-53-32.png" alt="Snipaste_2024-05-21_10-53-32" style="zoom:43%;">

<img src="Snipaste_2024-05-21_10-49-03.png" alt="Snipaste_2024-05-21_10-49-03" style="zoom:43%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--如果需要看到对应行的数值，则必须将 Enum 值转换为整数类型</span><br><span class="line">SELECT CAST(x, &#x27;Int8&#x27;) FROM t_enum;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2024-05-21_10-50-15.png" alt="Snipaste_2024-05-21_10-50-15" style="zoom:43%;">

<p><strong>使用场景：对一些状态、类型的字段算是一种空间优化，也算是一种数据约束。但是实际使用中往往因为一些数据内容的变化增加一定的维护成本，甚至是数据丢失问题。所以谨慎使用。</strong></p>
<h2 id="3-7-时间类型"><a href="#3-7-时间类型" class="headerlink" title="3.7 时间类型"></a>3.7 时间类型</h2><p>目前 ClickHouse 有三种时间类型</p>
<p>➢ <strong>Date</strong> 接受<strong>年-月-日</strong>的字符串比如 ‘2019-12-16’</p>
<p>➢ <strong>Datetime</strong> 接受<strong>年-月-日 时:分:秒</strong>的字符串比如 ‘2019-12-16 20:50:10’</p>
<p>➢ <strong>Datetime64</strong> 接受<strong>年-月-日 时:分:秒.亚秒</strong>的字符串比如‘2019-12-16 20:50:10.66’</p>
<p>日期类型，用两个字节存储，表示从 1970-01-01 (无符号) 到当前的日期值。</p>
<p>还有很多数据结构，可以参考官方文档：<a target="_blank" rel="noopener" href="https://clickhouse.yandex/docs/zh/data_types/">https://clickhouse.yandex/docs/zh/data_types/</a></p>
<h2 id="3-8-数组"><a href="#3-8-数组" class="headerlink" title="3.8 数组"></a>3.8 数组</h2><p>**Array(T)**：由 T 类型元素组成的数组。</p>
<p>T 可以是任意类型，包含数组类型。 但不推荐使用多维数组，ClickHouse 对多维数组的支持有限。例如，不能在 MergeTree 表中存储多维数组。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--（1））创建数组方式 1，使用 array 函数</span><br><span class="line">--toTypeName(x)函数，返回x列的数据类型</span><br><span class="line">select array(1,2,3) as x, toTypeName(x);</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2024-05-21_11-13-00.png" alt="Snipaste_2024-05-21_11-13-00" style="zoom:43%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--（2）创建数组方式 2：使用方括号</span><br><span class="line">select [1,2,3] as x, toTypeName(x);</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2024-05-21_11-13-00.png" alt="Snipaste_2024-05-21_11-13-00" style="zoom:43%;">

<h1 id="第四章-表引擎（重点）"><a href="#第四章-表引擎（重点）" class="headerlink" title="第四章 表引擎（重点）"></a>第四章 表引擎（重点）</h1><h2 id="4-1-表引擎的使用"><a href="#4-1-表引擎的使用" class="headerlink" title="4.1 表引擎的使用"></a>4.1 表引擎的使用</h2><p>表引擎是 ClickHouse 的一大特色。可以说， 表引擎决定了如何存储表的数据。包括：</p>
<p>➢ 数据的存储方式和位置，写到哪里以及从哪里读取数据。</p>
<p>➢ 支持哪些查询以及如何支持。</p>
<p>➢ 并发数据访问。</p>
<p>➢ 索引的使用（如果存在）。</p>
<p>➢ 是否可以执行多线程请求。</p>
<p>➢ 数据复制参数。</p>
<p><strong>表引擎的使用方式就是必须显式在创建表时定义该表使用的引擎，以及引擎使用的相关参数。</strong></p>
<p><strong>特别注意：引擎的名称大小写敏感</strong></p>
<h2 id="4-2-TinyLog（测试用）"><a href="#4-2-TinyLog（测试用）" class="headerlink" title="4.2 TinyLog（测试用）"></a>4.2 TinyLog（测试用）</h2><p>以列文件的形式保存在磁盘上，不支持索引，没有并发控制。一般保存少量数据的小表，生产环境上作用有限。可以用于平时练习测试用。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create table t_enum</span><br><span class="line">(</span><br><span class="line">    x Enum8(&#x27;hello&#x27; = 1, &#x27;world&#x27; = 2)</span><br><span class="line">)</span><br><span class="line">ENGINE = TinyLog;--TinyLog代表表引擎，表引擎在建表的时候必须使用</span><br></pre></td></tr></table></figure>

<h2 id="4-3-Memory（测试用）"><a href="#4-3-Memory（测试用）" class="headerlink" title="4.3 Memory（测试用）"></a>4.3 Memory（测试用）</h2><p>内存引擎，数据以未压缩的原始形式直接保存在内存当中，服务器重启数据就会消失。读写操作不会相互阻塞，不支持索引。简单查询下有非常非常高的性能表现（超过 10G&#x2F;s）。</p>
<p>一般用到它的地方不多，除了用来测试，就是在需要非常高的性能，同时数据量又不太大（上限大概 1 亿行）的场景。</p>
<h2 id="4-4-MergeTree（最常用）"><a href="#4-4-MergeTree（最常用）" class="headerlink" title="4.4 MergeTree（最常用）"></a>4.4 MergeTree（最常用）</h2><p>ClickHouse 中最强大的表引擎当属 MergeTree（合并树）引擎及该系列（*MergeTree）中的其他引擎，支持索引和分区，地位可以相当于 innodb 之于 Mysql。而且基于 MergeTree，还衍生除了很多小弟，也是非常有特色的引擎。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">--建表</span><br><span class="line">create table t_order_mt(</span><br><span class="line">   id UInt32,</span><br><span class="line">   sku_id String,</span><br><span class="line">   total_amount Decimal(16,2),</span><br><span class="line">   create_time Datetime</span><br><span class="line">) engine =MergeTree</span><br><span class="line">  partition by toYYYYMMDD(create_time) --toYYYYMMDD()日期格式化函数</span><br><span class="line">  primary key (id) --主键约束，注意：ck中的主键没有唯一性约束，也就是id可以重复</span><br><span class="line">  order by (id,sku_id);--在分区内先按照id排序，再按照sku_id排序</span><br><span class="line">--插入数据</span><br><span class="line">insert into t_order_mt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br><span class="line">--查询</span><br><span class="line">select * from t_order_mt;</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2024-05-21_11-55-53.png" alt="Snipaste_2024-05-21_11-55-53"></p>
<h3 id="4-4-1-partition-by分区（可选）"><a href="#4-4-1-partition-by分区（可选）" class="headerlink" title="4.4.1 partition by分区（可选）"></a>4.4.1 partition by分区（可选）</h3><p>（1）作用：分区的目的是降低扫描的范围，优化查询速度</p>
<p><strong>若不填，只会使用一个分区，目录名即为all</strong></p>
<p>（2）分区目录</p>
<p>MergeTree 是以列文件+索引文件+表定义文件组成的，但是如果设定了分区那么这些文件就会保存到不同的分区目录中。</p>
<p>（3）并行</p>
<p>分区后，面对涉及跨分区的查询统计，ClickHouse 会以分区为单位并行处理。<strong>一个分区一个线程</strong></p>
<p>（4）数据写入与分区合并</p>
<p>任何一个批次的数据写入都会产生一个临时分区，不会纳入任何一个已有的分区。写入后的某个时刻（大概 10-15 分钟后），ClickHouse 会自动执行合并操作（等不及也可以手动通过 optimize 执行），把临时分区的数据，合并到已有分区中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimize table xxxx（表名） final;</span><br></pre></td></tr></table></figure>

<h4 id="4-4-1-1-ck数据文件解析"><a href="#4-4-1-1-ck数据文件解析" class="headerlink" title="4.4.1.1 ck数据文件解析"></a>4.4.1.1 ck数据文件解析</h4><p>进入到本地磁盘中存储clickhouse数据文件的目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 metadata]# cd /var/lib/clickhouse/</span><br><span class="line">[root@hadoop102 clickhouse]# ll</span><br><span class="line">总用量 4</span><br><span class="line">drwxr-x---  2 clickhouse clickhouse 116 5月  19 23:08 access</span><br><span class="line">drwxr-x---  2 clickhouse clickhouse   6 5月  19 23:08 cores</span><br><span class="line">drwxr-x---  5 clickhouse clickhouse  47 5月  21 10:44 data</span><br><span class="line">drwxr-x---  2 clickhouse clickhouse   6 5月  19 23:08 dictionaries_lib</span><br><span class="line">drwxr-x---  2 clickhouse clickhouse   6 5月  19 23:08 flags</span><br><span class="line">drwxr-x---  2 clickhouse clickhouse   6 5月  19 23:08 format_schemas</span><br><span class="line">drwxr-x---  2 clickhouse clickhouse 100 5月  21 11:28 metadata</span><br><span class="line">drwxr-x---  2 clickhouse clickhouse   6 5月  19 23:08 metadata_dropped</span><br><span class="line">drwxr-x---  2 clickhouse clickhouse  41 5月  19 23:08 preprocessed_configs</span><br><span class="line">-rw-r-----  1 clickhouse clickhouse  58 5月  20 14:45 status</span><br><span class="line">drwxr-x--- 13 clickhouse clickhouse 127 5月  21 11:45 store</span><br><span class="line">drwxr-x---  2 clickhouse clickhouse   6 5月  19 23:08 tmp</span><br><span class="line">drwxr-x---  2 clickhouse clickhouse   6 5月  19 23:08 user_files</span><br></pre></td></tr></table></figure>

<p>进入ck的元数据目录，可以看到数据库和库中的数据表，还有每张表的建表语句</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]# cd metadata</span><br><span class="line">[root@hadoop102 metadata]# ll</span><br><span class="line">总用量 12</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  19 23:08 default -&gt; /var/lib/clickhouse/store/69c/69cbf2dd-f23d-4c51-a9cb-f2ddf23dfc51/</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 78 5月  19 23:08 default.sql</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  19 23:08 system -&gt; /var/lib/clickhouse/store/cda/cdadbf5e-1fef-466a-8dad-bf5e1fef366a/</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 78 5月  19 23:08 system.sql</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  21 10:44 test -&gt; /var/lib/clickhouse/store/122/122b549c-a10a-41ce-922b-549ca10a41ce/</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 78 5月  21 10:44 test.sql</span><br><span class="line">[root@hadoop102 metadata]# cd test</span><br><span class="line">[root@hadoop102 test]# ll</span><br><span class="line">总用量 8</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 151 5月  21 10:56 t_enum_1.sql</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 120 5月  21 10:44 t_enum.sql</span><br><span class="line">[root@hadoop102 test]# vim t_enum.sql </span><br></pre></td></tr></table></figure>

<img src="Snipaste_2024-05-21_14-20-32.png" alt="Snipaste_2024-05-21_14-20-32" style="zoom:43%;">

<p>进入ck的数据目录，可以用一个目录去存储一个数据库</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse]# cd data/</span><br><span class="line">[root@hadoop102 data]# ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse  24 5月  21 11:45 default</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse 113 5月  20 14:45 system</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse  36 5月  21 10:56 test</span><br></pre></td></tr></table></figure>

<p>先看一下使用TinyLog引擎的数据表：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 data]# cd test/</span><br><span class="line">[root@hadoop102 test]# ll</span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  21 10:44 t_enum -&gt; /var/lib/clickhouse/store/967/967e6ef6-91be-4d1e-967e-6ef691be9d1e/</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  21 10:56 t_enum_1 -&gt; /var/lib/clickhouse/store/7b5/7b59fc27-75f0-43d8-bb59-fc2775f0d3d8/</span><br><span class="line">[root@hadoop102 test]# cd t_enum_1/</span><br><span class="line">[root@hadoop102 t_enum_1]# ll</span><br><span class="line">总用量 16</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 84 5月  21 10:57 sizes.json</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 55 5月  21 10:57 x.bin</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 58 5月  21 10:57 y.bin</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 63 5月  21 10:57 z.bin</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#######</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可以发现使用TinyLog引擎的数据表的数据文件包含两类，一类是json文件，一类是bin文件代表每个列，每个列有一个文件，印证了ck是列式存储的事实（.bin文件存储的是真正的数据）</span></span><br></pre></td></tr></table></figure>

<p>再看一下使用MergeTree引擎的数据表：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 data]# cd default/</span><br><span class="line">[root@hadoop102 default]# ll</span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  21 11:45 t_order_mt -&gt; /var/lib/clickhouse/store/d55/d55da05f-ce1a-4e57-955d-a05fce1aae57/</span><br><span class="line">[root@hadoop102 default]# cd t_order_mt/</span><br><span class="line">[root@hadoop102 t_order_mt]# ll</span><br><span class="line">总用量 4</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse 203 5月  21 11:45 20200601_1_1_0</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse 203 5月  21 11:45 20200602_2_2_0</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse   6 5月  21 11:45 detached</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse   1 5月  21 11:45 format_version.txt</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#######</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可以发现使用MergeTree引擎的数据表的数据文件包含三类，一类是detached文件，一类是format_version文件，一类是分区数据，每一个分区拥有一个目录</span></span><br><span class="line">[root@hadoop102 t_order_mt]# cd 20200601_1_1_0/</span><br><span class="line">[root@hadoop102 20200601_1_1_0]# ll</span><br><span class="line">总用量 36</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 259 5月  21 11:45 checksums.txt</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 118 5月  21 11:45 columns.txt</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse   1 5月  21 11:45 count.txt</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 189 5月  21 11:45 data.bin</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 144 5月  21 11:45 data.mrk3</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse  10 5月  21 11:45 default_compression_codec.txt</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse   8 5月  21 11:45 minmax_create_time.idx</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse   4 5月  21 11:45 partition.dat</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse   8 5月  21 11:45 primary.idx</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#######</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">打开某个分区目录，可以看到以上文件</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">data.bin：记录了该分区下的数据</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">data.mrk3：记录了该分区的数据偏移量（标记文件），标记文件在idx索引文件和bin数据文件之间起到了桥梁的作用，以mrk3结尾的文件，表示该表启用了自适应索引间隔</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">count.txt：记录了该分区下的数据的行数</span></span><br><span class="line">[root@hadoop102 20200601_1_1_0]# cat count.txt </span><br><span class="line">5</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">columns.txt：记录了表的列的信息</span></span><br><span class="line">[root@hadoop102 20200601_1_1_0]# cat columns.txt </span><br><span class="line">columns format version: 1</span><br><span class="line">4 columns:</span><br><span class="line">`id` UInt32</span><br><span class="line">`sku_id` String</span><br><span class="line">`total_amount` Decimal(16, 2)</span><br><span class="line">`create_time` DateTime</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">checksums.txt：校验信息，用于校验各个文件的正确性。存放各个文件的size以及<span class="built_in">hash</span>值</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">primary.idx：主键的索引文件，用于加快查询效率</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">partition.dat：分区索引</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">minmax_create_time.idx：分区键的最大最小值</span></span><br></pre></td></tr></table></figure>

<p><strong>关于分区目录名20200601_1_1_0的解析</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">PartitionId_MinBlockNum_MaxBlockNum_Level</span><br><span class="line">分区值_最小分区块编号_最大分区块编号_合并层级</span><br><span class="line">    =》PartitionId</span><br><span class="line">        数据分区ID生成规则</span><br><span class="line">        数据分区规则由分区ID决定，分区ID由PARTITION BY分区键决定。根据分区键字段类型，ID生成规则可分为：</span><br><span class="line">            未定义分区键</span><br><span class="line">                没有定义PARTITION BY，默认生成一个目录名为all的数据分区，所有数据均存放在all目录下。</span><br><span class="line"></span><br><span class="line">            整型分区键</span><br><span class="line">                分区键为整型，那么直接用该整型值的字符串形式做为分区ID。</span><br><span class="line"></span><br><span class="line">            日期类分区键</span><br><span class="line">                分区键为日期类型，或者可以转化成日期类型。</span><br><span class="line"></span><br><span class="line">            其他类型分区键</span><br><span class="line">                String、Float类型等，通过128位的Hash算法取其Hash值作为分区ID。</span><br><span class="line">    =》MinBlockNum</span><br><span class="line">        最小分区块编号，自增类型，从1开始向上递增。每产生一个新的目录分区就向上递增一个数字。</span><br><span class="line">    =》MaxBlockNum</span><br><span class="line">        最大分区块编号，新创建的分区MinBlockNum等于MaxBlockNum的编号。</span><br><span class="line">    =》Level</span><br><span class="line">        合并的层级，被合并的次数。合并次数越多，层级值越大。</span><br></pre></td></tr></table></figure>

<h3 id="4-4-2-primary-key主键（可选）"><a href="#4-4-2-primary-key主键（可选）" class="headerlink" title="4.4.2 primary key主键（可选）"></a>4.4.2 primary key主键（可选）</h3><p>ClickHouse 中的主键，和其他数据库不太一样，<strong>它只提供了数据的一级索引，但是却不是唯一约束。</strong>这就意味着是可以存在相同 primary key 的数据的。</p>
<p>主键的设定主要依据是查询语句中的 where 条件。</p>
<p>根据条件通过对主键进行某种形式的二分查找，能够定位到对应的 index granularity,避免了全表扫描。</p>
<p>index granularity： 直接翻译的话就是索引粒度，指在稀疏索引中两个相邻索引对应数据的间隔。ClickHouse 中的 MergeTree 默认是 8192。官方不建议修改这个值，除非该列存在大量重复值，比如在一个分区中几万行才有一个不同数据。</p>
<p>稀疏索引：</p>
<p><img src="Snipaste_2024-05-21_15-20-34.png" alt="Snipaste_2024-05-21_15-20-34"></p>
<p>稀疏索引的好处就是可以用很少的索引数据，定位更多的数据，代价就是只能定位到索引粒度的第一行，然后再进行进行一点扫描。</p>
<h3 id="4-4-3-order-by（必选）"><a href="#4-4-3-order-by（必选）" class="headerlink" title="4.4.3 order by（必选）"></a>4.4.3 order by（必选）</h3><p><strong>注意：这里面工作有涉及</strong></p>
<p>order by 设定了<strong>分区内</strong>的数据按照哪些字段顺序进行有序保存。</p>
<p>order by 是 MergeTree 中唯一一个必填项，甚至比 primary key 还重要，因为当用户不设置主键的情况，很多处理会依照 order by 的字段进行处理（比如后面会讲的去重和汇总）。</p>
<p><strong>要求：主键必须是 order by 字段的前缀字段。</strong></p>
<p>比如 order by 字段是 (id,sku_id) 那么主键必须是 id 或者(id,sku_id)</p>
<h3 id="4-4-4-二级索引"><a href="#4-4-4-二级索引" class="headerlink" title="4.4.4 二级索引"></a>4.4.4 二级索引</h3><p>目前在 ClickHouse 的官网上二级索引的功能在 v20.1.2.4 之前是被标注为实验性的，在这个版本之后默认是开启的。（v20.1.2.4 开始，这个参数已被删除，默认开启）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set allow_experimental_data_skipping_indices=1;</span><br></pre></td></tr></table></figure>

<p>例子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">--建表</span><br><span class="line">create table t_order_mt2(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2),</span><br><span class="line">    create_time Datetime,</span><br><span class="line">    INDEX a total_amount TYPE minmax GRANULARITY 5</span><br><span class="line">) engine =MergeTree</span><br><span class="line">partition by toYYYYMMDD(create_time)</span><br><span class="line">primary key (id)</span><br><span class="line">order by (id, sku_id);</span><br><span class="line">--INDEX：二级索引语法</span><br><span class="line">--a total_amount：a表示索引名，total_amount表示以该列建立索引</span><br><span class="line">--TYPE minmax：索引类型位最小最大值，[min,max]</span><br><span class="line">--GRANULARITY N ：设定二级索引对于一级索引粒度的粒度，N个一级索引汇总一次变为一个二级索引</span><br><span class="line"></span><br><span class="line">--插入数据</span><br><span class="line">insert into t_order_mt2 values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">那么在使用下面语句进行测试，可以看出二级索引能够为非主键字段的查询发挥作用</span></span><br><span class="line">[root@hadoop102 20200601_1_1_0]#  clickhouse-client --send_logs_level=trace &lt;&lt;&lt; &#x27;select * from t_order_mt2 where total_amount &gt; toDecimal32(900., 2)&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2024-05-21_15-50-31.png" alt="Snipaste_2024-05-21_15-50-31"></p>
<p>进入对应表的分区目录中，可以看到对应的索引文件</p>
<p><img src="Snipaste_2024-05-21_15-53-01.png" alt="Snipaste_2024-05-21_15-53-01"></p>
<h3 id="4-4-5-数据TTL"><a href="#4-4-5-数据TTL" class="headerlink" title="4.4.5 数据TTL"></a>4.4.5 数据TTL</h3><p>TTL 即 Time To Live，MergeTree 提供了可以管理数据<strong>表</strong>或者<strong>列</strong>的生命周期的功能。超过过期时间，表级别TTL将符合条件的一行删除，列级别TTL将符合条件的列值置为默认值。</p>
<p>（1）列级别TTL</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">--建表</span><br><span class="line">create table t_order_mt3(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2) TTL create_time+interval 10 SECOND,--列级别TTL，生命周期为create_time列的值加上10s，也就是说10s后total_amount列的值过期</span><br><span class="line">    create_time Datetime</span><br><span class="line">) engine =MergeTree</span><br><span class="line">partition by toYYYYMMDD(create_time)</span><br><span class="line">primary key (id)</span><br><span class="line">order by (id, sku_id);</span><br><span class="line">--插入数据</span><br><span class="line">insert into t_order_mt3 values</span><br><span class="line">(106,&#x27;sku_001&#x27;,1000.00,&#x27;2024-05-21 16:06:30&#x27;),</span><br><span class="line">(107,&#x27;sku_002&#x27;,2000.00,&#x27;2024-05-21 16:06:40&#x27;),</span><br><span class="line">(110,&#x27;sku_003&#x27;,600.00,&#x27;2024-05-21 16:06:50&#x27;);</span><br><span class="line">--手动合并</span><br><span class="line">optimize table t_order_mt3 final;</span><br><span class="line">select * from t_order_mt3;</span><br></pre></td></tr></table></figure>

<p>发现超时的列值变为默认值</p>
<p><img src="Snipaste_2024-05-21_16-08-47.png" alt="Snipaste_2024-05-21_16-08-47"></p>
<p>（2）表级别TTL</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--下面的这条语句是数据会在 create_time 之后 10 秒丢失</span><br><span class="line">alter table t_order_mt3 MODIFY TTL create_time + INTERVAL 10 SECOND;</span><br><span class="line">select * from t_order_mt3;</span><br></pre></td></tr></table></figure>

<p>已经将超时的行都删除了</p>
<p><img src="Snipaste_2024-05-21_16-12-09.png" alt="Snipaste_2024-05-21_16-12-09"></p>
<p>涉及判断的字段必须是 Date 或者 Datetime 类型，推荐使用分区的日期字段。</p>
<p>能够使用的时间周期：</p>
<p>- SECOND</p>
<p>- MINUTE</p>
<p>- HOUR</p>
<p>- DAY</p>
<p>- WEEK</p>
<p>- MONTH</p>
<p>- QUARTER</p>
<p>- YEAR</p>
<h2 id="4-5-ReplacingMergeTree（去重）"><a href="#4-5-ReplacingMergeTree（去重）" class="headerlink" title="4.5 ReplacingMergeTree（去重）"></a>4.5 ReplacingMergeTree（去重）</h2><p>ReplacingMergeTree 是 MergeTree 的一个变种，它存储特性完全继承 MergeTree，只是多了一个<strong>去重</strong>的功能。 尽管 MergeTree 可以设置主键，但是 primary key 其实没有唯一约束的功能。如果你想处理掉重复的数据，可以借助这个 ReplacingMergeTree。<strong>该引擎的去重是按照排序字段去重的</strong></p>
<p>（1）去重时机</p>
<p><strong>数据的去重只会在合并的过程中出现</strong>。合并会在未知的时间在后台进行，所以你无法预先作出计划。有一些数据可能仍未被处理。</p>
<p>（2）去重范围</p>
<p><strong>如果表经过了分区，去重只会在分区内部进行去重，不能执行跨分区的去重。</strong></p>
<p>所以 ReplacingMergeTree 能力有限， ReplacingMergeTree 适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现。</p>
<p>（3）例子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">--建表</span><br><span class="line">create table t_order_rmt(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2) ,</span><br><span class="line">    create_time Datetime</span><br><span class="line">) engine =ReplacingMergeTree(create_time)--去重后保留create_time字段值最大的</span><br><span class="line">--ReplacingMergeTree() 填入的参数为版本字段，重复数据保留版本字段值最大的。如果不填版本字段，默认按照插入顺序保留最后一条。</span><br><span class="line">partition by toYYYYMMDD(create_time)</span><br><span class="line">primary key (id)</span><br><span class="line">order by (id, sku_id);--排序字段是(id,sku_id)，那么就是按照(id,sku_id)去重</span><br><span class="line">--插入数据</span><br><span class="line">insert into t_order_rmt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br><span class="line">--查询</span><br><span class="line">select * from t_order_rmt;</span><br></pre></td></tr></table></figure>

<p>可以发现，查询的结果已经是按规定去重之后的了</p>
<p><img src="Snipaste_2024-05-21_16-47-44.png" alt="Snipaste_2024-05-21_16-47-44"></p>
<p>（4）结论</p>
<p>➢ 实际上是使用 order by 字段作为唯一键</p>
<p>➢ 去重不能跨分区</p>
<p>➢ 只有同一批插入（新版本）或合并分区时才会进行去重</p>
<p>➢ 认定重复的数据保留，版本字段值最大的</p>
<p>➢ 如果版本字段相同则按插入顺序保留最后一笔</p>
<h2 id="4-6-SummingMergeTree（预聚合）"><a href="#4-6-SummingMergeTree（预聚合）" class="headerlink" title="4.6 SummingMergeTree（预聚合）"></a>4.6 SummingMergeTree（预聚合）</h2><p>对于不查询明细，只关心<strong>以维度进行汇总聚合结果</strong>的场景。如果只使用普通的MergeTree的话，无论是存储空间的开销，还是查询时临时聚合的开销都比较大。</p>
<p>ClickHouse 为了这种场景，提供了一种能够<strong>“预聚合”</strong>的引擎 SummingMergeTree</p>
<p>（1）例子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">--建表</span><br><span class="line">create table t_order_smt(</span><br><span class="line">    id UInt32,</span><br><span class="line">    sku_id String,</span><br><span class="line">    total_amount Decimal(16,2) ,</span><br><span class="line">    create_time Datetime</span><br><span class="line">) engine =SummingMergeTree(total_amount) --聚合字段为total_amount，进行sum(total_amount)</span><br><span class="line">partition by toYYYYMMDD(create_time)</span><br><span class="line">primary key (id)</span><br><span class="line">order by (id,sku_id );--排序字段是(id,sku_id)，那么就是按照(id,sku_id)去分组聚合</span><br><span class="line">--插入数据</span><br><span class="line">insert into t_order_smt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br><span class="line">--查询</span><br><span class="line">select * from t_order_smt;</span><br></pre></td></tr></table></figure>

<p>可以发现，查询结果已经是分组聚合（sum(total_amount) group by id,sku_id）</p>
<p><img src="Snipaste_2024-05-21_17-06-54.png" alt="Snipaste_2024-05-21_17-06-54"></p>
<p>（2）结论</p>
<p>➢ 以 SummingMergeTree（）中指定的列作为汇总数据列</p>
<p>➢ 可以填写多列必须数字列，如果不填，以所有非维度列且为数字列的字段为汇总数据列</p>
<p>➢ 以 order by 的列为准，作为维度列</p>
<p>➢ 其他的列按插入顺序保留第一行</p>
<p>➢ 不在一个分区的数据不会被聚合</p>
<p>➢ 只有在同一批次插入(新版本)或分片合并时才会进行聚合</p>
<p>（3）开发建议</p>
<p>设计聚合表的话，唯一键值、流水号可以去掉，所有字段全部是维度、度量或者时间戳。</p>
<p>（4）问题</p>
<p>能不能直接执行以下 SQL 得到汇总值 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select total_amount from XXX where province_name=’’ and create_date=’xxx’</span><br></pre></td></tr></table></figure>

<p>不行，可能会包含一些还没来得及聚合的临时明细，如果要是获取汇总值，还是需要使用 sum 进行聚合，这样效率会有一定的提高，但本身 ClickHouse 是列式存储的，效率提升有限，不会特别明显。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select sum(total_amount) from XXX province_name=’’ and create_date=‘xxx’</span><br></pre></td></tr></table></figure>

<h1 id="第五章-SQL操作"><a href="#第五章-SQL操作" class="headerlink" title="第五章 SQL操作"></a>第五章 SQL操作</h1><p>基本上来说传统关系型数据库（以 MySQL 为例）的 SQL 语句，ClickHouse 基本都支持，这里不会从头讲解 SQL 语法只介绍 ClickHouse 与标准 SQL（MySQL）不一致的地方。</p>
<h2 id="5-1-Insert"><a href="#5-1-Insert" class="headerlink" title="5.1 Insert"></a>5.1 Insert</h2><p>基本与标准 SQL（MySQL）基本一致</p>
<p>（1）标准</p>
<p>insert into [table_name] values(…),(….) </p>
<p>（2）从表到表的插入</p>
<p>insert into [table_name] select a,b,c from [table_name_2]</p>
<h2 id="5-2-Update和Delete"><a href="#5-2-Update和Delete" class="headerlink" title="5.2 Update和Delete"></a>5.2 Update和Delete</h2><p>ClickHouse 提供了 Delete 和 Update 的能力，这类操作被称为 Mutation 查询，它可以看做 Alter 的一种。</p>
<p>虽然可以实现修改和删除，但是和一般的 OLTP 数据库不一样，<strong>Mutation</strong> <strong>语句是一种很“重”的操作，而且不支持事务。</strong></p>
<p>“重”的原因主要是每次修改或者删除都会导致放弃目标数据的原有分区，重建新分区。所以尽量做批量的变更，不要进行频繁小数据的操作。</p>
<p>（1）删除操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alter table t_order_smt delete where sku_id =&#x27;sku_001&#x27;;</span><br><span class="line">select * from t_order_smt;</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2024-05-21_17-43-12.png" alt="Snipaste_2024-05-21_17-43-12"></p>
<p>（2）修改操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alter table t_order_smt update total_amount=toDecimal32(2000.00,2) where id = 102;</span><br><span class="line">select * from t_order_smt;</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2024-05-21_17-47-11.png" alt="Snipaste_2024-05-21_17-47-11"></p>
<p>由于操作比较“重”，所以 Mutation 语句分两步执行，同步执行的部分其实只是进行新增数据新增分区和并把旧分区打上逻辑上的失效标记。直到触发分区合并的时候，才会删除旧数据释放磁盘空间，一般不会开放这样的功能给用户，由管理员完成。</p>
<p><strong>由此引申几个问题</strong></p>
<p><strong>①Clickhouse是否支持事务     —不支持</strong></p>
<p><strong>②Clickhouse是否支持更新和删除       –支持，只是是间接实现的</strong></p>
<p><strong>③如何实现高性能update或delete</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">--实现高性能update或delete的思路：</span><br><span class="line">create table  A</span><br><span class="line">(</span><br><span class="line">a xxx,</span><br><span class="line">b xxx,</span><br><span class="line">c xxx,</span><br><span class="line">_sign UInt8,</span><br><span class="line">_version UInt32</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">==&gt; 更新 ：  插入一条新的数据，   _version + 1 </span><br><span class="line">    =》 查询的时候加上一个过滤条件，  where version最大</span><br><span class="line"></span><br><span class="line">==&gt; 删除： _sign,   0表示未删除，1表示已删除， 同时 version + 1</span><br><span class="line">    =》 查询的时候加上一个过滤条件， where  _sign=0 and version最大</span><br><span class="line">    </span><br><span class="line">==&gt; 时间久了，数据膨胀了 ==》 类似合并机制，怎么把过期数据清除掉</span><br></pre></td></tr></table></figure>

<h2 id="5-3-查询操作"><a href="#5-3-查询操作" class="headerlink" title="5.3 查询操作"></a>5.3 查询操作</h2><p>ClickHouse 基本上与标准 SQL 差别不大</p>
<p>➢ 支持子查询</p>
<p>➢ 支持 CTE(Common Table Expression 公用表表达式 with 子句)</p>
<p>➢ 支持各种 JOIN，但是 JOIN 操作无法使用缓存，所以即使是两次相同的 JOIN 语句，ClickHouse 也会视为两条新 SQL</p>
<p>➢ 窗口函数(官方正在测试中…)</p>
<p>➢ 不支持自定义函数</p>
<p>➢ GROUP BY 操作增加了 with rollup\with cube\with total 用来计算小计和总计。</p>
<p><strong>详细解析with rollup\with cube\with total</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">--rollup上卷</span><br><span class="line">select a,b,sum(c) from tmp group by a,b with rollup;</span><br><span class="line">--以上语句等价于</span><br><span class="line">select null,null,sum(c) from tmp </span><br><span class="line">union </span><br><span class="line">select a,null,sum(c) from tmp group by a</span><br><span class="line">union </span><br><span class="line">select a,b,sum(c) from tmp group by a,b</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">--cube多维分析</span><br><span class="line">select a,b,sum(c) from tmp group by a,b with cube;</span><br><span class="line">--以上语句等价于</span><br><span class="line">select null,null,sum(c) from tmp </span><br><span class="line">union </span><br><span class="line">select a,null,sum(c) from tmp group by a</span><br><span class="line">union </span><br><span class="line">select null,b,sum(c) from tmp group by b</span><br><span class="line">union </span><br><span class="line">select a,b,sum(c) from tmp group by a,b</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">--total总计</span><br><span class="line">select a,b,sum(c) from tmp group by a,b with total;</span><br><span class="line">--以上语句等价于</span><br><span class="line">select null,null,sum(c) from tmp  </span><br><span class="line">union </span><br><span class="line">select a,b,sum(c) from tmp group by a,b</span><br></pre></td></tr></table></figure>

<p>例子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">--先将表中数据都删除</span><br><span class="line">alter table t_order_mt delete where 1=1;</span><br><span class="line">--再重新插入数据</span><br><span class="line">insert into t_order_mt values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(101,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(103,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(104,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(105,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;),</span><br><span class="line">(106,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-04 12:00:00&#x27;),</span><br><span class="line">(107,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-04 12:00:00&#x27;),</span><br><span class="line">(108,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-04 12:00:00&#x27;),</span><br><span class="line">(109,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-04 12:00:00&#x27;),</span><br><span class="line">(110,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-01 12:00:00&#x27;);</span><br><span class="line">--查询</span><br><span class="line">select * from t_order_mt;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2024-05-22_20-46-29.png" alt="Snipaste_2024-05-22_20-46-29" style="zoom:43%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">--with rollup:从右至左去掉维度进行统计</span><br><span class="line">select id,</span><br><span class="line">       sku_id,</span><br><span class="line">       sum(total_amount)</span><br><span class="line">from t_order_mt</span><br><span class="line">group by id,sku_id with rollup;</span><br></pre></td></tr></table></figure>

<p>​                                                                           group by id, sku_id</p>
<img src="Snipaste_2024-05-22_20-56-12.png" alt="Snipaste_2024-05-22_20-56-12" style="zoom:43%;">

<p>​																			group by id</p>
<img src="Snipaste_2024-05-22_20-56-38.png" alt="Snipaste_2024-05-22_20-56-38" style="zoom:43%;">

<p>​																			group by </p>
<img src="Snipaste_2024-05-22_20-56-57.png" alt="Snipaste_2024-05-22_20-56-57" style="zoom:43%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">--with cube : 从右至左去掉维度进行统计，再从左至右去掉维度进行统计</span><br><span class="line">select id,</span><br><span class="line">       sku_id,</span><br><span class="line">       sum(total_amount)</span><br><span class="line">from t_order_mt</span><br><span class="line">group by id,sku_id with cube;</span><br></pre></td></tr></table></figure>

<p>​																		    group by id, sku_id</p>
<img src="Snipaste_2024-05-22_20-56-12.png" alt="Snipaste_2024-05-22_20-56-12" style="zoom:43%;">

<p>​																			group by id</p>
<img src="Snipaste_2024-05-22_20-56-38.png" alt="Snipaste_2024-05-22_20-56-38" style="zoom:43%;">

<p>​																			group by sku_id</p>
<img src="Snipaste_2024-05-22_21-04-48.png" alt="Snipaste_2024-05-22_21-04-48" style="zoom:43%;">

<p>​																			group by</p>
<img src="Snipaste_2024-05-22_20-56-57.png" alt="Snipaste_2024-05-22_20-56-57" style="zoom:43%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-- with totals: 只计算合计--注意：此处使用第三方数据库图形化操作工具查询出的结果不符合预期，使用clickhouse终端查询符合预期</span><br><span class="line">select id,</span><br><span class="line">       sku_id,</span><br><span class="line">       sum(total_amount)</span><br><span class="line">from t_order_mt</span><br><span class="line">group by id,sku_id with totals ;           </span><br></pre></td></tr></table></figure>

<img src="Snipaste_2024-05-22_21-13-09.png" alt="Snipaste_2024-05-22_21-13-09" style="zoom:43%;">

<h2 id="5-4-alter操作"><a href="#5-4-alter操作" class="headerlink" title="5.4 alter操作"></a>5.4 alter操作</h2><p>与Mysql的修改字段基本一致（<strong>注意：Hive表一般不会删除表的字段</strong>），ck是基于列式存储的，对于新增和删除字段自由度很大</p>
<p>（1）新增字段</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table tableName add column newcolname String after col1;</span><br></pre></td></tr></table></figure>

<p>（2）修改字段类型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table tableName modify column newcolname String;</span><br></pre></td></tr></table></figure>

<p>（3）删除字段</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table tableName drop column newcolname;</span><br></pre></td></tr></table></figure>

<h2 id="5-5-导出数据（很少使用）"><a href="#5-5-导出数据（很少使用）" class="headerlink" title="5.5 导出数据（很少使用）"></a>5.5 导出数据（很少使用）</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clickhouse-client --query &quot;select * from t_order_mt where create_time=&#x27;2020-06-01 12:00:00&#x27;&quot; --format CSVWithNames&gt; /opt/module/data/rs1.csv</span><br></pre></td></tr></table></figure>

<p>用ck存的表一般为宽表，一般直接用BI工具查询或者看板展示就可以了</p>
<p>更多支持格式参照：<a target="_blank" rel="noopener" href="https://clickhouse.tech/docs/en/interfaces/formats/">https://clickhouse.tech/docs/en/interfaces/formats/</a></p>
<h2 id="5-6-查看ck的执行计划"><a href="#5-6-查看ck的执行计划" class="headerlink" title="5.6 查看ck的执行计划"></a>5.6 查看ck的执行计划</h2><p>在 clickhouse 20.6 版本之前要查看 SQL 语句的执行计划需要设置日志级别为 trace 才能 可以看到，并且只能真正执行 sql，在执行日志里面查看。在 20.6 版本引入了原生的执行计 划的语法。在 20.6.3 版本成为正式版本的功能。 本文档基于目前较新稳定版 21.7.3.14。</p>
<h3 id="5-6-1-基本语法"><a href="#5-6-1-基本语法" class="headerlink" title="5.6.1 基本语法"></a>5.6.1 基本语法</h3><p><img src="image-20250521154605363.png" alt="image-20250521154605363"></p>
<h3 id="5-6-2-案例实操"><a href="#5-6-2-案例实操" class="headerlink" title="5.6.2 案例实操"></a>5.6.2 案例实操</h3><p>（1）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">explain plan <span class="keyword">select</span> arrayJoin([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="keyword">null</span>,<span class="keyword">null</span>]);</span><br></pre></td></tr></table></figure>

<p><img src="image-20250521155545146.png" alt="image-20250521155545146"></p>
<p>（2）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">explain <span class="keyword">select</span> database,<span class="keyword">table</span>,<span class="built_in">count</span>(<span class="number">1</span>) cnt <span class="keyword">from</span> system.parts <span class="keyword">where</span></span><br><span class="line">        database <span class="keyword">in</span> (<span class="string">&#x27;datasets&#x27;</span>,<span class="string">&#x27;system&#x27;</span>) <span class="keyword">group</span> <span class="keyword">by</span> database,<span class="keyword">table</span> <span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">        database,cnt <span class="keyword">desc</span> limit <span class="number">2</span> <span class="keyword">by</span> database;</span><br></pre></td></tr></table></figure>

<p><img src="image-20250521155713273.png" alt="image-20250521155713273"></p>
<p>（3）打开全部参数的plan执行计划</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN header<span class="operator">=</span><span class="number">1</span>, actions<span class="operator">=</span><span class="number">1</span>,description<span class="operator">=</span><span class="number">1</span> <span class="keyword">SELECT</span> number <span class="keyword">from</span></span><br><span class="line">    system.numbers limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure>

<p><img src="image-20250521160024893.png" alt="image-20250521160024893"></p>
<p>（4）查看AST语法树</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN AST <span class="keyword">SELECT</span> number <span class="keyword">from</span> system.numbers limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure>

<img src="image-20250521160155895.png" alt="image-20250521160155895" style="zoom:50%;">

<p>（5）SYNTAX 语法优化</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--先做一次查询</span></span><br><span class="line"><span class="keyword">SELECT</span> number <span class="operator">=</span> <span class="number">1</span> ? <span class="string">&#x27;hello&#x27;</span> : (number <span class="operator">=</span> <span class="number">2</span> ? <span class="string">&#x27;world&#x27;</span> : <span class="string">&#x27;atguigu&#x27;</span>) <span class="keyword">FROM</span></span><br><span class="line">    numbers(<span class="number">10</span>);</span><br></pre></td></tr></table></figure>

<img src="image-20250522204930754.png" alt="image-20250522204930754" style="zoom:50%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查看语法优化</span></span><br><span class="line">EXPLAIN SYNTAX <span class="keyword">SELECT</span> number <span class="operator">=</span> <span class="number">1</span> ? <span class="string">&#x27;hello&#x27;</span> : (number <span class="operator">=</span> <span class="number">2</span> ? <span class="string">&#x27;world&#x27;</span> :</span><br><span class="line">                                                       <span class="string">&#x27;atguigu&#x27;</span>) <span class="keyword">FROM</span> numbers(<span class="number">10</span>);</span><br></pre></td></tr></table></figure>

<img src="image-20250522205014230.png" alt="image-20250522205014230" style="zoom:50%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启三元运算符优化</span></span><br><span class="line"><span class="keyword">SET</span> optimize_if_chain_to_multiif <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"><span class="comment">--再次查看语法优化</span></span><br><span class="line">EXPLAIN SYNTAX <span class="keyword">SELECT</span> number <span class="operator">=</span> <span class="number">1</span> ? <span class="string">&#x27;hello&#x27;</span> : (number <span class="operator">=</span> <span class="number">2</span> ? <span class="string">&#x27;world&#x27;</span> :</span><br><span class="line">                                                       <span class="string">&#x27;atguigu&#x27;</span>) <span class="keyword">FROM</span> numbers(<span class="number">10</span>);</span><br></pre></td></tr></table></figure>

<img src="image-20250522205053655.png" alt="image-20250522205053655" style="zoom:50%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--返回优化后的语句</span></span><br><span class="line"><span class="keyword">SELECT</span> multiIf(number <span class="operator">=</span> <span class="number">1</span>, <span class="string">&#x27;hello&#x27;</span>, number <span class="operator">=</span> <span class="number">2</span>, <span class="string">&#x27;world&#x27;</span>, <span class="string">&#x27;xyz&#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> numbers(<span class="number">10</span>);</span><br></pre></td></tr></table></figure>

<img src="image-20250522205430269.png" alt="image-20250522205430269" style="zoom:33%;">

<p>（6）查看 PIPELINE</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN PIPELINE <span class="keyword">SELECT</span> <span class="built_in">sum</span>(number) <span class="keyword">FROM</span> numbers_mt(<span class="number">100000</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> number <span class="operator">%</span> <span class="number">20</span>;</span><br></pre></td></tr></table></figure>

<img src="image-20250522205819820.png" alt="image-20250522205819820" style="zoom:50%;">

<p>8代表八线程（虚拟机8线程），聚合了8次</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN PIPELINE header<span class="operator">=</span><span class="number">1</span>,graph<span class="operator">=</span><span class="number">1</span> <span class="keyword">SELECT</span> <span class="built_in">sum</span>(number) <span class="keyword">FROM</span></span><br><span class="line">    numbers_mt(<span class="number">10000</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> number<span class="operator">%</span><span class="number">20</span>;</span><br></pre></td></tr></table></figure>

<p>digraph<br>{<br>“  rankdir&#x3D;””LR””;”<br>  { node [shape &#x3D; box]<br>“        n2 [label&#x3D;””Limit””];”<br>“        n1 [label&#x3D;””Numbers””];”<br>    subgraph cluster_0 {<br>“      label &#x3D;””Aggregating””;”<br>      style&#x3D;filled;<br>      color&#x3D;lightgrey;<br>“      node [style&#x3D;filled,color&#x3D;white];”<br>      { rank &#x3D; same;<br>“        n4 [label&#x3D;””AggregatingTransform””];”<br>      }<br>    }<br>    subgraph cluster_1 {<br>“      label &#x3D;””Expression””;”<br>      style&#x3D;filled;<br>      color&#x3D;lightgrey;<br>“      node [style&#x3D;filled,color&#x3D;white];”<br>      { rank &#x3D; same;<br>“        n5 [label&#x3D;””ExpressionTransform””];”<br>      }<br>    }<br>    subgraph cluster_2 {<br>“      label &#x3D;””Expression””;”<br>      style&#x3D;filled;<br>      color&#x3D;lightgrey;<br>“      node [style&#x3D;filled,color&#x3D;white];”<br>      { rank &#x3D; same;<br>“        n3 [label&#x3D;””ExpressionTransform””];”<br>      }<br>    }<br>  }<br>“  n2 -&gt; n3 [label&#x3D;”””<br>“number UInt64 UInt64(size &#x3D; 0)””];”<br>“  n1 -&gt; n2 [label&#x3D;”””<br>“number UInt64 UInt64(size &#x3D; 0)””];”<br>“  n4 -&gt; n5 [label&#x3D;”””<br>“modulo(number, 20) UInt8 UInt8(size &#x3D; 0)”<br>“sum(number) UInt64 UInt64(size &#x3D; 0)””];”<br>“  n3 -&gt; n4 [label&#x3D;”””<br>number UInt64 UInt64(size &#x3D; 0)<br>“modulo(number, 20) UInt8 UInt8(size &#x3D; 0)””];”<br>}</p>
<h1 id="第六章-副本（ReplicatedMergeTree）"><a href="#第六章-副本（ReplicatedMergeTree）" class="headerlink" title="第六章 副本（ReplicatedMergeTree）"></a>第六章 副本（ReplicatedMergeTree）</h1><p>副本的目的主要是保障数据的高可用性，即使一台 ClickHouse 节点宕机，那么也可以从其他服务器获得相同的数据。</p>
<p><a target="_blank" rel="noopener" href="https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/replication/">https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/replication/</a></p>
<h2 id="6-1-副本写入流程"><a href="#6-1-副本写入流程" class="headerlink" title="6.1 副本写入流程"></a>6.1 副本写入流程</h2><img src="Snipaste_2024-05-23_14-08-48.png" alt="Snipaste_2024-05-23_14-08-48" style="zoom: 50%;">

<h2 id="6-2-配置步骤"><a href="#6-2-配置步骤" class="headerlink" title="6.2 配置步骤"></a>6.2 配置步骤</h2><p>（1）启动zookeeper集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse-server]# zk.sh start</span><br></pre></td></tr></table></figure>

<p>（2）在hadoop102的&#x2F;etc&#x2F;clickhouse-server&#x2F;config.d 目录下创建一个名为 metrika.xml的配置文件,内容如下：（注：也可以不创建外部文件，直接在 config.xml 中指定<zookeeper>）</zookeeper></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /etc/clickhouse-server/config.d/</span><br><span class="line">[root@hadoop102 config.d]# ll</span><br><span class="line">总用量 0</span><br><span class="line">[root@hadoop102 config.d]# vim metrika.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">yandex</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;1&quot;</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;2&quot;</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;3&quot;</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop104<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">yandex</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（3）修改用户组，并同步到hadoop103和hadoop104上</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 config.d]# chown clickhouse:clickhouse metrika.xml</span><br><span class="line">[root@hadoop102 config.d]# xsync /etc/clickhouse-server/config.d/metrika.xml</span><br></pre></td></tr></table></figure>

<p>（4）在hadoop102 的&#x2F;etc&#x2F;clickhouse-server&#x2F;config.xml 中增加</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse-server]# vim config.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">zookeeper</span> <span class="attr">incl</span>=<span class="string">&quot;zookeeper-servers&quot;</span> <span class="attr">optional</span>=<span class="string">&quot;true&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">include_from</span>&gt;</span>/etc/clickhouse-server/config.d/metrika.xml<span class="tag">&lt;/<span class="name">include_from</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（5）同步到hadoop103和hadoop104上</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse-server]# xsync config.xml</span><br></pre></td></tr></table></figure>

<p>重启hadoop102上的ck服务器，并开启hadoop103上的ck服务器</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse-server]# clickhouse start</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 ~]# clickhouse start</span><br></pre></td></tr></table></figure>

<p>（6）在 hadoop102 和 hadoop103 上分别建表</p>
<p><strong>副本只能同步数据，不能同步表结构，所以我们需要在每台机器上自己手动建表</strong></p>
<p>在hadoop102中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_rep2 (</span><br><span class="line"> id UInt32,</span><br><span class="line"> sku_id String,</span><br><span class="line"> total_amount Decimal(16,2),</span><br><span class="line"> create_time Datetime</span><br><span class="line">) engine =ReplicatedMergeTree(&#x27;/clickhouse/table/01/t_order_rep&#x27;,&#x27;rep_102&#x27;)</span><br><span class="line"> partition by toYYYYMMDD(create_time)</span><br><span class="line"> primary key (id)</span><br><span class="line"> order by (id,sku_id);</span><br></pre></td></tr></table></figure>

<p>在hadoop103中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table t_order_rep2 (</span><br><span class="line"> id UInt32,</span><br><span class="line"> sku_id String,</span><br><span class="line"> total_amount Decimal(16,2),</span><br><span class="line"> create_time Datetime</span><br><span class="line">) engine =ReplicatedMergeTree(&#x27;/clickhouse/table/01/t_order_rep&#x27;,&#x27;rep_103&#x27;)</span><br><span class="line"> partition by toYYYYMMDD(create_time)</span><br><span class="line"> primary key (id)</span><br><span class="line"> order by (id,sku_id);</span><br></pre></td></tr></table></figure>

<p><strong>参数解释</strong></p>
<p>ReplicatedMergeTree 中，<strong>第一个参数</strong>是分片的 zk_path 一般按照：</p>
<p>&#x2F;clickhouse&#x2F;table&#x2F;{shard}&#x2F;{table_name} 的格式写，如果只有一个分片就写 01 即可。<strong>shard表示分片信息，table_name是表名</strong></p>
<p><strong>第二个参数</strong>是副本名称，相同的分片副本名称不能相同。</p>
<p>查看zookeeper监控器：</p>
<p><img src="Snipaste_2024-05-23_23-37-03.png" alt="Snipaste_2024-05-23_23-37-03"></p>
<p> （7）在 hadoop102 上执行 insert 语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">insert into t_order_rep2 values</span><br><span class="line">(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(103,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(104,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;),</span><br><span class="line">(105,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;);</span><br></pre></td></tr></table></figure>

<p>（8）在 hadoop103 上执行 select，可以查询出结果，说明副本配置正确</p>
<img src="Snipaste_2024-05-23_23-33-47.png" alt="Snipaste_2024-05-23_23-33-47" style="zoom:43%;">

<h1 id="第七章-集群分片（Distributed，很少使用）"><a href="#第七章-集群分片（Distributed，很少使用）" class="headerlink" title="第七章 集群分片（Distributed，很少使用）"></a>第七章 集群分片（Distributed，很少使用）</h1><p>副本虽然能够提高数据的可用性，降低丢失风险，但是每台服务器实际上必须容纳全量数据，对数据的横向扩容没有解决。</p>
<p>要解决数据水平切分的问题，需要引入分片的概念。<strong>通过分片把一份完整的数据进行切分，不同的分片分布到不同的节点上，再通过 Distributed 表引擎把数据拼接起来一同使用。</strong></p>
<p><strong>Distributed 表引擎本身不存储数据</strong>，有点类似于 MyCat 之于 MySql，成为一种中间件，通过分布式逻辑表来写入、分发、路由来操作多台节点不同分片的分布式数据。</p>
<blockquote>
<p>注意：ClickHouse 的集群是表级别的，实际企业中，大部分做了高可用，但是没有用分片，避免降低查询性能以及操作集群的复杂性。</p>
</blockquote>
<h2 id="7-1-集群写入流程（3分片2副本共6个节点）"><a href="#7-1-集群写入流程（3分片2副本共6个节点）" class="headerlink" title="7.1 集群写入流程（3分片2副本共6个节点）"></a>7.1 集群写入流程（3分片2副本共6个节点）</h2><p><img src="Snipaste_2024-05-23_23-50-13.png" alt="Snipaste_2024-05-23_23-50-13"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">生产环境下一般将internal_replication设置为ture</span><br></pre></td></tr></table></figure>

<h2 id="7-2-集群读取流程（3分片2副本共6个节点）"><a href="#7-2-集群读取流程（3分片2副本共6个节点）" class="headerlink" title="7.2 集群读取流程（3分片2副本共6个节点）"></a>7.2 集群读取流程（3分片2副本共6个节点）</h2><p><img src="image-20250519112418484.png" alt="image-20250519112418484"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">优先选择errors_count小的副本读，如果error_count相同则选择以下四种方式：随机、顺序、随机（优先第一顺位）、host名称近似</span><br></pre></td></tr></table></figure>

<h2 id="7-3-3分片2副本6个节点集群配置（仅参考）"><a href="#7-3-3分片2副本6个节点集群配置（仅参考）" class="headerlink" title="7.3 3分片2副本6个节点集群配置（仅参考）"></a>7.3 3分片2副本6个节点集群配置（仅参考）</h2><p>仅供参考，因为我们只有三个节点</p>
<p>配置的位置还是在之前的&#x2F;etc&#x2F;clickhouse-server&#x2F;config.d&#x2F;metrika.xml，内容如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注：也可以不创建外部文件，直接在 config.xml 的&lt;remote_servers&gt;中指定</span><br></pre></td></tr></table></figure>

<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">yandex</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">remote_servers</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">gmall_cluster</span>&gt;</span> <span class="comment">&lt;!-- 集群名称，随便起名--&gt;</span> </span><br><span class="line">            <span class="tag">&lt;<span class="name">shard</span>&gt;</span> <span class="comment">&lt;!--集群的第一个分片--&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!--该分片的第一个副本--&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">replica</span>&gt;</span> </span><br><span class="line">                    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop101<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!--该分片的第二个副本--&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">replica</span>&gt;</span> </span><br><span class="line">                    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">shard</span>&gt;</span> <span class="comment">&lt;!--集群的第二个分片--&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">replica</span>&gt;</span> <span class="comment">&lt;!--该分片的第一个副本--&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">replica</span>&gt;</span> <span class="comment">&lt;!--该分片的第二个副本--&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop104<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">shard</span>&gt;</span> <span class="comment">&lt;!--集群的第三个分片--&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">replica</span>&gt;</span> <span class="comment">&lt;!--该分片的第一个副本--&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop105<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">replica</span>&gt;</span> <span class="comment">&lt;!--该分片的第二个副本--&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop106<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">gmall_cluster</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">remote_servers</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">yandex</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="7-4-配置三节点版本集群及副本"><a href="#7-4-配置三节点版本集群及副本" class="headerlink" title="7.4 配置三节点版本集群及副本"></a>7.4 配置三节点版本集群及副本</h2><h3 id="7-4-1-集群及副本规划（2个分片，只有第一个分片有副本）"><a href="#7-4-1-集群及副本规划（2个分片，只有第一个分片有副本）" class="headerlink" title="7.4.1 集群及副本规划（2个分片，只有第一个分片有副本）"></a>7.4.1 集群及副本规划（2个分片，只有第一个分片有副本）</h3><img src="image-20250519114545766.png" alt="image-20250519114545766" style="zoom:67%;">

<img src="image-20250519114232541.png" alt="image-20250519114232541" style="zoom: 67%;">

<h3 id="7-4-2-配置步骤"><a href="#7-4-2-配置步骤" class="headerlink" title="7.4.2 配置步骤"></a>7.4.2 配置步骤</h3><p>（1）在hadoop102的&#x2F;etc&#x2F;clickhouse-server&#x2F;config.d目录下创建metrika-shard.xml文件</p>
<blockquote>
<p>注：也可以不创建外部文件，直接在config.xml的中指定 </p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /etc/clickhouse-server/</span><br><span class="line">[root@hadoop102 clickhouse-server]# ll</span><br><span class="line">总用量 64</span><br><span class="line">drwxr-xr-x 2 clickhouse clickhouse    25 5月  23 2024 config.d</span><br><span class="line">-rwxr-xr-x 1 clickhouse clickhouse 56147 5月  23 2024 config.xml</span><br><span class="line">drwxr-xr-x 2 clickhouse clickhouse     6 5月  19 2024 users.d</span><br><span class="line">-rwxr-xr-x 1 clickhouse clickhouse  6053 7月  14 2021 users.xml</span><br><span class="line">[root@hadoop102 clickhouse-server]# cd config.d/</span><br><span class="line">[root@hadoop102 config.d]# ll</span><br><span class="line">总用量 4</span><br><span class="line">-rw-r--r-- 1 clickhouse clickhouse 420 5月  23 2024 metrika.xml</span><br><span class="line">[root@hadoop102 config.d]# vim metrika-shard.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">yandex</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--分片逻辑：数据按分片规则（如随机/哈希）分布到不同 Shard</span></span><br><span class="line"><span class="comment">        副本冗余：Shard 1 有 2 副本（高可用），Shard 2 有 1 副本（无冗余）</span></span><br><span class="line"><span class="comment">        写入策略：internal_replication=true 时，写入一个副本即返回，后台异步同步--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">remote_servers</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">gmall_cluster</span>&gt;</span> <span class="comment">&lt;!-- 集群名称，可以随便起--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">shard</span>&gt;</span>         <span class="comment">&lt;!--集群的第一个分片--&gt;</span> </span><br><span class="line">                 <span class="comment">&lt;!-- 分片内副本同步模式：</span></span><br><span class="line"><span class="comment">                 true：写操作只需一个副本确认即返回（更快）</span></span><br><span class="line"><span class="comment">                 false：需所有副本确认（更安全） --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">replica</span>&gt;</span>    <span class="comment">&lt;!--该分片的第一个副本--&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">replica</span>&gt;</span>    <span class="comment">&lt;!--该分片的第二个副本--&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">shard</span>&gt;</span>  <span class="comment">&lt;!--集群的第二个分片--&gt;</span> </span><br><span class="line">                <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">replica</span>&gt;</span>    <span class="comment">&lt;!--该分片的第一个副本--&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop104<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">gmall_cluster</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">remote_servers</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!--分布式协调：管理 ReplicatedMergeTree 表的元数据、副本状态</span></span><br><span class="line"><span class="comment">        故障恢复：通过 ZK 实现副本自动故障转移</span></span><br><span class="line"><span class="comment">        事务日志：记录分布式 DDL 操作--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- ZooKeeper节点列表（必须为奇数个，建议3节点以上） --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;1&quot;</span>&gt;</span><span class="comment">&lt;!-- 节点标识，不影响实际使用 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;2&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;3&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop104<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!--节点	     shard值	 replica值	   说明</span></span><br><span class="line"><span class="comment">		hadoop102	01	   rep_1_1	   分片1的第一个副本</span></span><br><span class="line"><span class="comment">        hadoop103	01	   rep_1_2	   分片1的第二个副本</span></span><br><span class="line"><span class="comment">        hadoop104	02	   rep_2_1	   分片2的唯一副本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">macros</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 当前节点所属分片编号（不同分片节点需不同，如01/02） --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span>01<span class="tag">&lt;/<span class="name">shard</span>&gt;</span>   <span class="comment">&lt;!--不同机器放的分片数不一样--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 当前节点副本标识（格式建议：rep_&#123;分片号&#125;_&#123;副本号&#125;） --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">replica</span>&gt;</span>rep_1_1<span class="tag">&lt;/<span class="name">replica</span>&gt;</span>  <span class="comment">&lt;!--不同机器放的副本数不一样--&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">macros</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">yandex</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）在服务端配置config.xml中引入metrika-shard.xml文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 config.d]# cd ..</span><br><span class="line">[root@hadoop102 clickhouse-server]# ll</span><br><span class="line">总用量 64</span><br><span class="line">drwxr-xr-x 2 clickhouse clickhouse    50 5月  21 12:00 config.d</span><br><span class="line">-rwxr-xr-x 1 clickhouse clickhouse 56147 5月  23 2024 config.xml</span><br><span class="line">drwxr-xr-x 2 clickhouse clickhouse     6 5月  19 2024 users.d</span><br><span class="line">-rwxr-xr-x 1 clickhouse clickhouse  6053 7月  14 2021 users.xml</span><br><span class="line">[root@hadoop102 clickhouse-server]# vim config.xml </span><br></pre></td></tr></table></figure>

<p><img src="image-20250521120729935.png" alt="image-20250521120729935"></p>
<p>（3）将hadoop102的metrika-shard.xml 同步到 103和 104 </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse-server]# xsync /etc/clickhouse-server/config.d/metrika-shard.xml </span><br><span class="line">============ hadoop102 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line"></span><br><span class="line">sent 55 bytes  received 12 bytes  134.00 bytes/sec</span><br><span class="line">total size is 2,798  speedup is 41.76</span><br><span class="line">============ hadoop103 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line">metrika-shard.xml</span><br><span class="line"></span><br><span class="line">sent 2,900 bytes  received 35 bytes  5,870.00 bytes/sec</span><br><span class="line">total size is 2,798  speedup is 0.95</span><br><span class="line">============ hadoop104 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line">metrika-shard.xml</span><br><span class="line"></span><br><span class="line">sent 2,900 bytes  received 35 bytes  5,870.00 bytes/sec</span><br><span class="line">total size is 2,798  speedup is 0.95</span><br></pre></td></tr></table></figure>

<p>（4）同步&#x2F;etc&#x2F;clickhouse-server&#x2F;config.xml 到 103 和 104</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse-server]# xsync config.xml </span><br><span class="line">============ hadoop102 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line"></span><br><span class="line">sent 73 bytes  received 12 bytes  56.67 bytes/sec</span><br><span class="line">total size is 56,153  speedup is 660.62</span><br><span class="line">============ hadoop103 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line">config.xml</span><br><span class="line"></span><br><span class="line">sent 1,146 bytes  received 521 bytes  3,334.00 bytes/sec</span><br><span class="line">total size is 56,153  speedup is 33.69</span><br><span class="line">============ hadoop104 ==============</span><br><span class="line">mkdir：无效选项 -- P</span><br><span class="line">Try &#x27;mkdir --help&#x27; for more information.</span><br><span class="line">sending incremental file list</span><br><span class="line">config.xml</span><br><span class="line"></span><br><span class="line">sent 1,146 bytes  received 521 bytes  3,334.00 bytes/sec</span><br><span class="line">total size is 56,153  speedup is 33.69</span><br></pre></td></tr></table></figure>

<p>（5）修改103和104中metrika-shard.xml 宏的配置 </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 ~]# cd /etc/clickhouse-server/config.d/</span><br><span class="line">[root@hadoop103 config.d]# ll</span><br><span class="line">总用量 8</span><br><span class="line">-rw-r--r-- 1 root       root       2798 5月  21 12:00 metrika-shard.xml</span><br><span class="line">-rw-r--r-- 1 clickhouse clickhouse  420 5月  23 2024 metrika.xml</span><br><span class="line">[root@hadoop103 config.d]# vim metrika-shard.xml</span><br></pre></td></tr></table></figure>

<p><img src="image-20250521121518970.png" alt="image-20250521121518970"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 ~]# cd /etc/clickhouse-server/config.d/</span><br><span class="line">[root@hadoop104 config.d]# ll</span><br><span class="line">总用量 8</span><br><span class="line">-rw-r--r-- 1 root       root       2798 5月  21 12:00 metrika-shard.xml</span><br><span class="line">-rw-r--r-- 1 clickhouse clickhouse  420 5月  23 2024 metrika.xml</span><br><span class="line">[root@hadoop104 config.d]# vim metrika-shard.xml </span><br></pre></td></tr></table></figure>

<p><img src="image-20250521121710957.png" alt="image-20250521121710957"></p>
<p>（6）启动zookeeper并且重启三台服务器上的ClickHouse服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 clickhouse-server]# zk.sh start</span><br><span class="line">---------- zookeeper hadoop102 启动 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">---------- zookeeper hadoop103 启动 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">---------- zookeeper hadoop104 启动 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure>

<p><strong>以下略，实际工作中不需要操作人员使用副本和分片，数仓人员不会直接操作副本和分片配置，只需要了解原理即可，推荐《ClickHouse原理解析与应用实践》，我们将副本和分片配置回退到最初，这样才能保证clickhouse正常运行，方便后续学习。</strong></p>
<h2 id="7-5-项目为了节省资源，就使用单节点，不用集群"><a href="#7-5-项目为了节省资源，就使用单节点，不用集群" class="headerlink" title="7.5 项目为了节省资源，就使用单节点，不用集群"></a>7.5 项目为了节省资源，就使用单节点，不用集群</h2><p>不需要求改文件引用，因为已经使用集群建表了，如果改为引用metrika-shard.xml的话， 启动会报错。我们以后用的时候只启动102即可。 </p>
<h1 id="第八章-建表优化"><a href="#第八章-建表优化" class="headerlink" title="第八章 建表优化"></a>第八章 建表优化</h1><h2 id="2-1-数据类型"><a href="#2-1-数据类型" class="headerlink" title="2.1 数据类型"></a>2.1 数据类型</h2><h3 id="2-1-1-时间字段的类型"><a href="#2-1-1-时间字段的类型" class="headerlink" title="2.1.1 时间字段的类型"></a>2.1.1 时间字段的类型</h3><p>建表时能用<strong>数值型或日期时间型</strong>表示的字段就不要用字符串，全 String 类型在以 Hive 为中心的数仓建设中常见，但 ClickHouse 环境不应受此影响。 </p>
<p>虽然 ClickHouse 底层将 DateTime 存储为时间戳 Long 类型，但不建议存储 Long 类型， 因为 DateTime 不需要经过函数转换处理，执行效率高、可读性好</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_order_mt(</span><br><span class="line">   id UInt32,</span><br><span class="line">   sku_id String,</span><br><span class="line">   total_amount <span class="type">Decimal</span>(<span class="number">16</span>,<span class="number">2</span>),</span><br><span class="line">   create_time Datetime <span class="comment">--直接写成Datetime类型</span></span><br><span class="line">) engine <span class="operator">=</span>MergeTree</span><br><span class="line">  <span class="keyword">partition</span> <span class="keyword">by</span> toYYYYMMDD(create_time) <span class="comment">--toYYYYMMDD()日期格式化函数</span></span><br><span class="line">  <span class="keyword">primary</span> key (id) <span class="comment">--主键约束，注意：ck中的主键没有唯一性约束，也就是id可以重复</span></span><br><span class="line">  <span class="keyword">order</span> <span class="keyword">by</span> (id,sku_id);<span class="comment">--在分区内先按照id排序，再按照sku_id排序</span></span><br></pre></td></tr></table></figure>

<h3 id="2-1-2-空值存储类型"><a href="#2-1-2-空值存储类型" class="headerlink" title="2.1.2 空值存储类型"></a>2.1.2 空值存储类型</h3><p>官方已经指出 Nullable 类型几乎总是会拖累性能，因为存储 Nullable 列时需要创建一个 额外的文件来存储 NULL 的标记，并且 Nullable 列无法被索引。</p>
<p>因此除非极特殊情况，应直 接使用字段默认值表示空，或者自行指定一个在业务中无意义的值（例如用-1 表示没有商品 ID）。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t_null(x Int8, y Nullable(Int8)) ENGINE TinyLog;</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> t_null <span class="keyword">VALUES</span> (<span class="number">1</span>, <span class="keyword">NULL</span>), (<span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line"><span class="keyword">SELECT</span> x <span class="operator">+</span> y <span class="keyword">FROM</span> t_null;</span><br></pre></td></tr></table></figure>

<img src="image-20250522212021785.png" alt="image-20250522212021785" style="zoom:50%;">

<p>在ck中Nullable的列会单独存</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /var/lib/clickhouse/data/default/</span><br><span class="line">[root@hadoop102 default]# ll</span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  22 21:19 t_null -&gt; /var/lib/clickhouse/store/48e/48e5a505-f7aa-4ee7-88e5-a505f7aaaee7/</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  21 13:35 t_order_mt -&gt; /var/lib/clickhouse/store/72a/72ab6a68-6b5a-4d3c-b2ab-6a686b5a4d3c/</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  21 13:41 t_order_mt2 -&gt; /var/lib/clickhouse/store/dac/dac240cb-7b10-4847-9ac2-40cb7b10a847/</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  21 13:56 t_order_mt3 -&gt; /var/lib/clickhouse/store/c97/c978ae1c-8e5f-488d-8978-ae1c8e5f388d/</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  21 13:57 t_order_rmt -&gt; /var/lib/clickhouse/store/97d/97d02cd6-412e-4e04-97d0-2cd6412e0e04/</span><br><span class="line">lrwxrwxrwx 1 clickhouse clickhouse 67 5月  21 13:57 t_order_smt -&gt; /var/lib/clickhouse/store/ba9/ba92adf6-064e-4899-ba92-adf6064e9899/</span><br><span class="line">[root@hadoop102 default]# cd t_null/</span><br><span class="line">[root@hadoop102 t_null]# ll</span><br><span class="line">总用量 16</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 91 5月  22 21:20 sizes.json</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 28 5月  22 21:20 x.bin</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 28 5月  22 21:20 y.bin</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 28 5月  22 21:20 y.null.bin</span><br></pre></td></tr></table></figure>

<h2 id="2-2-分区和索引"><a href="#2-2-分区和索引" class="headerlink" title="2.2 分区和索引"></a>2.2 分区和索引</h2><p>分区粒度根据业务特点决定，不宜过粗或过细。一般选择<strong>按天分区</strong>，也可以指定为 Tuple()， 以单表一亿数据为例，分区大小控制在 10-30 个为最佳。（<strong>也就是说如果以天分区，ck表的生命周期也就是三十天左右</strong>）</p>
<p>必须指定索引列，ClickHouse 中的<strong>索引列即排序列</strong>，通过 <strong>order by</strong> 指定，一般在查询条 件中经常被用来充当筛选条件的属性被纳入进来；可以是单一维度，也可以是组合维度的索 引；<strong>通常需要满足高级列在前、查询频率大的在前原则</strong>；还有基数特别大的不适合做索引列， 如用户表的 userid 字段；通常<strong>筛选后的数据满足在百万以内为最佳。</strong></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">Linux</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/12/01/clickhouse%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-大数据面试题总结" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/">大数据面试题总结</a>
    </h1>
  

        
        <a href="/2023/11/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/" class="archive-article-date">
  	<time datetime="2023-11-27T11:33:23.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-11-27</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第一章-技术及框架"><a href="#第一章-技术及框架" class="headerlink" title="第一章 技术及框架"></a>第一章 技术及框架</h1><h2 id="自我介绍"><a href="#自我介绍" class="headerlink" title="自我介绍"></a>自我介绍</h2><p>面试官你好，我叫王宇涵，26岁。本科就读于哈尔滨工程大学，硕士就读于大连理工大学，计算机技术专业，2025届应届生，秋招打算求职一份数仓开发或大数据开发的工作。</p>
<p>在研究生阶段一次偶然的机会阅读了Google大数据开山的三篇论文（File-System，BigTable，MapReduce），再加上我的研究方向是量化交易相关的，需要和大量股票数据和因子数据打交道，在此期间接触到了一些数仓理论和大数据计算引擎，从而对大数据行业产生浓厚的兴趣，结合研究生的课程和自学学习了很多大数据的技术组件和数仓建模理论，也做了一个离线数仓的项目。通过对项目也锻炼了我动手实操和理解业务的能力。</p>
<p>在今年三月入职快手数据平台部，担任数据研发实习生，主要工作内容是参与商业化广告流量相关业务的数仓建设，包括模型设计、ETL开发，数据同步、维度指标建设等，参与模型的性能优化，内容治理和日常维护的工作。参与了一些商业化广告流量项目的建设，比如生态体验-用户行为指标建设和CPM-广告竞价链路指标建设，还有横向的数仓模型设计（将主站的三张DWD表进行数据提炼汇总，汇总原则是数据收口在商业化范围内，供下游商业化广告数仓使用），涉及的技术栈有Hadoop（Mapreduce、HDFS、Yarn）、Spark、Hive、Clickhouse、维度建模等。</p>
<p>我的工作流程（简略版）：开需求评审会（了解需求、分析prd）-技术评审（介绍技术方案，QA给出提测排期）-模型设计，ETL开发-冒烟测试，提测-上线-将hive表数据同步到ck表中，配置标准数据集-日常运维优化</p>
<p>我的大数据学习路线是：视频（快速了解技术概况和开发操作）+权威书籍（对不懂的知识点进行查缺补漏）+官方文档（把控组件最新发展和bug说明，同时作为工具书在开发过程中查阅）+个人技术博客总结（总结课程笔记和读书笔记和相关知识点，浓缩精华，记录开发过程中遇到的问题和解决办法，供他人分享）</p>
<h2 id="一、⭐Linux-amp-Shell"><a href="#一、⭐Linux-amp-Shell" class="headerlink" title="一、⭐Linux&amp;Shell"></a>一、⭐Linux&amp;Shell</h2><h3 id="1-说一说你用到的Linux高级命令"><a href="#1-说一说你用到的Linux高级命令" class="headerlink" title="1. 说一说你用到的Linux高级命令"></a>1. 说一说你用到的Linux高级命令</h3><ul>
<li>scp：可以实现服务器与服务器之间的数据拷贝（所有文件都复制过去）</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ scp -r /opt/module/jdk1.8.0_212/ atguigu@hadoop103:/opt/module/</span><br></pre></td></tr></table></figure>

<ul>
<li>rsync：远程同步工具（与scp的差别是只对差异文件做更新）</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ rsync -av hadoop-3.1.3/ atguigu@hadoop103:/opt/module/hadoop-3.1.3/</span><br></pre></td></tr></table></figure>

<ul>
<li>jps：查看当前系统中正在运行的Java进程信息，包括进程ID和进程名称。</li>
<li>ps -ef（ps aux）:查看系统中的所有进程</li>
<li>kill [-9] 进程号：终止进程</li>
<li>top：实时监控系统进程状态</li>
<li>iotop：查看磁盘IO读写</li>
<li>netstat -nlp | grep 端口号：显示网络端口号占用情况</li>
<li>df -h：查看磁盘空间使用情况</li>
<li>tar -zxvf … &#x2F;目录：将..解压到&#x2F;目录下</li>
<li>mkdir：创建一个新目录</li>
<li>chmod：更改文件或目录的权限</li>
<li><img src="image-20240825224030662.png" alt="image-20240825224030662" style="zoom: 50%;"></li>
</ul>
<img src="image-20240825224115295.png" alt="image-20240825224115295" style="zoom: 50%;">

<h3 id="2-写过哪些shell脚本"><a href="#2-写过哪些shell脚本" class="headerlink" title="2. 写过哪些shell脚本"></a>2. 写过哪些shell脚本</h3><p>（1）启停脚本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line"> 	    for i in hadoop102 hadoop103 hadoop104</span><br><span class="line"> 		do</span><br><span class="line"> 			ssh $i &quot;绝对路径&quot;</span><br><span class="line"> 		done</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line"> 		for i in hadoop102 hadoop103 hadoop104</span><br><span class="line"> 		do</span><br><span class="line"> 			ssh $i &quot;绝对路径&quot;</span><br><span class="line"> 		done</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）集群分发脚本</p>
<p>（3）集群命令同时执行脚本</p>
<p>（4）与外部系统的导入导出（ods–&gt;ads）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">定义变量</span><br><span class="line">获取时间</span><br><span class="line">sql = &quot; 绝对路径  库名.表  库名.自定义函数&quot;</span><br><span class="line">执行sql</span><br></pre></td></tr></table></figure>

<h2 id="二、Hive"><a href="#二、Hive" class="headerlink" title="二、Hive"></a>二、Hive</h2><h3 id="1-⭐⭐Hive架构是什么样的？"><a href="#1-⭐⭐Hive架构是什么样的？" class="headerlink" title="1. ⭐⭐Hive架构是什么样的？"></a>1. ⭐⭐Hive架构是什么样的？</h3><p>定义Hive：Hive是一个基于Hadoop的数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能，本质是将HQL转化成MapReduce程序，底层由HDFS来提供数据的存储，程序执行在Yarn上。</p>
<p>架构：<strong>Hive架构由用户接口、元数据存储（Metastore）和驱动（解析器、语义分析器、逻辑计划生成器、逻辑优化器、物理计划生成器、物理优化器、执行器等）组成。</strong>用户通过接口创建HQL语句，由Metastore记录对应的元数据，通过一系列驱动进行HQL语句的分析优化和查询计划的生成（或者说翻译成MapReduce程序），生成的查询计划存储在HDFS中，随后在MapReduce调用执行，最后将计算结果返回给用户。</p>
<p><img src="111111%E5%9B%BE%E7%89%871.png" alt="111111图片1"></p>
<h3 id="2-⭐Hive-SQL编译成MapReduce的过程"><a href="#2-⭐Hive-SQL编译成MapReduce的过程" class="headerlink" title="2. ⭐Hive SQL编译成MapReduce的过程"></a>2. ⭐Hive SQL编译成MapReduce的过程</h3><img src="image-20240821001832336.png" alt="image-20240821001832336" style="zoom:50%;">

<img src="image-20240821001852413.png" alt="image-20240821001852413" style="zoom:50%;">

<p>①解析器（SQLParser）：将SQL字符串转换成抽象语法树（AST）</p>
<p>②语义分析（Semantic Analyzer）：将AST进一步划分为QeuryBlock</p>
<p>③逻辑计划生成器（Logical Plan Gen）：将QeuryBlock生成逻辑计划</p>
<p>④逻辑优化器（Logical Optimizer）：对逻辑计划进行优化</p>
<p>⑤物理计划生成器（Physical Plan Gen）：根据优化后的逻辑计划生成物理计划</p>
<p>⑥物理优化器（Physical Optimizer）：对物理计划进行优化</p>
<p>⑦执行器（Execution）：执行该计划，得到查询结果并返回给客户端</p>
<h3 id="3-说一说Hive的优缺点"><a href="#3-说一说Hive的优缺点" class="headerlink" title="3. 说一说Hive的优缺点"></a>3. 说一说Hive的优缺点</h3><p>优点：</p>
<ul>
<li>①操作接口采用类SQL语法，简单容易上手</li>
<li>②免去写MapReduce，减少学习成本</li>
<li>③适合实时性不高的场景，可以处理大量的数据</li>
<li>④支持用户自定义函数</li>
</ul>
<p>缺点：</p>
<ul>
<li>①Hive常用于数据分析，不能用于实时性高的场景，延迟较高</li>
<li>②HQL表达能力有限，迭代式无法表示</li>
<li>③实现不了数据挖掘类的算法</li>
<li>④Hive的调优比较困难，粒度比较粗</li>
</ul>
<h3 id="4-⭐⭐说一说Hive与传统数据库的区别"><a href="#4-⭐⭐说一说Hive与传统数据库的区别" class="headerlink" title="4. ⭐⭐说一说Hive与传统数据库的区别"></a>4. ⭐⭐说一说Hive与传统数据库的区别</h3><table>
<thead>
<tr>
<th></th>
<th>Hive</th>
<th>关系型数据库</th>
</tr>
</thead>
<tbody><tr>
<td>查询语言</td>
<td>HQL</td>
<td>SQL</td>
</tr>
<tr>
<td>数据存储位置</td>
<td>HDFS</td>
<td>本地文件系统</td>
</tr>
<tr>
<td>数据更新</td>
<td>不支持对数据的修改</td>
<td>经常修改</td>
</tr>
<tr>
<td>执行延迟</td>
<td>高</td>
<td>低</td>
</tr>
<tr>
<td>数据规模</td>
<td>大</td>
<td>小</td>
</tr>
<tr>
<td>是否支持主键或外键</td>
<td>不支持</td>
<td>支持</td>
</tr>
<tr>
<td>是否有索引</td>
<td>3.0版本之前支持，之后不支持</td>
<td>支持</td>
</tr>
</tbody></table>
<h3 id="5-⭐⭐Hive内部表和外部表的区别"><a href="#5-⭐⭐Hive内部表和外部表的区别" class="headerlink" title="5. ⭐⭐Hive内部表和外部表的区别"></a>5. ⭐⭐Hive内部表和外部表的区别</h3><ul>
<li>由external修饰的表为外部表，未被external修饰的表为内部表，默认创建的表都是内部表</li>
<li>内部表由Hive自身管理（包括元数据和HDFS中的数据）；外部表Hive只接管元数据，数据由HDFS管理</li>
<li>删除（drop）内部表会直接删除元数据及HDFS上的文件；删除（drop）外部表仅仅会删除元数据，HDFS上的文件并不会删除</li>
<li>清空（truncate）表操作只能清空内部表，清空外部表将报错</li>
<li>查询结果缓存只适用于托管表（内部表）</li>
</ul>
<p><strong>使用场景</strong>（官网翻译）：</p>
<p>每天采集的ng日志和埋点日志，在存储的时候建议使用外部表，因为日志数据是采集程序实时采集进来的，一旦被误删，恢复起来很麻烦。而且外部表方便数据的共享。</p>
<p>在做统计分析的时候用到的中间表、结果表建议使用内部表，因为这些数据不需要共享，使用内部表更为合适。并且很多时候结果分区我们只需要保留最近三天的数据，用外部表的时候删除分区时无法删除数据。</p>
<h3 id="6-⭐⭐Hive的数据类型及其类型间的转化"><a href="#6-⭐⭐Hive的数据类型及其类型间的转化" class="headerlink" title="6. ⭐⭐Hive的数据类型及其类型间的转化"></a>6. ⭐⭐Hive的数据类型及其类型间的转化</h3><p>基本数据类型：</p>
<table>
<thead>
<tr>
<th>Hive</th>
<th>说明</th>
<th>对应java</th>
</tr>
</thead>
<tbody><tr>
<td>tinyint</td>
<td>1byte有符号整数</td>
<td>byte</td>
</tr>
<tr>
<td>smallint</td>
<td>2byte有符号整数</td>
<td>short</td>
</tr>
<tr>
<td>int</td>
<td>4byte有符号整数</td>
<td>int</td>
</tr>
<tr>
<td>bigint</td>
<td>8byte有符号整数</td>
<td>long</td>
</tr>
<tr>
<td>boolean</td>
<td>布尔类型，true或false</td>
<td>boolean</td>
</tr>
<tr>
<td>float</td>
<td>单精度浮点数</td>
<td>float</td>
</tr>
<tr>
<td>double</td>
<td>双精度浮点数</td>
<td>double</td>
</tr>
<tr>
<td>decimal</td>
<td>十进制精准数据类型，<br>decimal(10,2)：整体最多10位，小数部分两位</td>
<td></td>
</tr>
<tr>
<td>varchar</td>
<td>字符序列，需要指定最大长度，varchar(32)</td>
<td>char</td>
</tr>
<tr>
<td>string</td>
<td>字符串，无需指定最大长度</td>
<td>String</td>
</tr>
<tr>
<td>timestamp</td>
<td>时间类型，可以存储整数，浮点型，字符串</td>
<td></td>
</tr>
<tr>
<td>binary</td>
<td>二进制数据</td>
<td></td>
</tr>
</tbody></table>
<p>复杂数据类型：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
<th>定义</th>
<th>取值</th>
</tr>
</thead>
<tbody><tr>
<td>array</td>
<td>一组相同数据类型的值的集合</td>
<td>array&lt;string&gt;</td>
<td>arr[0]</td>
</tr>
<tr>
<td>map</td>
<td>一组相同类型的键值堆集合</td>
<td>map&lt;string, int&gt;</td>
<td>map[‘key’]</td>
</tr>
<tr>
<td>struct</td>
<td>由多个属性组成，每个属性都有自己的属性名和数据类型</td>
<td>struct&lt;id:int, name:string&gt;</td>
<td>struct.id</td>
</tr>
</tbody></table>
<p>隐式转换：整数范围小的类型转换为范围大的类型，整形可转为float，布尔类型不能转换为其他任何类型</p>
<p>显式转换：cast(<code>字段</code> as <code>需要转换的类型</code>)</p>
<h3 id="7-运维如何对Hive进行调度？"><a href="#7-运维如何对Hive进行调度？" class="headerlink" title="7. 运维如何对Hive进行调度？"></a>7. 运维如何对Hive进行调度？</h3><ul>
<li>将 hive 的 sql 定义在脚本当中</li>
<li>使用 azkaban 或者 oozie 进行任务的调度</li>
<li>监控任务调度页面</li>
</ul>
<h3 id="8-⭐Hive中文件的存储格式，以及ORC、Parquet等列式存储的优点"><a href="#8-⭐Hive中文件的存储格式，以及ORC、Parquet等列式存储的优点" class="headerlink" title="8. ⭐Hive中文件的存储格式，以及ORC、Parquet等列式存储的优点"></a>8. ⭐Hive中文件的存储格式，以及ORC、Parquet等列式存储的优点</h3><p>Hive支持的存储格式：行式存储：TextFile（默认），SequenceFile；列式存储：ORC、Parquet</p>
<p>列式存储的优点：select 某几个字段效率高；</p>
<p>行式存储的优点：select *效率高；</p>
<p>ORC和Parquet都是高性能的列式存储方式。</p>
<p><strong>Parquet</strong>：首尾中间由若干个row group（行组）和Footer，一个行组对应逻辑表中的若干行，一个行组中的一列保存在一个列块中，一个列块数据会被划分为若干页；Footer中存储了行组和列块的元数据信息。</p>
<p><strong>行组（Row Group）</strong>：一个行组对应逻辑表中的若干行。 </p>
<p><strong>列块（Column Chunk）</strong>：一个行组中的一列保存在一个列块中。 </p>
<p><strong>页（Page）</strong>：一个列块的数据会划分为若干个页。 </p>
<img src="Snipaste_2024-08-09_14-15-42-17254540094474.png" alt="Snipaste_2024-08-09_14-15-42" style="zoom: 50%;">

<ul>
<li>支持嵌套的数据模型；</li>
<li>没有Map、Array这样的复杂数据结构；</li>
<li>以二进制方式存储，不可以直接读取和修改</li>
</ul>
<p>ORC：</p>
<img src="Snipaste_2024-08-09_14-19-18-17254540094473.png" alt="Snipaste_2024-08-09_14-19-18" style="zoom:50%;">

<img src="image-20240825224642085.png" alt="image-20240825224642085" style="zoom: 50%;">

<p>每个Orc文件由Header、Body和Tail三部分组成。</p>
<p>其中Header内容为ORC，用于表示文件类型。</p>
<p>Body由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，每个stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer。</p>
<p>Tail由File Footer和PostScript组成。File Footer中保存了各Stripe的其实位置、索引长度、数据长度等信息，各Column的统计信息等；PostScript记录了整个文件的压缩类型以及File Footer的长度信息等。</p>
<p>在读取ORC文件时，会先从最后一个字节读取PostScript长度，进而读取到PostScript，从里面解析到File Footer长度，进而读取FileFooter，从中解析到各个Stripe信息，再读各个Stripe，<strong>即从后往前读。</strong></p>
<ul>
<li>ORC 中使用了更加精确的索引信息，使得在读取数据时可以指定从任意一行开始读取</li>
<li>ORC 会尽可能合并多个离散的区间尽可能的减少 I&#x2F;O 次数</li>
<li>以二进制方式存储，不可以直接读取和修改</li>
</ul>
<p><img src="image-20240903103248436.png" alt="image-20240903103248436"></p>
<h3 id="9-⭐说说Hive的4个By区别"><a href="#9-⭐说说Hive的4个By区别" class="headerlink" title="9. ⭐说说Hive的4个By区别"></a>9. ⭐说说Hive的4个By区别</h3><ul>
<li>order by：order by a，<strong>全局按字段a排序，只有一个Reduce</strong>。当输入规模较大时，需要较长的计算时间（生产环境中慎用）</li>
<li>sort by：sort by a，<strong>为每个reduce产生一个排序文件</strong>。<strong>每个Reduce内部按字段a进行排序</strong>，对全局结果集来说不是排序</li>
<li>distrbute by：distribute by a sort by b，类似MapReduce中partition（自定义分区），<strong>按照字段a进行分区，每一个分区内按照字段b进行排序</strong>，结合sort by使用。 </li>
<li>cluster by：cluster by a，当distribute by和sort by字段相同时，可以使用cluster by方式。<strong>按字段a进行分区，每个分区内按照字段a进行排序</strong>。不能指定升序还是降序，默认是升序的</li>
</ul>
<h3 id="10-⭐⭐说一说Hive的窗口函数"><a href="#10-⭐⭐说一说Hive的窗口函数" class="headerlink" title="10. ⭐⭐说一说Hive的窗口函数"></a>10. ⭐⭐说一说Hive的窗口函数</h3><p>窗口函数的语法中主要包括“窗口”和“函数”两部分。<strong>其中“窗口”用于定义计算范围，“函数”用于定义计算逻辑</strong>。</p>
<img src="22226-34-30.png" alt="22226-34-30" style="zoom:50%;">

<p>lag：整列向下移动；lead：整列向上移动</p>
<p><img src="Snipaste_2023-10-18_20-52-54.png" alt="Snipaste_2023-10-18_20-52-54"></p>
<p><img src="552023-10-18_20-58-47.png" alt="552023-10-18_20-58-47"></p>
<p><img src="6ste_2023-10-18_21-15-06.png" alt="6ste_2023-10-18_21-15-06"></p>
<p><img src="image-20240821111250457.png" alt="image-20240821111250457"></p>
<p><img src="image-20240902134543775.png" alt="image-20240902134543775"></p>
<p>窗口函数写在select语句中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">函数() over (partition by 字段1 order by 字段2 [rows|range between...and...]) 别名</span><br></pre></td></tr></table></figure>

<ul>
<li>函数：max，min，sum，avg，count，lead，lag，rank，dense_rank，row_rank…</li>
<li>rows|range between…and…：自定义窗口，lag和lead函数不支持自定义窗口，rank 、dense_rank、row_number不支持自定义窗口</li>
<li>rank()：排序相同时会重复，总数不会变</li>
<li>dense_rank()：排序相同时会重复，总数会减少</li>
<li>row_number()：会根据顺序计算</li>
</ul>
<h3 id="11-⭐⭐在项目中是否自定义过-UDF-、UDAF、UDTF-函数，以及用他们处理了什么问题，及自定义步骤？"><a href="#11-⭐⭐在项目中是否自定义过-UDF-、UDAF、UDTF-函数，以及用他们处理了什么问题，及自定义步骤？" class="headerlink" title="11. ⭐⭐在项目中是否自定义过 UDF 、UDAF、UDTF  函数，以及用他们处理了什么问题，及自定义步骤？"></a>11. ⭐⭐在项目中是否自定义过 UDF 、UDAF、UDTF  函数，以及用他们处理了什么问题，及自定义步骤？</h3><p>自定义过</p>
<ul>
<li><p>UDF：一进一出</p>
</li>
<li><p>UDAF：多进一出</p>
</li>
<li><p>UDTF：一进多出</p>
</li>
<li><p>UDF：实现给定基本数据类型的长度或者给用户的敏感信息加密</p>
<p>步骤：创建一个类继承GenericUDF或UDF，重写initialize方法和evaluate方法（或evalute方法），在HDFS中上传jar包，执行创建函数语句</p>
</li>
<li><p>UDTF：没定义过</p>
<p>步骤：创建一个类继承GenericUDTF，重写initialize方法、process方法和close方法，在HDFS中上传jar包，执行创建函数语句</p>
</li>
</ul>
<p>添加jar包的UDF路径（该路径在上传jar包给平台的时候会获取到）</p>
<p>创建临时函数：create temporary function…</p>
<p>创建永久函数：create function…</p>
<p><strong>下面是一个例子</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用Description定义函数说明，这样用户可以使用&quot;show create function&quot;查看这些说明</span></span><br><span class="line"><span class="meta">@Description(name = &quot;example_arraysum&quot;, value = &quot;_FUNC_(expr) - Example UDAF that returns the sum&quot;)</span></span><br><span class="line"><span class="comment">//继承org.apache.hadoop.hive.ql.exec.UDF接口，实现evalute方法</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">UDFExampleArraySum</span> <span class="keyword">extends</span> <span class="title class_">UDF</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> Double <span class="title function_">evaluate</span><span class="params">(List&lt;Double&gt; a)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (a == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">double</span> <span class="variable">total</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; a.size(); i++) &#123;</span><br><span class="line">            <span class="type">Double</span> <span class="variable">e</span> <span class="operator">=</span> a.get(i);</span><br><span class="line">            <span class="keyword">if</span> (e != <span class="literal">null</span>) &#123;</span><br><span class="line">                total += e;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> total;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>打成jar包上传至HDFS，获取UDF路径<code>virefs:///hom/dp/data/udf/19/owesome-udf-example-1.0-SNAPSHOT.jar</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">add jar virefs:///hom/dp/data/udf/19/owesome-udf-example-1.0-SNAPSHOT.jar;</span><br><span class="line">create temporary function arrary_sum as &#x27;com.kuaishou.udf.examples.UDFExampleArraySum&#x27;;</span><br><span class="line">select array_sum(array(12,10.4,1));//23.4</span><br></pre></td></tr></table></figure>

<h3 id="12-⭐⭐Hive的常用函数"><a href="#12-⭐⭐Hive的常用函数" class="headerlink" title="12. ⭐⭐Hive的常用函数"></a>12. ⭐⭐Hive的常用函数</h3><p>可以通过<code>SHOW FUNCTIONS</code>查看Hive的内置函数。通过<code>DESCRIBE FUNCTION 函数名</code>;查看函数的输入输出功能描述</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">-- 1. substring：截取字符串</span><br><span class="line">-- 获取第二个字符以后的所有字符（正向索引从1开始,负向索引从-1开始）</span><br><span class="line">select substring(&quot;atguigu&quot;,2);</span><br><span class="line">--tguigu</span><br><span class="line">-- 获取倒数第三个字符以后的所有字符</span><br><span class="line">select substring(&quot;atguigu&quot;,-3);</span><br><span class="line">--igu</span><br><span class="line">-- 从第3个字符开始，向后获取2个字符</span><br><span class="line">select substring(&quot;atguigu&quot;,3,2);</span><br><span class="line">--gu</span><br><span class="line"></span><br><span class="line">--6. split ：字符串切割</span><br><span class="line">/**</span><br><span class="line">    语法：split(string str, string pat)</span><br><span class="line">    返回值：array</span><br><span class="line">    说明：按照正则表达式pat匹配到的内容分割str，分割后的字符串，以数组的形式返回。</span><br><span class="line"> */</span><br><span class="line">select split(&#x27;a-b-c-d&#x27;,&#x27;-&#x27;);</span><br><span class="line">-- [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;]</span><br><span class="line">select split(&#x27;192.168.10.102&#x27;, &#x27;\\.&#x27;);</span><br><span class="line">-- [&quot;192&quot;,&quot;168&quot;,&quot;10&quot;,&quot;102&quot;]</span><br><span class="line"></span><br><span class="line">--7. nvl ：替换null值</span><br><span class="line">/**</span><br><span class="line">  语法：nvl(A,B)</span><br><span class="line">  说明：若A的值不为null，则返回A，否则返回B</span><br><span class="line"> */</span><br><span class="line">select nvl(null,1);</span><br><span class="line">--1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--8. concat ：拼接字符串</span><br><span class="line">/**</span><br><span class="line">语法：concat(string A, string B, string C, ……)</span><br><span class="line">返回：string</span><br><span class="line">说明：将A,B,C……等字符拼接为一个字符串</span><br><span class="line"> */</span><br><span class="line">select concat(&#x27;beijing&#x27;,&#x27;-&#x27;,&#x27;shanghai&#x27;,&#x27;-&#x27;,&#x27;shenzhen&#x27;);</span><br><span class="line">-- beijing-shanghai-shenzhen</span><br><span class="line">select &#x27;beijing&#x27;||&#x27;-&#x27;||&#x27;shanghai&#x27;||&#x27;-&#x27;||&#x27;shenzhen&#x27;;</span><br><span class="line">-- beijing-shanghai-shenzhen</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--9.concat_ws：以指定分隔符拼接字符串或者字符串数组</span><br><span class="line">/**</span><br><span class="line">语法：concat_ws(string A, string…| array(string))</span><br><span class="line">返回值：string</span><br><span class="line">说明：使用分隔符A拼接多个字符串，或者一个数组的所有元素。</span><br><span class="line"> */</span><br><span class="line">select concat_ws(&#x27;-&#x27;,&#x27;beijing&#x27;,&#x27;shanghai&#x27;,&#x27;shenzhen&#x27;);</span><br><span class="line">-- beijing-shanghai-shenzhen</span><br><span class="line">select concat_ws(&#x27;-&#x27;,array(&#x27;beijing&#x27;,&#x27;shenzhen&#x27;,&#x27;shanghai&#x27;));</span><br><span class="line">--beijing-shenzhen-shanghai</span><br><span class="line"></span><br><span class="line">--一些日期函数</span><br><span class="line">select datediff(&#x27;2023-08-08&#x27;,&#x27;2022-10-09&#x27;);  --303</span><br><span class="line">select date_add(&quot;2023-04-18&quot;,100);  --2023-07-27</span><br><span class="line">select date_sub(&quot;2023-04-18&quot;,100);  --2023-01-08</span><br><span class="line">select date_format(&#x27;2023-04-18&#x27;, &#x27;yyyy年-MM月-dd日&#x27;); --2023年-04月-18日</span><br></pre></td></tr></table></figure>

<img src="image-20240903095932151.png" alt="image-20240903095932151" style="zoom:50%;">

<img src="image-20240903095957650.png" alt="image-20240903095957650" style="zoom:50%;">

<p><strong>nvl()与coalesce()的区别:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvl(参数1,参数2);--如果第一个参数为null，则返回第二个参数，如果第一个参数不为null，则返回第一个参数</span><br><span class="line">coalesce(参数1,参数2,参数3....参数n);--从左往右数，遇到第一个非null值，则返回该非null值</span><br></pre></td></tr></table></figure>

<p><strong>高级聚合函数：</strong></p>
<img src="Snipaste_2024-08-06_17-33-43-17254540094475.png" alt="Snipaste_2024-08-06_17-33-43" style="zoom: 50%;">

<p><strong>炸裂函数</strong>：</p>
<img src="Snipaste_2024-08-06_22-32-29-17254540094476.png" alt="Snipaste_2024-08-06_22-32-29" style="zoom: 33%;">



<h3 id="13-⭐使用过Hive解析JSON串吗？"><a href="#13-⭐使用过Hive解析JSON串吗？" class="headerlink" title="13. ⭐使用过Hive解析JSON串吗？"></a>13. ⭐使用过Hive解析JSON串吗？</h3><p>使用过。</p>
<p>（1）使用Hive中的get_json_object函数（最常用）</p>
<p>能够解析嵌套的json，但每次只能解析一次</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">--10.get_json_object：解析json字符串</span><br><span class="line">/**</span><br><span class="line">语法：get_json_object(string json_string, string path)</span><br><span class="line">返回值：string</span><br><span class="line">说明：解析json的字符串json_string，返回path指定的内容。如果输入的json字符串无效，那么返回NULL。</span><br><span class="line"> */</span><br><span class="line">--获取json数组里面的json具体数据，其中$指代的就是传入的Json字符串本身</span><br><span class="line">select get_json_object(&#x27;[&#123;&quot;name&quot;:&quot;大海海&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;25&quot;&#125;,&#123;&quot;name&quot;:&quot;小宋宋&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;47&quot;&#125;]&#x27;,&#x27;$.[0]&#x27;);</span><br><span class="line">--&#123;&quot;name&quot;:&quot;大海海&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;25&quot;&#125;</span><br><span class="line">select get_json_object(&#x27;[&#123;&quot;name&quot;:&quot;大海海&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;25&quot;&#125;,&#123;&quot;name&quot;:&quot;小宋宋&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;47&quot;&#125;]&#x27;,&#x27;$.[0].name&#x27;);</span><br><span class="line">--大海海</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--实际工作中</span><br><span class="line">nvl(get_json_object(f.experience_json,&#x27;$.short_link_cnt&#x27;),0)</span><br></pre></td></tr></table></figure>

<p>（2）使用json_tuple</p>
<p>每次能够同时解析多个字段，不能解析嵌套的json</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with tmp_1 as (</span><br><span class="line">    select json_tuple(&#x27;&#123;&quot;name&quot;:&quot;zhangshan&quot;,&quot;age&quot;:18,&quot;addr&quot;:&#123;&quot;province&quot;:&quot;广东省&quot;,&quot;city&quot;:&quot;广州市&quot;&#125;&#125;&#x27;, &#x27;addr&#x27;) as addr</span><br><span class="line">)</span><br><span class="line">select json_tuple(addr,&#x27;province&#x27;) from tmp_1;--广东省</span><br></pre></td></tr></table></figure>

<p>结合炸裂函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">with tmp_1 as (</span><br><span class="line">    select 1 as id,&#x27;&#123;&quot;name&quot;:&quot;zhangshan&quot;,&quot;age&quot;:18,&quot;addr&quot;:&#123;&quot;province&quot;:&quot;广东省&quot;,&quot;city&quot;:&quot;广州市&quot;&#125;&#125;&#x27; as info</span><br><span class="line">)</span><br><span class="line">select id,name,age from tmp_1</span><br><span class="line">lateral view json_tuple(info,&#x27;name&#x27;,&#x27;age&#x27;) info_view as name,age</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2024-08-06_23-06-42-17254540094477.png" alt="Snipaste_2024-08-06_23-06-42"></p>
<p>（2）使用serde处理</p>
<p>考虑使用专门负责JSON文件的JSON Serde，设计表字段时，表的字段与JSON字符串中的一级字段保持一致，对于具有嵌套结构的JSON字符串，考虑使用合适复杂数据类型保存其内容。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table teacher</span><br><span class="line">(</span><br><span class="line">    name     string,</span><br><span class="line">    friends  array&lt;string&gt;,</span><br><span class="line">    students map&lt;string,int&gt;,</span><br><span class="line">    address  struct&lt;city:string,street:string,postal_code:int&gt;</span><br><span class="line">)</span><br><span class="line">row format serde &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">location &#x27;/user/hive/warehouse/teacher&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="14-⭐Hive分区和分桶"><a href="#14-⭐Hive分区和分桶" class="headerlink" title="14. ⭐Hive分区和分桶"></a>14. ⭐Hive分区和分桶</h3><p>Hive中的分区就是把一张大表的数据按照业务需要分散的存储到<strong>多个目录</strong>，每个目录就称为该表的一个分区。在查询时通过where子句中的表达式选择查询所需要的分区，这样的查询效率会提高很多。<code>partitioned by(分区字段 字段类型)</code>分区字段要求放在create结构外</p>
<ul>
<li>分区字段一定是表外字段</li>
<li>不建议使用中文</li>
<li>不建议分区数过多（避免小文件过多）</li>
<li>不建议使用动态分区</li>
</ul>
<p>分桶：按照某字段hash值%分桶数，将结果相同的写入同一个文件中，一个表没有分区直接按照整体分桶。已经分区就在每个分好区的表内进行分桶。分桶是更细粒度的范围划分。<code>clustered by(分桶字段)</code>分桶字段要求放在create结构内</p>
<p>分区&#x2F;分桶区别：<strong>分区</strong>针对的是<strong>存储路径</strong>，<strong>分桶</strong>针对的是<strong>数据文件</strong>。</p>
<h3 id="15-⭐为什么Hive分区表不建议使用动态分区？什么时候会使用动态分区？（字节）"><a href="#15-⭐为什么Hive分区表不建议使用动态分区？什么时候会使用动态分区？（字节）" class="headerlink" title="15. ⭐为什么Hive分区表不建议使用动态分区？什么时候会使用动态分区？（字节）"></a>15. ⭐为什么Hive分区表不建议使用动态分区？什么时候会使用动态分区？（字节）</h3><p>动态分区是指向分区表insert数据时，被写往的分区不由用户指定【即只写明分区字段是谁，具体哪个分区不说】，而是由每行数据的最后一个字段的值来动态的决定。<strong>使用动态分区，可只用一个insert语句将数据写入多个分区。</strong></p>
<p><strong>情况1：一级动态分区</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">-- （1）创建目标分区表</span><br><span class="line">create table dept_partition_dynamic(</span><br><span class="line">    id int,</span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">partitioned by (loc int)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br><span class="line">-- 2）设置动态分区</span><br><span class="line">set hive.exec.dynamic.partition=true;</span><br><span class="line">set hive.exec.dynamic.partition.mode = nonstrict;</span><br><span class="line">insert into table dept_partition_dynamic</span><br><span class="line">partition(loc)  --不需要声明指定某个分区，只需要声明分区字段即可</span><br><span class="line">select</span><br><span class="line">    deptno,</span><br><span class="line">    dname,</span><br><span class="line">    loc  --按照loc字段的值进行动态分区</span><br><span class="line">from default.dept;</span><br></pre></td></tr></table></figure>
<p><strong>情况2：二级动态分区</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">-- 1）二级分区表建表语句</span><br><span class="line">create table dept_partition2(</span><br><span class="line">    deptno int,    -- 部门编号</span><br><span class="line">    dname string, -- 部门名称</span><br><span class="line">    loc string     -- 部门位置</span><br><span class="line">)</span><br><span class="line">partitioned by (day string, hour string) --采用二级分区</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br><span class="line">-- 2）设置动态分区</span><br><span class="line">set hive.exec.dynamic.partition=true;</span><br><span class="line">set hive.exec.dynamic.partition.mode = nonstrict;</span><br><span class="line">insert into table dept_partition2</span><br><span class="line">partition(day,hour)  --不需要声明指定某个分区，只需要声明分区字段即可</span><br><span class="line">select</span><br><span class="line">    deptno,</span><br><span class="line">    dname,</span><br><span class="line">    loc,</span><br><span class="line">    day,</span><br><span class="line">    hour  --按照day和hour字段的值进行动态分区</span><br><span class="line">from default.dept;</span><br></pre></td></tr></table></figure>
<p><strong>情况3：动静态分区混用</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">-- 1）二级分区表建表语句</span><br><span class="line">create table dept_partition2(</span><br><span class="line">    deptno int,    -- 部门编号</span><br><span class="line">    dname string, -- 部门名称</span><br><span class="line">    loc string     -- 部门位置</span><br><span class="line">)</span><br><span class="line">partitioned by (day string, hour string) --采用二级分区</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br><span class="line">-- 2）设置动态分区</span><br><span class="line">set hive.exec.dynamic.partition.mode = nonstrict;</span><br><span class="line">insert into table dept_partition2</span><br><span class="line">partition(day = &#x27;20240304&#x27;,hour)  --不需要声明指定某个分区，只需要声明分区字段即可</span><br><span class="line">select</span><br><span class="line">    deptno,</span><br><span class="line">    dname,</span><br><span class="line">    loc,</span><br><span class="line">    day,</span><br><span class="line">    hour  --按照day和hour字段的值进行动态分区</span><br><span class="line">from default.dept</span><br><span class="line">where day = &#x27;20240304&#x27;;</span><br></pre></td></tr></table></figure>

<ul>
<li>静态分区的情况下，使用where子句过滤确定的某一个分区（分区过滤），是发生在Map的上一个阶段，即在输入阶段进行路径的过滤，效率极高。动态分区由于分区不确定，所有记录都会进行<code>distribute by</code>，在reduce阶段才指导每条数据应该插到哪个分区，如果分区很多，在reduce端会造成数据瓶颈，性能很低。</li>
<li>动态分区通常是在数据写入时自动创建的，因此难以预知分区数量。在某些情况下，如果分区数量过多，在HDFS中创建大量的目录和文件，造成Namenode性能降低，也会对<strong>查询性能</strong>产生负面影响。</li>
<li>动态分区可能会导致元数据和HDFS数据内容的修改，这可能会增加管理和维护的<strong>复杂性</strong>。</li>
</ul>
<p>Hive的动态分区主要在以下场景中会被使用：</p>
<ol>
<li><strong>数据导入的需求</strong>：当有一张Hive表，里面有一个字段是时间字段，每天的数据是按这个时间字段导入进去的，这时候就需要动态分区。这是因为对于大数据批量导入来说，手动去指定分区显然是不现实的。动态分区可以在往表中插入数据的时候，动态的根据值来选择数据进入的分区。</li>
<li><strong>分区数量庞大（分区级数多）的情况</strong>：例如气象站的气温记录数据，根据年份分区之后，还有根据月份分区，下面可能还有根据日期分区。当分区变得非常多的时候，手动去指定分区显然是不现实的。这个时候，就需要使用到动态分区。</li>
</ol>
<h3 id="16-⭐⭐⭐数据倾斜怎么解决？"><a href="#16-⭐⭐⭐数据倾斜怎么解决？" class="headerlink" title="16. ⭐⭐⭐数据倾斜怎么解决？"></a>16. ⭐⭐⭐数据倾斜怎么解决？</h3><h4 id="表现："><a href="#表现：" class="headerlink" title="表现："></a>表现：</h4><p>MapReduce：</p>
<ul>
<li>有一个或多个Reduce卡住，卡在99.99%</li>
<li>有异常的reduce读写数据量远远大于其他reduce</li>
</ul>
<p>Spark：</p>
<ul>
<li>Driver OOM</li>
<li>Shuffle过程出错</li>
<li>单个job的stage执行时间特别久，整个任务卡在某阶段不动</li>
<li>比如我们在Spark Web UI或者本地log中发现，job0的stage1的某几个task执行得特别慢，判定job0-stage1出现了数据倾斜，那么就可以回到代码中，定位出job0-stage1主要包括了哪个shuffle类算子，此时基本就可以确定是是该算子导致了数据倾斜问题。<br>该Stage有Shuffle Read，所以为Shuffle Stage，出现Shuffle Stage的原因大概有三种：Join、Group by、count(distinct)，下面对具体的问题进行定位。</li>
</ul>
<h4 id="本质原因："><a href="#本质原因：" class="headerlink" title="本质原因："></a>本质原因：</h4><p>（1）map端：如果文件使用GZIP压缩等不支持文件分割操作的压缩方式时，若该不可分割的文件超大，被一个map读取时，就会发生map阶段的数据倾斜。</p>
<p>（2）reduce端（更容易出现）：map 到 reduce 会经过 shuffle 阶段，在 shuffle 中默认会按照 key进行重新分区，<strong>如果相同的 key 过多，那么重新分区的结果就是大量相同的 key 进入到同一个 reduce 中</strong>，导致数据倾斜</p>
<p>简而言之：旱的旱死涝的涝死</p>
<h4 id="各种数据倾斜的原因及解决方案："><a href="#各种数据倾斜的原因及解决方案：" class="headerlink" title="各种数据倾斜的原因及解决方案："></a>各种数据倾斜的原因及解决方案：</h4><p>（0）通用方法：提前在map端进行combine，减少shuffle过程中传输的数据量以及Reducer端的计算量；shuffle阶段，进入环形缓冲区之前，可以自定义分区</p>
<p>（1）空值引发的数据倾斜</p>
<ul>
<li><p>原因：实际业务中有些大量的 null 值或者一些无意义的数据参与到计算作业中，表中有大量的 null 值，如果表之间进行 join 操作，就会有 shuffle 产生，<strong>这样所有的 null 值都会被分配到一个 reduce 中，必然产生数据倾斜。</strong></p>
</li>
<li><p>解决方案：可以直接不让 null 值参与 join 操作，关联条件排除掉null值或者可以给null 值随机赋值，这样它们的 hash 结果就不一样，就会进到不同的 reduce 中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT *</span><br><span class="line">FROM log a</span><br><span class="line">LEFT JOIN users b ON CASE</span><br><span class="line">WHEN a.user_id IS NULL THEN concat(&#x27;hive_&#x27;, rand())</span><br><span class="line">ELSE a.user_id</span><br><span class="line">END = b.user_id;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>（2）关联字段的不同数据类型引发的数据倾斜</p>
<ul>
<li>原因：对于两个表join，表a中需要 join的字段key为 int，表b中key 字段既有string类型也有 int 类型。当按照 key 进行两个表的 join 操作时，默认的 Hash 操作会按 int 型的 id 来进行分配，这样所有的 string 类型都被分配成同一个 id，结果就是所有的 string 类型的字段进入到一个 reduce 中，引发数据倾斜。</li>
<li>解决方案：如果 key 字段既有 string 类型也有 int 类型，默认的 hash 就都会按 int 类型来分配，那我们直接把 int 类型都转为 string 就好了，这样 key 字段都为 string，hash 时就按照 string 类型分配了</li>
</ul>
<p>（3）不可拆分大文件引发的数据倾斜</p>
<ul>
<li>原因：如果文件使用GZIP压缩等不支持文件分割操作的压缩方式时，若该不可分割的文件超大，被一个map读取时，就会发生map阶段的数据倾斜。</li>
<li>解决方案：这种数据倾斜问题没有什么好的解决方案，只能将使用 GZIP 压缩等不支持文件分割的文件转为 bzip 和 zip 等支持文件分割的压缩方式。</li>
</ul>
<p>（4）⭐分组聚合导致的数据倾斜</p>
<ul>
<li><p>原因：如果group by分组字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题</p>
</li>
<li><p>解决方案：</p>
<p>开启map-side聚合：开启Map-Side聚合后，数据会现在Map端完成部分聚合工作。这样一来即便原始数据是倾斜的，经过Map端的初步聚合后，发往Reduce的数据也就不再倾斜了。最佳状态下，Map-端聚合能完全屏蔽数据倾斜问题。</p>
<p>开启Skew-GroupBy优化：Skew-GroupBy的原理是启动两个MR任务，第一个MR按照随机数分区，将数据分散发送到Reduce，完成部分聚合，第二个MR按照分组字段分区，完成最终聚合。</p>
</li>
</ul>
<p>举具体例子：项目中的order_detail表，大概2000万行数据，其中province_id字段99%以上都是1（北京），也就是大部分的订单都是北京这个地区产生的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">	province_id,</span><br><span class="line">	count(*)</span><br><span class="line">from order_detail</span><br><span class="line">group by province_id;</span><br></pre></td></tr></table></figure>

<p><img src="%E5%9B%BE%E7%89%871.png" alt="图片1"></p>
<p>开启map-side聚合：</p>
<img src="图片2.png" alt="图片2" style="zoom:33%;">

<p>开启Skew-GroupBy优化：</p>
<img src="图片3.png" alt="图片3" style="zoom:33%;">

<p>（5）⭐表join连接时引发的数据倾斜：</p>
<ul>
<li><p>原因：未经优化的join操作，默认是使用common join算法，也就是通过一个MapReduce Job完成计算。Map端负责读取join操作所需表的数据，并按照关联字段进行分区，通过Shuffle，将其发送到Reduce端，相同key的数据在Reduce端完成最终的Join操作。如果关联字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题</p>
</li>
<li><p>解决方案：</p>
<p>①map join：适用于<strong>大表join小表</strong>。通过<strong>两个只有Map阶段的Job完成一个Join工作</strong>：第一个Job读取小表数据上传HDFS缓存上，第二个Job先从HDFS缓存种读取小表数据，并缓存在Map Task的内存中，然后扫描大表数据。使用map join算法，join操作仅在map端就能完成，没有shuffle操作，没有reduce阶段，自然不会产生reduce端的数据倾斜。</p>
<p>②skew join： 适用于 <strong>inner join</strong> 时发生数据倾斜的场景，整体思路就是使用<strong>独立的作业和 Map join</strong> 来处理倾斜的键。为倾斜的大key单独启动一个map join任务进行计算，其余key进行正常的common join</p>
<p>③调整SQL语句：将数据倾斜的表的倾斜字段随机分成id_0和id_1两类（打散），将另一张表扩容一倍，一半id_0，一半id_1，再将这两张修改后的表join</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">--优化前</span><br><span class="line">select</span><br><span class="line">    *</span><br><span class="line">from A</span><br><span class="line">join B</span><br><span class="line">on A.id=B.id;</span><br><span class="line">--优化后</span><br><span class="line">select</span><br><span class="line">    *</span><br><span class="line">from(</span><br><span class="line">    select --打散操作</span><br><span class="line">        concat(id,&#x27;_&#x27;,cast(rand()*2 as int)) id,</span><br><span class="line">        value</span><br><span class="line">    from A</span><br><span class="line">)ta</span><br><span class="line">join(</span><br><span class="line">    select --扩容操作</span><br><span class="line">        concat(id,&#x27;_&#x27;,0) id,</span><br><span class="line">        value</span><br><span class="line">    from B</span><br><span class="line">    union all</span><br><span class="line">    select</span><br><span class="line">        concat(id,&#x27;_&#x27;,1) id,</span><br><span class="line">        value</span><br><span class="line">    from B</span><br><span class="line">)tb</span><br><span class="line">on ta.id=tb.id;</span><br></pre></td></tr></table></figure>

<p> 比如表A的id字段是倾斜的(id&#x3D;1的记录远大于其它id)，可以先把id&#x3D;1的记录(倾斜字段)筛选出来作为表C(临时表)，然后在id字段上加一个随机数后缀，然后表B膨胀相应的倍数并与C相关联，将表A无倾斜的数据和原来的表B相关联，最后union all两次关联的结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">select *</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select concat(id, &#x27;_&#x27;, FLOOR(RAND() * 5)+1)</span><br><span class="line">	from A</span><br><span class="line">	where id = 1</span><br><span class="line">) as t1</span><br><span class="line">left join</span><br><span class="line">(</span><br><span class="line">    select concat(id, &#x27;_&#x27;, 1) from B where id = 1</span><br><span class="line">    union all</span><br><span class="line">    select concat(id, &#x27;_&#x27;, 2) from B where id = 1</span><br><span class="line">    union all</span><br><span class="line">    select concat(id, &#x27;_&#x27;, 3) from B where id = 1</span><br><span class="line">    union all</span><br><span class="line">    select concat(id, &#x27;_&#x27;, 4) from B where id = 1</span><br><span class="line">    union all</span><br><span class="line">    select concat(id, &#x27;_&#x27;, 5) from B where id = 1</span><br><span class="line">) as t2</span><br><span class="line">on t1.id = t2.id</span><br><span class="line"></span><br><span class="line">union all</span><br><span class="line"></span><br><span class="line">select</span><br><span class="line">from A as t1 left join B as t2</span><br><span class="line">on A.id = B.id</span><br><span class="line">where A.id != 1 </span><br><span class="line"></span><br><span class="line">-- 总结：</span><br><span class="line">select A.id from A join B on A.id = B.id where A.id != 1</span><br><span class="line">union all </span><br><span class="line">select A.id from A join B on A.id = B.id where A.id = 1 and B.id = 1;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>举具体例子：项目中的order_detail表，大概2000万行数据，其中province_id字段99%以上都是1（北京），也就是大部分的订单都是北京这个地区产生的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">    *</span><br><span class="line">from order_detail od</span><br><span class="line">join province_info pi</span><br><span class="line">on od.province_id=pi.id;</span><br></pre></td></tr></table></figure>

<p><img src="%E5%9B%BE%E7%89%874.png" alt="图片4"></p>
<p>map join优化：只有map任务，没有reduce任务</p>
<img src="图片5.png" alt="图片5" style="zoom: 50%;">

<img src="图片6.png" alt="图片6" style="zoom:50%;">

<p>skew join：</p>
<img src="图片7.png" alt="图片7" style="zoom:50%;">

<p>（6）确实无法减少数据量引发的数据倾斜</p>
<p>这类问题最直接的方式就是调整 reduce 所执行的内存大小。调整 reduce 的内存大小使用 mapreduce.reduce.memory.mb 这个配置</p>
<p>（7）数据膨胀引发的数据倾斜</p>
<ul>
<li>原因：在多维聚合的时候，如果<strong>分组聚合的字段过多</strong>，且操作表的<strong>数据量很大</strong>，会导致数据在Map端产出到reduce端的数据量急速膨胀，从而导致内存溢出，如果含有倾斜的key还会导致数据倾斜。</li>
<li>解决方案：<br>配置参数<code>hive.new.job.grouping.set.cardinality</code>自动控制作业拆解，如果多维聚合的键值组合大于该值，会启用新的任务去处理大于该值之外的组合</li>
</ul>
<h3 id="17-⭐Hive小文件产生的原因"><a href="#17-⭐Hive小文件产生的原因" class="headerlink" title="17. ⭐Hive小文件产生的原因"></a>17. ⭐Hive小文件产生的原因</h3><p>（1）直接向表中插入数据：每次插入都会产生一个文件，多次插入少量数据就会出现多个小文件</p>
<p>（2）通过load方式加载数据，每导入一个文件hive表就会产生一个文件</p>
<p>（3）通过查询方式加载数据（最常见）：其中，文件数量&#x3D;ReduceTask数量×分区数（只有map阶段就是，文件数量&#x3D;MapTask数量×分区数）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table A  select s_id,c_name,s_score from B;</span><br></pre></td></tr></table></figure>

<h3 id="18-⭐HIve小文件过多的危害"><a href="#18-⭐HIve小文件过多的危害" class="headerlink" title="18. ⭐HIve小文件过多的危害"></a>18. ⭐HIve小文件过多的危害</h3><p>（1）对于HDFS而言，其本身就不适合存储大量小文件，小文件过多会导致NameNode元数据特别大，占用内存太多，影响HDFS性能。如果数据块设置的太小，一个大文件就会被分成多个数据块，增加整个大文件的寻址时间，寻址时间超过读取时间。</p>
<p>（2）对Hive而言，在进行查询时，每个小文件都会启动一个Map任务来完成，Map任务启动和初始化的时间远远大于逻辑处理的时间，本末倒置，造成资源浪费</p>
<h3 id="14-⭐Hive小文件过多怎么解决？"><a href="#14-⭐Hive小文件过多怎么解决？" class="headerlink" title="14. ⭐Hive小文件过多怎么解决？"></a>14. ⭐Hive小文件过多怎么解决？</h3><p>（1）Map端输入文件合并</p>
<p>合并Map端输入的小文件，是指将多个小文件划分到一个切片中，进而由一个Map Task去处理。目的是防止为单个小文件启动一个Map Task，浪费计算资源。也就是使用CombineHiveInputFormat</p>
<p>相关参数为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--可将多个小文件切片，合并为一个切片，进而由一个map任务处理</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>

<p>（2）Reduce端文件合并</p>
<p>合并Reduce端输出的小文件，是指将多个小文件合并成大文件。目的是减少HDFS小文件数量。其原理是根据计算任务输出文件的平均大小进行判断，若符合条件，则单独启动一个额外的任务进行合并。</p>
<p>相关参数为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">--开启合并map only任务输出的小文件</span><br><span class="line">set hive.merge.mapfiles=true;</span><br><span class="line"></span><br><span class="line">--开启合并map reduce任务输出的小文件</span><br><span class="line">set hive.merge.mapredfiles=true;</span><br><span class="line"></span><br><span class="line">--合并后的文件大小</span><br><span class="line">set hive.merge.size.per.task=256000000;</span><br><span class="line"></span><br><span class="line">--触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并</span><br><span class="line">set hive.merge.smallfiles.avgsize=16000000;</span><br></pre></td></tr></table></figure>

<p>（3）减少Reduce的数量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--reduce 的个数决定了输出的文件的个数，所以可以调整reduce的个数控制hive表的文件数量</span><br><span class="line">set mapreduce.job.reduces=10;</span><br></pre></td></tr></table></figure>

<p>（4）使用Hive自带的concatenate命令，自动合并小文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--对于非分区表</span><br><span class="line">alter table A concatenate;</span><br><span class="line"></span><br><span class="line">--对于分区表</span><br><span class="line">alter table B partition(day=20201224) concatenate;</span><br><span class="line">--使用concatenate命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令。 </span><br><span class="line">--当多次使用concatenate后文件数量不在变化，这个跟参数 mapreduce.input.fileinputformat.split.minsize=256mb 的设置有关，可设定每个文件的最小size。</span><br></pre></td></tr></table></figure>

<p>（5）使用hadoop的archive将小文件归档</p>
<p>Hadoop Archive简称HAR，是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样在减少namenode内存使用的同时，仍然允许对文件进行透明的访问</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">--用来控制归档是否可用</span><br><span class="line">set hive.archive.enabled=true;</span><br><span class="line">--通知Hive在创建归档时是否可以设置父目录</span><br><span class="line">set hive.archive.har.parentdir.settable=true;</span><br><span class="line">--控制需要归档文件的大小</span><br><span class="line">set har.partfile.size=1099511627776;</span><br><span class="line"></span><br><span class="line">--使用以下命令进行归档</span><br><span class="line">ALTER TABLE A ARCHIVE PARTITION(dt=&#x27;2020-12-24&#x27;, hr=&#x27;12&#x27;);</span><br><span class="line"></span><br><span class="line">--对已归档的分区恢复为原文件</span><br><span class="line">ALTER TABLE A UNARCHIVE PARTITION(dt=&#x27;2020-12-24&#x27;, hr=&#x27;12&#x27;);</span><br></pre></td></tr></table></figure>

<p>（6）减少JVM开关时间</p>
<p>如果有大量小文件要处理，那么频繁启动和关闭JVM会浪费大量时间，所以减少JVM开关时间可以减少作业处理时间</p>
<p>（7）如何解决分区过多（动态分区）容易产生小文件的问题？</p>
<ul>
<li>增加<code>spark.sql.shuffle.partitions</code> 参数。<br>Spark的默认shuffle分区数较高，特别是在数据规模较小的情况下，这可能导致每个分区的数据量非常少，进而产生小文件。根据数据量适当减少<code>shuffle</code>分区的数量，例如将<code>spark.sql.shuffle.partitions</code>从默认的200减少到50或更少。这样做会使得每个分区的数据量增大，从而减少生成的小文件数量。</li>
<li>合并文件<br>使用<code>coalesce</code>或<code>repartition</code>来合并小文件。例如，在保存到HDFS之前使用<code>coalesce</code>将分区数减少，从而减少输出的文件数。</li>
<li>动态分区数调优<br>在进行多级动态分区时，若每个分区的数据量很小，可能会导致大量小文件。根据数据分布情况，设置合理的动态分区数限制，避免数据被分得过于细致。例如，在Hive中使用<code>SET hive.exec.dynamic.partition.mode=nonstrict;</code> 和 <code>SET hive.exec.max.dynamic.partitions.pernode=1000;</code> 来控制每个节点的最大分区数。调整分区字段的组合，例如将粒度较细的字段合并为一个较粗的分区字段，以减少分区数</li>
<li>合理设置分区字段<br>分区字段的选择和粒度会影响文件的生成，过细的分区会产生大量小文件。在定义分区字段时，选择能够适当分割数据但不过度细化的字段。例如，避免以<code>timestamp</code>作为分区字段，而是使用<code>date</code>、<code>month</code>、<code>year</code>等较粗的时间粒度。使用辅助字段，如根据数据的使用频率或查询需求，将某些分区字段调整为非分区字段，减少分区数量。</li>
</ul>
<h3 id="19-⭐⭐⭐Hive优化有哪些？"><a href="#19-⭐⭐⭐Hive优化有哪些？" class="headerlink" title="19. ⭐⭐⭐Hive优化有哪些？"></a>19. ⭐⭐⭐Hive优化有哪些？</h3><p>（0）资源配置</p>
<p><strong>MapReduce引擎</strong>：</p>
<ul>
<li>MapTask申请container内存大小：<code>mapreduce.map.memory.mb</code></li>
<li>MapTask申请container的CPU核数：<code>mapreduce.map.cpu.vcores</code></li>
<li>ReduceTask申请container内存大小：<code>mapreduce.reduce.memory.mb</code></li>
<li>ReduceTask申请container的CPU核数：<code>mapreduce.reduce.cpu.vcores</code></li>
</ul>
<p><strong>Spark引擎</strong>：</p>
<ul>
<li>Executor申请container的CPU核数：<code>spark.executor.cores</code></li>
<li>Executor申请container的JVM进程堆内内存大小：<code>spark.executor.memory</code></li>
<li>Executor申请container的JVM进程堆外内存大小：<code>spark.executor.memoryOverhead</code></li>
<li>Executor的个数：静态：<code>spark.executor.instances</code>；动态：<img src="Snipaste_2024-08-07_17-15-20-17254540094478.png" alt="Snipaste_2024-08-07_17-15-20" style="zoom:33%;">还可以设置Executor的初始值，最大值和最小值</li>
<li>Driver申请container的JVM进程堆内内存大小：<code>spark.driver.memory</code></li>
<li>Driver申请container的JVM进程堆外内存大小：<code>spark.driver.memoryOverhead</code></li>
</ul>
<p>（1）数据存储及压缩优化</p>
<p>一般表格存储格式选用ORC（占有更少的存储），压缩格式选用snappy。因为Hive底层使用MR计算框架，数据流从HDFS到磁盘再到HDFS，使用ORC+snappy可以降低IO读写，还能降低网络传输量。</p>
<p>（2）有效地减小数据集将大表拆分成子表，结合使用分区表和分桶表</p>
<p>（3）HQL语法优化</p>
<ul>
<li><strong>分组聚合优化</strong>：主要围绕着减少Shuffle数据量进行，具体做法是<strong>map-side聚合</strong>。在map端维护一个<strong>hash table</strong>，完成部分聚合，然后再发送至reduce端</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--启用map-side聚合</span><br><span class="line">set hive.map.aggr=true;</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>Join优化</strong>：</p>
<p>①Common Join：Hive种最稳定的Join算法，通过一个MapReduce Job完成一个Join操作。Map端负责读取join操作所需表的数据，并按照关联字段进行分区，通过Shuffle，将其发送到Reduce端，相同key的数据在Reduce端完成最终的Join操作。</p>
<p>②Map Join：适用于<strong>大表join小表</strong>。通过<strong>两个只有Map阶段的Job完成一个Join工作</strong>：第一个Job读取小表数据上传HDFS缓存上，第二个Job先从HDFS缓存种读取小表数据，并缓存在Map Task的内存中，然后扫描大表数据；Hive优化器根据参与Join表的数据量大小，自动触发开启Map Join优化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--启用Map Join自动转换。</span><br><span class="line">set hive.auto.convert.join=true;</span><br><span class="line">--使用无条件转Map Join。</span><br><span class="line">set hive.auto.convert.join.noconditionaltask=true;</span><br></pre></td></tr></table></figure>

<p>③Bucket Map Join：对Map Join算法的改进，其打破了Map Join只适用于大表join小表的限制，可用于<strong>大表join大表</strong>的场景，<strong>若能保证参与join的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍</strong>，就能保证参与join的两张表的分桶之间具有明确的关联关系，所以就可以在两表的分桶间进行Map Join操作了。这样一来，第二个Job的Map端就无需再缓存小表的全表数据了，而只需缓存其所需的分桶即可。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--启用bucket map join优化功能</span><br><span class="line">set hive.optimize.bucketmapjoin = true;</span><br></pre></td></tr></table></figure>

<p>④Sort Merge Bucket Map Join：用于<strong>大表join小表</strong>，<strong>参与join的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍。</strong>SMB Map Join与Bucket Map Join相比，在进行Join操作时，Map端是无需对整个Bucket构建hash table，也无需在Map端缓存整个Bucket数据的，每个Mapper只需按顺序逐个key读取两个分桶的数据进行join即可。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--启动Sort Merge Bucket Map Join优化</span><br><span class="line">set hive.optimize.bucketmapjoin.sortedmerge=true;</span><br><span class="line">--使用自动转换SMB Join</span><br><span class="line">set hive.auto.convert.sortmerge.join=true;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>（4）数据倾斜的优化（看前面）</p>
<p>（5）小文件过多优化（看前面）</p>
<p>（6）任务并行度优化</p>
<ul>
<li><p>设置Map端个数（并行度）：</p>
<p>①减少map个数：若查询表中存在大量小文件，使用Hive提供的CombineHiveInputFormat，多个小文件合并为一个切片，从而控制map task个数</p>
<p>②增加map个数：：若SQL语句中有正则替换、json解析等复杂耗时的查询逻辑时，map端的计算会相对慢一些。若想加快计算速度，在计算资源充足的情况下，可考虑增大map端的并行度，令map task多一些，每个map task计算的数据少一些。</p>
</li>
<li><p>设置Reduce端个数（并行度）：</p>
<p>可以根据用户自己指定，也可以由Hive自行根据MR job输入的文件大小进行估算</p>
</li>
</ul>
<p>（5）其他优化</p>
<ul>
<li><p>CBO优化：基于计算成本（CPU、本地IO、HDFS IO）的优化。目前CBO在hive的MR引擎下主要用于join的优化，例如多表join的join顺序。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--是否启用cbo优化 </span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>
</li>
<li><p>谓词下推优化：尽量将过滤操作前移，以减少后续计算步骤的数据量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--是否启动谓词下推（predicate pushdown）优化</span><br><span class="line">set hive.optimize.ppd = true;</span><br></pre></td></tr></table></figure>
</li>
<li><p>矢量化查询：依赖于CPU的矢量化计算</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.vectorized.execution.enabled=true;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Fetch抓取：Hive中对某些情况的查询可以不必使用MapReduce计算。例如：select * from emp;在这种情况下，Hive可以简单地读取emp对应的存储目录下的文件，然后输出查询结果到控制台。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">--是否在特定场景转换为fetch 任务</span><br><span class="line">--设置为none表示不转换</span><br><span class="line">--设置为minimal表示支持select *，分区字段过滤，Limit等</span><br><span class="line">--设置为more表示支持select 任意字段,包括函数，过滤，和limit等</span><br><span class="line">set hive.fetch.task.conversion=more;</span><br></pre></td></tr></table></figure>
</li>
<li><p>本地模式：通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--开启自动转换为本地模式</span><br><span class="line">set hive.exec.mode.local.auto=true; </span><br></pre></td></tr></table></figure>
</li>
<li><p>并行模式：Hive会将一个SQL语句转化成一个或者多个Stage，并行执行没有依赖关系的Stage</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--启用并行执行优化</span><br><span class="line">set hive.exec.parallel=true;  </span><br></pre></td></tr></table></figure>
</li>
<li><p>严格模式：开启严格模式，防止危险操作</p>
</li>
<li><p>相关性优化：可以减少重复的shuffle操作。如果一个join操作后输出的结果是作为group by阶段的操作的输入，这种情况下group by操作没有必要重新shuffle，这种操作与操作之间的相关性在开启上述参数会被感知到并且减少shuflle操作</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.optimize.correlation = true;</span><br></pre></td></tr></table></figure>

<h3 id="20-⭐Hive的严格模式"><a href="#20-⭐Hive的严格模式" class="headerlink" title="20. ⭐Hive的严格模式"></a>20. ⭐Hive的严格模式</h3><p>①用户不允许扫描所有分区表，只能用where过滤一些分区字段。因为分区表通常非常大，扫描所有会浪费巨大资源</p>
<p>②对于使用了order by的语句的查询，必须使用limit语句，这样可以可以防止Reduce额外执行很长一段时间</p>
<p>③限制笛卡尔积查询</p>
<h3 id="21-HiveSQL语句的正确执行顺序"><a href="#21-HiveSQL语句的正确执行顺序" class="headerlink" title="21. HiveSQL语句的正确执行顺序"></a>21. HiveSQL语句的正确执行顺序</h3><p><strong>from .. where .. join .. on .. select .. group by .. select .. having .. distinct .. order by .. limit .. union&#x2F;union all</strong></p>
<blockquote>
<p>可以看到 group by 是在两个 select 之间，我们知道 Hive 是默认开启 map 端的 group by 分组的，所以<strong>在 map 端是 select 先执行，在 reduce 端是 group by 先执行</strong>。</p>
</blockquote>
<p>在执行计划中，每个stage都是一个独立的MR。</p>
<h3 id="22-Tez引擎的优点？"><a href="#22-Tez引擎的优点？" class="headerlink" title="22. Tez引擎的优点？"></a>22. Tez引擎的优点？</h3><p>Tez 可以将多个有依赖的作业转换为一个作业，这样只需写一次 HDFS，且中间节点较少，从而大大提升作业的计算性能。Tez支持DAG作业的计算，绕过了MapReduce很多不必要的中间的数据存储和读取的过程，直接在一个作业中表达了MapReduce需要多个作业共同协作才能完成的事情。</p>
<p>相比于MapReduce，当查询需要有多个reduce逻辑时，Hive的MapReduce引擎会将计划分解，每个Reduce提交一个MR作业。这个链中的所有MR作业都需要逐个调度，每个作业都必须从HDFS中重新读取上一个作业的输出并重新洗牌。而在Tez中，几个reduce接收器可以直接连接，数据可以流水线传输，而不需要临时HDFS文件，这种模式称之为MRR（Map-reduce-reduce）</p>
<h3 id="23-Hive计算引擎"><a href="#23-Hive计算引擎" class="headerlink" title="23. Hive计算引擎"></a>23. Hive计算引擎</h3><p>目前 Hive 支持 MapReduce、Tez 和 Spark 三种计算引擎</p>
<ol>
<li>MapReduce：这是Hive最初使用的计算引擎，也是Hadoop生态系统中的主要计算模型。MapReduce通过将任务分解成多个小任务（Map和Reduce阶段）来处理数据，这使得它非常适合大规模数据处理。然而，MapReduce的批处理特性使得它在处理交互式查询和实时数据处理方面不够高效。</li>
<li>Tez：完全基于内存，Tez将MapReduce的两次扫描优化为一次扫描，提高了查询性能。<strong>一般用于快速出结果，数据量较小的场景</strong></li>
<li>Spark：Spark是另一种高性能的计算引擎，它提供了内存计算和DAG（有向无环图）执行引擎，使得数据处理速度更快。<strong>一般处理天指标</strong></li>
</ol>
<h3 id="24-Hive与HBase的区别"><a href="#24-Hive与HBase的区别" class="headerlink" title="24. Hive与HBase的区别"></a>24. Hive与HBase的区别</h3><table>
<thead>
<tr>
<th></th>
<th>Hive</th>
<th>HBase</th>
</tr>
</thead>
<tbody><tr>
<td>数据模型</td>
<td>基于Hadoop的数据仓库工具，数据查询转化成MapReduce执行</td>
<td>基于HDFS上的Nosql数据源，基于数据库本身实时查询，不运行MapReduce</td>
</tr>
<tr>
<td>SQL支持</td>
<td>支持SQL查询</td>
<td>本身不支持，需要集成Phoenix</td>
</tr>
<tr>
<td>查询速度</td>
<td>默认MR引擎，查询速度较慢</td>
<td>有自己的一级索引，rowkey，基于一级索引进行数据查询，查询较快</td>
</tr>
<tr>
<td>表结构</td>
<td>纯逻辑表，只有表的定义</td>
<td>物理表，有独立的物理数据结构</td>
</tr>
<tr>
<td>运行依赖</td>
<td>依赖HDFS存储，MapReduce计算</td>
<td>依赖HDFS存储，zookeeper协调服务</td>
</tr>
<tr>
<td>应用场景</td>
<td>构建离线数仓，海量数据分析</td>
<td>大数据实时查询，海量数据存储</td>
</tr>
</tbody></table>
<h3 id="25-讲一下Union和Join的区别"><a href="#25-讲一下Union和Join的区别" class="headerlink" title="25. 讲一下Union和Join的区别"></a>25. 讲一下Union和Join的区别</h3><p>UNION是两张表进行上下拼接，产生的两个记录集(字段要一样的)并在一起，成为一个新的记录集，分为UNION和UNION ALL两种方法；JOIN 是两张表进行左右连接，条件匹配的记录将合并产生一个记录集，有LEFT JOIN、RIGHT JOIN、INNER JOIN、OUTER JOIN等多种方法。</p>
<h3 id="26-SQL语句转换成MapReduce作业的基本原理"><a href="#26-SQL语句转换成MapReduce作业的基本原理" class="headerlink" title="26. SQL语句转换成MapReduce作业的基本原理"></a>26. SQL语句转换成MapReduce作业的基本原理</h3><p>用MapReduce实现连接操作</p>
<p>假设连接（join）的两个表分别是用户表User(uid,name)和订单表Order(uid,orderid)，具体的SQL命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT name, orderid FROM User u JOIN Order o ON u.uid=o.uid;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2024-01-06_20-49-10.png" alt="Snipaste_2024-01-06_20-49-10" style="zoom:50%;">

<p>上图描述了连接操作转换为MapReduce操作任务的具体执行过程。</p>
<p>首先，在Map阶段，</p>
<p>User表以uid为key，以name和表的标记位（这里User的标记位记为1）为value，进行Map操作，把表中记录转换生成一系列KV对的形式。比如，User表中记录(1,Lily)转换为键值对(1,&lt; 1,Lily&gt;)，其中第一个“1”是uid的值，第二个“1”是表User的标记位，用来标示这个键值对来自User表；</p>
<p>同样，Order表以uid为key，以orderid和表的标记位（这里表Order的标记位记为2）为值进行Map操作，把表中的记录转换生成一系列KV对的形式；</p>
<p>接着，在Shuffle阶段，把User表和Order表生成的KV对按键值进行Hash，然后传送给对应的Reduce机器执行。比如KV对(1,&lt; 1,Lily&gt;)、(1,&lt; 2,101&gt;)、(1,&lt; 2,102&gt;)传送到同一台Reduce机器上。当Reduce机器接收到这些KV对时，还需按表的标记位对这些键值对进行排序，以优化连接操作；</p>
<p>最后，在Reduce阶段，对同一台Reduce机器上的键值对，根据“值”（value）中的表标记位，对来自表User和Order的数据进行笛卡尔积连接操作，以生成最终的结果。比如键值对(1,&lt; 1,Lily&gt;)与键值对(1,&lt; 2,101&gt;)、(1,&lt; 2,102&gt;)的连接结果是(Lily,101)、(Lily,102)。</p>
<p>用MR实现分组操作</p>
<p>假设分数表Score(rank, level)，具有rank（排名）和level（级别）两个属性，需要进行一个分组（Group By）操作，功能是把表Score的不同片段按照rank和level的组合值进行合并，并计算不同的组合值有几条记录。SQL语句命令如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT rank,level,count(*) as value FROM score GROUP BY rank,level;</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2024-01-06_20-51-01.png" alt="Snipaste_2024-01-06_20-51-01"></p>
<p>上图描述分组操作转化为MapReduce任务的具体执行过程。</p>
<p>首先，在Map阶段，对表Score进行Map操作，生成一系列KV对，其键为&lt; rank, level&gt;，值为“拥有该&lt; rank, level&gt;组合值的记录的条数”。比如，Score表的第一片段中有两条记录(A,1)，所以进行Map操作后，转化为键值对(&lt; A,1&gt;,2);</p>
<p>接着在Shuffle阶段，对Score表生成的键值对，按照“键”的值进行Hash，然后根据Hash结果传送给对应的Reduce机器去执行。比如，键值对(&lt; A,1&gt;,2)、(&lt; A,1&gt;,1)传送到同一台Reduce机器上，键值对(&lt; B,2&gt;,1)传送另一Reduce机器上。然后，Reduce机器对接收到的这些键值对，按“键”的值进行排序；</p>
<p>在Reduce阶段，把具有相同键的所有键值对的“值”进行累加，生成分组的最终结果。比如，在同一台Reduce机器上的键值对(&lt; A,1&gt;,2)和(&lt; A,1&gt;,1)Reduce操作后的输出结果为(A,1,3)。</p>
<h3 id="27-⭐⭐Hive元数据存储了什么内容？"><a href="#27-⭐⭐Hive元数据存储了什么内容？" class="headerlink" title="27. ⭐⭐Hive元数据存储了什么内容？"></a>27. ⭐⭐Hive元数据存储了什么内容？</h3><p>元数据包括：数据库（默认是default）、表名、表的拥有者、列&#x2F;分区字段、表的类型（是否是外部表）、表的数据所在目录等。</p>
<p><strong>Hive的元数据主要分为5大部分</strong>：<strong>数据库相关的元数据</strong>、<strong>表相关的元数据</strong>、<strong>分区相关的元数据</strong>、<strong>文件存储的元数据</strong>、<strong>其他</strong></p>
<ul>
<li>数据库相关的元数据：所有数据库的库名、存储地址、属性、权限等等</li>
<li>表相关的元数据：数据表所属的数据库、创建时间、创建者、表的类型、属性、表统计信息（存储的文件个数、总文件大小、总行数、分区数、数据量）</li>
<li>分区相关的元数据：分区列，分区创建时间、分区统计时间</li>
<li>文件存储的元数据：文件存储的HDFS路径，输入输出格式</li>
<li>其他：事务信息，锁信息</li>
</ul>
<h3 id="28-⭐说一说SQL执行计划的各个操作Operator"><a href="#28-⭐说一说SQL执行计划的各个操作Operator" class="headerlink" title="28. ⭐说一说SQL执行计划的各个操作Operator"></a>28. ⭐说一说SQL执行计划的各个操作Operator</h3><p>整体上分为Map operator Tree（Map端执行计划操作树）和Reduce Operator Tree（Reduce端执行计划操作树）</p>
<p><strong>TableScan</strong>：表扫描操作</p>
<p><strong>Filter Operator</strong>：过滤操作</p>
<p><strong>Select Operator</strong>：列投影操作</p>
<p><strong>Group By Operator</strong>：分组聚合操作</p>
<p><strong>Reduce Output Operator</strong>：Map端聚合后的信息输出到reduce操作</p>
<p><strong>sort order</strong>：排序操作</p>
<p><strong>PTF Operator</strong>：窗口函数操作</p>
<p><strong>Join Operator</strong>：表连接操作</p>
<p><strong>File Output Operator</strong>：文件输出操作</p>
<h3 id="29-⭐⭐详细说一下count-distinct-优化原理"><a href="#29-⭐⭐详细说一下count-distinct-优化原理" class="headerlink" title="29. ⭐⭐详细说一下count(distinct)优化原理"></a>29. ⭐⭐详细说一下count(distinct)优化原理</h3><p><strong>只有一个count(distinct)语句</strong>，如果系统引擎会自动做优化，那就看是否进行两部聚合（内层groupby+count，外层sum）效率更高。如果系统不会自动进行优化，需要我们手动优化，我们就需要看count(distinct)的字段是否分布很不均匀，如果均匀没必要优化，不均匀有可能出现数据倾斜，有必要优化。配置<code>set hive.optimize.countdistinct=true</code>，即使出现数据倾斜也能实现自动优化。</p>
<p><strong>当有多个count(distinct)语句</strong>，distinct算子在处理过程中会将原有数据膨胀，有几个distinct关键字就会在Map端膨胀几倍。有两种优化手段：第一种就是代码层面拆分为两个两层，内层groupby+count，外层sum；第二种方法是设置<code>spark.sql.files.maxPartitionBytes</code>为更小的值，让每一个task处理的数据少一些</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">--代码1</span><br><span class="line">select ft_type,</span><br><span class="line">       app_id,</span><br><span class="line">       count(distinct case when flag = 0 then user_id else null end) as A_cnt,</span><br><span class="line">       count(distinct case when flag = 1 then user_id else null end) as B_cnt</span><br><span class="line">from tmp</span><br><span class="line">where dt &gt;= &#x27;20240304&#x27;</span><br><span class="line">group by ft_type,</span><br><span class="line">         app_id</span><br><span class="line">     </span><br><span class="line">     </span><br><span class="line">--代码2</span><br><span class="line">set spark.sql.files.maxPartitionBytes = 20M;</span><br><span class="line">select ft_type,</span><br><span class="line">       app_id,</span><br><span class="line">       sum(case when A_cnt&gt; 0 then 1 else 0 end) as A_cnt,</span><br><span class="line">       sum(case when B_cnt&gt; 0 then 1 else 0 end) as B_cnt</span><br><span class="line">from (</span><br><span class="line">    select ft_type,</span><br><span class="line">           app_id,</span><br><span class="line">           count(case when flag = 0 then user_id else null end) as A_cnt,</span><br><span class="line">           count(case when flag = 1 then user_id else null end) as B_cnt</span><br><span class="line">    from tmp</span><br><span class="line">    where dt &gt;= &#x27;20240304&#x27;</span><br><span class="line">    group by ft_type,</span><br><span class="line">             app_id,</span><br><span class="line">             user_id</span><br><span class="line">    )t</span><br><span class="line">group by ft_type,</span><br><span class="line">         app_id</span><br></pre></td></tr></table></figure>

<h3 id="30-⭐Hive中join的底层实现以及几种常见的join语法的差别"><a href="#30-⭐Hive中join的底层实现以及几种常见的join语法的差别" class="headerlink" title="30. ⭐Hive中join的底层实现以及几种常见的join语法的差别"></a>30. ⭐Hive中join的底层实现以及几种常见的join语法的差别</h3><p><img src="image-20240821002221119.png" alt="image-20240821002221119"></p>
<p>innor join（join）:内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来</p>
<p>left join:只要左表存在的都会返回，右表不存在的补null</p>
<p>right join:只要右表存在的都会返回，左表不存在的补null</p>
<p>full join：返回两个表的并基，左表和右表中不存在的都补null</p>
<p>left semi join:返回左表&#x2F;数据集中与右表&#x2F;数据集匹配的记录，不会返回左表的所有记录，不会因此形成null值。用于判断一个表的数据在另一个表中是否有相同的数据。可用于替代Hive中in&#x2F;exists类型的子查询（执行计划相同）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select * from tab1</span><br><span class="line">where col1 in (select col1 from tab2)</span><br><span class="line">--用left semi join写法如下</span><br><span class="line">select * from tab1</span><br><span class="line">left semi join tab2 on tab1.col = tab2.col</span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong>hive不支持右半连接，semi join比通常的inner join要更高效，因为对于左表中一条指定的记录，在右边表中一旦找到匹配的记录，Hive就会立即停止扫描，左边表中选择的列是可以预测的</p>
<p>left semi join的限制是，右侧表只能在连接条件（on子句）中引用，而不能在where或select子句中引用。</p>
<p>笛卡尔积：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-- 笛卡尔积写法1</span><br><span class="line">select *</span><br><span class="line">from emp, dept;</span><br><span class="line">-- 笛卡尔积写法2</span><br><span class="line">select *</span><br><span class="line">from emp join dept;</span><br><span class="line">-- 笛卡尔积写法3</span><br><span class="line">select *</span><br><span class="line">from emp join dept</span><br><span class="line">on true;</span><br></pre></td></tr></table></figure>

<p>cross join：返回左右两表连接字段的笛卡尔积</p>
<h3 id="31-⭐高级分组聚合"><a href="#31-⭐高级分组聚合" class="headerlink" title="31. ⭐高级分组聚合"></a>31. ⭐高级分组聚合</h3><p>（1）grouping sets</p>
<p>在一个GROUP BY查询中，根据不同的维度组合进行聚合，等价于将不同维度的GROUP BY结果集进行Union ALL</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">select a,b,c from tmp group by a,b,c grouping sets((a,b,c),(a,b),(b,c),(a,c),(a),(b),(c),())</span><br><span class="line">--等价于</span><br><span class="line">select a,b,c from tmp group by a,b,c</span><br><span class="line">union all</span><br><span class="line">select a,b,null from tmp group by a,b</span><br><span class="line">union all</span><br><span class="line">select null,b,c from tmp group by b,c</span><br><span class="line">union all</span><br><span class="line">select a,null,c from tmp group by a,c</span><br><span class="line">union all</span><br><span class="line">select a,null,null from tmp group by a</span><br><span class="line">union all</span><br><span class="line">select null,b,null from tmp group by b</span><br><span class="line">union all </span><br><span class="line">select null,null,c from tmp group by c</span><br></pre></td></tr></table></figure>

<p>（2）cube：根据group by的维度的所有组合进行聚合</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">group by a,b,c with cube</span><br><span class="line">--等价于</span><br><span class="line">group by a,b,c grouping sets ((a,b,c),(a,b),(b,c),(a,c),(a),(b),(c),())</span><br></pre></td></tr></table></figure>

<p>（3）rollup：是cube的子集，<strong>以最左侧</strong>的维度为主，从该维度进行层级聚合</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">group by a,b,c with rollup</span><br><span class="line">--等价于</span><br><span class="line">group by a,b,c grouping sets ((a,b,c),(a,b),(a),())</span><br></pre></td></tr></table></figure>

<h3 id="32-Hive3-0的物化视图"><a href="#32-Hive3-0的物化视图" class="headerlink" title="32. Hive3.0的物化视图"></a>32. Hive3.0的物化视图</h3><img src="image-20240821112700942.png" alt="image-20240821112700942" style="zoom: 67%;">

<h2 id="三、Hadoop"><a href="#三、Hadoop" class="headerlink" title="三、Hadoop"></a>三、Hadoop</h2><h3 id="3-1-前置知识"><a href="#3-1-前置知识" class="headerlink" title="3.1 前置知识"></a>3.1 前置知识</h3><h4 id="0-一句话介绍hadoop"><a href="#0-一句话介绍hadoop" class="headerlink" title="0. 一句话介绍hadoop"></a>0. 一句话介绍hadoop</h4><p><img src="image-20240828155310608.png" alt="image-20240828155310608"></p>
<h4 id="1-Hadoop的运行模式"><a href="#1-Hadoop的运行模式" class="headerlink" title="1. Hadoop的运行模式"></a>1. Hadoop的运行模式</h4><p>本地模式（单机，仅运行官方案例），伪分布式模式（单机，模拟分布式环境），完全分布式模式（多台服务器分布式集群）</p>
<h4 id="2-说一说你的Hadoop集群配置"><a href="#2-说一说你的Hadoop集群配置" class="headerlink" title="2. 说一说你的Hadoop集群配置"></a>2. 说一说你的Hadoop集群配置</h4><p><img src="Snipaste_2023-12-05_14-42-39.png" alt="Snipaste_2023-12-05_14-42-39"></p>
<p>NameNode是HDFS的主节点，ResourceManager是YARN的主节点。</p>
<p>HDFS中的DataNode主要负责存储数据，每个服务器节点都部署一台，NameNode和SecondaryNameNode分别部署在102和104节点上。YARN的NodeManager是单个节点服务器上的资源和任务管理器，在每台节点上都部署一台，ResourceManager主要负责集群整体的资源调度工作，非常消耗内存，不要和同样消耗内存的NameNode配置在同一台节点上（所以将其配置在103节点上）。除此之外，在102上配置一个历史服务器，用于回溯历史任务运行情况</p>
<h4 id="3-Hadoop的配置文件"><a href="#3-Hadoop的配置文件" class="headerlink" title="3. Hadoop的配置文件"></a>3. Hadoop的配置文件</h4><ul>
<li>core-site.xml：主要用于将分布式文件系统HDFS的NameNode的<strong>入口地址</strong>和分布式文件系统中的<strong>数据</strong> 存储于服务器本地磁盘中进行配置。</li>
<li>hdfs-site.xml：主要对HDFS的属性进行配置</li>
<li>yarn-site.xml：主要对YARN的属性进行配置</li>
<li>mapred-site.xml：主要对MR的属性进行配置</li>
<li>workers：配置从节点角色</li>
</ul>
<h4 id="4-Hadoop的常用端口号"><a href="#4-Hadoop的常用端口号" class="headerlink" title="4. Hadoop的常用端口号"></a>4. Hadoop的常用端口号</h4><img src="Snipaste_2023-09-15_21-10-19.png" alt="Snipaste_2023-09-15_21-10-19" style="zoom:43%;">

<h4 id="5-hadoop1-0-2-0-3-0的区别"><a href="#5-hadoop1-0-2-0-3-0的区别" class="headerlink" title="5. hadoop1.0 2.0 3.0的区别"></a>5. hadoop1.0 2.0 3.0的区别</h4><img src="Snipaste_2024-08-19_22-11-44-17254540094479.png" alt="Snipaste_2024-08-19_22-11-44" style="zoom:50%;">

<h3 id="3-2-HDFS"><a href="#3-2-HDFS" class="headerlink" title="3.2 HDFS"></a>3.2 HDFS</h3><h4 id="1-HDFS的定义及优缺点"><a href="#1-HDFS的定义及优缺点" class="headerlink" title="1. HDFS的定义及优缺点"></a>1. HDFS的定义及优缺点</h4><p>（1）定义：Hadoop分布式文件系统，适用于一次写入，多次读取的场景</p>
<p>（2）优点：</p>
<ul>
<li>高容错性（多个副本），数据文件上传后，可以自动存储为多个副本</li>
<li>适合处理大数据</li>
<li>可以构建在廉价的机器上</li>
</ul>
<p>（3）缺点：</p>
<ul>
<li>不适合低时间延迟的数据访问</li>
<li>无法存储大量小文件（对NameNode内存占用大，消耗性能；小文件存储寻址时间超过读取时间，造成资源浪费）</li>
<li>文件不支持并发写入操作，不允许多个线程同时写文件。对于一个文件的写入操作，只能以“仅添加”的模式在文件末尾写入数据，不支持在文件的任意位置进行修改。</li>
</ul>
<h4 id="2-⭐介绍一下HDFS的基本架构"><a href="#2-⭐介绍一下HDFS的基本架构" class="headerlink" title="2. ⭐介绍一下HDFS的基本架构"></a>2. ⭐介绍一下HDFS的基本架构</h4><p>HDFS 主要包括三个部分，namenode，datanode 以及 secondary namenode。这里主要讲一下他们的作用：</p>
<ul>
<li>namenode 主要负责存储数据的元数据信息，不存储实际的数据块，</li>
<li>datanode 就是存储实际的数据块，</li>
<li>secondary namenode 主要是定期合并 FsImage 和 edits 文件</li>
</ul>
<p>（这里可以进行扩展，讲一下为什么有他们的存在？首先 namenode 存储的元数据信息是会放在内存中，因为会经常进行读写操作，放在磁盘的话效率就太低了，那么这时候就会有一个问题，如果断电了，元数据信息不就丢失了吗？所以也需要将元数据信息存在磁盘上，因此就有了用来备份元数据信息的 FsImage 文件，那么是不是每次更新元数据信息，都需要操作FsImage 文件呢？当然不是，这样效率不就又低了吗，所以我们就引入了 edits文件，用来存储对元数据的所有更新操作，并且是顺序写的方式，效率也不会太低，这样，一旦重启 namenode，那么首先就会进行 FsImage 文件和 edits 文件的合并，形成最新的元数据信息。这里还会有一个问题，但是如果一直向 edits 文件进行写入数据，这个文件就会变得很大，那么重启的时候恢复元数据就会很卡，所以这里就有了 secondary namenode 在 namenode 启动的时候定期来进行 fsimage和 edits 文件的合并，这样在重启的时候就会很快完成元数据的合并）</p>
<p>（1）Client：客户端</p>
<ul>
<li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</li>
<li>与NameNode交互，获取文件的<strong>位置信息和文件块信息</strong>。</li>
<li>与DataNode交互，读取或写入数据。</li>
<li>使用命令访问和管理HDFS</li>
</ul>
<p>（2）NameNode：主节点，存储数据的<strong>元数据信息</strong>，不存储具体的数据</p>
<ul>
<li>管理HDFS的名称空间</li>
<li>配置副本策略</li>
<li>管理数据块（Block）映射信息</li>
<li>处理客户端读写<strong>请求</strong></li>
</ul>
<p>（3）DataNode：从节点，NameNode下达命令，DataNode执行实际的操作</p>
<ul>
<li>存储实际的数据块</li>
<li>执行数据块的读写<strong>操作</strong></li>
</ul>
<p>（4）SecondaryNameNode：并不是NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提高服务</p>
<ul>
<li>辅助NameNode，分担其工作量，定期合并Fsimage和Edits，并推送给NameNode</li>
<li>在紧急情况下，可辅助恢复Name Node</li>
</ul>
<h4 id="3-⭐HDFS的文件块大小，为什么块的大小不能设置太大也不能设置太小？"><a href="#3-⭐HDFS的文件块大小，为什么块的大小不能设置太大也不能设置太小？" class="headerlink" title="3. ⭐HDFS的文件块大小，为什么块的大小不能设置太大也不能设置太小？"></a>3. ⭐HDFS的文件块大小，为什么块的大小不能设置太大也不能设置太小？</h4><p>2.x、3.x版本中是128M。（HDFS块大小设置主要取决于磁盘传输速率）</p>
<p>HDFS中平均寻址时间大约是10ms，根据经验寻址时间为传输时间的1%为最佳，则传输时间为10ms&#x2F;0.01&#x3D;1000ms&#x3D;1s，目前磁盘普遍传输速率为100MB&#x2F;s，所以最佳block大小为100MB&#x2F;s * 1s &#x3D; 100MB</p>
<ul>
<li>如果数据块设置的太小，一个大文件就会被分成多个数据块，增加整个大文件寻址的时间，出现小文件问题（对NameNode造成存储负担）</li>
<li>如果数据块设置的的太大，磁盘传输数据的时间占比会增大。</li>
</ul>
<h4 id="4-⭐⭐请说一下HDFS读写数据流程"><a href="#4-⭐⭐请说一下HDFS读写数据流程" class="headerlink" title="4. ⭐⭐请说一下HDFS读写数据流程"></a>4. ⭐⭐请说一下HDFS读写数据流程</h4><p><img src="Snipaste_2023-12-05_16-25-29.png" alt="Snipaste_2023-12-05_16-25-29"></p>
<p>HDFS写数据流程：<code>hadoop fs -put a.tat /user/sl/</code></p>
<ul>
<li>Client客户端发送上传请求，<strong>通过 RPC 与 NameNode 建立通信</strong>，NameNode检查该用户是否有上传权限，以及上传的文件是否在 HDFS 对应的目录下重名，如果这两者有任意一个不满足，则直接报错，如果两者都满足，则返回给客户端一个可以上传的信息；</li>
<li>Client 根据文件的大小进行切分，默认 128M 一块，切分完成之后给NameNode 发送请求第一个 block 块上传到哪些服务器上；</li>
<li>NameNode 收到请求之后，根据机架感知的副本放置策略进行文件分配，返回可用的 DataNode 的地址；（<strong>数据文件默认在 HDFS 上存放三份,存储策略为本地一份，同机架内其它某一节点上一份, 不同机架的某一节点上一份</strong>）</li>
<li>客户端收到地址之后与服务器地址列表中的一个节点如 A 进行通信，本质上就是 RPC 调用，建立 pipeline，A 收到请求后会继续调用 B，B 在调用 C，将整个 pipeline 建立完成，逐级返回 Client</li>
<li>Client 开始向 A 上发送第一个 block（<strong>先从磁盘读取数据然后放到本地内存缓存</strong>），<strong>以 packet（数据包，64kb）为单位，A 收到一个 packet 就会发送给B，然后 B 发送给 C，A 每传完一个 packet 就会放入一个应答队列等待应答</strong>；</li>
<li>数据被分割成一个个的 packet 数据包在 pipeline 上依次传输，<strong>在 pipeline反向传输中，逐个发送 ack（命令正确应答）</strong>，最终由 pipeline 中第一个DataNode 节点 A 将 pipelineack 发送给 Client；</li>
<li>当一个 block 传输完成之后, Client 再次请求 NameNode 上传第二个 block，NameNode 重新选择三台 DataNode 给 Client。</li>
</ul>
<p><strong>面试速背：</strong></p>
<p>（1）Client先与NameNode通信，表示要写入数据，NameNode会在NameSpace中添加一个新的空文件，并告知Client已经做好写的准备工作</p>
<p>（2）Client接到通知，向NameNode申请DataNode的数据块，NameNode会返回一个数据块，包含所有数据节点所在DataNode的位置信息</p>
<p>（3）Client得到目标数据块位置，建立socket流，并向目标数据块写入数据</p>
<p><img src="Snipaste_2023-12-05_16-34-37.png" alt="Snipaste_2023-12-05_16-34-37"></p>
<p>HDFS读数据流程：<code>hadoop fs -get a.txt /opt/module/hadoop/data/</code></p>
<ul>
<li>Client 向 NameNode 发送 RPC 请求。请求文件 block 的位置</li>
<li>NameNode 收到请求之后会检查用户权限以及是否有这个文件，如果都符合，则会返回 block 列表，对于每个 block，NameNode都会返回含有该 block 副本的 DataNode 地址；这些返回的 DataNode 地址，会按照集群拓扑结构得出 DataNode 与客户端的距离，然后进行排序。</li>
<li>Client 选取排序靠前的 DataNode 来读取 block</li>
<li>当读完列表的 block 后，若文件读取还没有结束，客户端会继续向NameNode 获取下一批的 block 列表</li>
<li><strong>read 方法是并行的读取 block 信息，不是一块一块的读取</strong></li>
<li>最终读取来所有的 block 会合并成一个完整的最终文件；</li>
</ul>
<p><strong>面试速背：</strong></p>
<p>（1）Client先与NameNode进行交互，获取数据所在DataNode的地址和文件块信息</p>
<p>（2）Client根据从NameNode得到的消息找到DataNode及对应的文件块，建立socket流</p>
<p>（3）DataNode读取磁盘中的文件回传给Client</p>
<h4 id="5-HDFS-在读取文件的时候，如果其中一个块突然损坏了怎么办？"><a href="#5-HDFS-在读取文件的时候，如果其中一个块突然损坏了怎么办？" class="headerlink" title="5. HDFS 在读取文件的时候，如果其中一个块突然损坏了怎么办？"></a>5. HDFS 在读取文件的时候，如果其中一个块突然损坏了怎么办？</h4><p>客户端读取完 DataNode 上的块之后会进行 checksum 验证，也就是把客户端读取到本地的块与 HDFS 上的原始块进行校验，如果发现校验结果不一致，客户端会通知 NameNode，然后再<strong>从下一个拥有该 block 副本的 DataNode 继续读</strong>。</p>
<h4 id="6-HDFS-在上传文件的时候，如果其中一个-DataNode-突然挂掉了怎么办"><a href="#6-HDFS-在上传文件的时候，如果其中一个-DataNode-突然挂掉了怎么办" class="headerlink" title="6. HDFS 在上传文件的时候，如果其中一个 DataNode 突然挂掉了怎么办?"></a>6. HDFS 在上传文件的时候，如果其中一个 DataNode 突然挂掉了怎么办?</h4><p><img src="Snipaste_2024-08-09_15-10-39-172545400944710.png" alt="Snipaste_2024-08-09_15-10-39"></p>
<p>DataNode在启动后向NameNode注册，在注册通过后，每隔一段时间都会向NameNode上报所有的数据块信息，时间间隔默认是6小时。</p>
<p>DataNode每隔3秒都会向NameNode发送心跳信息，心跳返回结果中包含NameNode对该DataNode的命令，如复制数据块、删除数据块等。如果DataNode进程挂掉，或者发生网络故障，导致DataNode无法与NameNode正常通信，那么NameNode不会立即将该节点判定为死亡，它会在一段时间（称为超时时长，默认配置为10分钟30秒）后将该节点判定为死亡。</p>
<p>当 DataNode 突然挂掉了，客户端接收不到这个 DataNode 发送的 ack 确认，客户端会通知 NameNode，NameNode 会通知 DataNode 去复制副本，并将挂掉的 DataNode 作下线处理，不再让它参与文件上传与下载。</p>
<h4 id="7-⭐SecondaryNameNode的工作机制"><a href="#7-⭐SecondaryNameNode的工作机制" class="headerlink" title="7. ⭐SecondaryNameNode的工作机制"></a>7. ⭐SecondaryNameNode的工作机制</h4><p>Secondary Namenode 主要是用于 edit logs 和 fsimage 的合并，edit logs 记录了对 namenode 元数据的增删改操作，fsimage 记录了最新的元数据检查点，在namenode 重启的时候，会把 edit logs 和 fsimage 进行合并，形成新的 fsimage文件</p>
<p>⚫ 工作机制：</p>
<p>◼ Secondary NameNode 询问 NameNode 是否需要 checkpoint。直接带回NameNode 是否检查结果</p>
<p>◼ Secondary NameNode 请求执行 checkpoint </p>
<p>◼ NameNode 滚动正在写的 edits 日志</p>
<p>◼ 将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode </p>
<p>◼ Secondary NameNode 加载编辑日志和镜像文件到内存，并合并</p>
<p>◼ 生成新的镜像文件 fsimage.chkpoint</p>
<p>◼ 拷贝 fsimage.chkpoint 到 NameNode</p>
<p>◼ NameNode 将 fsimage.chkpoint 重新命名成 fsimage</p>
<p>◼ 所以如果 NameNode 中的元数据丢失，是可以从 Secondary NameNode 恢复一部分元数据信息的，但不是全部，因为 NameNode 正在写的 edits 日志还没有拷贝到 Secondary NameNode，这部分恢复不了</p>
<h4 id="8-⭐Hadoop中的数据压缩的优缺点，压缩原则，以及常见的压缩格式"><a href="#8-⭐Hadoop中的数据压缩的优缺点，压缩原则，以及常见的压缩格式" class="headerlink" title="8. ⭐Hadoop中的数据压缩的优缺点，压缩原则，以及常见的压缩格式"></a>8. ⭐Hadoop中的数据压缩的优缺点，压缩原则，以及常见的压缩格式</h4><p>优点：减少磁盘IO、减少磁盘存储空间；缺点：增加CPU开销</p>
<p>压缩原则：运算密集型的job少用压缩，IO密集型的job多用压缩</p>
<p>常见压缩格式：</p>
<p>速记：<strong>压缩率高：zz；支持切片：B罗；压缩解压缩速度：S罗</strong></p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>压缩率</th>
<th>压缩解压缩速度</th>
<th>是否支持切片</th>
</tr>
</thead>
<tbody><tr>
<td>GZip</td>
<td>较高</td>
<td>一般</td>
<td>不支持</td>
</tr>
<tr>
<td>bzip2</td>
<td>高</td>
<td>慢</td>
<td>支持</td>
</tr>
<tr>
<td>LZO</td>
<td>一般</td>
<td>较快</td>
<td>支持</td>
</tr>
<tr>
<td>Snappy</td>
<td>一般</td>
<td>快</td>
<td>不支持</td>
</tr>
</tbody></table>
<p>选择压缩算法时需要考虑的维度</p>
<ol>
<li><strong>压缩比（Compression Ratio）</strong><ul>
<li>这是文件压缩后大小与原始文件大小的比率。更高的压缩比意味着压缩后的文件更小，节省更多的存储空间和网络带宽。但通常压缩比与压缩&#x2F;解压速度成反比。</li>
</ul>
</li>
<li><strong>压缩和解压缩速度（Compression&#x2F;Decompression Speed）</strong><ul>
<li>速度是选择压缩算法时非常重要的因素，尤其是在处理大规模数据时。压缩和解压缩速度直接影响到数据处理的时间。像Snappy和LZ4虽然压缩比相对较低，但它们的速度非常快，适合实时或批处理要求很高的场景。</li>
</ul>
</li>
<li><strong>是否支持切分（Splittable）</strong><ul>
<li>在Hadoop中，大数据文件通常需要被切分成多个块以供MapReduce任务并行处理。非切分（Non-Splittable）的压缩格式（如Gzip和Bzip2）不适合处理大文件，因为它们不能被MapReduce有效并行处理。</li>
</ul>
</li>
<li><strong>兼容性（Compatibility）</strong><ul>
<li>需要考虑压缩算法的生态系统支持情况，例如是否与Hadoop、Hive、Spark等工具兼容，是否支持文件的直接读取和写入。</li>
</ul>
</li>
<li><strong>应用场景</strong><ul>
<li>具体应用场景对压缩算法的选择也有影响。例如，如果你的应用对空间节省的要求很高，并且不太关心解压速度，Bzip2可能是一个不错的选择；而对于实时流处理，Snappy或LZO则更为合适。</li>
</ul>
</li>
</ol>
<h4 id="9-为什么HDFS要做成三副本？"><a href="#9-为什么HDFS要做成三副本？" class="headerlink" title="9. 为什么HDFS要做成三副本？"></a>9. 为什么HDFS要做成三副本？</h4><img src="image-20240828142424230.png" alt="image-20240828142424230" style="zoom: 80%;">

<h4 id="10-HDFS的副本机制如何运转？"><a href="#10-HDFS的副本机制如何运转？" class="headerlink" title="10. HDFS的副本机制如何运转？"></a>10. HDFS的副本机制如何运转？</h4><p>（1）数据块划分</p>
<p><img src="image-20240828142654905.png" alt="image-20240828142654905"></p>
<p>（2）副本放置策略</p>
<p><img src="image-20240828142716996.png" alt="image-20240828142716996"></p>
<p>（3）写操作过程</p>
<p>（4）读操作过程</p>
<p>（5）副本监控与修复</p>
<p><img src="image-20240828144138588.png" alt="image-20240828144138588"></p>
<p>（6）副本删除与再分配</p>
<p><img src="image-20240828144208315.png" alt="image-20240828144208315"></p>
<h4 id="11-为什么Hadoop有多副本而MySQL，Oracle没有多副本？"><a href="#11-为什么Hadoop有多副本而MySQL，Oracle没有多副本？" class="headerlink" title="11. 为什么Hadoop有多副本而MySQL，Oracle没有多副本？"></a>11. 为什么Hadoop有多副本而MySQL，Oracle没有多副本？</h4><p>（1）设计目标和应用场景</p>
<img src="image-20240828144538227.png" alt="image-20240828144538227" style="zoom:80%;">

<p>（2）数据存储和访问模型</p>
<img src="image-20240828145529671.png" alt="image-20240828145529671" style="zoom: 80%;">

<p>（3）故障处理策略</p>
<img src="image-20240828145600338.png" alt="image-20240828145600338" style="zoom:80%;">

<h4 id="12-压缩策略及算法"><a href="#12-压缩策略及算法" class="headerlink" title="12. 压缩策略及算法"></a>12. 压缩策略及算法</h4><p><strong>Snappy</strong></p>
<ul>
<li><strong>特点</strong>: Snappy是Google开发的一种压缩算法，主要用于高效的压缩和解压缩，目标是实现非常快的处理速度。</li>
<li>优点<ul>
<li>压缩和解压速度非常快，特别适合大规模数据处理场景。</li>
<li>广泛用于列式存储格式（如Parquet、ORC）和Spark的内存数据压缩。</li>
</ul>
</li>
<li><strong>缺点</strong>: 压缩比相对较低，但适合对速度要求高的应用。</li>
</ul>
<p><strong>LZO</strong></p>
<ul>
<li><strong>特点</strong>: LZO也是一种专注于高速度的压缩算法，通常用于实时处理和数据流。</li>
<li>优点<ul>
<li>压缩和解压速度非常快，支持切分，可以用于MapReduce和Spark任务。</li>
<li>在需要频繁随机访问或大规模数据流处理时非常有用。</li>
</ul>
</li>
<li><strong>缺点</strong>: 压缩比不如Bzip2、Gzip等算法高。</li>
</ul>
<p><strong>Gzip</strong></p>
<ul>
<li><strong>特点</strong>: 基于DEFLATE算法的Gzip提供了较好的压缩比，适合需要高效压缩的场景。</li>
<li>优点<ul>
<li>提供良好的压缩比，适合需要节省存储空间的场景。</li>
<li>兼容性好，很多工具和系统都支持Gzip。</li>
</ul>
</li>
<li>缺点<ul>
<li>压缩和解压速度相对较慢，尤其是在处理大规模数据时。</li>
<li>不支持切分，影响并行处理效率。</li>
</ul>
</li>
</ul>
<p><strong>Bzip2</strong></p>
<ul>
<li><strong>特点</strong>: Bzip2是一种高压缩比的压缩算法，常用于需要高效空间利用的场景。</li>
<li>优点<ul>
<li>压缩比高，能够显著减少存储空间需求。</li>
</ul>
</li>
<li>缺点<ul>
<li>压缩和解压速度慢，尤其在大数据场景中不太常用。</li>
<li>不支持切分，不适合大规模分布式数据处理任务。</li>
</ul>
</li>
</ul>
<p><strong>LZ4</strong></p>
<ul>
<li><strong>特点</strong>: LZ4是一种非常快速的压缩算法，适用于需要高吞吐量的场景。</li>
<li>优点<ul>
<li>压缩和解压速度非常快，支持切分，适合实时和批处理任务。</li>
</ul>
</li>
<li>缺点<ul>
<li>压缩比相对较低。</li>
</ul>
</li>
</ul>
<p><strong>Zstandard (ZSTD)</strong></p>
<ul>
<li><strong>特点</strong>: Zstandard（ZSTD）是Facebook开发的压缩算法，专注于实现高压缩比和高速压缩&#x2F;解压。</li>
<li>优点<ul>
<li>在高压缩比和压缩速度之间实现了良好的平衡。</li>
<li>支持多种压缩级别，用户可以根据需要调整。</li>
</ul>
</li>
<li>缺点<ul>
<li>相对新，兼容性可能不如Gzip或Snappy广泛。</li>
</ul>
</li>
</ul>
<h4 id="13-（百度）hadoop提交任务之前要进行split，说一说split的过程"><a href="#13-（百度）hadoop提交任务之前要进行split，说一说split的过程" class="headerlink" title="13. （百度）hadoop提交任务之前要进行split，说一说split的过程"></a>13. （百度）hadoop提交任务之前要进行split，说一说split的过程</h4><p>在Hadoop中，<strong>Split</strong> 是指将大数据集分割成更小的数据块，这些小块可以并行处理。这个过程通常在提交MapReduce任务之前进行，目的是为了充分利用集群的计算资源，提高并行处理的效率。</p>
<p>Split的过程概述</p>
<ol>
<li><strong>输入格式（InputFormat）确定Split逻辑</strong>：<ul>
<li>在Hadoop中，<code>InputFormat</code>类决定了如何将输入数据划分为Splits。Hadoop提供了不同的<code>InputFormat</code>类来处理不同类型的数据。例如，处理文本文件的<code>TextInputFormat</code>、处理压缩文件的<code>CompressedInputFormat</code>等。</li>
<li>每个<code>InputFormat</code>都有自己的逻辑来决定Split的大小和方式。</li>
</ul>
</li>
<li><strong>确定Split大小</strong>：<ul>
<li>Split大小通常由<code>InputFormat</code>类决定。默认情况下，Hadoop根据块大小（Block Size）和数据的物理存储位置来确定Split的大小。HDFS中的默认块大小通常为128MB或64MB。</li>
<li>Split大小可以通过Hadoop的配置参数<code>mapreduce.input.fileinputformat.split.maxsize</code>和<code>mapreduce.input.fileinputformat.split.minsize</code>进行调节。</li>
<li>如果文件很大，可能会分成多个Splits；如果文件较小，多个文件也可能合并为一个Split。</li>
</ul>
</li>
<li><strong>逻辑上划分数据块</strong>：<ul>
<li><code>InputFormat</code>类将输入数据逻辑上分成多个Splits。这里的“逻辑上”意味着这些Splits不需要严格按照HDFS中的物理块边界划分，而是可以跨越多个HDFS块。</li>
<li>例如，一个128MB的文件可能会被分成两个64MB的Splits，但这并不意味着它们严格对应HDFS的块边界。</li>
</ul>
</li>
<li><strong>为每个Split创建一个RecordReader</strong>：<ul>
<li>Hadoop为每个Split创建一个<code>RecordReader</code>对象。<code>RecordReader</code>负责将Split中的字节流转换为键值对，供Map任务处理。</li>
<li>每个<code>RecordReader</code>从Split的开始位置开始读取，直到读取完毕。</li>
</ul>
</li>
<li><strong>生成Splits列表</strong>：<ul>
<li>最终，Hadoop生成一个Splits列表，每个Split代表要被Map任务处理的一部分数据。</li>
<li>这些Splits随后分配给Map任务，通常每个Split对应一个Map任务。</li>
</ul>
</li>
</ol>
<p>Split的过程详细步骤</p>
<ol>
<li><strong>获取输入文件列表</strong>：<ul>
<li><code>InputFormat</code>首先获取要处理的所有输入文件的列表。这些文件通常存储在HDFS中。</li>
</ul>
</li>
<li><strong>计算每个文件的Split大小</strong>：<ul>
<li>对于每个文件，Hadoop根据文件大小和配置的Split大小来计算应该如何划分文件。假设文件大小为256MB，块大小为128MB，那么这个文件将被分为两个Splits，每个128MB。</li>
</ul>
</li>
<li><strong>计算Split的起始位置和长度</strong>：<ul>
<li><code>InputFormat</code>计算每个Split的起始位置和长度。例如，第一个Split的起始位置是0，长度是128MB；第二个Split的起始位置是128MB，长度是128MB。</li>
</ul>
</li>
<li><strong>检查Split的边界</strong>：<ul>
<li>Hadoop在计算Split时会考虑记录的边界。例如，对于文本文件，Hadoop通常会确保每个Split不会在行的中间截断。这样可以避免处理不完整的记录。</li>
</ul>
</li>
<li><strong>生成Split对象</strong>：<ul>
<li>对于每个计算出的Split，Hadoop生成一个<code>InputSplit</code>对象，包含文件路径、起始位置、长度等信息。</li>
</ul>
</li>
<li><strong>分配Splits给Map任务</strong>：<ul>
<li>生成的<code>InputSplit</code>对象被提交给作业调度器，作业调度器根据任务执行环境的资源情况，将这些Splits分配给不同的Map任务进行并行处理。</li>
</ul>
</li>
</ol>
<p>Split的优化</p>
<p>Hadoop的Split过程具有一定的灵活性，可以通过以下方式进行优化：</p>
<ul>
<li><strong>调整Split大小</strong>：根据任务需求，通过配置参数调整Split的最大和最小大小。</li>
<li><strong>自定义InputFormat</strong>：为特定类型的数据设计自定义的<code>InputFormat</code>，实现更精细的Split控制。</li>
<li><strong>合并小文件</strong>：对于大量小文件，Hadoop可以通过CombineFileInputFormat类将多个小文件合并为一个Split，以减少Map任务的数量。</li>
</ul>
<p>总结</p>
<p>Split过程是Hadoop任务处理的第一步，它将大数据集划分为更小的逻辑单元，供Map任务并行处理。Split的大小和划分方式直接影响任务的执行效率和性能。理解并优化Split过程，对于提高Hadoop任务的性能至关重要。</p>
<h3 id="3-3-MapReduce"><a href="#3-3-MapReduce" class="headerlink" title="3.3 MapReduce"></a>3.3 MapReduce</h3><h4 id="1-什么是MapReduce？"><a href="#1-什么是MapReduce？" class="headerlink" title="1. 什么是MapReduce？"></a>1. 什么是MapReduce？</h4><p>MapReduce是一个分布式计算程序的编程框架。MapReduce的核心功能是将<strong>用户编写的业务逻辑代码</strong>和<strong>自带的默认组件</strong>整合成一个完整的分布式计算程序并且将其<strong>并发运行</strong>在一个Hadoop集群上。</p>
<p>MapReduce计算框架将数据的计算过程分为<strong>数据输入阶段</strong>、<strong>map阶段</strong>、<strong>reduce阶段</strong>和<strong>数据输出阶段</strong>。</p>
<img src="Snipaste_2024-08-09_20-00-47-172545400944711.png" alt="Snipaste_2024-08-09_20-00-47" style="zoom:50%;">

<p>编写MapReduce程序：<strong>继承Mapper类重写map方法</strong>，<strong>继承Reduce类重写reduce方法</strong>，<strong>在main方法中编写Driver驱动器</strong></p>
<h4 id="2-⭐请说一下Map-Task的工作机制"><a href="#2-⭐请说一下Map-Task的工作机制" class="headerlink" title="2. ⭐请说一下Map Task的工作机制"></a>2. <del>⭐请说一下Map Task的工作机制</del></h4><p><img src="Snipaste_2023-10-03_14-11-03.png" alt="Snipaste_2023-10-03_14-11-03"></p>
<ul>
<li>Read阶段：inputFile 通过 split 被切割为多个 split 文件，通过 Record 按行读取内容给 map（自己写的处理逻辑的方法） </li>
<li>Map阶段：使用map()方法对数据进行处理</li>
<li>Collect阶段：数据被 map 处理完之后交给 OutputCollect 收集器，对其结果 key 进行分区（默认使用的 hashPartitioner），然后写入 buffer，<strong>每个 map task 都有一个内存缓冲区</strong>（环形缓冲区），存放着 map 的输出结果</li>
<li>溢写阶段：当缓冲区快满的时候(80%)需要将缓冲区的数据以一个临时文件的方式溢写到磁盘</li>
<li>Merge阶段：当整个 map task 结束后再对磁盘中这个 map task 产生的所有临时文件做Merge合并，生成最终的正式输出文件，然后等待 reduce task 的拉取。</li>
</ul>
<h4 id="3-⭐请说一下Reduce-Task的工作机制"><a href="#3-⭐请说一下Reduce-Task的工作机制" class="headerlink" title="3. ⭐请说一下Reduce Task的工作机制"></a>3<del>. ⭐请说一下Reduce Task的工作机制</del></h4><p><img src="image-20231003141616898.png" alt="image-20231003141616898"></p>
<ul>
<li>Copy阶段：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</li>
<li>Sort阶段：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。合并后的文件进行一次全局归并排序，使得key相同的数据聚集在一起。</li>
<li>Reduce阶段：reduce()函数将计算结果写到 HDFS 上。</li>
</ul>
<h4 id="4-⭐⭐说一说Map-Task和Reduce-Task的并行度（个数）决定机制？"><a href="#4-⭐⭐说一说Map-Task和Reduce-Task的并行度（个数）决定机制？" class="headerlink" title="4. ⭐⭐说一说Map Task和Reduce Task的并行度（个数）决定机制？"></a>4. ⭐⭐说一说Map Task和Reduce Task的并行度（个数）决定机制？</h4><ul>
<li><p><strong>Map Task并行度（也就是Map Task数量）</strong>由切片个数决定，一个数据切片可以对应启动一个MapTask，切片个数由输入文件和切片规则决定（默认情况下，数据切片大小与数据块大小相同，或者是数据块大小的倍数，即128M或128M的倍数）。<br>（1）默认情况下Map的个数&#x3D;目标文件或数据的总大小totalSize&#x2F;hdfs集群文件块的大小blockSize</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaultNum = totalSize/blockSize</span><br></pre></td></tr></table></figure>

<p>（2）当用户指定<code>mapred.map.tasks</code>时，即userExpNum，这个期望值计算引擎不会直接采纳，会先算出默认情况下Map个数与用户期望的Map个数的最大值，用expMaxNum表示<code>expMaxNum=max(defaultNum,userExpNum)</code>作为待选项；计算真实分片数realSplitNum，分片大小为参数<code>mapred.min.split.size</code>和blockSize间的较大值，再将目标文件或数据的总大小除以这个数得到真实的分片数。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">defaultNum = totalSize/blockSize;</span><br><span class="line">userExpNum = mapred.map.tasks;</span><br><span class="line">expMaxNum = max(defaultNum, userExpNum);</span><br><span class="line">realSplitNum = totalSize/max(mapred.min.split.size, blockSize);</span><br><span class="line">实际的map个数 = min(realSplitNum, expMaxNum);</span><br></pre></td></tr></table></figure>


<p>（3）一般情况下无需手动调节，如果查询表中存在大量小文件，需要使用CombineHiveInputFormat将多个小文件合并为一个切片，从而控制map task个数。</p>
</li>
<li><p><strong>Reduce Task并行度（也就是Reduce Task数量）</strong>可以直接手动设置，默认为1，设置为0表示没有reduce阶段。</p>
</li>
</ul>
<p>在mapreduce编程框架中设置reduce task个数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>

<p>在HIvesql中指定reduce task个数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.job.reduces = 10;--老版本中的参数是mapred.reduce.tasks</span><br></pre></td></tr></table></figure>

<h4 id="5-⭐⭐⭐请说一下Shuffle阶段，MapTask工作机制、ReduceTask工作机制"><a href="#5-⭐⭐⭐请说一下Shuffle阶段，MapTask工作机制、ReduceTask工作机制" class="headerlink" title="5. ⭐⭐⭐请说一下Shuffle阶段，MapTask工作机制、ReduceTask工作机制"></a>5. ⭐⭐⭐请说一下Shuffle阶段，MapTask工作机制、ReduceTask工作机制</h4><p><img src="Snipaste_2023-09-28_13-48-05.png" alt="Snipaste_2023-09-28_13-48-05"></p>
<p><del>（1）Map 方法之后 Reduce 方法之前这段处理过程叫 Shuffle，<strong>本质就是数据的重新分区</strong></del></p>
<p><del>（2）map 方法之后， 数据首先进入到分区方法， 把数据标记好分区， 然后把数据发送到 环形缓冲区；环形缓冲区默认大小 100m，环形缓冲区达到 80%时，进行溢写；溢写前对数 据进行排序，排序按照对 key 的索引进行字典顺序排序， 排序的手段快排；溢写产生大量溢 写文件，需要对溢写文件进行归并排序；对溢写的文件也可以进行 Combiner 操作， 前提是汇总操作， 求平均值不行。之后还可以进行数据压缩，最后将文件按照分区存储到磁盘， 等待 Reduce 端拉取。</del></p>
<p><del>（3）每个 Reduce 拉取 Map 端对应分区的数据。拉取数据后先存储到内存中， 内存不够了，再存储到磁盘。拉取完所有数据后，采用归并排序将内存和磁盘中的数据都进行排序。在进入 Reduce 方法前， 可以对数据进行分组操作。</del></p>
<p><strong>Shuffle定义</strong>：Map方法之后Reduce方法之前的这段处理过程叫Shuffle，<strong>本质就是数据的重新分区</strong></p>
<p>细节：Shuffle阶段一共三次排序：Map阶段：快排，归并；Reduce阶段：归并</p>
<p>①~⑤为MapTask阶段，②~⑨为shuffle阶段，⑧~⑩为ReduceTask阶段</p>
<p>①InputFile切割成多个split文件，通过RecordReader按行读取内容给map方法（<strong>Map Task的Read阶段</strong>）</p>
<p>②使用map()方法对数据进行处理（<strong>Map Task的Map阶段</strong>）</p>
<p>③map方法之后，交给OutputCollect收集器，按照key的hash进行分区，发送到环形缓冲区（默认大小100M）（<strong>Map Task的Collect阶段</strong>）</p>
<p>④达到80%，将数据以临时文件的方式溢写到磁盘（<strong>Map Task的溢写阶段</strong>）</p>
<p>⑤溢写前，对key的索引进行快排；溢写后，对溢写文件进行归并排序（<strong>Map Task的Merge阶段</strong>）</p>
<p>⑥在归并排序的前后还可以进行Combiner操作（前提是汇总操作），之后还可以进行数据压缩</p>
<p>⑦将文件按分区存储在磁盘上，等待Reduce端拉取</p>
<p>⑧每个Reduce拉取Map端对应分区的数据，先存储在内存中，后磁盘中。（<strong>Reduce Task的Copy阶段</strong>）</p>
<p>⑨拉取完所有的数据后，采用归并排序将内存和磁盘中的数据进行排序，使key相同的数据聚集在一起，之后按照相同的key进行分组（<strong>Reduce Task的Sort阶段</strong>）</p>
<p>⑩使用reduce()函数将计算结果写到HDFS上（<strong>Reduce Task的Reduce阶段</strong>）</p>
<blockquote>
<p>环形缓冲区的底层实现</p>
</blockquote>
<img src="image-20240821112152155.png" alt="image-20240821112152155" style="zoom:67%;">

<h4 id="6-说一说Combiner合并"><a href="#6-说一说Combiner合并" class="headerlink" title="6. 说一说Combiner合并"></a>6. 说一说Combiner合并</h4><ol>
<li>Combiner是MR程序中Mappe和Reducer之外的一种组件</li>
<li>Combiner组件的父类就是Reducer</li>
<li>Combiner和Reducer的区别在于运行的位置，<strong>Combiner是在每一个MapTask所在的节点运行，Reducer是接收全局所有Mapper的输出结果</strong></li>
<li>Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量</li>
<li><strong>Combiner能够应用的前提是不影响最终的业务逻辑</strong>，而且Combiner的输出kv应该跟Reducer的输入kv类型要对应起来。</li>
</ol>
<h4 id="7-Shuffle阶段的数据压缩机制了解吗？"><a href="#7-Shuffle阶段的数据压缩机制了解吗？" class="headerlink" title="7. Shuffle阶段的数据压缩机制了解吗？"></a>7. Shuffle阶段的数据压缩机制了解吗？</h4><p>压缩可以减少磁盘IO，节约磁盘存储空闲。</p>
<p>我们使用的压缩方式是Snappy，特点是速度快，缺点是无法切分</p>
<h4 id="9-在MapReduce任务中压缩位置的选择"><a href="#9-在MapReduce任务中压缩位置的选择" class="headerlink" title="9. 在MapReduce任务中压缩位置的选择"></a>9. 在MapReduce任务中压缩位置的选择</h4><ul>
<li>输入端采用压缩：重点考虑支持切片的压缩格式Bzip2和LZO</li>
<li>Mapper输出采用压缩：为了减少MapTask和ReduceTask之间的网络IO，重点考虑压缩和解压缩快的LZO，Snappy</li>
<li>Reducer输出采用压缩：永久保存考虑采用压缩率较高的GZip和bzip2，如果作为下一个MapReduce的输入，需要考虑支持切片</li>
</ul>
<h4 id="10-⭐join原理"><a href="#10-⭐join原理" class="headerlink" title="10.⭐join原理"></a>10.⭐join原理</h4><img src="Snipaste_2024-08-19_22-05-19-172545400944712.png" alt="Snipaste_2024-08-19_22-05-19" style="zoom:50%;">



<h3 id="3-4-YARN"><a href="#3-4-YARN" class="headerlink" title="3.4 YARN"></a>3.4 YARN</h3><h4 id="1-⭐YARN集群的架构和工作原理知道多少？"><a href="#1-⭐YARN集群的架构和工作原理知道多少？" class="headerlink" title="1. ⭐YARN集群的架构和工作原理知道多少？"></a>1. ⭐YARN集群的架构和工作原理知道多少？</h4><p>YARN是一个资源调度平台。</p>
<p>YARN采用了常见的Master-Slaver架构，其中，资源管理器<strong>ResourceManager</strong>担任Master角色，负责<strong>整个框架的资源统一管理和调度</strong>；<strong>NodeManager</strong>担任Slave角色，负责<strong>任务的执行及当前节点的资源管理</strong>。</p>
<p><img src="Snipaste_2023-10-05_16-12-45.png" alt="Snipaste_2023-10-05_16-12-45"></p>
<h4 id="2-⭐⭐YARN工作机制"><a href="#2-⭐⭐YARN工作机制" class="headerlink" title="2. ⭐⭐YARN工作机制"></a>2. ⭐⭐YARN工作机制</h4><img src="Snipaste_2024-08-10_16-25-39-172545400944713.png" alt="Snipaste_2024-08-10_16-25-39" style="zoom:50%;">

<p>（1）客户端向YARN提交一个作业（Application）</p>
<p>（2）作业提交后，RM根据从NM收集的资源信息，在有足够资源的节点分配一个容器，并与对应的NM进行通信，要求它在该容器中启动AM</p>
<p>（3）AM创建成功后向RM中的ASM注册自己，表示自己可以去管理一个作业（job）</p>
<p>（4）AM注册成功后，会对作业需要处理的数据进行切分，然后向RM申请资源，RM会根据给定的调度策略提供给请求的资源AM</p>
<p>（5）AM申请到资源成功后，会与集群中的NM通信，要求它启动任务</p>
<p>（6）NM接收到AM的要求后，根据作业提供的信息，启动对应的任务</p>
<p>（7）启动后的每个任务会定时向AM提供自己的状态信息和执行的进度</p>
<p>（8）作业运行完成后AM会向ASM注销和关闭自己</p>
<h4 id="3-⭐YARN-的资源调度三种模型了解吗"><a href="#3-⭐YARN-的资源调度三种模型了解吗" class="headerlink" title="3. ⭐YARN 的资源调度三种模型了解吗?"></a>3. ⭐YARN 的资源调度三种模型了解吗?</h4><p>资源调度器负责集群的资源调度工作。</p>
<ul>
<li>FIFO调度器</li>
<li>容量调度器（Apache Hadoop3.1.3默认使用）</li>
<li>公平调度器（CDH框架默认使用）</li>
</ul>
<p>（1）FIFO调度器</p>
<p>单队列调度器，它将任务按照其到达的时间进行排序，<strong>队列中先到达的任务先获得资源</strong></p>
<p>（2）容量调度器（生产环境中会用）</p>
<ul>
<li>多队列，每个队列配置一定资源，每个队列采用FIFO</li>
<li>每个队列都有资源最低保证和使用上限</li>
<li>如果一个队列资源有剩余可以暂时共享给需要资源的队列</li>
<li>分配资源时，先找资源占用率最低的那个队列，然后按照提交作业的优先级或提交时间顺序，最后按容器优先级（相同就看本地性原则），给任务分配资源</li>
</ul>
<p>（3）公平调度器（生产环境中会用，对并行度要求较高）</p>
<ul>
<li>多队列，每个队列配置一定资源，每个队列采用FIFO</li>
<li>每个队列都有资源最低保证和使用上限</li>
<li>如果一个队列资源有剩余可以暂时共享给需要资源的队列</li>
</ul>
<p>多队列，每个队列内部按照缺额大小分配资源启动任务，每个队列可以单独设置资源分配方式，同一时间队列中有多个任务执行</p>
<p>注意：hadoop中默认单队列，企业中按照部门和业务划分队列</p>
<h4 id="4-Hadoop宕机？"><a href="#4-Hadoop宕机？" class="headerlink" title="4. Hadoop宕机？"></a>4. Hadoop宕机？</h4><ul>
<li>如果 MR 造成系统宕机。此时要控制 Yarn 同时运行的任务数，和每个任务申请的最大内存。调整参数：yarn.scheduler.maximum-allocation-mb（单个任务可申请的最多物理内存量，默认是 8192MB）</li>
<li>如果写入文件过量造成 NameNode 宕机。那么调高 Kafka 的存储大小，控制从 Kafka 到 HDFS 的写入速度。高峰期的时候用 Kafka 进行缓存，高峰期过去数据同步会自动跟上。</li>
</ul>
<h3 id="3-5-高可用HA"><a href="#3-5-高可用HA" class="headerlink" title="3.5 高可用HA"></a>3.5 高可用HA</h3><h4 id="1-什么是高可用HA"><a href="#1-什么是高可用HA" class="headerlink" title="1. 什么是高可用HA"></a>1. 什么是高可用HA</h4><p>7*24小时不中断服务。实现高可用的关键策略是消除单点故障，分为HDFS HA和YARN HA</p>
<h4 id="2-简单说一说HDFS如何实现高可用"><a href="#2-简单说一说HDFS如何实现高可用" class="headerlink" title="2. 简单说一说HDFS如何实现高可用"></a>2. 简单说一说HDFS如何实现高可用</h4><p>HDFS HA功能通过配置多个NameNode实现，要保证这两个NameNode的元数据信息必须要同步，而且一个NameNode挂掉之后另一个要马上补上。</p>
<ul>
<li>如何保证多个NN元数据信息一致：其中一个NN负责生成快照文件FsImage，其他NN拉取同步；引进新角色<strong>JournalNode</strong>，保证多个NN的EditLog数据一致性</li>
<li>如何使多个NN其中一个处于Active状态，其他处于Standby状态：在每一个NN上启动<strong>ZKFC</strong>（由zookeeper实现的故障转移控制器），当ZKFC运行在处于Active状态的NN上时，会<strong>在发现其状态不正常</strong>时向ZooKeeper中写入数据。当ZKFC运行在处于Standby状态的NN上时，会从ZooKeeper中读取数据，从而感知处于Active状态的NN是否在正常工作，以便顺利完成故障转移工作。</li>
</ul>
<h4 id="3-在NameNode-HA中，会出现脑裂问题吗-怎么解决脑裂问题？"><a href="#3-在NameNode-HA中，会出现脑裂问题吗-怎么解决脑裂问题？" class="headerlink" title="3. 在NameNode HA中，会出现脑裂问题吗?怎么解决脑裂问题？"></a>3. 在NameNode HA中，会出现脑裂问题吗?怎么解决脑裂问题？</h4><p>某时刻NN1为Active状态，NN2为Standby状态，如果下一时刻NN1对应的ZKFC进程发生假死，那么zookeeper服务端会认为NN1挂掉了，NN2会代替NN1进入Active状态，这时NN1和NN2都处于Active状态，都对外提供服务，这种现象叫脑裂。</p>
<p>如何解决：</p>
<ul>
<li>首先尝试用调用旧的Active NameNode的RPC接口的transitionToStandby 方法，看能不能把它转换为 Standby 状态</li>
<li>如果方法调用失败，使用Hadoop的两种隔离措施：①ssh发送kill指令。②调用用户自定义的脚本程序</li>
</ul>
<h2 id="四、Kafka"><a href="#四、Kafka" class="headerlink" title="四、Kafka"></a>四、Kafka</h2><h3 id="1-什么是Kafka？什么是消息队列？"><a href="#1-什么是Kafka？什么是消息队列？" class="headerlink" title="1. 什么是Kafka？什么是消息队列？"></a>1. 什么是Kafka？什么是消息队列？</h3><p>Kafka是一个分布式的基于<strong>发布&#x2F;订阅</strong>模式的消息队列，或者说其是一个分布式<strong>事件流平台</strong>。</p>
<p>发布&#x2F;订阅：消息的发布者将消息分为不同的类型，订阅者只接收感兴趣的消息。</p>
<p>消息队列就是一个存放消息的容器，参与消息传递的双方称为<strong>生产者</strong>和<strong>消费者</strong>，生产者负责发送消息，消费者负责处理消息</p>
<h3 id="2-为什么要用kafka（kafka消息队列的应用场景）？（消息队列的作用？）"><a href="#2-为什么要用kafka（kafka消息队列的应用场景）？（消息队列的作用？）" class="headerlink" title="2. 为什么要用kafka（kafka消息队列的应用场景）？（消息队列的作用？）"></a>2. 为什么要用kafka（kafka消息队列的应用场景）？（消息队列的作用？）</h3><p>（1）缓冲&#x2F;消峰：先将短时间高并发产生的事务消息存储在消息队列中，然后下游再慢慢根据自己的能力去消费这些信息。有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况</p>
<p>（2）解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵循同样的接口约束</p>
<p>（3）异步通信：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们</p>
<p>（4）数据流处理：在大数据场景，消息队列可以实时或批量收集业务日志、监控数据、用户行为等，并将其导入到大数据引擎中。</p>
<h3 id="3-消息队列的两种模式"><a href="#3-消息队列的两种模式" class="headerlink" title="3. 消息队列的两种模式"></a>3. 消息队列的两种模式</h3><ul>
<li>点对点模式：消费者主动拉取数据，消息收到后清除消息</li>
<li>发布&#x2F;订阅模式：消息的发布者将消息分为不同的类型（多个topic主题），订阅者只接收感兴趣的消息。</li>
</ul>
<h3 id="4-⭐简单介绍一下Kafka的架构"><a href="#4-⭐简单介绍一下Kafka的架构" class="headerlink" title="4. ⭐简单介绍一下Kafka的架构"></a>4. ⭐简单介绍一下Kafka的架构</h3><p>先说几个概念：</p>
<ul>
<li>Producer（生产者）：生产消息的一方</li>
<li>Consumer（消费者）：消费消息的乙方</li>
<li>Broker（代理）：可以看作一个独立的Kafka实例，多个Broker组成一个Kafka Cluster</li>
<li>Topic（主题）：生产者将消息发送到特定的主题，消费者通过订阅特定的主题来消费信息</li>
<li>Partition（分区）：分区属于Topic的一部分，一个Topic可以有多个Partition，并且同一个Topic下的Partition可以分布在不同的Broker上，即一个Topic可以横跨多个Broker。</li>
</ul>
<p><img src="Snipaste_2023-10-23_13-10-47.png" alt="Snipaste_2023-10-23_13-10-47"></p>
<p>Kafka整体上分为Producer（生产者）-Topic（主题）-Consumer（消费者）组成。</p>
<p>为了实现扩展性，会把一个非常大的主题分成多个分区（Partition）分布到多个broker（可以理解为服务器）上，相应地为了配合分区的概念，消费者可以分成消费者组，消费者组由多个Consumer组成，组内每个消费者负责消费不同分区的数据。</p>
<p>为了提高可用性，为每个Partition增加若干副本，一个Leader和多个Follower。其中生产者发送数据和消费者消费数据的对象都是Leader，Follower负责实时从Leader中同步数据，Leader发生故障时，某个Follower会成为新的Leader。</p>
<p>zookeeper中记录各服务器节点运行的状态（Broker注册）和各分区的Leader相关信息（Topic注册）。Kafka2.8以后可以不采用zookeeper</p>
<h3 id="5-kafka数据分区和消费者的关系"><a href="#5-kafka数据分区和消费者的关系" class="headerlink" title="5. kafka数据分区和消费者的关系"></a>5. kafka数据分区和消费者的关系</h3><p>每个分区只能由同一个消费组内的一个消费者(consumer)来消费，可以由不同的消费组的消费者来消费，同组的消费者则起到并发的效果。</p>
<h3 id="6-Kafka的分区数，副本数，Topic数应如何设定？"><a href="#6-Kafka的分区数，副本数，Topic数应如何设定？" class="headerlink" title="6. Kafka的分区数，副本数，Topic数应如何设定？"></a>6. Kafka的分区数，副本数，Topic数应如何设定？</h3><ul>
<li>Kafka分区数：分区数不是越多越好，一般分区数不要超过集群机器数量，分区数越多占用内存越大，一般3~10个</li>
<li>副本数设定：一般设置成2~3个</li>
<li>Topic数：一般情况下，有多少个日志种类就有多少个Topic</li>
</ul>
<h3 id="7-Kafka杂项问题"><a href="#7-Kafka杂项问题" class="headerlink" title="7. Kafka杂项问题"></a>7. Kafka杂项问题</h3><ul>
<li>Kafka的日志保存时间：7天</li>
<li>Kafka单条日志传输大小：最大1M</li>
<li>Kafka的硬盘大小：每天的数据量*7 天&#x2F;70%</li>
<li>Kafka监控：kafkaeagle</li>
</ul>
<h3 id="8-采集数据为什么选择Kafka？（什么时候用Kafka，什么时候用Flume？）"><a href="#8-采集数据为什么选择Kafka？（什么时候用Kafka，什么时候用Flume？）" class="headerlink" title="8. 采集数据为什么选择Kafka？（什么时候用Kafka，什么时候用Flume？）"></a>8. 采集数据为什么选择Kafka？（什么时候用Kafka，什么时候用Flume？）</h3><p>Flume：Flume是一个基于流式架构的高可用、高可靠、分布式海量日志<strong>采集、聚合、传输</strong>的系统，最主要的作用就是，实时读取服务器本地磁盘的数据，将数据写入到HDFS。</p>
<p>Kafka：Kafka是一个分布式的基于<strong>发布&#x2F;订阅</strong>模式的消息队列，或者说其是一个分布式<strong>事件流平台</strong>。</p>
<p>两者对比：Flume 是一个专用工具被设计为旨在往 HDFS，HBase 发送数据。它对HDFS 有特殊的优化，并且集成了 Hadoop 的安全特性。议如果数据被<strong>多个系统</strong>消费的话，使用 kafka；如果数据被设计给 Hadoop 使用，使用 Flume。</p>
<h3 id="9-⭐Kafka的为什么能高效读写数据？（Kafka的数据是放在磁盘上还是内存上）"><a href="#9-⭐Kafka的为什么能高效读写数据？（Kafka的数据是放在磁盘上还是内存上）" class="headerlink" title="9. ⭐Kafka的为什么能高效读写数据？（Kafka的数据是放在磁盘上还是内存上）"></a>9. ⭐Kafka的为什么能高效读写数据？（Kafka的数据是放在磁盘上还是内存上）</h3><p>磁盘；</p>
<ul>
<li>Kafka本身是分布式集群，可以采用分区技术，并行度高</li>
<li>读数据采用稀疏索引，可以快速定位要消费的数据</li>
<li>顺序写磁盘，省去了大量磁头寻址的时间</li>
<li>页缓存（读数据先从页缓存中找，找不到再去磁盘中读）+零拷贝技术（不走应用层，传输速率高。只用将磁盘文件的数据复制到页面缓冲区一次，然后将数据从页面缓冲区直接发送到网络中，这样就避免了在内核空间和用户空间之间的拷贝）</li>
</ul>
<img src="image-20240821000152051.png" alt="image-20240821000152051" style="zoom: 50%;">

<h3 id="10-⭐Kafka生产者消息发送流程"><a href="#10-⭐Kafka生产者消息发送流程" class="headerlink" title="10. ⭐Kafka生产者消息发送流程"></a>10. ⭐Kafka生产者消息发送流程</h3><p><img src="Snipaste_2023-10-23_15-38-44.png" alt="Snipaste_2023-10-23_15-38-44"></p>
<p>首先Kafka生产者在<strong>main线程</strong>中创建一个Producer对象，数据通过拦截器、序列化器和分区器输送到一个<strong>双端队列</strong>（默认大小32M）中，双端队列中数据以批次（ProducerBatch，默认16K）存在。</p>
<p>sender线程会主动从双端队列中拉取数据，当数据累积到16K或达到等待时间，sender就会向Kafka集群（broker）发送数据，broker给出应答，如果某个broker节点无应答，最多可以缓存5个发送请求。</p>
<p>应答类型：</p>
<ul>
<li>ack &#x3D; 0，生产者发送过来数据就不管了，一直发，可靠性差，效率高</li>
<li>ack &#x3D; 1，生产者发送过来数据，等待Leader应答才继续发送，可靠性中等，效率中等</li>
<li>ack &#x3D; -1，生产者发送过来数据，等待Leader和ISR队列里面所有的Follower应答才继续发送，可靠性高，效率低</li>
</ul>
<h3 id="11-Kafka的生产者发送消息分区好处与策略"><a href="#11-Kafka的生产者发送消息分区好处与策略" class="headerlink" title="11. Kafka的生产者发送消息分区好处与策略"></a>11. Kafka的生产者发送消息分区好处与策略</h3><p>（1）好处</p>
<ul>
<li>便于合理使用存储资源</li>
<li>提高并行度</li>
</ul>
<p>（2）策略</p>
<ul>
<li>指明partition的情况下，直接将数据写入该分区</li>
<li>没有指明partition但指明key的情况下，将key的hash值与分区数取余得到partition值</li>
<li>既没有partition也没有key值的情况下，kafka采用黏性分区器，会随机选择一个分区直到该分区batch已满或已完成，kafka再随机选择另外一个分区。</li>
</ul>
<h3 id="12-⭐谈一谈Kafka数据的可靠性"><a href="#12-⭐谈一谈Kafka数据的可靠性" class="headerlink" title="12. ⭐谈一谈Kafka数据的可靠性"></a>12. ⭐谈一谈Kafka数据的可靠性</h3><p>数据完全可靠条件&#x3D;ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2</p>
<p>可靠性总结：</p>
<ul>
<li>ack &#x3D; 0，生产者发送过来数据就不管了，一直发，可靠性差，效率高</li>
<li>ack &#x3D; 1，生产者发送过来数据，等待Leader应答才继续发送，可靠性中等，效率中等</li>
<li>ack &#x3D; -1，生产者发送过来数据，等待Leader和ISR队列里面所有的Follower应答才继续发送，可靠性高，效率低</li>
</ul>
<p><img src="image-20240826195630672.png" alt="image-20240826195630672"></p>
<img src="image-20240826195621373.png" alt="image-20240826195621373" style="zoom:50%;">

<h3 id="13-⭐⭐kafka如何保证消息不重复（谈一谈Kafka的幂等性）"><a href="#13-⭐⭐kafka如何保证消息不重复（谈一谈Kafka的幂等性）" class="headerlink" title="13. ⭐⭐kafka如何保证消息不重复（谈一谈Kafka的幂等性）"></a>13. ⭐⭐kafka如何保证消息不重复（谈一谈Kafka的幂等性）</h3><p>（1）幂等性：</p>
<p>0.11 版本之后，kafka 提出了一个非常重要的特性，幂等性（默认是开启的），也就是说无论 producer 发送多少次重复的数据，kafka 只会持久化一条数据，把这个特性和至少一次语义（ack 级别设置为-1+副本数&gt;&#x3D;2+ISR 最小副本数&gt;&#x3D;2）结合在一起，就可以实现精确一次性（既不丢失又不重复）。我大致介绍一下它的底层原理：在 producer 刚启动的时候会分配一个 PID，然后发送到同一个分区的消息都会携带一个SequenceNum（单调自增的），broker 会对&lt;PID,partition,SeqNum&gt;做缓存，也就是把它当做主键，如果有相同主键的消息提交时，broker 只会持久化一条数据。但是这个机制只能保证单会话的精准一次性，如果想要保证跨会话的精准一次性，那么就需要事务的机制来进行保证（producer 在使用事务功能之前，必须先自定义一个唯一的事务 id，这样，即使客户端重启，也能继续处理未完成的事务；并且这个事务的信息会持久化到一个特殊的主题当中）</p>
<ul>
<li>幂等性就是kafka生产者无论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证不重复。</li>
<li>当&lt;PID，Partition，SeqNumber&gt;相同的主键消息提交时，Broker只会持久化一条。其中PID是Kafka每次重启都会分配一个新的，Partition表示分区号，SeqNumber是单调自增的。</li>
</ul>
<p><img src="Snipaste_2023-10-23_21-41-19.png" alt="Snipaste_2023-10-23_21-41-19"></p>
<p>（2）生产者事务：</p>
<ul>
<li>生产者可以跨分区和会话，要么全部成功，要么全部失败。</li>
<li>开启事务，必须开启幂等性。</li>
</ul>
<p>（3）Kafka保证消息不丢不重：</p>
<ul>
<li>幂等性 + 事务 + ack设置为-1</li>
<li>如真的重复，如hive中的dwd层去重，去重手段：distinct、分组、按照id开窗只取第一个值</li>
</ul>
<h3 id="14-⭐⭐Kafka数据怎么保证不丢失？"><a href="#14-⭐⭐Kafka数据怎么保证不丢失？" class="headerlink" title="14. ⭐⭐Kafka数据怎么保证不丢失？"></a>14. ⭐⭐Kafka数据怎么保证不丢失？</h3><p>分为生产者端，消费者端，broker端</p>
<p>（1）生产者数据的不丢失</p>
<p>kafka 的 ack 机制：在 kafka 发送数据的时候，每次发送消息都会有一个确认反馈机制，确保消息正常的能够被收到，其中状态有 0，1，-1。</p>
<ul>
<li>ack &#x3D; 0，生产者发送过来数据就不管了，一直发，可靠性差，效率高</li>
<li>ack &#x3D; 1，生产者发送过来数据，等待Leader应答才继续发送，可靠性中等，效率中等</li>
<li>ack &#x3D; -1，生产者发送过来数据，等待Leader和ISR队列里面所有的Follower应答才继续发送，可靠性高，效率低</li>
</ul>
<p>（2）消费者数据的不丢失</p>
<p>通过offset commit 来保证数据的不丢失，kafka自己记录了每次消费的<strong>offset数值</strong>，下次继续消费的时候，会接着上次的 offset 进行消费。offset信息在0.9版本之前保存在zookeeper中，0.9版本之后保存在系统主题之中，即使消费者在运行过程中挂掉了，再次启动的时候会找到 offset 的值，找到之前消费消息的位置，接着消费。由于 offset 的信息写入的时候并不是每条消息消费完成后都写入的（先消费，再写入offset），所以这种情况有可能会造成重复消费，但是不会丢失消息。</p>
<p>（3）broker数据的不丢失</p>
<p>每个Broker中的partition我们一般都会设置副本的个数（副本就是Leader和Follower），生产者发送消息时候先根据分区策略（有partition按partition，有key按key的hash对分区数取余，都没有用黏性策略）写入到Leader中，Follower再跟Leader同步数据。这样有了备份，也可以保证消息数据的不丢失。</p>
<h3 id="15-⭐⭐Kafka是如何保证数据有序？"><a href="#15-⭐⭐Kafka是如何保证数据有序？" class="headerlink" title="15. ⭐⭐Kafka是如何保证数据有序？"></a>15. ⭐⭐Kafka是如何保证数据有序？</h3><p>kafka 只能保证 partition 内是有序的，但是 partition 间的有序是没办法的。</p>
<p>保证partition内部有序的策略：</p>
<ul>
<li>未开启幂等性时，max.in.flight.requests.per.connection需要设置为1，只有一个需求，请求成功传下一个，强制保证按顺序到达。</li>
<li>开启幂等性时，max.in.flight.requests.per.connection设置小于等于5即可，开启幂等性，收集到的数据会再服务端按照SeqNumber重新排序保证单调递增再传递，且生产者最多缓存5个请求，一定能保证这5个请求是有序的，超过5个就不保证了。</li>
</ul>
<p>除此之外，还可以设置一个Topic只对应一个Partition；发送消息时候指定key&#x2F;Partition（推荐）。</p>
<h3 id="16-分区副本是如何在broker上分配的？"><a href="#16-分区副本是如何在broker上分配的？" class="headerlink" title="16. 分区副本是如何在broker上分配的？"></a>16. 分区副本是如何在broker上分配的？</h3><p>Kafka分区引入了副本机制，分区中的多个副本之间会有一个Leader，其余的是follower，我们发送消息会被发送到leader副本，然后follower副本才能从leader副本中拉取消息进行同步。</p>
<p>在每个broker上尽可能均匀地分布着Leader和Follower，如果在生产环境中，可以根据实际需要手动调整分区副本的分配。</p>
<p>kafka通过特定Topic指定多个分区，各个分区可以分布在不同的Broker上，提供较好的负载均衡的能力。分区可以指定对应的副本数，极大提高了消息存储的安全性和容灾能力。</p>
<h3 id="17-⭐说一说Kafka的消费方式以及消费者工作流程"><a href="#17-⭐说一说Kafka的消费方式以及消费者工作流程" class="headerlink" title="17. ⭐说一说Kafka的消费方式以及消费者工作流程"></a>17. ⭐说一说Kafka的消费方式以及消费者工作流程</h3><p>消费方式：消费者采用从broker<strong>主动拉取</strong>数据的方式。</p>
<p>消费者消费原则：</p>
<p>消费可以一对一（一个消费者消费一个Broker中的Leader），可以多对一（一个消费者消费不同Broker中的Leader），这是无条件的。如果一对多，对应的消费者一定是分属于不同消费者组的。每个消费者的offset由消费者提交到系统主题保存。</p>
<p><img src="Snipaste_2023-10-26_15-37-22.png" alt="Snipaste_2023-10-26_15-37-22"></p>
<p>工作流程：</p>
<p>消费者首先创建一个ConsumerNetworkClient，调用sendFetches发送消费请求，当达到每批次最小抓取大小或达到超时时间，调用send方法拉取数据。拉取过来的数据放在消息队列中，经过反序列化，拦截器，最后处理数据。</p>
<p><img src="Snipaste_2023-10-26_16-13-12.png" alt="Snipaste_2023-10-26_16-13-12"></p>
<h3 id="18-Kafka消费者消费数据的分区分配策略"><a href="#18-Kafka消费者消费数据的分区分配策略" class="headerlink" title="18. Kafka消费者消费数据的分区分配策略"></a>18. Kafka消费者消费数据的分区分配策略</h3><p>一个Consumer group中有多个消费者，一个topic中有多个partition分区，到底由哪个消费者来消费哪个partition的数据？</p>
<p>Kafka内部存在两种默认的分区分配策略：Range和RoundRobin；</p>
<ul>
<li>Range：Range 是对每个 Topic 而言的 ，首先对同一个 Topic 里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。然后用 Partitions 分区的个数除以消费者总数来决定每个消费者线程消费几个分区。如果除不尽， 那么前面几个消费者线程将会多消费一个分区。</li>
</ul>
<img src="Snipaste_2023-12-15_18-21-18.png" alt="Snipaste_2023-12-15_18-21-18" style="zoom:33%;">

<ul>
<li>RoundRobin：RoundRobin是针对集群中所有Topic而言的，把所有的partition和所有的consumer都列出来，然后按照hashcod进行排序，最后通过轮询算法来分配partition给到各个消费者。</li>
</ul>
<img src="Snipaste_2023-12-15_18-23-24.png" alt="Snipaste_2023-12-15_18-23-24" style="zoom:33%;">

<ul>
<li>Sitcky：尽量均匀且随机分配</li>
</ul>
<h3 id="19-什么是漏消费和重复消费？如何解决？"><a href="#19-什么是漏消费和重复消费？如何解决？" class="headerlink" title="19. 什么是漏消费和重复消费？如何解决？"></a>19. 什么是漏消费和重复消费？如何解决？</h3><ul>
<li>重复消费：自动提交offset引起。<strong>消费快，offset提交慢</strong>，consumer挂了之后重启从上一次提交的offset处继续消费，导致重复消费</li>
<li>漏消费：设置offset为手动提交，<strong>offset提交快，消费慢</strong>，consumer挂了之后重启从上一次提交的offset处继续消费，导致漏消费</li>
<li>如何解决：Kafka消费端将消费过程和提交offset过程做原子绑定，并且下游消费者支持事务机制</li>
</ul>
<h3 id="20-Kafka消费数据积压，Kafka消费能力不足怎么处理？（消费者如何提高吞吐量）"><a href="#20-Kafka消费数据积压，Kafka消费能力不足怎么处理？（消费者如何提高吞吐量）" class="headerlink" title="20. Kafka消费数据积压，Kafka消费能力不足怎么处理？（消费者如何提高吞吐量）"></a>20. Kafka消费数据积压，Kafka消费能力不足怎么处理？（消费者如何提高吞吐量）</h3><p>（1）如果是Kafka消费能力不足，则可以考虑增加Topic分区数，并且同时提升消费者组的消费者数量，消费者数&#x3D;分区数（两者缺一不可）。</p>
<p>（2）如果是下游的数据处理不及时，可以提高每批次拉取的数据量</p>
<h3 id="21-Kafka宕机了如何解决？"><a href="#21-Kafka宕机了如何解决？" class="headerlink" title="21. Kafka宕机了如何解决？"></a>21. Kafka宕机了如何解决？</h3><p>首先考虑业务是否受到影响：若服务提供没问题，实现做好了集群的容灾机制，就不必担心，</p>
<p>节点排错与恢复：通过日志分析查看</p>
<ul>
<li>查看Flume记录</li>
<li>查看kafka日志</li>
<li>短期没事</li>
</ul>
<h3 id="22-Kafka重启是否会导致数据丢失？"><a href="#22-Kafka重启是否会导致数据丢失？" class="headerlink" title="22. Kafka重启是否会导致数据丢失？"></a>22. Kafka重启是否会导致数据丢失？</h3><ul>
<li>Kafka是将数据写入到磁盘的，一般数据不会丢失</li>
<li>但是在重启 kafka 过程中，如果有消费者消费消息，那么 kafka 如果来不及提交 offset，可能会造成数据的不准确（丢失或者重复消费）。</li>
</ul>
<h3 id="23-为什么Kafka不支持读写分离？"><a href="#23-为什么Kafka不支持读写分离？" class="headerlink" title="23. 为什么Kafka不支持读写分离？"></a>23. 为什么Kafka不支持读写分离？</h3><p>读写分离有两个明显的缺点：</p>
<ul>
<li>数据一致性问题</li>
<li>延时问题</li>
</ul>
<h3 id="24-采集数据的时候为什么在Flume中间加一个Kafka？"><a href="#24-采集数据的时候为什么在Flume中间加一个Kafka？" class="headerlink" title="24. 采集数据的时候为什么在Flume中间加一个Kafka？"></a>24. 采集数据的时候为什么在Flume中间加一个Kafka？</h3><p>Kafka作为一个消息中间件起到日志缓冲的作用，避免同时发生的大量读&#x2F;写请求造成HDFS性能下降，能对Kafka的日志生产采集过程进行实时监控，避免消费层Flume在落盘HDFS过程中产生大量的小数据文件，从而降低HGFS运行性能，并对落盘数据采取适当的压缩措施，尽量节省存储空间，降低网络IO。</p>
<h3 id="25-RPC和消息队列的区别"><a href="#25-RPC和消息队列的区别" class="headerlink" title="25. RPC和消息队列的区别"></a>25. RPC和消息队列的区别</h3><ul>
<li>从用途上看：RPC主要用来解决两个服务远程通信的问题，消息队列主要用来实现异步传输、消峰和降低系统耦合</li>
<li>从通信方式上看：RPC是双向直接网络通讯，消息队列是单向引入中间载体的网络通讯</li>
<li>从架构上看：消息队列需要把消息存储起来，RPC没有这个要求</li>
<li>从请求处理的时效性来看：通过RPC发出的调用一般会立即被处理，存放在消息队列中的消息并不一定会立即被处理</li>
</ul>
<h3 id="26-几种消息队列的对比"><a href="#26-几种消息队列的对比" class="headerlink" title="26. 几种消息队列的对比"></a>26. 几种消息队列的对比</h3><p>吞吐量方面：RabbitMQ比RocketMQ和Kafka低一个数量级</p>
<p>可用性方面：都可以实现高可用，RabbitMQ基于主从架构实现高可用性，RocketMQ基于分布式架构。Kafka也是分布式的，一个数据多个副本。</p>
<p>时效性方面：RabbitMQ并发能力很强，性能极好，延时很低</p>
<p>由于RabbitMQ基于Erlang开发，国内很少有公司做到对其的源码级研究，而RocketMQ基于java开发，阿里巴巴开源，可以进行企业实际业务定制。Kafka拥有高吞吐量，可靠性可用性，分布式任意扩展，在大数据领域的日志采集和实时计算，是业内标准，社区活跃度很高。</p>
<h3 id="27-Kafka文件存储机制了解吗？"><a href="#27-Kafka文件存储机制了解吗？" class="headerlink" title="27. Kafka文件存储机制了解吗？"></a>27. Kafka文件存储机制了解吗？</h3><img src="webwxgetmsgimg (2)-172545400944715.png" alt="webwxgetmsgimg (2)" style="zoom:50%;">

<p>在Kafka中，一个Topic会被分割成多个Partition，而Partition由多个更小的segment的元素组成。由log、index、timeindex三个文件组成一个segment。log中存储的是消息内容，而index和timeindex分别是一些索引信息。kafka会根据log.segment.bytes的配置来决定单个segment文件的大小，当写入数据达到这个大小时就会创建新的segment。</p>
<h3 id="28-⭐Kafka的事务是如何实现的？"><a href="#28-⭐Kafka的事务是如何实现的？" class="headerlink" title="28. ⭐Kafka的事务是如何实现的？"></a>28. ⭐Kafka的事务是如何实现的？</h3><p>Kafka 的事务机制是为了确保生产者和消费者之间的数据一致性，即提供“<strong>exactly-once</strong>”语义，保证数据在多个主题分区间的一致性和原子性。事务的实现涉及多个关键组件和机制：</p>
<p>（1） <strong>事务日志 (Transaction Log)</strong>:</p>
<p>Kafka 在集群中引入了一个特殊的内部主题叫做 <code>__transaction_state</code>，用于存储事务的状态信息。这个主题包含了事务的开始、提交、回滚等信息。事务的状态会以日志记录的方式存储在这个主题中。</p>
<p>（2）<strong>事务协调者 (Transaction Coordinator)</strong>:</p>
<p>每个 Kafka broker 中都运行着一个事务协调者，负责管理和协调事务。具体来说，它处理事务的开始、提交和回滚操作。事务协调者会将事务相关的元数据（例如事务ID、事务状态）写入 <code>__transaction_state</code> 主题中。</p>
<p>（3）<strong>事务ID (Transactional ID)</strong>:</p>
<p>Kafka 中的事务是通过一个唯一的 <code>transactional.id</code> 来标识的。这是由生产者在配置中指定的，Kafka 通过这个ID来跟踪和管理每个事务。当一个事务启动时，事务协调者会为其分配一个唯一的事务序列号 (Transaction Sequence Number, TSN)。</p>
<p>（4）<strong>生产者 (Producer) 端的事务处理</strong>:</p>
<p>生产者在发送消息时，必须指定 <code>transactional.id</code>。当生产者启动一个事务时，会向事务协调者发出 <code>BeginTransaction</code> 请求。生产者发送的所有消息都会在事务范围内进行标记，直到事务提交或回滚。当提交事务时，生产者会发送 <code>EndTransaction</code> 请求给事务协调者，协调者会将事务状态标记为已提交或已回滚。</p>
<p>（5）<strong>消费者 (Consumer) 端的事务处理</strong>:</p>
<p>事务性消费者会检测到未提交的事务消息，并跳过这些消息，直到它们被正式提交（或回滚）。如果事务被回滚，消费者将永远不会处理这些消息。如果事务提交成功，消费者将按顺序消费这些消息，从而确保“exactly-once”语义。</p>
<p>（6）<strong>幂等性 (Idempotence) 和事务的结合</strong>:</p>
<p>Kafka 的幂等性和事务机制结合使用以提供强大的数据一致性保障。幂等性确保生产者发送的每条消息在特定分区上仅被写入一次，避免了重复消费。事务则确保消息在多个分区间的一致性，即使在失败和重启的情况下。</p>
<p>（7）<strong>事务中的原子性</strong>:</p>
<p>Kafka 事务的一个关键特性是消息的原子性，即要么所有消息都被成功写入，要么所有消息都被回滚。这是通过事务协调者和事务日志来实现的。</p>
<p>（8）<strong>事务超时</strong>:</p>
<p>Kafka 设置了事务超时机制。如果事务在指定时间内没有被提交或回滚，事务协调者会自动回滚该事务，确保不会有长时间悬而未决的事务。</p>
<p>总结</p>
<p>Kafka 的事务机制通过事务日志、事务协调者、事务ID等组件实现了跨多个分区的一致性和原子性操作，为用户提供了强大的数据一致性保障，适用于金融交易、订单处理等需要高一致性的数据场景。</p>
<h2 id="五、Zookeeper"><a href="#五、Zookeeper" class="headerlink" title="五、Zookeeper"></a>五、Zookeeper</h2><h3 id="1-什么是Zookeeper？"><a href="#1-什么是Zookeeper？" class="headerlink" title="1.什么是Zookeeper？"></a>1.什么是Zookeeper？</h3><p>Zookeeper是一个开源的基于观察者模式的分布式协调服务，负责存储数据+事件监听。在Kafka中，Zookeeper主要为kafka提供Broker和Topic的注册以及多个Partition的负载均衡等功能。在Hadoop中，zookeeper为NameNode提供高可用的支持。</p>
<h3 id="2-Zookeeper的特点有哪些？"><a href="#2-Zookeeper的特点有哪些？" class="headerlink" title="2. Zookeeper的特点有哪些？"></a>2. Zookeeper的特点有哪些？</h3><p>（1）ZooKeeper由一个领导者（Leader）和多个跟随着（Follower）组成的集群</p>
<p>（2）集群中只要有<strong>半数以上</strong>节点存活，Zookeeper集群就能正常服务，所以ZooKeeper适合安装奇数台服务器</p>
<p>（3）<strong>全局数据一致性</strong>：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的</p>
<p>（4）<strong>更新请求顺序执行</strong>：来自同一个Client的更新请求按其发送顺序依次执行</p>
<p>（5）数据更新<strong>原子性</strong>：一次数据要么成功，要么失败</p>
<p>（6）实时性：在一定使劲按范围内，Client能读到最新数据</p>
<h3 id="3-说说zookeeper的应用场景"><a href="#3-说说zookeeper的应用场景" class="headerlink" title="3. 说说zookeeper的应用场景"></a>3. 说说zookeeper的应用场景</h3><ul>
<li>统一命名服务：可以通过 ZooKeeper 的顺序节点生成全局唯一 ID。</li>
<li>统一管理配置：通过zookeeper的监听机制，当数据发布到zookeeper被监听的节点上，其他机器可以通过监听zookeeper上节点的变化来实现配置的动态更新和</li>
<li>动态感知服务器的上下线</li>
<li>软负载均衡：在zookeeper中记录每台服务器的访问数，让访问数量最少的服务器去处理最新的客户端请求</li>
</ul>
<h3 id="4-⭐说说zookeeper的数据结构"><a href="#4-⭐说说zookeeper的数据结构" class="headerlink" title="4. ⭐说说zookeeper的数据结构"></a>4. ⭐说说zookeeper的数据结构</h3><p>整体上可以看成一根树，每个节点称作一个ZNode，ZNode可以存储数据（默认1MB）也可以存储其他节点。</p>
<p>ZNode分为4大类：</p>
<ul>
<li>持久化目录节点：客户端与zookeeper断开连接后，该节点依旧存在</li>
<li>临时目录节点（-e）：客户端与zookeeper断开连接后，该节点被删除</li>
<li>持久化顺序编号目录节点（-s）：客户端与zookeeper断开连接后，该节点依旧存在，zookeeper给该节点名称进行顺序编号</li>
<li>临时顺序编号目录节点（-e -s）：客户端与zookeeper断开连接后，该节点被删除，zookeeper给该节点名称进行顺序编号</li>
</ul>
<p>每个ZNode由两部分组成：</p>
<ul>
<li>data：节点存放的数据</li>
<li>stat：状态信息</li>
</ul>
<h3 id="5-说一说zookeeper中的常用命令"><a href="#5-说一说zookeeper中的常用命令" class="headerlink" title="5. 说一说zookeeper中的常用命令"></a>5. 说一说zookeeper中的常用命令</h3><img src="Snipaste_2023-12-01_17-46-33.png" alt="Snipaste_2023-12-01_17-46-33" style="zoom:43%;">

<h3 id="6-说一说zookeeper的监听器原理"><a href="#6-说一说zookeeper的监听器原理" class="headerlink" title="6. 说一说zookeeper的监听器原理"></a>6. 说一说zookeeper的监听器原理</h3><p>客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、节点删除、子目录节点增加删除）时，ZooKeeper 会通知客户端。</p>
<p>（1）首先要有一个main()线程</p>
<p>（2）在main线程中创建zookeeper客户端，这时就会创建两个线程，一个负责网络连接通信（connect），一个负责监听（listener）</p>
<p>（3）通过connect线程将注册的监听事件发送给zookeeper</p>
<p>（4）zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程</p>
<h3 id="7-⭐Zookeeper的选举机制"><a href="#7-⭐Zookeeper的选举机制" class="headerlink" title="7. ⭐Zookeeper的选举机制"></a>7. ⭐Zookeeper的选举机制</h3><p>Zookeeper在工作时，有一台节点服务器为Leader，其他节点服务器均为Follower，Leader是通过内部的选举机制临时产生的。</p>
<p><img src="Snipaste_2023-10-19_16-04-00.png" alt="Snipaste_2023-10-19_16-04-00"></p>
<p>（1）<strong>zookeeper第一次启动时的选举</strong>：每当一个服务器启动的时候，发起一次选举，服务器首先投自己一票，然后该服务器将自己所有的票投给比自己myid大的服务器，若没有半数以上结果，服务器就保持LOOKING状态，接着下一台服务器启动做相同的事情，直到某台服务器得票超过半数，该服务器更改状态为LEADING，其余服务器更改状态为FOLLOWING。总结：投票过半数时，服务器 id 大的胜出。（注意：Leader选出后，后续启动的所有服务器都是follower）</p>
<p><img src="Snipaste_2023-10-19_16-18-31.png" alt="Snipaste_2023-10-19_16-18-31"></p>
<p>（2）<strong>如果leader宕机了，再进行一次选举（超过半数服务器存活）</strong>：</p>
<p>SID：服务器ID，和myid一致</p>
<p>ZXID：事务ID，用来标识一次服务器状态的变更</p>
<p>Epoch：每个Leader任期的代号</p>
<p>①Epoch 大的直接胜出</p>
<p>②Epoch 相同，事务 ID 大的胜出</p>
<p>③事务 ID 相同，服务器 ID 大的胜出</p>
<h3 id="8-ZooKeeper集群为啥最好是奇数台？"><a href="#8-ZooKeeper集群为啥最好是奇数台？" class="headerlink" title="8. ZooKeeper集群为啥最好是奇数台？"></a>8. ZooKeeper集群为啥最好是奇数台？</h3><p>因为Zookeeper服务器超过半数存活才可用，比如我们有三台zookeeper服务器，最多允许宕掉1台zookeeper，而有4台的时候最多也允许宕掉1台。</p>
<p>生产经验：10台服务器：3台zookeeper；20台服务器：5台zookeeper</p>
<h3 id="9-Zookeeper选举过半机制是如何防止“脑裂”现象产生的？"><a href="#9-Zookeeper选举过半机制是如何防止“脑裂”现象产生的？" class="headerlink" title="9. Zookeeper选举过半机制是如何防止“脑裂”现象产生的？"></a>9. Zookeeper选举过半机制是如何防止“脑裂”现象产生的？</h3><p><strong>脑裂</strong>：zookeeper集群中同时出现两个Leader（Hadoop集群中有两个NameNode处于Active状态），很危险，会带来数据一致性的问题</p>
<p>由于Zookeeper选举过半机制，只有当选票超过集群总数一半时才会选为Leader，假设集群中有两个Leader，这两个Leader都须获得超过半数的选票，这显然不肯能，所以不可能选出两个Leader</p>
<h3 id="10-Paxos算法和ZAB协议"><a href="#10-Paxos算法和ZAB协议" class="headerlink" title="10. Paxos算法和ZAB协议"></a>10. Paxos算法和ZAB协议</h3><p>Paxos算法：</p>
<img src="image-20240820235031265.png" alt="image-20240820235031265" style="zoom:50%;">

<img src="image-20240820235050806.png" alt="image-20240820235050806" style="zoom:50%;">

<p>Zab协议：</p>
<img src="image-20240820235202634.png" alt="image-20240820235202634" style="zoom:67%;">

<img src="image-20240820235230773.png" alt="image-20240820235230773" style="zoom:50%;">

<img src="image-20240820235249766.png" alt="image-20240820235249766" style="zoom: 50%;">

<h3 id="11-什么是CAP原则？Zookeeper符合这个原则的哪两个？"><a href="#11-什么是CAP原则？Zookeeper符合这个原则的哪两个？" class="headerlink" title="11. 什么是CAP原则？Zookeeper符合这个原则的哪两个？"></a>11. 什么是CAP原则？Zookeeper符合这个原则的哪两个？</h3><img src="Snipaste_2024-08-19_22-24-39-172545400944714.png" alt="Snipaste_2024-08-19_22-24-39" style="zoom: 67%;">

<p>为什么只能满足其中的两项？</p>
<p>比如当两个机器之间的数据同步出现了问题：</p>
<img src="Snipaste_2024-08-19_22-25-41-172545400944716.png" alt="Snipaste_2024-08-19_22-25-41" style="zoom:67%;">

<h3 id="12-zookeeper如何实现分布式锁"><a href="#12-zookeeper如何实现分布式锁" class="headerlink" title="12. zookeeper如何实现分布式锁"></a>12. zookeeper如何实现分布式锁</h3><img src="image-20240821112457768.png" alt="image-20240821112457768" style="zoom:67%;">

<h2 id="六、Flume"><a href="#六、Flume" class="headerlink" title="六、Flume"></a>六、Flume</h2><h3 id="1-Flume是什么？"><a href="#1-Flume是什么？" class="headerlink" title="1. Flume是什么？"></a>1. Flume是什么？</h3><p>Flume是一个基于流式架构的高可用、高可靠、分布式海量日志<strong>采集、聚合、传输</strong>的系统，最主要的作用就是，实时读取服务器本地磁盘的数据，将数据写入到HDFS。</p>
<h3 id="2-⭐Flume的基础框架（组成）"><a href="#2-⭐Flume的基础框架（组成）" class="headerlink" title="2. ⭐Flume的基础框架（组成）"></a>2. ⭐Flume的基础框架（组成）</h3><p>Flume以<strong>Agent</strong>为最小独立运行单元，单个Agent由Source、Channel、Sink三大组件组成。</p>
<ul>
<li><p><strong>Source</strong>：是<strong>负责接收数据到 Agent 的组件</strong>，可以处理各种类型的数据，比如Taildir Source（Flume1.7以后提供）：断点续传【断点续传主要保证在服务器挂掉的情况下，再次启动服务数据不会丢失的问题;其原理就是在底层维护了⼀个 offset 偏移量(也就是每次读取文件的偏移量)Flume 会通过这个偏移量来找到上次文件读取的位置从⽽实现了断点续传的功能】、多目录</p>
</li>
<li><p><strong>Sink</strong>：用于把数据发送到目的地的组件</p>
</li>
<li><p><strong>Channel</strong>：Channel 是位于 Source 和 Sink 之间的缓冲区，因此，Channel 允许 Source 和 Sink 运作在<strong>不同的速率</strong>上。</p>
<p>File Channel：数据存储在磁盘，宕机数据可以保存。但是传输速率慢。适合对数据传输可靠性要求高的场景，比如，金融行业。</p>
<p>Memory Channel：数据存储在内存中，宕机数据丢失。传输速率快。适合对数据传输可靠性要求不高的场景，比如，普通的日志数据。</p>
<p>Kafka Channel：减少了 Flume 的 Sink 阶段，提高了传输效率。</p>
</li>
<li><p><strong>Event</strong>：数据传输基本单元，由Header和Body两部分组成</p>
</li>
</ul>
<h3 id="3-⭐在你的数仓项目中，怎么使用Flume的？三大组件使用的类型都是什么？"><a href="#3-⭐在你的数仓项目中，怎么使用Flume的？三大组件使用的类型都是什么？" class="headerlink" title="3. ⭐在你的数仓项目中，怎么使用Flume的？三大组件使用的类型都是什么？"></a>3. ⭐在你的数仓项目中，怎么使用Flume的？三大组件使用的类型都是什么？</h3><ul>
<li><p>采集日志Flume：</p>
<p>Source：TailDir Source（断点续传，多目录）</p>
<p>Channel：Kafka Channel</p>
</li>
<li><p>消费Kafka数据的Flume：</p>
<p>Source：kafkaSource</p>
<p>Channel：FileChanne</p>
<p>Sink：HDFS Sink</p>
</li>
</ul>
<h3 id="4-谈一谈Flume的事务机制"><a href="#4-谈一谈Flume的事务机制" class="headerlink" title="4. 谈一谈Flume的事务机制"></a>4. 谈一谈Flume的事务机制</h3><p>Flume 使用两个独立的事务分别负责从Soucrce 到 Channel，以及从 Channel 到 Sink 的事件传递。在数据传递过程中，Flume会进行<strong>错误检测</strong>、<strong>错误恢复</strong>以及<strong>日志记录</strong>等操作，以保证数据的可靠性和一致性。</p>
<p>该事务机制主要包括三个阶段：<strong>启动事务</strong>、<strong>进行业务逻辑</strong>、<strong>提交或回滚事务</strong>。</p>
<p>在启动事务阶段，Flume会初始化事务所需的资源，并记录事务的起始时间戳。同时，Flume还会检查前一个事务是否已经成功提交或回滚，以确保事务的顺序性和一致性。</p>
<h3 id="5-什么是Put事务或Take事务？"><a href="#5-什么是Put事务或Take事务？" class="headerlink" title="5. 什么是Put事务或Take事务？"></a>5. 什么是Put事务或Take事务？</h3><p>Source 到 Channel 是 Put 事务，Channel 到 Sink 是 Take 事务</p>
<h3 id="6-Flume采集数据会丢失吗？"><a href="#6-Flume采集数据会丢失吗？" class="headerlink" title="6. Flume采集数据会丢失吗？"></a>6. Flume采集数据会丢失吗？</h3><p>根据 Flume 的架构原理，Flume 是不可能丢失数据的，其内部有完善的事务机制，Source 到 Channel 是事务性的，Channel 到 Sink 是事务性的，因此这两个环节不会出现数据的丢失，<strong>唯一</strong>可能丢失数据的情况是 Channel 采用 memoryChannel，agent 宕机导致数据丢失，或者<strong>Channel</strong> 存储数据<strong>已满</strong>，导致 Source 不再写入，未写入的数据丢失。</p>
<p>Flume 不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由 Sink 发出，但是没有接收到响应，Sink 会再次发送数据，此时可能会导致数据的重复。</p>
<h3 id="7-Flume-Agent的配置文件怎么写？"><a href="#7-Flume-Agent的配置文件怎么写？" class="headerlink" title="7. Flume Agent的配置文件怎么写？"></a>7. Flume Agent的配置文件怎么写？</h3><ul>
<li>定义agent的名称a1</li>
<li>定义a1的Source名称、Sink名称、Channel名称</li>
<li>描述a1的Source类型（数据源地址），Sink类型（输出地址）、Channel类型以及对应细节</li>
<li>自定义定义拦截器</li>
<li>将Source和Channel连接起来，将Channel和Sink连接起来</li>
</ul>
<h3 id="8-说一说Flume的拓扑结构"><a href="#8-说一说Flume的拓扑结构" class="headerlink" title="8. 说一说Flume的拓扑结构"></a>8. 说一说Flume的拓扑结构</h3><ul>
<li>简单串联：将多个Flume按顺序连接起来，前一个Agent的Sink通过RPC连接下一个Agent的Source</li>
<li>复制和多路复用：将相同的数据复制到多个channel中，或者将不同的数据发送到不同的channel中【根据 event 中 Header 的某个 key 的值，将不同的 event 发送到不同的 Channel中】（多channel多sink）</li>
<li>负载均衡和故障转移：将多个sink逻辑上分到一个sink组，sink组配合可以实现负载均衡和错误恢复的功能。（一channel多sink）</li>
<li>聚合（最常见，实用）：每台服务器部署一个Flume，最后传送到一个集中日志的Flume，再由此 flume 上传到 hdfs、hive、hbase 等，进行日志分析。</li>
</ul>
<h3 id="9-说一说Flume的选择器（Channel-Selectors）"><a href="#9-说一说Flume的选择器（Channel-Selectors）" class="headerlink" title="9. 说一说Flume的选择器（Channel Selectors）"></a>9. 说一说Flume的选择器（Channel Selectors）</h3><p>Channel Selectors，可以让不同的项目日志通过不同的Channel到不同的Sink中去。Replicating Channel Selector (default)（复制）和 Multiplexing Channel Selector（多路复用），前者会将source过来的events发往所有channel，后者可以选择该发往哪些channel。</p>
<h3 id="10-⭐自定义Flume拦截器的步骤"><a href="#10-⭐自定义Flume拦截器的步骤" class="headerlink" title="10. ⭐自定义Flume拦截器的步骤"></a>10. ⭐自定义Flume拦截器的步骤</h3><p>ETL拦截器：主要是用来判断json是否完整。没有做复杂的清洗操作主要是防止过多的降低传输速率。</p>
<p>时间戳拦截器：主要是解决零点漂移问题</p>
<ul>
<li><p>①定义类并实现Interceptor接口</p>
</li>
<li><p>②重写四个方法：</p>
<p>initialize()：初始化</p>
<p>public Event intercept(Event event)：处理单个Event</p>
<p>public List<Event> intercept(List<Event> events)：处理多个Event</Event></Event></p>
<p>close()</p>
</li>
<li><p>③创建一个静态内部类，实现 Interceptor.Builder</p>
</li>
</ul>
<h3 id="11-如何实现Flume数据传输的监控的"><a href="#11-如何实现Flume数据传输的监控的" class="headerlink" title="11. 如何实现Flume数据传输的监控的"></a>11. 如何实现Flume数据传输的监控的</h3><p>Ganglia</p>
<h3 id="12-Flume参数调优"><a href="#12-Flume参数调优" class="headerlink" title="12. Flume参数调优"></a>12. Flume参数调优</h3><p>（1）Source</p>
<p>Soure个数：增大Source可以适当增大Source读取数据的能力。</p>
<p>batchSize 参数：决定 Source 一次批量运输到 Channel 的 event 条数，适当调大这个参数可以提高 Source 搬运 Event 到 Channel 时的性能。</p>
<p>（2）Channel</p>
<p>type：memory channel和file channel（前面介绍过了）</p>
<p>Capacity：决定 Channel 可容纳最大的 event 条数</p>
<p>transactionCapacity：决定每次 Source 往 channel 里面写的最大 event 条数和每次 Sink 从 channel 里面读的最大event 条数</p>
<p>（3）Sink</p>
<p>Sink个数：增加 Sink 的个数可以增加 Sink 消费 event 的能力。Sink 也不是越多越好够用就行，过多的 Sink 会占用系统资源，造成系统资源不必要的浪费。</p>
<p>batchSize 参数：决定 Sink 一次批量从 Channel 读取的 event 条数，适当调大这个参数可以提高 Sink 从 Channel 搬出 event 的性能。</p>
<h3 id="13-⭐HDFS-Sink的小文件处理"><a href="#13-⭐HDFS-Sink的小文件处理" class="headerlink" title="13. ⭐HDFS Sink的小文件处理"></a>13. ⭐HDFS Sink的小文件处理</h3><p>小文件危害;</p>
<ul>
<li><p>对于HDFS而言，其本身就不适合存储大量小文件，小文件过多会导致NameNode元数据特别大，占用内存太多，影响HDFS性能。</p>
</li>
<li><p>对Hive而言，在进行查询时，每个小文件都会启动一个Map任务来完成，Map任务启动和初始化的时间远远大于逻辑处理的时间，本末倒置，造成资源浪费</p>
</li>
</ul>
<p>在对HDFS Sink进行配置时，可以通过调整Flume官方提供的三个参数避免写入HDFS大量小文件，基于以上hdfs.rollInterval&#x3D;3600，hdfs.rollSize&#x3D;134217728，hdfs.rollCount &#x3D;0几个参数综合作用，效果如下：</p>
<ul>
<li>文件在达到128M时会滚动生成新文件</li>
<li>文件创建超3600秒时会滚动生成新文件</li>
<li>不通过Event个数来决定何时滚动生成新文件</li>
</ul>
<h3 id="14-浏览器的页面日志采集分类和采集原理"><a href="#14-浏览器的页面日志采集分类和采集原理" class="headerlink" title="14. 浏览器的页面日志采集分类和采集原理"></a>14. 浏览器的页面日志采集分类和采集原理</h3><p>（1）页面浏览（展示）日志采集</p>
<p>一个页面被浏览器加载呈现时采集的日志，是所有互联网产品的两大基本指标：页面浏览量（Page View，PV，每刷新一次页面就记录一次）和访客数（Unique Visitor，UV，对用户id1进行去重）</p>
<p>用户访问淘宝首页（<a target="_blank" rel="noopener" href="http://www.taobao.com)/">www.taobao.com）</a></p>
<ul>
<li>步骤1：用户在浏览器内点击淘宝首页链接（<a target="_blank" rel="noopener" href="http://www.taobao.com)/">www.taobao.com）</a></li>
<li>步骤2：浏览器向淘宝服务器发起HTTP请求。一个标准的HTTP请求包含请求行（最重要的是URL）、请求报头、请求正文</li>
<li>步骤3：服务器接收并解析请求，将处理结果以HTTP响应（状态行、响应报头、响应正文）形式发回浏览器</li>
<li>步骤4：浏览器接收到服务器的响应内容，并将其按照文档规范展现给用户，从而完成一次请求</li>
</ul>
<p>采集日志的动作，放在第四步，浏览器开始解析文档时才能进行。</p>
<p>（2）页面交互日志采集</p>
<p>当页面加载和渲染完成之后，用户可以在页面上执行各类操作，这些操作（互动）要求采集用户的互动行为数据，以便通过量化获知用户的兴趣点或者体验优化点。</p>
<h2 id="七、数据仓库"><a href="#七、数据仓库" class="headerlink" title="七、数据仓库"></a>七、数据仓库</h2><h3 id="1-数仓的概念"><a href="#1-数仓的概念" class="headerlink" title="1. 数仓的概念"></a>1. 数仓的概念</h3><p>数据仓库是为了<strong>数据分析</strong>而设计的企业级数据管理系统，通过海量数据的整合、分析给企业提供决策支持。它具有面向主题、不可更新且时变的特点。</p>
<p>数仓是一个<strong>面向主题的、集成的、相对稳定的、反应历史变化</strong>的数据集合，用于支持管理决策。</p>
<ul>
<li>面向主题的：主题为与业务相关的数据的类别，每个主题基本对应一个宏观的分析领域，如销售主题，考试主题</li>
<li>集成的：数仓中的数据需要经过一系列加工、整理和汇总，这些历史数据记录了企业从过去某个时间点到当前时间点的全部信息</li>
<li>相对稳定的：数据一旦进入数仓就不会再发生变化，当有改变的操作型数据进入数据仓库时，会产生新的记录，而不是覆盖原有记录，这样就保证了数据仓库中保存了变化的全部数据。</li>
</ul>
<h3 id="2-⭐离线数仓和实时数仓的区别？"><a href="#2-⭐离线数仓和实时数仓的区别？" class="headerlink" title="2. ⭐离线数仓和实时数仓的区别？"></a>2. ⭐离线数仓和实时数仓的区别？</h3><p>（1）数据处理时效性</p>
<ul>
<li><strong>离线数仓</strong>：主要处理批量数据，数据的延迟较高，通常是天、小时级别。离线数仓通过定时任务处理已经发生的数据，适用于需要较长时间周期来分析的场景，如日终报表、月度分析等。</li>
<li><strong>实时数仓</strong>：能够处理实时数据，数据的延迟非常低，通常为秒、分钟级别。实时数仓可以实时处理和分析正在产生的数据，适合对时效性要求较高的场景，如实时监控、实时推荐系统等。</li>
</ul>
<p>（2）<strong>使用场景</strong></p>
<ul>
<li><strong>离线数仓</strong>：多用于报表生成、历史数据分析、趋势预测等需要处理大量历史数据的场景，适合定期的数据分析任务。例如，离线数仓可以用来生成日终销售报表、月度用户行为分析等。</li>
<li><strong>实时数仓</strong>：适用于对数据的实时性有要求的业务场景，比如实时监控、风险控制、实时推荐等。例如，电商平台的实时推荐系统、金融系统的实时风控、物联网的实时设备监控等。</li>
</ul>
<p>（3） <strong>技术架构</strong></p>
<ul>
<li><strong>离线数仓</strong>：通常基于批处理系统构建，如Hadoop、Spark等。数据的采集、存储和处理流程是按照一定的时间间隔批量执行的，ETL（抽取、转换、加载）流程比较长。</li>
<li><strong>实时数仓</strong>：依赖于流处理技术，如Flink、Kafka、Spark Streaming等，能够实现数据的实时采集、计算和输出。数据是流式处理的，ETL流程较短，数据几乎是实时进入系统并被处理。</li>
</ul>
<p>（4） <strong>数据一致性</strong></p>
<ul>
<li><strong>离线数仓</strong>：因为处理批量数据，所以能保证较高的一致性。数据在经过批处理后，会被严格校验、清洗和转化，确保一致性。</li>
<li><strong>实时数仓</strong>：由于处理的是实时流数据，数据一致性可能较低，特别是在面对数据延迟、丢失等情况时，需要额外的机制（如容错、补偿）来确保数据一致性。</li>
</ul>
<p>（5） <strong>计算模式</strong></p>
<ul>
<li><strong>离线数仓</strong>：采用批处理模式，即将大量数据按照预定的周期进行处理，适合对大规模历史数据进行分析、聚合。</li>
<li><strong>实时数仓</strong>：采用流处理模式，能够持续不断地处理数据流，支持实时计算和分析，如滑动窗口计算、增量计算等。</li>
</ul>
<p>（6） <strong>典型应用</strong></p>
<ul>
<li><strong>离线数仓</strong>：历史趋势分析、长期业务决策支持、固定周期的KPI报表等。</li>
<li><strong>实时数仓</strong>：实时风险预警、实时推荐系统、实时广告竞价、实时用户行为分析等。</li>
</ul>
<p>总结来说，离线数仓适合处理大批量、历史数据，适用于定期分析和决策支持；而实时数仓适合需要及时响应和决策的场景，能够处理实时数据流并提供即时反馈。</p>
<h3 id="2-⭐数据库与数据仓库的区别OLTP和OLAP（字节）"><a href="#2-⭐数据库与数据仓库的区别OLTP和OLAP（字节）" class="headerlink" title="2. ⭐数据库与数据仓库的区别OLTP和OLAP（字节）"></a>2. ⭐数据库与数据仓库的区别OLTP和OLAP（字节）</h3><p>数据库与数据仓库的区别实际讲的是 <strong>OLTP</strong> 与 <strong>OLAP</strong> 的区别。</p>
<table>
<thead>
<tr>
<th></th>
<th>数据库</th>
<th>数据仓库</th>
</tr>
</thead>
<tbody><tr>
<td>处理方式</td>
<td>联机事务处理OLTP</td>
<td>联机分析处理OLAP</td>
</tr>
<tr>
<td>读特性</td>
<td>每次查询返回少量记录</td>
<td>对大量数据进行汇总查询</td>
</tr>
<tr>
<td>写特性</td>
<td>随机、低延时写入</td>
<td>批量导入</td>
</tr>
<tr>
<td>数据存储</td>
<td>业务数据</td>
<td>历史数据</td>
</tr>
<tr>
<td>设计理念</td>
<td>面向事务设计，为了捕获数据，避免冗余</td>
<td>面向主题设计，为了分析数据，引入冗余</td>
</tr>
<tr>
<td>数据量</td>
<td>GB</td>
<td>TB、PB</td>
</tr>
</tbody></table>
<h3 id="3-数据仓库建模的意义"><a href="#3-数据仓库建模的意义" class="headerlink" title="3. 数据仓库建模的意义"></a>3. 数据仓库建模的意义</h3><ul>
<li>快速查询所需要的数据</li>
<li>减少重复计算</li>
<li>更好地组织和存储数据，以便能在性能、成本、效率和质量之间取得最佳平衡。</li>
</ul>
<h3 id="1-⭐说一说你了解到的数仓建模方法"><a href="#1-⭐说一说你了解到的数仓建模方法" class="headerlink" title="1. ⭐说一说你了解到的数仓建模方法"></a>1. ⭐说一说你了解到的数仓建模方法</h3><p>（1）ER（范式）建模法：</p>
<p>数据仓库之父Bill Inmon提出的建模方法是从全企业的高度，用实体关系（Entity Relationship，ER）模型来描述企业业务，并用规范化的方式表示出来，在关系型数据库中常用，<strong>在范式理论上符合3NF</strong>。范式就是指在设计关系型数据库时，需要遵从的不同的规范。关系型数据库的范式一共有<strong>六种</strong>，分别是<strong>第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）</strong>。遵循的范式级别越高，数据冗余性就越低。这种模型并不适合直接用于分析统计。一个符合第三范式的关系必须具有以下三个条件 :</p>
<ul>
<li>每个属性值唯一，不具有多义性 ;</li>
<li>每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;</li>
<li>每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。</li>
</ul>
<p>1NF：确保每列保持原子性，也就是列不能可分</p>
<p>2NF：在1NF的基础上，要求数据库中的每行必须可被唯一地区分</p>
<p>3NF：在2NF的基础上，要求非主键列完全依赖于整个候选列</p>
<p>（2）维度建模法：</p>
<p>源于Ralph Kimall的《数据仓库工具箱》这本书中的理论。维度模型将复杂的业务通过<strong>事实</strong>和<strong>维度</strong>两个概念进行呈现。<strong>事实</strong>通常对应<strong>业务过程</strong>，而<strong>维度</strong>通常对应<strong>业务过程发生时所处的环境</strong>。虽然存在数据冗余但是方便查询分析。典型代表如星形模型，雪花模型，星座模型。</p>
<h3 id="2-维度建模和范式建模的区别（字节）"><a href="#2-维度建模和范式建模的区别（字节）" class="headerlink" title="2. 维度建模和范式建模的区别（字节）"></a>2. 维度建模和范式建模的区别（字节）</h3><p>范式建模要求满足3NF（展开说），3NF的最终目的是降低数据冗余，保证数据一致性，但数据关联逻辑复杂，不适合分析统计。</p>
<p>维度建模存在数据冗余，违反了三范式，但能够提高查询性能，便于分析统计。（星形模型，雪花模型，星座模型）</p>
<h3 id="3-⭐⭐维度建模中表的类型（字节）"><a href="#3-⭐⭐维度建模中表的类型（字节）" class="headerlink" title="3. ⭐⭐维度建模中表的类型（字节）"></a>3. ⭐⭐维度建模中表的类型（字节）</h3><p>（1）<strong>事实表</strong>（细长）</p>
<ul>
<li><p><input disabled type="checkbox"> 
定义：发生在现实世界中的操作型事件，其所产生的<strong>可度量数值</strong>（个数、件数、次数、金额），存储在事实表中</p>
</li>
<li><p><input disabled type="checkbox"> 
特点：</p>
</li>
<li><p>数据量比较大</p>
</li>
<li><p>内容列少行多，比较细长，表里没有存放实际的内容，他是一堆主键的集合，这些ID分别能对应到维度表中的一条记录</p>
</li>
<li><p>经常发生变化，每天增加新数据</p>
</li>
<li><p><input disabled type="checkbox"> 
分类：</p>
</li>
<li><p>事务事实表：用来记录各业务过程，它保存的是各业务过程的原子操作事件，即最细粒度的操作事件。</p>
</li>
<li><p>周期快照事实表：以具有规律性的、可预见的<strong>时间间隔</strong>来记录事实，主要用于分析一些<strong>存量型</strong>（例如购物车存量，账户余额）或者<strong>状态型</strong>（空气温度，行驶速度）指标。</p>
</li>
<li><p>累积快照事实表：基于一个业务流程中的多个关键业务过程联合处理而构建的事实表，主要用于分析业务过程（里程碑）之间的时间间隔等需求。用来表述过程开始和结束之间的关键步骤事件，覆盖过程的整个生命周期。</p>
</li>
</ul>
<p>（2）<strong>维度表</strong>（宽）</p>
<ul>
<li><p><input disabled type="checkbox"> 
定义：维度表则围绕业务过程所处的环境进行设计。维度表主要包含一个主键和各种维度字段，维度字段称为维度属性。维度表的主键可以作为与之关联的任何事实表的外键。（没有度量值，都是描述信息）</p>
</li>
<li><p><input disabled type="checkbox"> 
特点：</p>
</li>
<li><p>内容列多行少，比较宽，通常具有很多属性</p>
</li>
<li><p>内容相对固定，不会轻易发生变化</p>
</li>
<li><p><input disabled type="checkbox"> 
分类：</p>
</li>
<li><p>全量快照表：每天保存一份全量的维度数据，缺点：浪费存储空间，尤其是当数据的变化比例比较低时</p>
</li>
<li><p>拉链表：记录每条信息的生命周期，<strong>能够更加高效的保存维度信息的历史状态</strong></p>
<p>拉链表是维护历史状态及最新状态数据的一种表，用于记录信息的生命周期，一旦一条信息的生命周期结束，就重新开始记录一条新的信息，并把当前日期放入生效开始日期。</p>
<p><strong>为什么要做拉链表</strong>：拉链表适用于如下场景，数据量比较大，且数据部分字段会发生变化，变化的比例不大且频率不高，若采用每日全量同步策略导入数据则会占用大量内存，且保存很多不变的信息。在此情况下使用拉链表，既能反应数据的历史状态，又能最大限度地节省存储空间。</p>
<p><strong>在拉链表中想要获取某个时间点的数据全量切片，可以通过生效开始时间 &lt;&#x3D; 某个日期且生效结束时间 &gt;&#x3D; 某个日期得到</strong></p>
</li>
</ul>
<p>随时加发生变化的维度叫缓慢变化维</p>
<p>处理缓慢变化维的方法：</p>
<ul>
<li>保留原始值</li>
<li>改写属性值</li>
<li>增加维度新行</li>
<li>增加维度新列</li>
</ul>
<h3 id="4-⭐⭐维度建模的过程"><a href="#4-⭐⭐维度建模的过程" class="headerlink" title="4. ⭐⭐维度建模的过程"></a>4. ⭐⭐维度建模的过程</h3><p><strong>粒度</strong>：事实表中一行数据所表达的业务细节程度</p>
<p>根据《数据仓库工具箱》中的总结，维度建模分四步走：</p>
<p>（1）<strong>选择业务过程</strong>：在整个业务流程中选择我们需要建模的业务（一般根据运营提供的需求），业务过程可以是单个业务事件（交易的支付、退款），也可以是某个事件的状态（当前账户的余额），还可以是一系列相关业务事件组成的业务流程。比如大效果广告的总览，信息流广告的检索，流量等等。</p>
<p>（2）<strong>声明粒度</strong>：在事件分析中，我们要预判所有分析需要<strong>细分的程度</strong>，从而决定选择的粒度。<strong>粒度是维度的一个组合。比如用户粒度，就是用户id维度，用户请求维度，用户登录设备id维度的组合。</strong>在同一事实表中，必须具有相同的粒度（一个用户有一个身份证号，多个手机号，那么用户粒度和身份证粒度相同，比两者更细的粒度就是手机号粒度）。对于有明确需求，就建立针对需求的粒度，需求不明就建立原子（最细）粒度。</p>
<p>（3）<strong>确认维度</strong>：选择好粒度之后，就需要基于此粒度设计维度表，包括维度属性，用于分析时进行分组和筛选。确保维度表中不能出现重复数据，应使维度主键唯一</p>
<p>（4）<strong>确认事实</strong>：确定分析需要衡量的<strong>指标</strong>。同一事实表中的所有度量必须具有相同的粒度。最实用的事实就是数值类型和可加类事实</p>
<h3 id="5-⭐数据建模用的哪些模型？（或维度建模的三种模式）"><a href="#5-⭐数据建模用的哪些模型？（或维度建模的三种模式）" class="headerlink" title="5. ⭐数据建模用的哪些模型？（或维度建模的三种模式）"></a>5. ⭐数据建模用的哪些模型？（或维度建模的三种模式）</h3><p>（1）星型模型</p>
<p>星形模式(Star Schema)是最常用的维度建模方式。星型模式是以<strong>事实表为中心</strong>，所有的维度表直接连接在事实表上，像星星一样。 星形模式的维度建模由<strong>一个事实表</strong>和<strong>一组维度表</strong>成，且具有以下特点：</p>
<ul>
<li>维表只和事实表关联，维表之间没有关联；</li>
<li>以事实表为核心，维表围绕核心呈星形分布</li>
</ul>
<p>（2）雪花模型</p>
<p>雪花模型就是有一张或多张维度表没有直接连接到事实表上，而是通过其他维度表连接到事实表上。虽然这种模型相比星型更规范一些，但是由于这种模型不太容易理解，维护成本比较高，性能比星型模型要低。</p>
<p>（3）星座模型</p>
<p><strong>星座模式是基于多张事实表的，而且多张事实表之间共享维度信息</strong>。在业务发展后期，绝大部分维度建模都采用的是星座模式。</p>
<h3 id="6-⭐为什么要对数仓分层？"><a href="#6-⭐为什么要对数仓分层？" class="headerlink" title="6. ⭐为什么要对数仓分层？"></a>6. ⭐为什么要对数仓分层？</h3><p>原始数据层（ODS）—公共维度层（DIM）—明细数据层（DWD）—汇总数据层（DWS）—数据应用层（ADS）</p>
<ul>
<li>把复杂的问题简单化，减少重复开发</li>
<li>隔离原始数据；如果不分层，原始数据业务规则发生变化将会影响后续整个数据清洗工作，牵一发而动全身</li>
<li>用空间换时间，用大量预处理来提升效率，但数仓中也会存在大量冗余数据</li>
<li>方便数据血缘追踪（相当于数据家谱，方便寻亲寻祖）</li>
</ul>
<h3 id="7-⭐⭐⭐详细说说数仓的分层结构"><a href="#7-⭐⭐⭐详细说说数仓的分层结构" class="headerlink" title="7. ⭐⭐⭐详细说说数仓的分层结构"></a>7. ⭐⭐⭐详细说说数仓的分层结构</h3><p>（1）原始数据层（ODS层）：存放未经处理的原始数据，结构上与源系统保持一致，是数据仓库的数据准备区。保持数据原貌，不进行任何修改，起到数据备份的作用。要保存全部历史数据，压缩格式选择压缩比较高的Gzip；建立分区表，避免后续对表查询时候全表扫描</p>
<p>（2）公共维度层（DIM）：基于维度建模理论进行构建，存放维度表。根据每个业务过程所处的环境设计维度表。ORC列式存储+snappy压缩</p>
<p>（3）明细数据层（DWD）：基于维度建模理论进行构建，存放事实表，最细粒度的明细数据（多采用一些维度退化的手法，将维度退化至事实表中，减少事实表和维度表的关联）；这一层主要是原始数据层与数据仓库的主要隔离层，需要对原始数据进行初步的清洗和规范化的操作。</p>
<p>（4）汇总数据层（DWS）：基于上层的指标需求，以分析的主题对象作为驱动，构建公共统计粒度（最大公约粒度）的汇总表（加强指标的维度退化，采用更多的宽表化手段构建公共指标数据层，提升公共指标的复用性，减少重复加工）。这一层是给ADS层服务的，也可以说是给ADS层提供提前的计算，提供给ADS层多个需求共同使用的相同子查询。通常按照业务主题进行组织，如销售汇总、客户行为分析等</p>
<p>（5）数据集市层（DM）：为特定业务部门或用户群体设计的（生态体验DA），相对独立，更加聚焦和针对性强，主题明确（例如用户行为指标）。相比直接在DWD和DWS层查询的时候性能显著提升</p>
<p>（6）数据应用层（ADS）：存放各项统计指标结果，直接面向业务应用。它存储的是用于具体业务场景和报表的最终数据。ADS 层的数据通常经过高度的定制和优化，以满足特定的应用需求。</p>
<p>DWD层和ODS层的区别？DWD层的数据清洗有哪些？DWS层和DWD层的区别？</p>
<p>分层要和业务强相关，比如快手的数据仓库分层是ODS-DIM-DWD-DWS-DM-APP（和ADS类似），多了一个DM层，DWD和DWS是区分数据域的，要想从各个数据域整合，就要用到DM层。一般而言，紧急需求，APP就直接从DWD层取，如果不紧急优先考虑先建一张DWD或者DM（同时考虑任务的复用性），再建APP，目前只有APP层需要新建对应的盖亚标准数据集。</p>
<p><strong>如果上一层级字段发生变更，下一层级如何感知变化？</strong></p>
<p>（1）<strong>数据血缘分析</strong>:</p>
<ul>
<li>通过数据血缘分析，能够追踪数据在各层之间的流动路径，识别出哪些下游数据依赖于上游的字段。当上游字段变化时，血缘分析工具可以提示相关的下游表或数据集需要更新。</li>
</ul>
<p>（2）<strong>变更数据捕获（CDC，Change Data Capture）</strong>:</p>
<ul>
<li>对于 ODS 层，CDC 是一种常用的技术，用于检测和捕获业务系统中数据的变化。这些变化被记录下来，并通过 ETL 传递到 DWD 层和 DWS 层。</li>
</ul>
<p>（3）<strong>定期全量刷新与增量更新</strong>:</p>
<ul>
<li>对于维度表和汇总表，通常会采用定期全量刷新或增量更新的策略。如果上游数据发生变化，ETL 任务会定期重新加载或更新下游表中的数据。</li>
</ul>
<p>（4）<strong>时间戳与版本控制</strong>:</p>
<ul>
<li>可以在数据表中添加时间戳或版本号，以标记数据的最后更新时间。下游层级可以通过比较时间戳或版本号，判断数据是否需要更新。</li>
</ul>
<p>（5）<strong>通知与调度机制</strong>:</p>
<ul>
<li>建立一种通知和调度机制，当上游层级的数据发生变更时，自动触发下游层级的更新任务。调度工具（如 Apache Airflow、Oozie 等）可以用于管理这些依赖关系和更新任务。</li>
</ul>
<p>（6）<strong>元数据管理与监控</strong>:</p>
<ul>
<li>元数据管理工具可以帮助追踪数据变化，并提供告警机制。当元数据（如表结构、字段含义等）发生变化时，工具会自动通知相关的数据工程师和业务用户。</li>
</ul>
<h3 id="8-⭐什么是数据漂移"><a href="#8-⭐什么是数据漂移" class="headerlink" title="8. ⭐什么是数据漂移"></a>8. ⭐什么是数据漂移</h3><p>通常是指ods表的同一个业务⽇期数据中包含了前一天或后一天凌晨附近的数据或者丢失当天变更的数据，这种现象就叫做漂移，且在⼤部分公司中都会遇到的场景</p>
<p>比如一条从source发来的数据时间是2022.11.10 23:59，到达HDFS Sink的时间已经是2022.11.11 00:05，按照Sink系统内Linux时间戳就会将本应该分区到11.10号的数据落盘到11.11号的分区。</p>
<h3 id="9-⭐如何解决数据漂移？"><a href="#9-⭐如何解决数据漂移？" class="headerlink" title="9. ⭐如何解决数据漂移？"></a>9. ⭐如何解决数据漂移？</h3><p>在消费Kafka数据Flume中添加拦截器。</p>
<p>由于flume默认会用linux系统时间，作为输出到HDFS路径的时间。如果数据是23:59分产生的。Flume消费kafka里面的数据时，有可能已经是第二天了，那么这部分数据会被发往第二天的HDFS路径。我们希望的是根据日志里面的实际时间，发往HDFS的路径，所以下面拦截器作用是获取日志中的实际时间。</p>
<p>拦截JSON日志，通过fastjson框架解析JSON，获取实际时间ts。将获取的ts时间写入拦截器header中，header的key必须是timestamp，因为Flume框架会根据这个key值识别时间，并将数据写入HDFS对应时间的路径下。</p>
<hr>
<p><strong>《阿里巴巴大数据之路》给出的实际公司如何解决数据漂移</strong><br>（1）首先根据log_time（日志数据更新时间）分别冗余前一天15分钟和后一天凌晨开始15分钟的数据，并且用modified_time（表数据更新时间）过滤非当天的数据，确保数据不会因为系统问题而遗漏</p>
<p>（2）然后根据log_time（日志数据更新时间）获取后一天15分钟的数据，针对此数据，按照主键根据log_time做升序排列去重</p>
<p>（3）最后将前两步的结果数据做全外连接，通过限制业务时间proc_time来获取我们需要的数据</p>
<h3 id="10-⭐谈谈对元数据的理解"><a href="#10-⭐谈谈对元数据的理解" class="headerlink" title="10. ⭐谈谈对元数据的理解"></a>10. ⭐谈谈对元数据的理解</h3><p><strong>元数据（Meta Date），主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态</strong>。一般会通过元数据资料库（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。</p>
<p>元数据狭义来讲就是用来描述数据的数据。在数仓中，元数据可以帮助开发人员方便找到他们所关心的数据。</p>
<ul>
<li><p>在数据仓库中，<strong>元数据</strong>（Metadata）是指描述数据的数据。它为数据仓库中的数据提供上下文、结构和管理信息，使用户能够理解、访问和使用这些数据。元数据通常分为技术元数据和业务元数据两类。</p>
<p>（1） <strong>技术元数据</strong></p>
<ul>
<li><strong>数据结构描述</strong>: 包括数据库表的结构（如表名、列名、数据类型、索引、约束等）、视图定义、存储过程、数据模型等。它描述了数据仓库中的数据如何存储和组织。</li>
<li><strong>ETL 过程信息</strong>: 包括数据抽取、转换和加载（ETL）过程的描述，如数据来源、转换规则、加载时间、数据流图等。它记录了数据从源系统到目标数据仓库的流转过程。</li>
<li><strong>数据血缘（Data Lineage）</strong>: 描述数据的来源和流向，说明某一数据项如何从源系统被提取、转换，并加载到数据仓库。数据血缘信息对于数据追踪、问题诊断和影响分析非常重要。</li>
<li><strong>数据质量信息</strong>: 包括数据的准确性、完整性、及时性和一致性等信息，用来评估数据的质量。</li>
<li><strong>存储信息</strong>: 包括数据的存储位置、存储格式、分区信息等，以及数据的备份和恢复策略。</li>
</ul>
<p>（2） <strong>业务元数据</strong></p>
<ul>
<li><strong>业务术语和定义</strong>: 描述业务相关的术语、概念及其定义，帮助业务用户理解数据的意义。例如，某一业务指标的定义、计算逻辑、用途等。</li>
<li><strong>数据使用信息</strong>: 说明哪些业务部门或应用程序在使用哪些数据集，这些数据的业务意义以及它们在报告和分析中的应用。</li>
<li><strong>访问权限和安全性</strong>: 描述谁有权访问哪些数据，访问数据的权限设置和审计信息等。</li>
<li><strong>业务规则</strong>: 描述与数据相关的业务规则和约束，如数据的有效性规则、业务流程中的数据处理规则等。</li>
</ul>
<p>（3） <strong>元数据的作用</strong></p>
<ul>
<li><strong>数据发现和理解</strong>: 元数据为用户提供数据的上下文信息，帮助用户查找和理解数据的含义和用途，从而更有效地利用数据仓库进行分析和决策。</li>
<li><strong>数据管理和维护</strong>: 技术元数据为数据管理员提供了数据仓库的结构和运行信息，支持数据仓库的管理、维护和优化。</li>
<li><strong>数据一致性和质量控制</strong>: 元数据帮助确保数据在不同系统和部门间的一致性，并支持数据质量管理，通过数据血缘和质量信息，用户可以追踪和验证数据的来源和处理过程。</li>
<li><strong>支持数据治理</strong>: 元数据在数据治理中起到关键作用，通过记录和管理数据的定义、使用情况、质量和安全性，支持组织的整体数据管理策略。</li>
</ul>
<p>（4）<strong>元数据管理工具</strong></p>
<p><em>Apache Atlas</em>*</p>
<ul>
<li><p><strong>简介</strong>: Apache Atlas 是由 Apache 软件基金会开发的一个可扩展和可组合的开源元数据管理和治理框架。</p>
</li>
<li><p>主要特性</p>
<ul>
<li><strong>元数据目录</strong>: 提供对数据资产的集中管理和搜索功能。</li>
<li><strong>数据血缘追踪</strong>: 能够追踪数据的来源和流向，帮助理解数据的生命周期。</li>
<li><strong>安全和访问控制</strong>: 支持细粒度的安全策略和访问控制。</li>
<li><strong>可扩展性</strong>: 支持自定义元数据模型和类型。</li>
<li><strong>集成性</strong>: 与 Apache Hadoop 生态系统紧密集成，如 Hive、HBase、Spark 等。</li>
</ul>
</li>
<li><p><strong>适用场景</strong>: 适合在 Hadoop 环境中需要统一管理和治理元数据的组织。</p>
<ul>
<li>为了管理和使用元数据，许多组织会使用元数据管理工具，如 Informatica Metadata Manager、Collibra、Talend Metadata Manager 等。这些工具可以自动化收集、组织和维护元数据，并提供用户友好的界面和查询功能，帮助用户和管理员更好地理解和管理数据。</li>
</ul>
</li>
</ul>
<p>通过元数据，数据仓库不仅仅是数据的存储库，更是一个可以被解释、管理和充分利用的数据资源库。元数据为数据仓库的所有者和使用者提供了必不可少的工具，使他们能够更好地管理和利用数据仓库中的数据。</p>
</li>
</ul>
<h3 id="11-如何确定数据域（主题域）"><a href="#11-如何确定数据域（主题域）" class="headerlink" title="11. 如何确定数据域（主题域）"></a>11. 如何确定数据域（主题域）</h3><p>数据仓库模型设计除横向的分层外，通常也需要根据业务情况进行纵向划分数据域。</p>
<p>划分数据域的意义是<strong>便于数据的管理和应用</strong>。</p>
<p>通常可以根据业务过程或者部门进行划分，本项目根据业务过程进行划分，需要注意的是一个业务过程只能属于一个数据域。</p>
<p>比如交易域、流量域、用户域、考试域等，每个数据域有对应的业务过程。</p>
<h3 id="12-在处理大数据过程中，如何保证得到期望值？"><a href="#12-在处理大数据过程中，如何保证得到期望值？" class="headerlink" title="12. 在处理大数据过程中，如何保证得到期望值？"></a>12. 在处理大数据过程中，如何保证得到期望值？</h3><p>（1）数据采集的时候不丢失数据</p>
<p>（2）数据处理过程中不丢失数据</p>
<p>（3）离线计算进行数据校对，保证数据的完整性</p>
<h3 id="13-你感觉数仓建设中最重要的是什么？"><a href="#13-你感觉数仓建设中最重要的是什么？" class="headerlink" title="13. 你感觉数仓建设中最重要的是什么？"></a>13. 你感觉数仓建设中最重要的是什么？</h3><p>数据的准确性，数据的真正价值在于<strong>数据驱动决策</strong>，通过数据指导运营，在一个不准确的数据驱动下，得到的一定是错误的数据分析，影响的是公司的业务发展决策，最终导致公司的策略调控失败</p>
<h3 id="14-⭐什么样的数仓是好数仓？"><a href="#14-⭐什么样的数仓是好数仓？" class="headerlink" title="14. ⭐什么样的数仓是好数仓？"></a>14. ⭐什么样的数仓是好数仓？</h3><ul>
<li>完善度：完善度衡量的是数仓模型是否能够满足业务需求，即通过ADS层和DWD层直接得出结果的查询指标占所有指标的比例。<img src="image-20240901205257844.png" alt="image-20240901205257844" style="zoom: 50%;"></li>
<li>复用性高：指的是DWD和DWS（汇总数据层）层的数据能否被下游应用广泛复用，即下游直接产出的表的数量。<img src="image-20240901205354600.png" alt="image-20240901205354600" style="zoom:50%;"></li>
<li>规范性：规范性指的是数据模型设计是否遵循企业或行业标准，命名、分层、编码等是否统一，数据的生命周期和治理流程是否清晰。<img src="image-20240901205442442.png" alt="image-20240901205442442" style="zoom:50%;"></li>
<li>稳定性：稳定性指的是数据仓库模型在长时间运行中能够持续提供高质量的数据支持，不受数据增长、业务变更等因素的显著影响。<img src="image-20240901210612541.png" alt="image-20240901210612541" style="zoom:50%;"></li>
<li>准确性： 准确性是指数仓中数据的一致性、准确性以及与源数据的同步性<img src="image-20240901210637345.png" alt="image-20240901210637345" style="zoom:50%;"></li>
<li>健壮性：健壮性是指数仓模型能够应对异常情况（如数据突增、硬件故障、系统更新等）并保持功能稳定和数据可靠。<img src="image-20240901210701575.png" alt="image-20240901210701575" style="zoom:50%;"></li>
<li>成本：成本包括开发、维护、运行等方面的资源投入，数仓的架构设计应尽可能提高性能和性价<img src="image-20240901210724417.png" alt="image-20240901210724417" style="zoom:50%;"></li>
</ul>
<h3 id="15-⭐⭐一些重要的名词术语定义"><a href="#15-⭐⭐一些重要的名词术语定义" class="headerlink" title="15. ⭐⭐一些重要的名词术语定义"></a>15. ⭐⭐一些重要的名词术语定义</h3><img src="Snipaste_2024-07-27_15-43-36.png" alt="Snipaste_2024-07-27_15-43-36" style="zoom:50%;">

<ul>
<li><p>&#x3D;&#x3D;数据域&#x3D;&#x3D;：对企业的业务数据进行区域划分。将同类型数据放在一起，便于快速查找需要的内容，将业务过程或者维度进行抽象的集合</p>
</li>
<li><p>&#x3D;&#x3D;业务过程&#x3D;&#x3D;：企业活动中中的事件，一个个不可拆分的行为事件，每个业务过程对应企业数据仓库的总线矩阵的一行。在业务过程之下，可以定义指标</p>
</li>
<li><p>修饰词：指<strong>除了统计维度以外</strong>指标的业务场景限定抽象</p>
</li>
<li><p>&#x3D;&#x3D;原子指标（度量）&#x3D;&#x3D;：基于某一业务过程的度量值，是业务中不可再拆解的指标。其核心功能就是对指标的聚合逻辑进行了定义。<strong>原子指标包含三要素：业务过程、度量值和聚合逻辑</strong></p>
</li>
<li><p>&#x3D;&#x3D;派生指标&#x3D;&#x3D;：基于<strong>原子指标、时间周期和维度</strong>，对原子指标业务统计范围的圈定。派生指标分为事务型指标（具体业务活动）、存量型指标（某些状态，历史截至当前某个时间）、复合型指标（衍生指标）</p>
</li>
<li><p>&#x3D;&#x3D;衍生指标（复合指标）&#x3D;&#x3D;：在一个或多个派生指标的基础上，通过各种逻辑运算符合而成的，如比率、比例、变化率、均值、分位数、排名等类型的指标</p>
</li>
<li><p>&#x3D;&#x3D;维度&#x3D;&#x3D;：维度是度量的环境，用来反应业务的一类属性，<strong>这类属性的集合构成一个维度</strong></p>
</li>
<li><p>&#x3D;&#x3D;退化维度&#x3D;&#x3D;：指那个一些常用的维度属性<strong>直接写到事实表</strong>中的维度操作。这样设计的主要目的是减少下游用户使用时关联多个表的操作</p>
</li>
<li><p>&#x3D;&#x3D;下钻&#x3D;&#x3D;：<strong>数据明细从粗粒度到细粒度的过程</strong>，会细化某些维度。下钻仅需要在查询上增加一个维度属性，附加在SQL的group by语句中。属性可以来自任何与查询使用的事实表关联的维度。下钻不需要存在层次的定义或者是下钻路径</p>
</li>
<li><p>&#x3D;&#x3D;上卷&#x3D;&#x3D;：数据的汇总聚合，从细粒度到粗粒度的过程，会无视某些维度</p>
</li>
<li><p>宽表：字段比较多的表，通常是指将业务主题相关的指标与维度、属性关联在一起的表</p>
</li>
<li><p>&#x3D;&#x3D;粒度&#x3D;&#x3D;：是指数据仓库的数据单位中保存数据的细化或综合程度的级别。细化程度越高，粒度级越小；相反，细化程度越低，粒度级就越大。<strong>粒度就是维度的组合</strong></p>
</li>
<li><p>&#x3D;&#x3D;业务总线矩阵&#x3D;&#x3D;：矩阵的行表示业务过程，列表示维度。矩阵中的点表示维度与给定的业务过程是否存在关联关系。</p>
</li>
<li><p>&#x3D;&#x3D;规范化&#x3D;&#x3D;：当属性层次被实例化为一系列维度（表），而不是单一的维度（表）时，被称为雪花模型。在规范化后，一张表的字段会拆分到多张表。大多数OLTP的底层数据结构在设计时采用这种规范化技术，删除数据冗余。</p>
</li>
<li><p>&#x3D;&#x3D;反规范化&#x3D;&#x3D;：将维度的属性合并到单个维度（表）中的操作称为反规范化。反规范化会产生包含全部信息的宽表，形成数据冗余，常用于OLAP。将多张表的数据冗余到一张表，目的是减少表之间的关联操作，提高查询性能。反规范化会得到星形模型</p>
</li>
</ul>
<h3 id="16-OneData模型实施过程"><a href="#16-OneData模型实施过程" class="headerlink" title="16. OneData模型实施过程"></a>16. OneData模型实施过程</h3><p>（1）业务调研</p>
<p>构建数据仓库，需要了解各个业务领域，业务线的业务有什么共同点和不同点，以及各个业务线可以细分为哪几个业务模块。每个业务模块具体的业务流程是怎么样的</p>
<p>（2）需求调研</p>
<p>收集数据的使用者的需求，可以去找DA，运营了解他们有什么数据诉求，此时更多的就是报表需求。需求调研的途径有两种：一种是根据与数据分析师、业务运营人员的沟通获知需求（一般是DPM数据产品经理牵线搭桥）；二是对报表系统中<strong>现有的报表</strong>进行研究分析。</p>
<p>（3）架构设计</p>
<ul>
<li>数据域划分：对企业的业务数据进行区域划分。将同类型数据放在一起，便于快速查找需要的内容，将业务过程或者维度进行抽象的集合。数据域需要抽象提炼，并且长期维护和更新，不轻易变动</li>
<li>构建业务总线矩阵：明确每个数据域下有哪些业务过程；业务过程与哪些维度相关，并定义每个数据域下的业务过程和维度。</li>
</ul>
<p>（4）规范定义</p>
<p>定义指标体系，包含原子指标、修饰词、时间周期和派生指标</p>
<p>（5）模型设计</p>
<h3 id="17-如何设计维度"><a href="#17-如何设计维度" class="headerlink" title="17. 如何设计维度"></a>17. 如何设计维度</h3><ul>
<li>选择维度或新建维度，必须保证维度的唯一性，有且只允许有一个维度定义</li>
<li>确定主维表，主维表一般是ODS表，直接与业务系统同步</li>
<li>确定相关维表。根据对业务系统的梳理，确定哪些表和主维表存在关联关系，并选择其中的某些表用于生成维度属性</li>
<li>确定维度属性。从主维表和相关维表中选择维度属性或生成新的维度属性</li>
</ul>
<h3 id="18-⭐什么是缓慢变化维及其解决办法？"><a href="#18-⭐什么是缓慢变化维及其解决办法？" class="headerlink" title="18. ⭐什么是缓慢变化维及其解决办法？"></a>18. ⭐什么是缓慢变化维及其解决办法？</h3><p>定义：在现实世界中，维度的属性（维度表的字段）不是静态的，它会随着时间的流逝发生缓慢的变化，与数据增长较为快速的事实表相比，维度变化相对缓慢。</p>
<p>解决办法：</p>
<p>（1）重写维度值，不保留历史数据，始终取最新数据。</p>
<p>（2）插入新的维度行，采用这种方式，保留历史数据，维度值变化前的事实表和过去的维度值关联，维度值变化后的事实和当前的维度值关联。</p>
<p>（3）添加维度列。保留历史数据，可以使用任何一个属性列。</p>
<p>（4）全量快照表：按照每天的周期，每天保留一份全量快照数据，优点是开发和维护成本低，缺点是浪费存储空间，尤其是数据变化率比较低的时候</p>
<p>（5）拉链表：记录每条信息的生命周期，新增两个字段“生效开始日期”和“生效结束时间”</p>
<img src="image-20240821125552183.png" alt="image-20240821125552183" style="zoom:67%;">

<img src="image-20240821125618072.png" alt="image-20240821125618072" style="zoom:67%;">

<h3 id="19-多值维度定义及其解决办法"><a href="#19-多值维度定义及其解决办法" class="headerlink" title="19. 多值维度定义及其解决办法"></a>19. 多值维度定义及其解决办法</h3><p>定义：事实表中一条记录在某个维度表中有多条记录与之对应，称为多值维度</p>
<p>解决：</p>
<ul>
<li>降低事实表的粒度（粒度变细）</li>
<li>在事实表中采用多字段保存多个维度值，每个字段保存一个维度id</li>
</ul>
<h3 id="20-多值属性定义及其解决办法"><a href="#20-多值属性定义及其解决办法" class="headerlink" title="20. 多值属性定义及其解决办法"></a>20. 多值属性定义及其解决办法</h3><p>定义：维度表中某个属性同时有多个值</p>
<p>解决：</p>
<ul>
<li>将多值属性放到一个字段，该字段内容为kv对的形式</li>
<li>将多值属性放到多个字段，每个字段对应一个属性</li>
</ul>
<h3 id="21-指标（度量）的类型"><a href="#21-指标（度量）的类型" class="headerlink" title="21. 指标（度量）的类型"></a>21. 指标（度量）的类型</h3><p>（1）可加性事实（指标）：按照与事实表关联的任意维度进行汇总</p>
<p>（2）半可加性事实（指标）：只能按照特定维度汇总，不能对所有维度汇总</p>
<p>（3）不可加性事实（指标）：完全不可加的指标，比如比率性指标</p>
<h3 id="22-⭐事实表的设计过程（举例说明）"><a href="#22-⭐事实表的设计过程（举例说明）" class="headerlink" title="22. ⭐事实表的设计过程（举例说明）"></a>22. ⭐事实表的设计过程（举例说明）</h3><h4 id="（1）以淘宝交易事务事实表为例设计事务事实表"><a href="#（1）以淘宝交易事务事实表为例设计事务事实表" class="headerlink" title="（1）以淘宝交易事务事实表为例设计事务事实表"></a>（1）以淘宝交易事务事实表为例设计事务事实表</h4><ul>
<li>选择业务过程：交易订单的四个业务过程：下单、支付、发货、成功完结</li>
<li>确定粒度：针对上一步的每个业务过程确定一个粒度，即事务事实表每一行所表达的细节层次。下单、支付、成功完结三个业务过程为交易子订单粒度，发货为物流单粒度（因为现实中同一个子订单可以拆开成多个物流单进行发货）</li>
<li>确定维度：按照经常用于统计分析的场景，确定维度包含：卖家，买家，商品，类目，发货地址，收获地址等等。</li>
<li>确定事实：不同的业务过程有不同的事实。比如在下单业务过程中，需要包含下单金额，下单数量，支付过程中，包含支付金额，分摊邮费，红包金额，积分金额，在完结过程中包含确认收获金额。</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>单事务事实表</th>
<th>多事务事实表</th>
</tr>
</thead>
<tbody><tr>
<td>业务过程</td>
<td>一个</td>
<td>多个</td>
</tr>
<tr>
<td>粒度</td>
<td>相互间不相关</td>
<td>相同粒度</td>
</tr>
<tr>
<td>维度</td>
<td>相互闲不相关</td>
<td>一致</td>
</tr>
<tr>
<td>事实</td>
<td>只取当前业务过程中的事实</td>
<td>保留多个业务过程中的事实，非当前业务过程中的事实需要置零处理</td>
</tr>
<tr>
<td>冗余维度</td>
<td>多个业务过程，则需要冗余多次</td>
<td>不同的业务过程只需要冗余一次</td>
</tr>
<tr>
<td>理解程度</td>
<td>容易理解，不会混淆</td>
<td>难以理解，需要通过标签来限定</td>
</tr>
<tr>
<td>计算存储成本</td>
<td>较多，每个业务过程都需要计算存储一次</td>
<td>较少，不同业务过程融合在一起，降低了存储计算成本，单是非当前业务过程的度量存在大量零值</td>
</tr>
</tbody></table>
<h4 id="（2）淘宝卖家历史至今快照事实表为例设计周期快照事实表"><a href="#（2）淘宝卖家历史至今快照事实表为例设计周期快照事实表" class="headerlink" title="（2）淘宝卖家历史至今快照事实表为例设计周期快照事实表"></a>（2）淘宝卖家历史至今快照事实表为例设计周期快照事实表</h4><p>周期快照事实表以具有规律性的、可预见的时间间隔来记录事实，主要用于分析存量或状态类型指标。</p>
<ul>
<li>确定粒度：每天针对卖家的历史截至当日的下单支付金额进行快照</li>
<li>确定状态度量（事实）：淘宝卖家历史至今汇总事实表，包含了历史截至当日的下单金额、历史截至当日的支付金额等度量</li>
</ul>
<h4 id="（3）淘宝交易累计快照事实表为例设计累计快照事实表"><a href="#（3）淘宝交易累计快照事实表为例设计累计快照事实表" class="headerlink" title="（3）淘宝交易累计快照事实表为例设计累计快照事实表"></a>（3）淘宝交易累计快照事实表为例设计累计快照事实表</h4><p>累计快照事实表通过具有多个日期字段，分析里程碑间隔之间的需求。<strong>累计快照事实表的另外一个作用就是保存全量数据。</strong></p>
<ul>
<li>选择业务过程：下单、支付、发货、成功完结</li>
<li>确定粒度：针对上一步的每个业务过程确定一个粒度，即事务事实表每一行所表达的细节层次。下单、支付、成功完结三个业务过程为交易子订单粒度，发货为物流单粒度（因为现实中同一个子订单可以拆开成多个物流单进行发货）</li>
<li>确定维度：在事务事实表的基础上增加下单时间、支付时间、发货时间、确认收货时间，对应于日期维表</li>
<li>确定事实：将业务过程对应的事实均放入事实表中，再将每个过程的时间间隔作为事实放在表中</li>
</ul>
<h3 id="23-⭐⭐哪些表适合全量同步，哪些表适合增量同步？"><a href="#23-⭐⭐哪些表适合全量同步，哪些表适合增量同步？" class="headerlink" title="23. ⭐⭐哪些表适合全量同步，哪些表适合增量同步？"></a>23. ⭐⭐哪些表适合全量同步，哪些表适合增量同步？</h3><p><strong>适合全量同步的表：</strong></p>
<ol>
<li><strong>小规模数据表</strong>：<ul>
<li>数据量较小，记录数不多，执行全量同步的时间和资源开销都比较低。</li>
</ul>
</li>
<li><strong>低更新频率的表</strong>：<ul>
<li>数据变化不频繁，增量同步的优势不明显。全量同步能够确保数据的一致性。</li>
</ul>
</li>
<li><strong>静态数据表</strong>：<ul>
<li>表的数据基本不变，例如字典表、配置表等。这类表通常一次同步即可，无需频繁更新。</li>
</ul>
</li>
<li><strong>历史数据表</strong>：<ul>
<li>数据已经不再更新，只是保留历史记录，可以通过一次全量同步完成数据传输。</li>
</ul>
</li>
</ol>
<p><strong>适合增量同步的表：</strong></p>
<ol>
<li><strong>大规模数据表</strong>：<ul>
<li>数据量很大，全量同步的时间和资源消耗过高，通过增量同步减少传输的数据量和时间。</li>
</ul>
</li>
<li><strong>高更新频率的表</strong>：<ul>
<li>数据变化频繁，全量同步效率较低，增量同步能够及时更新变化的数据，保持同步的实时性。</li>
</ul>
</li>
<li><strong>关键业务表</strong>：<ul>
<li>包含重要的实时数据，如订单表、交易记录表等。增量同步可以确保数据快速更新，并避免全量同步可能带来的系统负载。</li>
</ul>
</li>
<li><strong>日志表或事务表</strong>：<ul>
<li>这类表通常是追加操作，数据只会增加，通过增量同步可以只同步新增部分，提高效率。</li>
</ul>
</li>
</ol>
<p><strong>综合考虑：</strong></p>
<p>在实际操作中，往往需要根据表的特点、业务需求、数据库性能、网络带宽等多个方面综合考虑，决定使用全量同步还是增量同步。对于大型且关键的业务系统，通常会选择增量同步来降低资源消耗并提高数据一致性。而对于一些变动不大的表，初次同步时可以进行全量同步，后续再根据需求进行增量更新。</p>
<h3 id="23-⭐-有一些表进行逻辑删除，物理删除，修改和新增这四种操作，这种表是否可以进行增量同步？"><a href="#23-⭐-有一些表进行逻辑删除，物理删除，修改和新增这四种操作，这种表是否可以进行增量同步？" class="headerlink" title="23.+⭐ 有一些表进行逻辑删除，物理删除，修改和新增这四种操作，这种表是否可以进行增量同步？"></a>23.+⭐ 有一些表进行逻辑删除，物理删除，修改和新增这四种操作，这种表是否可以进行增量同步？</h3><p>可以进行增量同步，但需要考虑如何准确捕捉和处理这四种操作（逻辑删除、物理删除、修改、新增）。以下是实现增量同步的几种策略：</p>
<p>（1） <strong>操作标识字段</strong></p>
<ul>
<li><strong>添加操作类型字段</strong>: 在表中添加一个字段（例如 <code>operation_type</code>），用来标识每一行的操作类型（如 <code>INSERT</code>、<code>UPDATE</code>、<code>LOGICAL_DELETE</code>、<code>PHYSICAL_DELETE</code>）。在数据同步时，根据这个字段判断操作类型并进行相应的处理。</li>
<li><strong>时间戳字段</strong>: 记录每条记录的最后一次修改时间（如 <code>updated_at</code>），并基于此时间戳来进行增量同步。例如，只同步自上次同步时间以来有变化的记录。</li>
</ul>
<p>（2）<strong>逻辑删除的处理</strong></p>
<ul>
<li><strong>标记逻辑删除</strong>: 对于逻辑删除操作，通常通过在记录中设置一个标志位（如 <code>is_deleted</code>）或状态字段来表示。增量同步时，需要将此字段也同步到目标系统，并在目标系统中实现相应的处理逻辑（如在目标系统中标记记录为已删除）。</li>
<li><strong>同步逻辑删除的记录</strong>: 逻辑删除并不删除记录本身，因此可以将这些记录正常同步，但需要目标系统知道这是逻辑删除操作，以便进行相应处理。</li>
</ul>
<p>（3） <strong>物理删除的处理</strong></p>
<ul>
<li><strong>删除日志表</strong>: 如果需要增量同步物理删除的记录，可以维护一个删除日志表（<code>delete_log</code>），记录被物理删除记录的主键和删除时间。同步时，可以查找删除日志表，将相应的删除操作同步到目标系统。</li>
<li><strong>数据库触发器</strong>: 使用数据库触发器在物理删除时，将被删除记录的信息插入到一个日志表中，确保同步时能处理这些删除操作。</li>
</ul>
<p>（4） <strong>修改操作的处理</strong></p>
<ul>
<li><strong>版本控制</strong>: 对每条记录进行版本控制，每次修改时递增版本号（如 <code>version</code> 字段）。增量同步时，同步修改后的最新版本，并在目标系统中替换旧版本。</li>
<li><strong>更新字段</strong>: 通过 <code>updated_at</code> 字段来捕捉最近修改的记录，只同步那些在上次同步后被修改的记录。</li>
</ul>
<p>（5）<strong>新增操作的处理</strong></p>
<ul>
<li><strong>直接插入</strong>: 新增的记录可以通过增量同步机制直接插入到目标系统中。通过 <code>created_at</code> 字段可以方便地识别新增的记录。</li>
</ul>
<p>（6） <strong>同步机制选择</strong></p>
<ul>
<li><strong>基于时间戳的增量同步</strong>: 定期检查数据表中 <code>updated_at</code> 或 <code>created_at</code> 字段的变化，只同步自上次同步以来有变化的记录。</li>
<li><strong>基于变更数据捕获 (CDC) 的同步</strong>: 使用 CDC 工具（如 Debezium、GoldenGate）来捕获数据库表的所有变化（插入、更新、删除），然后将这些变化实时同步到目标系统。</li>
<li><strong>批量同步与实时同步结合</strong>: 在大多数情况下，可以通过批量增量同步的方式处理，结合实时同步机制处理高频率的更新或删除操作。</li>
</ul>
<p>（7）<strong>数据一致性检查</strong></p>
<ul>
<li><strong>数据校验</strong>: 定期对源系统和目标系统的数据进行对比和校验，确保同步后的数据与源数据一致，特别是要确保删除操作的同步不遗漏。</li>
<li><strong>数据完整性</strong>: 处理逻辑删除时，要确保在目标系统中相应地标记删除，而不是保留为活跃记录。</li>
</ul>
<p>总结</p>
<p>表中包含逻辑删除、物理删除、修改和新增操作时，可以进行增量同步，但需要设计一个健全的机制来正确捕捉和处理这些操作，确保同步过程中的数据一致性和完整性。通过使用操作标识字段、时间戳、删除日志和CDC工具，可以有效实现这一目标。</p>
<h3 id="24-数据采集过程中，如何保证数据不丢失"><a href="#24-数据采集过程中，如何保证数据不丢失" class="headerlink" title="24. 数据采集过程中，如何保证数据不丢失"></a>24. 数据采集过程中，如何保证数据不丢失</h3><p>在数据仓库（数仓）建设过程中，数据采集是确保数据完整性和可靠性的关键环节。为了保证数据采集过程中数据不丢失，可以采取以下措施：</p>
<p>（1） <strong>数据源的可靠性</strong></p>
<ul>
<li><strong>数据源高可用性设计</strong>: 确保数据源本身是高可用的，避免因数据源的不可用导致数据丢失。</li>
<li><strong>容错机制</strong>: 数据源接口应具备容错能力，能够在出现错误或异常时进行自动重试或报警。</li>
</ul>
<p>（2） <strong>数据传输的保障</strong></p>
<ul>
<li><strong>幂等性设计</strong>: 在数据采集过程中，尤其是在数据传输时，应设计幂等性机制，确保即使某些数据多次采集也不会产生重复或丢失。</li>
<li><strong>传输协议选择</strong>: 选择具有可靠性保障的传输协议，如 HTTPS、TCP，而不是不可靠的协议（如 UDP）。</li>
<li><strong>重试机制</strong>: 在数据传输失败的情况下，配置重试机制，并记录重试日志，以便在多次失败后能够进行人工干预。</li>
<li><strong>数据校验</strong>: 在数据传输过程中，通过校验和（如 MD5、CRC）确保数据未被篡改或丢失。</li>
</ul>
<p>（3） <strong>数据存储的可靠性</strong></p>
<ul>
<li><strong>分布式存储</strong>: 使用分布式存储系统（如 HDFS、Ceph）来存储采集的数据，分布式存储通常具备数据冗余和自动恢复能力。</li>
<li><strong>数据快照和备份</strong>: 定期对数据进行快照和备份，确保在发生数据丢失或损坏时能够及时恢复。</li>
</ul>
<p>（4）<strong>数据采集任务的调度和监控</strong></p>
<ul>
<li><strong>任务调度系统</strong>: 使用可靠的任务调度系统（如 Apache Airflow、Azkaban）来管理数据采集任务，确保任务按时执行，避免遗漏。</li>
<li><strong>任务监控和报警</strong>: 对数据采集任务进行实时监控，配置报警机制。当任务执行失败、延迟或数据量异常时，能够及时报警并进行处理。</li>
</ul>
<p>（5） <strong>数据质量管理</strong></p>
<ul>
<li><strong>数据落地前的校验</strong>: 在数据入库前进行校验，确保采集的数据完整且无误。如果发现异常，进行相应的补救措施，如重新采集或修正错误数据。</li>
<li><strong>数据审计</strong>: 定期对采集的数据进行审计，检查数据的完整性、一致性，并与源数据进行对比，确保没有遗漏或丢失。</li>
</ul>
<p>（6） <strong>数据流中的事务支持</strong></p>
<ul>
<li><strong>事务性操作</strong>: 在可能的情况下，使用事务性操作确保数据在处理过程中要么全部成功，要么全部失败，以避免部分数据处理成功、部分失败导致的数据不一致或丢失。</li>
<li><strong>Kafka 等消息队列的使用</strong>: 使用支持事务的消息队列（如 Kafka）在数据采集过程中进行数据缓冲和传输，确保在数据采集过程中即使出现故障，数据也不会丢失。</li>
</ul>
<p>（7） <strong>异常数据处理</strong></p>
<ul>
<li><strong>异常数据捕获</strong>: 对采集过程中可能出现的异常数据进行捕获和存储，避免数据因格式错误或内容异常而丢失。</li>
<li><strong>数据补偿机制</strong>: 针对采集过程中可能遗漏的数据，设置数据补偿机制，定期或在发现数据丢失时进行重新采集。</li>
</ul>
<p>（8） <strong>日志和审计</strong></p>
<ul>
<li><strong>详细日志记录</strong>: 对数据采集过程中的所有操作进行详细的日志记录，包括成功和失败的采集操作、数据量、时间戳等，以便在数据丢失时能够追溯原因。</li>
<li><strong>审计追踪</strong>: 定期审查和分析数据采集日志，确保数据采集过程的透明性和可追溯性。</li>
</ul>
<p>通过这些措施，可以有效保障数据采集过程的完整性和可靠性，防止数据丢失，确保数仓的数据质量和一致性。</p>
<h3 id="25-如何保证指标建设的规范性？"><a href="#25-如何保证指标建设的规范性？" class="headerlink" title="25. 如何保证指标建设的规范性？"></a>25. 如何保证指标建设的规范性？</h3><img src="image-20240901222048449.png" alt="image-20240901222048449" style="zoom: 50%;">

<p>为了保证数据仓库中指标建设的规范性，可以从以下几个方面入手：</p>
<p>（1） <strong>制定统一的指标定义标准</strong></p>
<ul>
<li><strong>指标词典（指标库）</strong>: 创建一个标准化的指标词典或指标库，明确每个指标的定义、计算逻辑、维度、数据来源等信息。所有指标的定义必须在指标词典中注册并得到审批，确保统一理解和使用。</li>
<li><strong>命名规范</strong>: 规定指标的命名规则，统一指标命名的格式、大小写、缩写使用等，避免出现同一指标不同名称或不同指标名称相似的情况。</li>
<li><strong>元数据管理</strong>: 利用元数据管理工具对指标的所有相关信息进行管理，包括指标的创建者、创建时间、数据来源、使用场景等，确保指标建设的透明性和规范性。</li>
</ul>
<p>（2） <strong>统一的计算逻辑和口径</strong></p>
<ul>
<li><strong>计算标准化</strong>: 确保相同的指标在不同场景下的计算逻辑和口径一致。对于常用指标，可以将其计算逻辑封装成标准函数或视图，供全公司范围内的人员调用。</li>
<li><strong>复用指标</strong>: 在可能的情况下，复用已有指标的计算逻辑，避免重复建设或因个人理解差异导致的指标不一致。</li>
</ul>
<p>（3） <strong>清晰的指标层次架构</strong></p>
<ul>
<li><strong>指标分类</strong>: 将指标分为基础指标（如 PV、UV）、派生指标（如转化率、增长率）、复合指标（如 ROI）等不同层次，清晰地定义每个层次的作用和使用场景。</li>
<li><strong>分层模型设计</strong>: 根据数据仓库的分层模型（如ODS层、DWD层、DWS层、ADS层）进行指标的设计和计算，确保指标的分层合理性和规范性。</li>
</ul>
<p>（4） <strong>版本控制与变更管理</strong></p>
<ul>
<li><strong>指标版本管理</strong>: 对每个指标的定义和计算逻辑进行版本控制，记录每次变更的内容、原因和影响，确保在需要时能够回溯到历史版本。</li>
<li><strong>变更审批流程</strong>: 建立严格的指标变更审批流程，确保任何指标的修改都经过必要的评估和审核，防止随意修改带来的不一致性问题。</li>
</ul>
<p>（5） <strong>审核与评估机制</strong></p>
<ul>
<li><strong>指标评审委员会</strong>: 设立指标评审委员会，定期对新增或修改的指标进行审核，确保其符合公司标准和业务需求。</li>
<li><strong>定期审计与优化</strong>: 对已有指标进行定期审计，检查其定义和计算逻辑的合理性、准确性，并根据业务变化进行适当优化和调整。</li>
</ul>
<p>（6） <strong>自动化工具的使用</strong></p>
<ul>
<li><strong>指标自动化管理工具</strong>: 使用专门的指标管理工具（如 Looker、Tableau 的指标管理模块），自动化管理指标定义、计算逻辑和使用情况，确保全公司范围内的一致性。</li>
<li><strong>监控和报警系统</strong>: 建立指标监控和报警系统，自动检测指标异常波动或计算错误，及时发现并纠正问题。</li>
</ul>
<p>（7） <strong>培训与文档</strong></p>
<ul>
<li><strong>指标使用培训</strong>: 定期对业务人员和数据分析师进行培训，讲解指标的定义、使用方法、注意事项等，确保所有相关人员都能够正确理解和使用指标。</li>
<li><strong>文档完善</strong>: 详细记录指标的定义、计算逻辑、使用场景等信息，形成完整的文档体系，供团队成员查阅。</li>
</ul>
<p>（8） <strong>数据治理框架</strong></p>
<ul>
<li><strong>数据治理委员会</strong>: 成立数据治理委员会，监督和管理整个数据仓库的建设，包括指标的规范性建设，确保数据的统一性和一致性。</li>
<li><strong>数据治理政策</strong>: 制定数据治理政策，明确数据管理和使用的规范，包括指标建设的原则和流程。</li>
</ul>
<p>通过以上方法，可以有效保证数据仓库中指标建设的规范性，减少因指标定义不一致或计算逻辑错误导致的数据质量问题，确保数据分析和业务决策的准确性和可靠性。</p>
<h2 id="八、DataX"><a href="#八、DataX" class="headerlink" title="八、DataX"></a>八、DataX</h2><h3 id="1-什么是DataX？"><a href="#1-什么是DataX？" class="headerlink" title="1. 什么是DataX？"></a>1. 什么是DataX？</h3><p>阿里巴巴开源的一个异构数据源离线同步工具，支持的Source和Sink要更多一些</p>
<h3 id="2-DataX框架设计"><a href="#2-DataX框架设计" class="headerlink" title="2. DataX框架设计"></a>2. DataX框架设计</h3><p>Framework + plugin架构构建</p>
<img src="Snipaste_2023-11-12_19-26-50.png" alt="Snipaste_2023-11-12_19-26-50" style="zoom:50%;">

<p>Framework处理缓冲、流程控制、并发、上下文加载等高速数据交换的大部分技术问题，并提供简单的接口与插件接入。</p>
<p>数据在DataX中以中间状态存在，并且在目标数据系统中将中间状态的数据转换为对应的数据格式后写入</p>
<h3 id="3-DataX调优"><a href="#3-DataX调优" class="headerlink" title="3. DataX调优"></a>3. DataX调优</h3><p>调整速度</p>
<p>调整内存</p>
<h3 id="4-⭐在使用DataX的时候，是否遇到问题？"><a href="#4-⭐在使用DataX的时候，是否遇到问题？" class="headerlink" title="4. ⭐在使用DataX的时候，是否遇到问题？"></a>4. ⭐在使用DataX的时候，是否遇到问题？</h3><p>遇到过。</p>
<p>MySql中的Null对应Hive中的\N，但是dataX不能自动转换成\N，只能在Hive建表的时候执行null值存储格式为空字符串（’’）</p>
<h3 id="5-DataX配置文件生成脚本"><a href="#5-DataX配置文件生成脚本" class="headerlink" title="5. DataX配置文件生成脚本"></a>5. DataX配置文件生成脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python gen_import_config.py -d database -t table</span><br></pre></td></tr></table></figure>

<h3 id="6-数据同步的三种方式"><a href="#6-数据同步的三种方式" class="headerlink" title="6. 数据同步的三种方式"></a>6. 数据同步的三种方式</h3><p>（1）直连同步</p>
<p>通过定义好的规范接口API和基于动态链接库的方式直接连接业务库，容易实现，但是当大批量数据同步时会降低甚至拖垮业务系统的性能，不太适合从业务系统到数据仓库系统的同步。</p>
<p>（2）数据文件同步</p>
<p>数据文件同步通过约定好的文件编码、大小、格式等，直接从源系统生成数据的文本文件，由专门的文件服务器，如FTP服务器传输到目标系统后，加载到目标数据库系统中。</p>
<p>（3）数据库日志解析同步</p>
<p>变更数据抓取软件。它会实时监控MySQL数据库中的变更操作（insert，update，delete），并将变更数据以JSON字符串发送给Kafka等流数据处理平台</p>
<h2 id="九、MaxWell"><a href="#九、MaxWell" class="headerlink" title="九、MaxWell"></a>九、MaxWell</h2><h3 id="1-什么是MaxWell，为什么选择MaxWell？"><a href="#1-什么是MaxWell，为什么选择MaxWell？" class="headerlink" title="1.什么是MaxWell，为什么选择MaxWell？"></a>1.什么是MaxWell，为什么选择MaxWell？</h3><p>MySQL变更数据抓取软件。它会实时监控MySQL数据库中的变更操作（insert，update，delete），并将变更数据以JSON字符串发送给Kafka等流数据处理平台。</p>
<p>为什么选：断点续传，自动根据库名和表名把数据发往kafka的对应主题。</p>
<h3 id="2-⭐MaxWell底层原理"><a href="#2-⭐MaxWell底层原理" class="headerlink" title="2. ⭐MaxWell底层原理"></a>2. ⭐MaxWell底层原理</h3><p><strong>mysql主从复制</strong>。Maxwell原理就是将自己伪装成slave，实时读取MySQL数据库的二进制日志（Binlog），从master同步数据。</p>
<h2 id="十、DolphinScheduler"><a href="#十、DolphinScheduler" class="headerlink" title="十、DolphinScheduler"></a>十、DolphinScheduler</h2><h3 id="1-你使用的版本"><a href="#1-你使用的版本" class="headerlink" title="1. 你使用的版本"></a>1. 你使用的版本</h3><p>2.0.3支持的报警信息更全一些，配置更容易。102配置master、worker，103配置worker，104配置worker</p>
<h3 id="2-什么时候执行？"><a href="#2-什么时候执行？" class="headerlink" title="2. 什么时候执行？"></a>2. 什么时候执行？</h3><p>00:10  业务数据</p>
<p>00:30   用户行为数据</p>
<h3 id="3-出现异常怎么办？"><a href="#3-出现异常怎么办？" class="headerlink" title="3. 出现异常怎么办？"></a>3. 出现异常怎么办？</h3><p>发邮件，打电话，查看任务，手动重试</p>
<h2 id="十一、Spark"><a href="#十一、Spark" class="headerlink" title="十一、Spark"></a>十一、Spark</h2><h3 id="0-一句话介绍Spark"><a href="#0-一句话介绍Spark" class="headerlink" title="0. 一句话介绍Spark"></a>0. 一句话介绍Spark</h3><p><img src="image-20240828155146635.png" alt="image-20240828155146635"></p>
<h3 id="1-⭐Spark解决了什么问题？和hadoop的MR对比一下"><a href="#1-⭐Spark解决了什么问题？和hadoop的MR对比一下" class="headerlink" title="1. ⭐Spark解决了什么问题？和hadoop的MR对比一下"></a>1. ⭐Spark解决了什么问题？和hadoop的MR对比一下</h3><img src="image-20240821002453757.png" alt="image-20240821002453757" style="zoom:50%;">

<p>Hadoop主要解决的是海量数据的存储和海量数据的分析计算，Spark主要解决海量数据的分析计算。Spark是基于MR开发的，与Hadoop相比，Spark实现了基于内存的数据流转，但是更快，当资源紧张的时候，Spark程序可能跑不起来，而使用MR虽然慢，但是最终可以跑完。</p>
<p>Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流，计算的中间结果是存储在内存中的。</p>
<table>
<thead>
<tr>
<th></th>
<th>Spark</th>
<th>MapReduce</th>
</tr>
</thead>
<tbody><tr>
<td>数据模型</td>
<td>RDD</td>
<td>抽象的&lt;K,V&gt;格式</td>
</tr>
<tr>
<td>计算逻辑</td>
<td>DAG有向无环图，可以迭代计算</td>
<td>一次计算</td>
</tr>
<tr>
<td>数据依赖关系</td>
<td>窄依赖、宽依赖</td>
<td>固定map-shuffle-reduce，只有ShuffleDependency</td>
</tr>
<tr>
<td>中间数据</td>
<td>内存（有一套内存管理系统）</td>
<td>磁盘，重复读写hdfs</td>
</tr>
<tr>
<td>底层操作</td>
<td>转换算子和行动算子，操作更丰富</td>
<td>只有map和reduce，表达能力欠缺</td>
</tr>
<tr>
<td>适合场景</td>
<td>低时延，迭代时计算</td>
<td>高时延</td>
</tr>
<tr>
<td>任务完成度</td>
<td>可能由于OOM内存溢出，导致程序无法运行</td>
<td>虽然运行缓慢，但至少可以慢慢运行完</td>
</tr>
</tbody></table>
<h3 id="2-⭐Spark由哪些模块？"><a href="#2-⭐Spark由哪些模块？" class="headerlink" title="2. ⭐Spark由哪些模块？"></a>2. ⭐Spark由哪些模块？</h3><p><img src="Snipaste_2024-07-29_09-43-46-172545400944717.png" alt="Snipaste_2024-07-29_09-43-46"></p>
<p><strong>Spark Core</strong>：实现了Spark的基本功能，包含<strong>任务调度、内存管理、错误恢复、与存储系统交互</strong>等模块。Spark Core中还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称<strong>RDD</strong>)的API定义。 </p>
<p><strong>Spark SQL</strong>：是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用 SQL或者Apache Hive版本的HQL来查询数据。Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等。</p>
<p><strong>Spark Streaming</strong>：是Spark提供的对<strong>实时数据</strong>进行流式计算的组件。提供了用来操作数据流的API，并且与Spark Core中的 RDD API高度对应。 </p>
<p><strong>Spark MLlib</strong>：提供常见的机器学习功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。 </p>
<p><strong>Spark GraphX</strong>：主要用于图形并行计算和图挖掘系统的组件。</p>
<p><strong>集群管理器</strong>：Spark设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器（Cluster Manager）上运行，包括Hadoop YARN、Apache Mesos，以及Spark自带的一个简易调度器，叫作<strong>独立调度器。</strong></p>
<h3 id="3-Spark的运行（部署）模式"><a href="#3-Spark的运行（部署）模式" class="headerlink" title="3. Spark的运行（部署）模式"></a>3. Spark的运行（部署）模式</h3><p>（1）Local模式：在本地部署单个Spark服务</p>
<p>（2）Standalone模式：使用Spark自带的独立调度器进行部署</p>
<p>（3）<strong>Yarn模式</strong>：使用Hadoop的YARN组件进行资源与任务调度（国内最常用）</p>
<p>（4）Mesos：使用Mesos平台进行资源与任务调度</p>
<h3 id="4-⭐简述YarnCluster模式作业提交流程及后续的运行流程"><a href="#4-⭐简述YarnCluster模式作业提交流程及后续的运行流程" class="headerlink" title="4. ⭐简述YarnCluster模式作业提交流程及后续的运行流程"></a>4. ⭐简述YarnCluster模式作业提交流程及后续的运行流程</h3><p><strong>注意：Yarn模式分为两种：yarn-client模式和yarn-cluster模式，区别是，前者的Driver程序运行在客户端，在输出日志中可以找到计算结果；后者的运行程序运行在由RM启动的NM上的APPMaster上，在输出日志中不会展示计算结果。</strong></p>
<p><img src="Snipaste_2024-07-29_14-02-59-172545400944718.png" alt="Snipaste_2024-07-29_14-02-59"></p>
<p>（1）脚本启动–master yarn –deploy-mode cluster，sparksubmit解析参数（–class类，–master模式），并创建客户端yarnclient，该客户端提交任务信息给<strong>RM</strong></p>
<p>（2）<strong>RM</strong>选择一台<strong>NM</strong>，在<strong>NM</strong>中启动<strong>APPMaster</strong></p>
<p>（3）<strong>APPMaster</strong>根据参数，启动<strong>Driver线程</strong>并初始化SparkContext</p>
<p>（4）<strong>APPMaster</strong>向<strong>RM</strong>申请计算资源，<strong>RM</strong>给其返回资源可用列表</p>
<p>（5）<strong>APPMaster</strong>向另外一台<strong>NM</strong>发送启动ExecutorBackend命令，ExectorBackend启动后通过通信模块向<strong>APPMaster</strong>请求注册Executor</p>
<p>（6）注册成功后，ExectorBackend创建<strong>Exector计算对象（进程）</strong></p>
<blockquote>
<p>以下步骤为作业运行过程</p>
</blockquote>
<p>（7）SpackContext构建DAG有向无环图，将DAG分解成Stage</p>
<p>（8）把Stage发送给TaskScheduler</p>
<p>（9）Excutor向SparkContext申请Task，TaskScheduler将Task发送给Excutor运行，同时SparkContext将应用程序代码发送给Executor</p>
<h3 id="5-Spark提交作业时的几个重要参数"><a href="#5-Spark提交作业时的几个重要参数" class="headerlink" title="5. Spark提交作业时的几个重要参数"></a>5. Spark提交作业时的几个重要参数</h3><p>executor-cores —— 每个executor使用的内核数，默认为1，官方建议2-5个，我们企业是4个</p>
<p>num-executors —— 启动executors的数量，默认为2</p>
<p>executor-memory —— executor内存大小，默认1G</p>
<p>driver-cores —— driver使用内核数，默认为1</p>
<p>driver-memory —— driver内存大小，默认512M</p>
<h3 id="6-Spark有哪些组件？"><a href="#6-Spark有哪些组件？" class="headerlink" title="6. Spark有哪些组件？"></a>6. Spark有哪些组件？</h3><ul>
<li>Master（在YARN中是一个NM）：管理集群和节点，不参与计算</li>
<li>Worker（在YARN中是另一个NM）：计算节点，启动Executor来执行具体的Spark任务，还负责与Master通信</li>
<li>Driver：运行程序的main方法，创建Sparkcontext对象</li>
<li>Executor：Spark执行器，负责执行具体的计算</li>
<li>Task：Spark应用的计算任务，是Spark中最小的计算单位，不能再拆分。task以线程方式运行在Executor进程中，执行具体的计算任务</li>
</ul>
<h3 id="7-⭐什么是RDD？DataFrame和DataSet？"><a href="#7-⭐什么是RDD？DataFrame和DataSet？" class="headerlink" title="7. ⭐什么是RDD？DataFrame和DataSet？"></a>7. ⭐什么是RDD？DataFrame和DataSet？</h3><p>RDD为弹性分布式数据集，是Spark中最基本的数据抽象，代表一个弹性的、不可变、可分区、里面元素可并行计算的集合，将输入、输出、中间数据抽象表示为统一的数据模型，每个输入输出、中间数据都可以是一个具体的实例化的RDD。RDD只是一个逻辑概念，在内存中不会真正地为某个RDD分配存储空间（除非该RDD需要被缓存）。</p>
<p>RDD可以包含多个数据分区，不同数据分区可以由不同的任务（task）在不同的节点进行处理。</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>区别</th>
<th>应用场景</th>
</tr>
</thead>
<tbody><tr>
<td>RDD（Resilient Distributed Dataset）</td>
<td>是 Spark 最底层的数据抽象，不支持 Schema，操作比较底层和灵活，但效率相对较低。</td>
<td>适用于需要进行底层数据操作和自定义转换逻辑的场景，例如复杂的数据清洗和预处理。</td>
</tr>
<tr>
<td>DataFrame</td>
<td>具有结构化的 Schema 信息，类似于关系型数据库中的表。操作更丰富，基于优化的执行引擎，性能较好。</td>
<td>适合处理结构化数据，进行数据分析和探索，以及与 SQL 结合使用。</td>
</tr>
<tr>
<td>Dataset</td>
<td>是 DataFrame 的扩展，在 Java 和 Scala 中具有强类型检查，性能更优。</td>
<td>对类型安全和性能要求极高的场景，尤其是在大型企业级应用中。</td>
</tr>
</tbody></table>
<p>例如，在处理大规模的日志数据时，如果需要进行复杂的自定义转换和处理逻辑，可能会首先使用 RDD。而对于常见的结构化数据分析任务，如报表生成和数据探索，DataFrame 会更方便。在一些对性能和类型安全有严格要求的企业级应用中，Dataset 则能发挥更好的作用。</p>
<p><strong>三者的共同性</strong>：</p>
<p>（1）RDD、DataFrame、DataSet全都是Spark平台下的分布式弹性数据集，为处理超大型数据提供便利。</p>
<p>（2）三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action行动算子如foreach时，三者才会开始遍历运算。</p>
<p>（3）三者有许多共同的函数，如filter，排序等。</p>
<p>（4）三者都会根据Spark的内存情况自动缓存运算。</p>
<p>（5）三者都有分区的概念。</p>
<h3 id="8-RDD的创建方法"><a href="#8-RDD的创建方法" class="headerlink" title="8. RDD的创建方法"></a>8. RDD的创建方法</h3><p>（1）从集合中创建</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//单值</span></span><br><span class="line">List&lt;String&gt; names = Arrays.asList(<span class="string">&quot;zhangsan&quot;</span>, <span class="string">&quot;lisi&quot;</span>, <span class="string">&quot;wangwu&quot;</span>);</span><br><span class="line">JavaRDD&lt;String&gt; rdd = jsc.parallelize(names);</span><br><span class="line"></span><br><span class="line"><span class="comment">//kv对</span></span><br><span class="line">Tuple2&lt;String, Integer&gt; a = <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>);</span><br><span class="line">Tuple2&lt;String, Integer&gt; b = <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>);</span><br><span class="line">Tuple2&lt;String, Integer&gt; c = <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;c&quot;</span>, <span class="number">3</span>);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd = jsc.parallelizePairs(Arrays.asList(a, b, c));</span><br></pre></td></tr></table></figure>

<p>（2）从外部存储系统中创建</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; rdd = jsc.textFile(<span class="string">&quot;D:\\idea-maven-space\\SparkCore\\src\\main\\data\\test.txt&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>（3）从其他RDD创建</p>
<h3 id="9-RDD如何设置分区数？以及常用的分区方法？"><a href="#9-RDD如何设置分区数？以及常用的分区方法？" class="headerlink" title="9. RDD如何设置分区数？以及常用的分区方法？"></a>9. RDD如何设置分区数？以及常用的分区方法？</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//从集合中创建RDD</span></span><br><span class="line">List&lt;String&gt; names = Arrays.asList(<span class="string">&quot;zhangsan&quot;</span>, <span class="string">&quot;lisi&quot;</span>, <span class="string">&quot;wangwu&quot;</span>);</span><br><span class="line">JavaRDD&lt;String&gt; rdd = jsc.parallelize(names,<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//从文件中创建RDD</span></span><br><span class="line">JavaRDD&lt;String&gt; rdd = jsc.textFile(<span class="string">&quot;src/main/data/test.txt&quot;</span>,<span class="number">2</span>);</span><br></pre></td></tr></table></figure>

<p><strong>注意：单值类型的RDD没有分区器partitioner，只有key-value类型的RDD才有分区器partitioner</strong></p>
<p>spark中常见的数据分区器方法（partitioner）：</p>
<ul>
<li>水平划分：按照记录的索引进行划分</li>
<li>Hash划分（HashPartitioner）：使用记录的Hash值来对数据进行划分</li>
<li>Range划分（RangePartitioner）：用于排序任务，按照元素的大小关系将其划分到不同分区，尽量保证每个分区中数量均匀，分区间有序，分区内无序。</li>
</ul>
<h3 id="⭐spark的并行度设置"><a href="#⭐spark的并行度设置" class="headerlink" title="⭐spark的并行度设置"></a>⭐spark的并行度设置</h3><p>最后一个RDD的分区个数就是（决定）task个数，各个stage的task的个数就是（决定）spark作业在各个stage的并行度</p>
<img src="image-20240821082506366.png" alt="image-20240821082506366" style="zoom:50%;">

<h3 id="10-⭐Spark中有哪些算子，两种算子之间的区别"><a href="#10-⭐Spark中有哪些算子，两种算子之间的区别" class="headerlink" title="10. ⭐Spark中有哪些算子，两种算子之间的区别"></a>10. ⭐Spark中有哪些算子，两种算子之间的区别</h3><p>两种，transformation转换算子和action行动算子</p>
<ul>
<li>transformation转换算子可以不断生成新的RDD（RDD本身不可变，所以生成新的RDD），转换算子都是懒加载，并不会立即执行。</li>
<li>action行动算子一般是对数据结果进行后处理，产生输出结果，而且会触发Spark提交job真正执行数据处理任务</li>
</ul>
<p>区分两种算子最直观的方法就是看返回值，transformation()操作一般返回RDD类型，而action()操作一般返回数值、数据结构（如Map）或者不返回任何值（如写磁盘）</p>
<h3 id="11-⭐说一说Spark的Transformation转换算子，及其功能"><a href="#11-⭐说一说Spark的Transformation转换算子，及其功能" class="headerlink" title="11. ⭐说一说Spark的Transformation转换算子，及其功能"></a>11. ⭐说一说Spark的Transformation转换算子，及其功能</h3><h4 id="（1）单Value"><a href="#（1）单Value" class="headerlink" title="（1）单Value"></a>（1）<strong>单Value</strong></h4><ul>
<li>**map()**：映射，使用func对rdd1中的每个记录进行处理，输出一个新记录。实际上map可以单值-》单值，单值-》KV对，KV对-》KV对，KV对-》单值</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd1 = jsc.parallelize(Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>));</span><br><span class="line">JavaRDD&lt;Integer&gt; rdd2 = rdd1.map(v1 -&gt; v1 * <span class="number">2</span>);</span><br><span class="line">rdd2.collect().forEach(System.out::println);<span class="comment">//2，4，6，8</span></span><br></pre></td></tr></table></figure>

<img src="map-172545400944719.png" alt="map" style="zoom: 50%;">

<ul>
<li>**mapPartitions(func)**：对RDD中的每个分区进行fubc操作，输出新的一组数据。每次处理一个分区的数据。函数的参数是一个迭代器，返回值也是另一个迭代器。</li>
</ul>
<img src="mappartitions-172545400944720.png" alt="mappartitions" style="zoom:50%;">

<ul>
<li>**filter()**：过滤，接收一个返回值为布尔类型的函数作为参数，如果func为true，则该元素会被添加到新的RDD中</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd1 = jsc.parallelize(Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>));</span><br><span class="line">JavaRDD&lt;Integer&gt; rdd2 = rdd1.filter(v1 -&gt; v1 % <span class="number">2</span> == <span class="number">1</span>);</span><br><span class="line">rdd2.collect().forEach(System.out::println);<span class="comment">//1，3</span></span><br></pre></td></tr></table></figure>

<img src="filter-172240229161914.png" alt="filter" style="zoom:50%;">

<ul>
<li>**flatMap()**：扁平化，对rdd1中的每一个元素（List）执行func操作，然后将所有新元素组合得到rdd2（降维，扁平化）</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">List&lt;List&lt;Integer&gt;&gt; data = Arrays.asList(</span><br><span class="line">                Arrays.asList(<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">                Arrays.asList(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">);</span><br><span class="line">JavaRDD&lt;List&lt;Integer&gt;&gt; rdd = jsc.parallelize(data);</span><br><span class="line">JavaRDD&lt;Integer&gt; flatMapRDD2 = rdd.flatMap(</span><br><span class="line">     (list) -&gt; list.iterator()<span class="comment">//flatMap方法最后一定要返回一个集合的迭代器</span></span><br><span class="line">);</span><br><span class="line"> flatMapRDD2.collect().forEach(System.out::println);<span class="comment">//1 2 3 4</span></span><br></pre></td></tr></table></figure>

<img src="flatMap-172545400944723.png" alt="flatMap" style="zoom:50%;">

<ul>
<li>**groupBy()**：分组，按照传入函数的返回值进行分组。将每一条数据增加一个标记（返回值），相同标记的数据会放置一个组中（将相同的key对应的值放入一个迭代器），这个标记其实就是组名。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>);</span><br><span class="line">JavaPairRDD&lt;Boolean, Iterable&lt;Integer&gt;&gt; groupByRDD = rdd.groupBy(</span><br><span class="line">        v1 -&gt; v1 % <span class="number">2</span> == <span class="number">0</span><span class="comment">//按照v1 % 2 == 0将数据进行分组</span></span><br><span class="line">);<span class="comment">//得到的结果是一个KV键值对的RDD</span></span><br><span class="line">groupByRDD.collect().forEach(System.out::println);</span><br><span class="line"><span class="comment">//(false,[1,3])</span></span><br><span class="line"><span class="comment">//(true,[2,4])</span></span><br></pre></td></tr></table></figure>

<ul>
<li>**distinct()**：去重，将去重后的元素放到新的RDD中</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), <span class="number">3</span>);</span><br><span class="line">rdd.distinct().collect().forEach(System.out::println);<span class="comment">//1 2</span></span><br></pre></td></tr></table></figure>

<img src="distinct-172545400944848.png" alt="distinct" style="zoom:50%;">

<ul>
<li>**sortBy(func)**：排序，在排序之前，可以将数据通过f函数进行处理，之后按照func函数处理的结果进行排序，默认为正序排列。排序后新产生的RDD的分区数与原RDD的分区数一致。Spark的排序结果是全局有序。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">33</span>, <span class="number">11</span>), <span class="number">2</span>);</span><br><span class="line"><span class="comment">//按照字符串规则排序</span></span><br><span class="line">rdd.sortBy(v1 -&gt; <span class="string">&quot;&quot;</span> + v1,<span class="literal">true</span>,<span class="number">2</span>).collect().forEach(System.out::println);<span class="comment">//1 11 2 3 33 4</span></span><br></pre></td></tr></table></figure>

<img src="sortBy-172545400944721.png" alt="sortBy" style="zoom:50%;">

<ul>
<li>**coalesce()**：合并（缩减）分区</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>), <span class="number">2</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; coalesceRDD = filterRDD.coalesce(<span class="number">1</span>);<span class="comment">//减少分区个数，可能产生数据倾斜</span></span><br><span class="line">JavaRDD&lt;Integer&gt; coalesceRDD1 = filterRDD.coalesce(<span class="number">1</span>,<span class="literal">true</span>);<span class="comment">//使用shuffle来减少分区个数，可以解释数据倾斜</span></span><br><span class="line">JavaRDD&lt;Integer&gt; coalesceRDD2 = filterRDD.coalesce(<span class="number">3</span>, <span class="literal">true</span>);<span class="comment">//使用shuffle来增加分区个数</span></span><br></pre></td></tr></table></figure>

<img src="coalesce-172545400944724.png" alt="coalesce" style="zoom:67%;">

<ul>
<li>**repartition()**：重分区，repartition方法其实就是设定shuffle为true的coalesce方法</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>), <span class="number">2</span>);<span class="comment">//[1,3,5] [2,4,6]</span></span><br><span class="line">filterRDD.repartition(<span class="number">3</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>**glom()**：将rdd1中每个分区的记录合并到一个List中</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; list = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD = jsc.parallelize(list,<span class="number">2</span>);</span><br><span class="line">listRDD.glom().collect().forEach(System.out::println);</span><br><span class="line"><span class="comment">//[1, 2]</span></span><br><span class="line"><span class="comment">//[3, 4, 5]</span></span><br></pre></td></tr></table></figure>

<img src="glom-172545400944722.png" alt="glom" style="zoom: 33%;">

<h4 id="（2）双vlaue"><a href="#（2）双vlaue" class="headerlink" title="（2）双vlaue"></a>（2）<strong>双vlaue</strong></h4><ul>
<li>**intersection(rdd)**：求交集，将rdd1和rdd2的共同元素抽取出来，形成新的rdd3</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; list = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">List&lt;Integer&gt; list1 = Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD = jsc.parallelize(list);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD1 = jsc.parallelize(list1);</span><br><span class="line">JavaRDD&lt;Integer&gt; rdd = listRDD.intersection(listRDD1);</span><br><span class="line">rdd.collect().forEach(System.out::println);<span class="comment">//1 2</span></span><br></pre></td></tr></table></figure>

<img src="intersection-172545400944725.png" alt="intersection" style="zoom:50%;">

<ul>
<li>**union(rdd)**：将rdd1和rdd2中的元素合并在一起得到rdd3</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; list = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">List&lt;Integer&gt; list1 = Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD = jsc.parallelize(list);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD1 = jsc.parallelize(list1);</span><br><span class="line">listRDD.union(listRDD1).collect().forEach(System.out::print);<span class="comment">//1234512789</span></span><br></pre></td></tr></table></figure>

<img src="union-172545400944726.png" alt="union" style="zoom:50%;">

<ul>
<li>**zip(rdd)**：将rdd1和rdd2中的元素按照一一对应的关系组合成kv对，K来自rdd1，V来自rdd2，要求两个rdd分区数相同且每个分区包含的元素个数相同</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; list = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">List&lt;Integer&gt; list1 = Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD = jsc.parallelize(list,<span class="number">3</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD1 = jsc.parallelize(list1,<span class="number">3</span>);</span><br><span class="line">listRDD.zip(listRDD1).collect().forEach(System.out::print);<span class="comment">//(1,1)(2,2)(3,7)(4,8)(5,9)</span></span><br></pre></td></tr></table></figure>

<img src="zip-172545400944828.png" alt="zip" style="zoom:50%;">

<ul>
<li>**subtract(rdd)**：计算在rdd1中而不在rdd2中的记录</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; list = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">List&lt;Integer&gt; list1 = Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD = jsc.parallelize(list,<span class="number">3</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD1 = jsc.parallelize(list1,<span class="number">3</span>);</span><br><span class="line">listRDD.subtract(listRDD1).collect().forEach(System.out::print);<span class="comment">//345</span></span><br></pre></td></tr></table></figure>

<img src="subtract-172545400944727.png" alt="subtract" style="zoom:50%;">

<h4 id="（3）Key-Value"><a href="#（3）Key-Value" class="headerlink" title="（3）Key-Value"></a>（3）<strong>Key-Value</strong></h4><ul>
<li>**mapValues()**：只对值映射，使用func对rdd1中的每个记录的值进行处理，输出一个新记录</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Tuple2&lt;String, Integer&gt; a = <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>);</span><br><span class="line">Tuple2&lt;String, Integer&gt; b = <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>);</span><br><span class="line">Tuple2&lt;String, Integer&gt; c = <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;c&quot;</span>, <span class="number">3</span>);</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd = jsc.parallelizePairs(Arrays.asList(a, b, c));</span><br><span class="line">rdd.mapValues(v1 -&gt; v1 * <span class="number">2</span>).collect().forEach(System.out::println);</span><br><span class="line"><span class="comment">//(a,2)</span></span><br><span class="line"><span class="comment">//(b,4)</span></span><br><span class="line"><span class="comment">//(c,6)</span></span><br></pre></td></tr></table></figure>

<img src="mapvalues-172545400944829.png" alt="mapvalues" style="zoom:50%;">

<ul>
<li>**groupByKey()**：对每个key进行分组聚合（这种聚合只是聚集在一起，不进行融合计算），类似SQL中Groupby子句，一般会有重分区（默认Hash划分），所以会发生shuffle。【注意：分组和分区不一样，一个分区中可能有多个分组，同一个分组只能在同一个分区中】</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd = jsc.parallelizePairs(Arrays.asList(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">3</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)</span><br><span class="line">));</span><br><span class="line">rdd.groupByKey().collect().forEach(System.out::println);</span><br><span class="line"><span class="comment">//(a,[1, 3])</span></span><br><span class="line"><span class="comment">//(b,[2, 4])</span></span><br></pre></td></tr></table></figure>

<img src="groupBy-172545400944830.png" alt="groupBy" style="zoom:50%;">

<ul>
<li>**reduceByKey(func)**：该操作可以将RDD[K,V]中的元素按照相同的K对V进行聚合。其存在多种重载形式，还可以设置新RDD的分区数。相比于groupByKey，reduceByKey可以在Shuffle之前使用func对数据进行预聚合（类似于MR中的map-side聚合），减少了数据传输量和内存用量，效率较高。在Shuffle之前进行一个本地的combine()预聚合操作（分区内），在Shuffle后再进行reduce()聚合（分区间），这两次聚合使用的逻辑相同都是func。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//TODO 将分组聚合功能进行简化-reduceByKey</span></span><br><span class="line"><span class="comment">//   reduceByKey方法的作用：将KV类型的数据按照 K 对 V 进行reduce（将多个值聚合成一个值）操作</span></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd = jsc.parallelizePairs(Arrays.asList(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">3</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)</span><br><span class="line">));</span><br><span class="line">rdd.reduceByKey(</span><br><span class="line">        (v1,v2) -&gt; v1 + v2</span><br><span class="line">).collect().forEach(System.out::println);</span><br></pre></td></tr></table></figure>

<img src="reduceByKey-172545400944831.png" alt="reduceByKey" style="zoom:50%;">

<ul>
<li>**aggregateByKey(zeroValue,seqOp,comOp)**：一个通用的聚合操作，可以看作更一般reduceByKey，将combine()和reduce两个函数的计算逻辑分开，可以使用不同的聚合逻辑，combine()使用seqOp将同一个分区中的KV对聚合在一起，而reduce()使用combineOp将经过seqOp聚合后的KV对进一步聚合</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Tuple2&lt;String, Integer&gt;&gt; kv = Arrays.asList(</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;c&quot;</span>, <span class="number">3</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;c&quot;</span>, <span class="number">4</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">5</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">7</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;d&quot;</span>, <span class="number">1</span>)</span><br><span class="line">        );</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; kvRDD = jsc.parallelizePairs(kv,<span class="number">4</span>);</span><br><span class="line"><span class="comment">//第一个参数是初始值，第二个参数是分区数，第三个参数是shuffle之前的combine聚合逻辑，第四个参数是shuffle之后的reduce聚合逻辑</span></span><br><span class="line">kvRDD.aggregateByKey(<span class="string">&quot;x&quot;</span>,<span class="number">2</span>,(v1,v2)-&gt; v1 + <span class="string">&quot;_&quot;</span> + v2,(v1,v2) -&gt; v1 + <span class="string">&quot;@&quot;</span> + v2)</span><br><span class="line">    .collect().forEach(System.out::println);</span><br><span class="line"><span class="comment">//(d,x_1)</span></span><br><span class="line"><span class="comment">//(b,x_2)</span></span><br><span class="line"><span class="comment">//(a,x_1@x_5@x_7)</span></span><br><span class="line"><span class="comment">//(c,x_3@x_4)</span></span><br></pre></td></tr></table></figure>

<img src="aggregateByKey-172545400944832.png" alt="aggregateByKey" style="zoom:50%;">

<ul>
<li>**combineByKey(createCombiner，mergeValue，mergeCombiners，[numPartitions])**：最通用的基础聚合操作，第一个参数是一个初始化函数，第二个参数是shuffle之前的combine聚合逻辑，第三个参数是shuffle之后的reduce聚合逻辑，最后一个参数是分区数。</li>
</ul>
<img src="combineByKey-172545400944833.png" alt="combineByKey" style="zoom:50%;">

<ul>
<li>**foldByKey()**：简化版的aggregateByKey，seqOp和combineOp共用一套逻辑。相比于reduceBykey，foldByKey多了初始值zero Value</li>
</ul>
<img src="Snipaste_2024-07-31_13-11-48-172545400944834.png" alt="Snipaste_2024-07-31_13-11-48" style="zoom:50%;">

<ul>
<li>**sortByKey()**：在一个(K,V)的RDD上调用，返回一个按照key进行排序的(K,V)的RDD</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd = jsc.parallelizePairs(Arrays.asList(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">3</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)</span><br><span class="line">));</span><br><span class="line"></span><br><span class="line"><span class="comment">//TODO sortByKey方法</span></span><br><span class="line"><span class="comment">//     按照K排序，参数true升序，false降序</span></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; sortRDD = rdd.sortByKey();</span><br><span class="line">sortRDD.collect().forEach(System.out::println);</span><br><span class="line"><span class="comment">//(a,1)</span></span><br><span class="line"><span class="comment">//(a,3)</span></span><br><span class="line"><span class="comment">//(b,2)</span></span><br><span class="line"><span class="comment">//(b,4)</span></span><br></pre></td></tr></table></figure>

<img src="sortBuKey-172545400944835.png" alt="sortBuKey" style="zoom: 33%;">

<ul>
<li>**partitionBy()**：使用新的partitioner分区器对rdd重新分区</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd = jsc.parallelizePairs(Arrays.asList(</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">3</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)</span><br><span class="line">        ));</span><br><span class="line">        rdd.partitionBy(<span class="keyword">new</span> <span class="title class_">HashPartitioner</span>(<span class="number">2</span>)).collect().forEach(System.out::print);</span><br></pre></td></tr></table></figure>

<img src="partitionBy-172545400944836.png" alt="partitionBy" style="zoom:50%;">

<ul>
<li>**cogroup(rdd)**：将多个RDD中具有相同key的Value聚合在一起，与groupBy的区别是可以将多个RDD聚合为一个RDD</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd = jsc.parallelizePairs(Arrays.asList(</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">3</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)</span><br><span class="line">));</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd1 = jsc.parallelizePairs(Arrays.asList(</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">9</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">7</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">0</span>)</span><br><span class="line">));</span><br><span class="line">rdd.cogroup(rdd1,<span class="number">2</span>).collect().forEach(System.out::println);</span><br><span class="line"><span class="comment">//(b,([2, 4],[9, 0]))</span></span><br><span class="line"><span class="comment">//(a,([1, 3],[2, 7]))</span></span><br></pre></td></tr></table></figure>

<img src="cogroup-172545400944837.png" alt="cogroup" style="zoom:50%;">

<ul>
<li>**join()**：将两个RDD中的数据关联在一起，与SQL中的join类似</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd = jsc.parallelizePairs(Arrays.asList(</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">2</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;c&quot;</span>, <span class="number">3</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;d&quot;</span>, <span class="number">4</span>)</span><br><span class="line">));</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd1 = jsc.parallelizePairs(Arrays.asList(</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">9</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;c&quot;</span>, <span class="number">7</span>),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;d&quot;</span>, <span class="number">0</span>)</span><br><span class="line">));</span><br><span class="line">rdd.join(rdd1,<span class="number">2</span>).collect().forEach(System.out::println);</span><br><span class="line"><span class="comment">//(d,(4,0))</span></span><br><span class="line"><span class="comment">//(b,(2,9))</span></span><br><span class="line"><span class="comment">//(a,(1,2))</span></span><br><span class="line"><span class="comment">//(c,(3,7))</span></span><br></pre></td></tr></table></figure>

<img src="join-172545400944838.png" alt="join" style="zoom:50%;">

<h4 id="（4）其他"><a href="#（4）其他" class="headerlink" title="（4）其他"></a>（4）其他</h4><ul>
<li>**mapToPair()**：将数据转换为kv的形式，输入的RDD可以是单值形式的（JavaRDD），也可以是KV对形式的（JavaPairRDD）</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将JavaRDD转化为JavaPairRDD</span></span><br><span class="line">List&lt;Integer&gt; list = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD = jsc.parallelize(list,<span class="number">3</span>);</span><br><span class="line">listRDD.mapToPair(v1 -&gt; <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(v1 + <span class="string">&quot;&quot;</span>,v1 * <span class="number">2</span>))</span><br><span class="line">    .collect().forEach(System.out::println);</span><br><span class="line"><span class="comment">//(1,2)</span></span><br><span class="line"><span class="comment">//(2,4)</span></span><br><span class="line"><span class="comment">//(3,6)</span></span><br><span class="line"><span class="comment">//(4,8)</span></span><br><span class="line"><span class="comment">//(5,10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//将JavaPairRDD的k和V进行任意映射变换，也可以颠倒KV的位置（使用map也可以）</span></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd = jsc.parallelizePairs(Arrays.asList(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">4</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">9</span>)</span><br><span class="line">));</span><br><span class="line"><span class="comment">//由于sortByKey只能按照key进行排序，当我们key相同的时候，想要对V进行排序，可以按照如下方法做</span></span><br><span class="line"><span class="comment">//使用mapToPair方法将KV对的键值交换，并且使得输出还是KV格式，在按照key进行排序，再使用mapToPair将KV对的键值进行交换</span></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; rdd1 = rdd.mapToPair(</span><br><span class="line">        kv -&gt; <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(kv._2, kv._1)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">rdd1.collect().forEach(System.out::println);</span><br><span class="line"><span class="comment">//(1,a)</span></span><br><span class="line"><span class="comment">//(4,a)</span></span><br><span class="line"><span class="comment">//(2,a)</span></span><br><span class="line"><span class="comment">//(9,a)</span></span><br></pre></td></tr></table></figure>

<h3 id="12-map和mapPartitions区别"><a href="#12-map和mapPartitions区别" class="headerlink" title="12. map和mapPartitions区别"></a>12. map和mapPartitions区别</h3><p>map：每次处理一条数据；mapPartitions：每次处理一个分区数据</p>
<h3 id="13-Repartition和Coalesce区别"><a href="#13-Repartition和Coalesce区别" class="headerlink" title="13. Repartition和Coalesce区别"></a>13. Repartition和Coalesce区别</h3><p>1）关系：</p>
<p>两者都是用来改变RDD的partition数量的，repartition底层调用的就是coalesce方法：coalesce(numPartitions, shuffle &#x3D; true)</p>
<p>2）区别：</p>
<p><strong>repartition一定会发生shuffle，coalesce根据传入的参数来判断是否发生shuffle</strong></p>
<p>一般情况下增大rdd的partition数量使用repartition，减少partition数量时使用coalesce</p>
<h3 id="14-⭐reduceByKey与groupByKey的区别"><a href="#14-⭐reduceByKey与groupByKey的区别" class="headerlink" title="14. ⭐reduceByKey与groupByKey的区别"></a>14. ⭐reduceByKey与groupByKey的区别</h3><p>groupByKey的分组聚合只是简单地聚在一起，分组内不进行融合计算，并且没有预聚合操作。缺点是Shuffle时会产生大量的中间数据，占用内存大。</p>
<p>reduceByKey的分组聚合在聚合过程中使用func对组内数据进行融合计算，并且在Shuffle之前【shuffle W rite阶段】进行一个本地的combine()预聚合操作，在Shuffle后【Shuffle Read阶段】再进行reduce()聚合，这两次聚合使用的逻辑相同都是func，这样减少了数据传输量和内存用量，效率较高</p>
<h3 id="15-⭐reduceByKey、foldByKey、aggregateByKey、combineByKey区别"><a href="#15-⭐reduceByKey、foldByKey、aggregateByKey、combineByKey区别" class="headerlink" title="15. ⭐reduceByKey、foldByKey、aggregateByKey、combineByKey区别"></a>15. ⭐reduceByKey、foldByKey、aggregateByKey、combineByKey区别</h3><table>
<thead>
<tr>
<th>算子名</th>
<th>初始值情况</th>
<th>两次聚合的逻辑</th>
</tr>
</thead>
<tbody><tr>
<td>ReduceByKey</td>
<td>没有初始值</td>
<td>分区内和分区间聚合逻辑相同</td>
</tr>
<tr>
<td>foldByKey</td>
<td>有初始值</td>
<td>分区内和分区间聚合逻辑相同</td>
</tr>
<tr>
<td>aggregateByKey</td>
<td>有初始值</td>
<td>分区内和分区间聚合逻辑可以不同</td>
</tr>
<tr>
<td>combineByKey</td>
<td>初始化函数</td>
<td>分区内和分区间聚合逻辑可以不同</td>
</tr>
</tbody></table>
<h3 id="16-cogroup与groupByKey的区别"><a href="#16-cogroup与groupByKey的区别" class="headerlink" title="16. cogroup与groupByKey的区别"></a>16. cogroup与groupByKey的区别</h3><p>groupByKey针对<strong>一个 RDD</strong> 中相同的 key 进行合并。而 cogroup 针对<strong>多个 RDD</strong> 中相同的 key 的元素进行合并。而且可能一部分RDD有shuflle，一部分RDD不shuffle（不shuffle就意味着rdd和CoGroupedRDD具有相同的分区器partitioner且分区个数相同）</p>
<h3 id="17-⭐说一说Spark的action算子及其功能"><a href="#17-⭐说一说Spark的action算子及其功能" class="headerlink" title="17. ⭐说一说Spark的action算子及其功能"></a>17. ⭐说一说Spark的action算子及其功能</h3><ul>
<li>**collect()**：以数组的形式返回数据集，将rdd中的记录收集到Driver端</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd1 = jsc.parallelize(Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>));</span><br><span class="line">JavaRDD&lt;Integer&gt; rdd2 = rdd1.map(v1 -&gt; v1 * <span class="number">2</span>);</span><br><span class="line">rdd2.collect().forEach(System.out::println);<span class="comment">//2，4，6，8</span></span><br></pre></td></tr></table></figure>

<ul>
<li>**count()**：返回RDD中元素个数，返回一个Long值</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>);</span><br><span class="line"><span class="comment">//TODO count获取结果数量</span></span><br><span class="line">System.out.println(rdd.count());<span class="comment">//4</span></span><br></pre></td></tr></table></figure>

<img src="count-172545400944839.png" alt="count" style="zoom:50%;">

<ul>
<li>**countByKey()**：统计rdd中每个key出现的次数，返回一个Map</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;String, String&gt; rdd = jsc.parallelizePairs(Arrays.asList(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;@234&quot;</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;233&quot;</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="string">&quot;$%%&amp;^&quot;</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="string">&quot;(*)&amp;&quot;</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;c&quot;</span>, <span class="string">&quot;_+(&quot;</span>)</span><br><span class="line">));</span><br><span class="line"><span class="comment">//TODO countBuKey:将结果按照Key计算数量</span></span><br><span class="line">Map&lt;String, Long&gt; map = rdd.countByKey();</span><br><span class="line">System.out.println(map);</span><br><span class="line"><span class="comment">//&#123;a=2, b=2, c=1&#125;</span></span><br></pre></td></tr></table></figure>

<img src="countBuKey-172545400944840.png" alt="countBuKey" style="zoom:50%;">

<ul>
<li>**first()**：返回RDD中的第一个元素，等价于take(1)</li>
<li>**take(num)**：将rdd中的前num个记录去除，形成一个数组</li>
</ul>
<img src="take-172545400944841.png" alt="take" style="zoom: 67%;">

<ul>
<li>**takeOrdered(num)**：取出rdd中最小的num个，要求rdd中的记录可比较</li>
</ul>
<img src="takeOrdered-172545400944844.png" alt="takeOrdered" style="zoom: 67%;">

<ul>
<li>**top(num)**：取出rdd中最大的num个，要求rdd中的记录可比较</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; list = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD = jsc.parallelize(list,<span class="number">3</span>);</span><br><span class="line">listRDD.take(<span class="number">2</span>).forEach(System.out::println);<span class="comment">//1 2</span></span><br><span class="line">System.out.println(listRDD.first());<span class="comment">//1</span></span><br><span class="line">listRDD.takeOrdered(<span class="number">2</span>).forEach(System.out::println);<span class="comment">//1 2</span></span><br><span class="line">listRDD.top(<span class="number">2</span>).forEach(System.out::println);<span class="comment">//5 4</span></span><br></pre></td></tr></table></figure>

<ul>
<li>**saveAsTextFile()、saveAsObjectFile()、saveAsHadoopFile()、saveAsSequenceFile()**：将rdd保存为文本文件、序列化对象形式的文件、Hadoop HDFS文件等等</li>
<li>**foreach(func)**：遍历RDD中每一个元素，进行func计算后一般会直接输出结果，并不形成新的RDD</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>);</span><br><span class="line"><span class="comment">//此处的foreach是spark中的行动算子，每次处理一个数据，分布式循环，在Executor端循环</span></span><br><span class="line">rdd.foreach(v1 -&gt; System.out.println(v1));<span class="comment">//1 3 2 4</span></span><br></pre></td></tr></table></figure>

<img src="foreach-172545400944846.png" alt="foreach" style="zoom:50%;">

<ul>
<li>**foreachPartition (func)**：遍历RDD中每一个分区，进行func计算后一般直接输出，并不形成新的RDD</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd = jsc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>);</span><br><span class="line"><span class="comment">//foreachPartition ()遍历RDD中每一个分区，每次处理一个分区中的所有数据，执行效率高，但是依托于内存大小</span></span><br><span class="line">rdd.foreachPartition(</span><br><span class="line">        iter -&gt; &#123;</span><br><span class="line">            <span class="keyword">while</span> (iter.hasNext())&#123;</span><br><span class="line">                System.out.print(iter.next());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">);<span class="comment">//3 4      </span></span><br><span class="line">  <span class="comment">//1 2</span></span><br></pre></td></tr></table></figure>

<img src="foreachpartition-172545400944842.png" alt="foreachpartition" style="zoom:50%;">

<ul>
<li>**fold(zeroValue，func)**：将rdd中的记录按照func进行聚合，combine和reduce聚合使用同样的func逻辑，有初始值</li>
</ul>
<img src="fold-172545400944843.png" alt="fold" style="zoom:50%;">

<ul>
<li>**reduce(func)**：将rdd中的记录按照func进行聚合，combine和reduce聚合使用同样的func逻辑</li>
</ul>
<img src="reduce-172545400944847.png" alt="reduce" style="zoom:50%;">

<ul>
<li>**aggregate(zeroValue，seqOp，comOp)**：将rdd中的记录按照func进行聚合，combine聚合使用seqOp逻辑，reduce聚合使用comOp逻辑，有初始值</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; list = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; listRDD = jsc.parallelize(list,<span class="number">3</span>);</span><br><span class="line">System.out.println(listRDD.reduce((v1, v2) -&gt; v1 + v2));<span class="comment">//15</span></span><br></pre></td></tr></table></figure>

<img src="aggregate-172545400944845.png" alt="aggregate" style="zoom:50%;">

<h3 id="18-Kryo序列化"><a href="#18-Kryo序列化" class="headerlink" title="18. Kryo序列化"></a>18. Kryo序列化</h3><p>Java的序列化能够序列化任何的类。但是比较重，序列化后对象的体积也比较大。</p>
<p>Spark出于性能的考虑，Spark2.0开始支持另外一种Kryo序列化机制。Kryo速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化。</p>
<h3 id="19-⭐RDD中血缘的概念（数据依赖关系）"><a href="#19-⭐RDD中血缘的概念（数据依赖关系）" class="headerlink" title="19. ⭐RDD中血缘的概念（数据依赖关系）"></a>19. ⭐RDD中血缘的概念（数据依赖关系）</h3><p>RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。通过toDebugString()查看血缘关系。</p>
<p>RDD中的数据依赖关系，分为窄依赖和宽依赖，通过rdd().dependencies()方法打印依赖关系</p>
<p>（1）<strong>窄依赖（NarrowDependency）【不会产生Shuffle】</strong></p>
<ul>
<li>一对一依赖（OneToOneDependency）：子RDD和父RDD中分区个数相同，并存在一一映射关系</li>
<li>区域依赖（RangeDependency）：表示子RDD和父RDD的分区经过区域化后存在一一映射的关系</li>
<li>多对一依赖：表示子RDD的一个分区同时依赖多个父RDD中的分区</li>
<li>多对多依赖：表示子RDD中一个分区依赖父RDD中的多个分区</li>
</ul>
<p>（2）<strong>宽依赖（ShuffleDependency）【会产生Shuffle】</strong></p>
<p>一对多</p>
<p>宽依赖表示新生成的子RDD中的分区依赖父RDD中的每个分区的一部分。具有宽依赖的算子包括，sortByKey，reduceByKey，groupByKey，join等等</p>
<p><strong>如果父RDD的一个或多个分区中的数据需要全部流入子RDD的某个或多个分区，则是窄依赖；如果父RDD分区中的数据需要一部分流入子RDD的某个分区，另外一部分流入子RDD的另外分区，则是宽依赖。</strong></p>
<h3 id="20-为什么要设计窄依赖和宽依赖？"><a href="#20-为什么要设计窄依赖和宽依赖？" class="headerlink" title="20. 为什么要设计窄依赖和宽依赖？"></a>20. 为什么要设计窄依赖和宽依赖？</h3><ul>
<li>窄依赖的多个分区可以并行计算，互不影响，进行流水线操作，不需要进行shuffle，窄依赖的一个分区的数据如果丢失只需要重新计算对应的数据就可以了。</li>
<li>宽依赖需要进行Shuffle，宽依赖必须等到上一阶段计算完成才能计算下一阶段（根据宽依赖将job划分为执行阶段stage）</li>
</ul>
<h3 id="21-⭐DAG是什么？如何划分DAG的stage？DAG中为什么要划分Stage？"><a href="#21-⭐DAG是什么？如何划分DAG的stage？DAG中为什么要划分Stage？" class="headerlink" title="21. ⭐DAG是什么？如何划分DAG的stage？DAG中为什么要划分Stage？"></a>21. ⭐DAG是什么？如何划分DAG的stage？DAG中为什么要划分Stage？</h3><ul>
<li>DAG：DAG有向无环图，数据转换执行的过程。</li>
<li>如何划分stage：对于窄依赖，partition的转换处理在stage中完成计算，不划分；对于宽依赖，但凡有一个shuffle的存在，就将shuffle前后划分为两个stage</li>
<li>为什么要划分stage：一个复杂的业务逻辑如果有shuffle，说明下一个阶段的计算要依赖上一个阶段的数据，按照shuffle（宽依赖）将一个DAG划分为多个stage，在同一个stage中，会有多个算子形成流水线操作，流水线内的多个平行分区可以并行执行。</li>
</ul>
<h3 id="22-DAG划分stage的算法"><a href="#22-DAG划分stage的算法" class="headerlink" title="22. DAG划分stage的算法"></a>22. DAG划分stage的算法</h3><p>核心算法：<strong>回溯算法</strong></p>
<p>对于每个job，从其最后的RDD往前回溯整个逻辑处理过程，如果遇到窄依赖，则将其当前RDD的父RDD纳入，并继续往前回溯。当遇到宽依赖时，停止回溯，将当前已经纳入的所有RDD按照其依赖关系建立一个执行阶段，命名为stagei。</p>
<h3 id="23-⭐Spark的物理执行计划任任务划分及过程"><a href="#23-⭐Spark的物理执行计划任任务划分及过程" class="headerlink" title="23. ⭐Spark的物理执行计划任任务划分及过程"></a>23. ⭐Spark的物理执行计划任任务划分及过程</h3><img src="Snipaste_2024-07-31_12-57-47-172545400944849.png" alt="Snipaste_2024-07-31_12-57-47" style="zoom:50%;">

<ul>
<li>首先根据action()操作顺序将应用划分为作业（job）【一个action算子就会产生一个job】</li>
<li>然后根据每个job的逻辑流程中的宽依赖（ShuffleDependency）依赖关系，将job划分为执行阶段stage【stage数等于宽依赖个数加1】</li>
<li>最后在每个stage中，根据最后一个的RDD的分区个数生成多个计算任务task【最后一个RDD的分区个数就是task个数】，可以根据参数设置，spark.sql.shuffle.partitions &#x3D; 50; task就是每个Executor的最小计算单元</li>
</ul>
<h3 id="24-job、stage、task的计算顺序"><a href="#24-job、stage、task的计算顺序" class="headerlink" title="24. job、stage、task的计算顺序"></a>24. job、stage、task的计算顺序</h3><ul>
<li>当应用程序执行到rdd.action()时，就会立即将rdd.action()形成的job提交给Spark。</li>
<li>stage从包含输入数据的stage开始，从前到后依次执行，仅当上游的stage都执行完成后，再执行下游的stage。</li>
<li>stage中的每个task并行运行没有先后之分，流水线操作</li>
</ul>
<h3 id="25-⭐⭐⭐Spark的Shuffle机制是怎么样的？请列举会引起Shuffle过程的Spark算子，"><a href="#25-⭐⭐⭐Spark的Shuffle机制是怎么样的？请列举会引起Shuffle过程的Spark算子，" class="headerlink" title="25. ⭐⭐⭐Spark的Shuffle机制是怎么样的？请列举会引起Shuffle过程的Spark算子，"></a>25. ⭐⭐⭐Spark的Shuffle机制是怎么样的？请列举会引起Shuffle过程的Spark算子，</h3><img src="image-20240821082054925.png" alt="image-20240821082054925" style="zoom:50%;">

<p>运行在不同stage上的不同节点上task间进行数据传递的过程被称为Shuffle机制。<strong>重新分区</strong>就会产生Shuffle，<strong>重新</strong>的含义是原来在同一个分区的数据通过操作被分到不同的分区，所以减少分区不一定产生Shuffle，增加分区一定产生Shuffle。</p>
<img src="image-20240821082213929.png" alt="image-20240821082213929" style="zoom: 67%;">

<p>Shuffle机制分为<strong>Shuffle Write</strong>和<strong>Shuffle Read</strong>两个阶段：</p>
<p>（1）<strong>Shuffle Write</strong>：主要解决上游stage输出数据的<strong>聚合、排序、分区</strong>问题（聚排分）【聚合combine和排序sort可选】</p>
<p><img src="Snipaste_2024-07-31_11-33-54-172545400944850.png" alt="Snipaste_2024-07-31_11-33-54"></p>
<ul>
<li>上游task输出record及其partitionId</li>
<li>建立一个HashMap对输出的record进行combine聚合，HashMap中的Key是“partitionId+Key”，Value是经过相同combine的聚合结果（如果HashMap存放不下，则会先扩容为两倍大小，还存不下就将HashMap中的record排序后溢写到磁盘上）</li>
<li>聚合完成后，再将HashMap中的数据放入类似Array的数据结构中进行排序，可以按照partitionId或者partitionId+key排序</li>
<li>最后根据partitionId将数据（通过HashMap排序的数据和溢写到磁盘上已经排好序的数据merge一下）写入不同分区中，存在到本地磁盘上</li>
</ul>
<p>（2）<strong>Shuffle Read</strong>：主要解决下游stage从上游stage<strong>数据获取、聚合、排序</strong>问题（取聚排）【聚合aggregate和排序sort可选】</p>
<p><img src="Snipaste_2024-07-31_11-34-54-172545400944851.png" alt="Snipaste_2024-07-31_11-34-54"></p>
<ul>
<li>从上游各个分区文件中获取数据，用buffer暂存</li>
<li>建立一个HashMap对获取的record进行aggregate聚合，HashMap中的Key是“Key”，Value是经过相同aggregate的聚合结果（如果HashMap存放不下，则会先扩容为两倍大小，还存不下就将HashMap中的record排序后溢写到磁盘上）</li>
<li>聚合完成后，再将HashMap中的数据放入类似Array的数据结构中进行排序，按照key进行排序</li>
<li>最后通过HashMap排序的数据和溢写到磁盘上已经排好序的数据merge一下，将结果输出给后续操作</li>
</ul>
<p>会引起Shuffle的算子：groupByKey，reduceByKey，aggregateByKey，sortByKey，foldByKey，combineByKey，distinct等等。<strong>spark的数据倾斜只会发生在shuffle过程中，这些引起shuffle的算子也可能引发数据倾斜</strong>。</p>
<h3 id="26-⭐⭐⭐MR的Shuffle和Spark的Shuffle有什么相同与不同？"><a href="#26-⭐⭐⭐MR的Shuffle和Spark的Shuffle有什么相同与不同？" class="headerlink" title="26. ⭐⭐⭐MR的Shuffle和Spark的Shuffle有什么相同与不同？"></a>26. ⭐⭐⭐MR的Shuffle和Spark的Shuffle有什么相同与不同？</h3><p><strong>相同点</strong>（也就是Shuffle通用定义）：都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个stage 里的 ShuffleMapTask，也可能是 ResultTask）</p>
<p><strong>不同点</strong>：</p>
<ul>
<li>MapReduce 强制按照key排序的（hadoop的默认行为）；spark提供了按partitionId或key排序等灵活排序，而且排序是可选的</li>
<li>MR采取独立阶段聚合，不管是map端还是reduce端都是先将数据存放在内存或磁盘上后，再执行聚合操作；Spark采用在线聚合，将record插入HashMap时自动完成聚合过程。</li>
<li>MapReduce 可以划分成 split，map()、spill、merge、shuffle、sort、reduce()等阶段，流程固定，阶段分明。spark 没有明显的阶段划分，只有不同的 stage 和算子操作。</li>
</ul>
<p><strong>MR的shuffle</strong>：</p>
<p>①InputFile切割成多个split文件，通过RecordReader按行读取内容给map方法（<strong>Map Task的Read阶段</strong>）</p>
<p>②使用map()方法对数据进行处理（<strong>Map Task的Map阶段</strong>）</p>
<p>③map方法之后，交给OutputCollect收集器，按照key的hash进行分区，发送到环形缓冲区（默认大小100M）（<strong>Map Task的Collect阶段</strong>）</p>
<p>④达到80%，将数据以临时文件的方式溢写到磁盘（<strong>Map Task的溢写阶段</strong>）</p>
<p>⑤溢写前，对key的索引进行快排；溢写后，对溢写文件进行归并排序（<strong>Map Task的Merge阶段</strong>）</p>
<p>⑥在归并排序的前后还可以进行Combiner操作（前提是汇总操作），之后还可以进行数据压缩</p>
<p>⑦将文件按分区存储在磁盘上，等待Reduce端拉取</p>
<p>⑧每个Reduce拉取Map端对应分区的数据，先存储在内存中，后磁盘中。（<strong>Reduce Task的Copy阶段</strong>）</p>
<p>⑨拉取完所有的数据后，采用归并排序将内存和磁盘中的数据进行排序，使key相同的数据聚集在一起，之后按照相同的key进行分组（<strong>Reduce Task的Sort阶段</strong>）</p>
<p>⑩使用reduce()函数将计算结果写到HDFS上（<strong>Reduce Task的Reduce阶段</strong>）</p>
<h3 id="27-⭐⭐⭐为什么要减少shuffle，如何减少shuflle？"><a href="#27-⭐⭐⭐为什么要减少shuffle，如何减少shuflle？" class="headerlink" title="27. ⭐⭐⭐为什么要减少shuffle，如何减少shuflle？"></a>27. ⭐⭐⭐为什么要减少shuffle，如何减少shuflle？</h3><p>在<strong>MapReduce</strong>和<strong>Spark</strong>等分布式计算框架中，<strong>shuffle</strong> 是指数据在节点之间的重新分配和传输过程，通常发生在需要对数据进行重新排序、分区或聚合的计算阶段。虽然 shuffle 是分布式计算中必不可少的一部分，但它通常被认为是昂贵和低效的操作，因此需要尽量减少其产生。以下是减少 shuffle 的原因：</p>
<p>（1）. <strong>高 I&#x2F;O 和网络开销</strong></p>
<ul>
<li><strong>数据传输成本高</strong>：在 shuffle 过程中，数据需要从某些节点（map阶段）传输到其他节点（reduce阶段），这涉及大量的磁盘读写操作（I&#x2F;O）和网络传输。特别是在数据量大、网络带宽有限的情况下，shuffle 的 I&#x2F;O 和网络传输会显著降低性能。</li>
<li><strong>磁盘读写性能瓶颈</strong>：在很多框架中，如 Hadoop 的 MapReduce 和 Spark，shuffle 过程中可能会将数据写到磁盘再读取，这会增加磁盘的压力，尤其是当数据集较大时，磁盘 I&#x2F;O 成为系统的瓶颈，影响计算速度。</li>
</ul>
<p>（2）. <strong>内存开销大</strong></p>
<ul>
<li><strong>内存压力</strong>：Spark 等引擎在处理 shuffle 时，通常会将数据放入内存中进行聚合、排序等操作。如果内存不足，数据就会溢出到磁盘，导致性能下降。此外，内存的高消耗可能导致频繁的垃圾回收，进一步拖慢系统性能。</li>
<li><strong>内存竞争</strong>：当多个任务在同一个节点上并行运行时，shuffle 过程中需要占用大量内存，可能导致不同任务之间的内存争夺，从而影响整体系统的稳定性。</li>
</ul>
<p>（3）. <strong>任务延迟和计算瓶颈</strong></p>
<ul>
<li><strong>延长计算时间</strong>：由于 shuffle 涉及跨节点数据传输，导致任务之间的依赖性增加。某些节点可能需要等待其他节点的数据传输完成后才能继续计算，因此 shuffle 操作会显著增加任务的延迟，特别是在处理海量数据时，这种延迟影响更为明显。</li>
<li><strong>资源不均衡</strong>：shuffle 会导致数据重新分配，可能产生数据倾斜问题，某些节点分配到的数据量过大，而其他节点的数据量较少。资源利用不均衡会导致一些节点处于繁忙状态，而其他节点闲置，从而降低整个集群的效率。</li>
</ul>
<p>（4）. <strong>数据倾斜问题</strong></p>
<ul>
<li><strong>产生数据倾斜</strong>：shuffle 过程中，某些 key 或分区可能聚集了大量的数据，导致某些节点承担了过多的计算任务，这种现象称为<strong>数据倾斜</strong>。数据倾斜会导致部分节点处理时间过长，拖慢整体作业的执行时间。减少 shuffle 可以降低数据倾斜的风险，避免个别节点过载。</li>
</ul>
<p>（5）. <strong>故障恢复成本高</strong></p>
<ul>
<li><strong>高容错成本</strong>：在分布式系统中，shuffle 过程如果出现节点故障，恢复成本很高。因为 shuffle 涉及大量跨节点的数据传输，如果某些任务在 shuffle 过程中失败，整个过程可能需要重新执行，进一步增加了计算时间和资源消耗。</li>
</ul>
<p>（6）. <strong>消耗集群带宽</strong></p>
<ul>
<li><strong>网络带宽占用</strong>：shuffle 涉及的数据在不同节点之间传输，会占用大量集群带宽。如果多个作业同时进行 shuffle，可能会导致网络拥塞，影响整个集群的计算效率。因此，减少 shuffle 操作可以减少带宽消耗，提升网络效率。</li>
</ul>
<img src="image-20240908234620100.png" alt="image-20240908234620100" style="zoom: 67%;">



<h3 id="27-如何利用Spark实现TopN的获取"><a href="#27-如何利用Spark实现TopN的获取" class="headerlink" title="27. 如何利用Spark实现TopN的获取"></a>27. 如何利用Spark实现TopN的获取</h3><p>如果是单值数据RDD，直接使用算子top(num);</p>
<p>如果是KV对数据RDD，使用sortByKey，按照key降序排列，然后取take(num);</p>
<h3 id="28-⭐说一说RDD的缓存机制"><a href="#28-⭐说一说RDD的缓存机制" class="headerlink" title="28. ⭐说一说RDD的缓存机制"></a>28. ⭐说一说RDD的缓存机制</h3><p>在Apache Spark中，缓存（或持久化）数据是一个优化操作，用于避免在重复使用数据时重新计算整个数据集。通常，Spark会在以下几种情况下主动缓存数据：</p>
<ol>
<li><strong>显式调用缓存操作</strong>：</li>
</ol>
<ul>
<li>当用户显式调用<code>cache()</code>或<code>persist()</code>方法时，Spark会将数据集（如RDD、DataFrame或Dataset）缓存到内存或磁盘中，以便在后续操作中重复使用时加快访问速度。</li>
</ul>
<ol start="2">
<li><strong>重复使用RDD、DataFrame或Dataset</strong>：</li>
</ol>
<ul>
<li>如果一个数据集在同一个作业中被多次使用且没有显式缓存，Spark可能会建议用户缓存它。虽然Spark本身不会主动缓存数据，但在用户多次操作同一个数据集时，缓存操作可以显著提高性能。</li>
</ul>
<ol start="3">
<li><strong>Shuffle操作后</strong>：</li>
</ol>
<ul>
<li>在一些复杂的操作（如<code>groupBy</code>、<code>reduceByKey</code>、<code>join</code>等）之后，Spark会产生中间结果，称为Shuffle文件。虽然这些文件会暂时保存在磁盘上，但它们不会自动缓存到内存中，除非用户显式要求。这种情况也提醒用户，缓存这些中间结果可能有助于后续操作的效率。</li>
</ul>
<ol start="4">
<li><strong>广播变量（Broadcast Variables）</strong>：</li>
</ol>
<ul>
<li>Spark在处理广播变量时，会将数据广播到各个节点并缓存，以便在任务执行过程中快速访问。广播变量的缓存是在底层执行的，但它只适用于广播的只读数据，不是一般意义上的数据集缓存。</li>
</ul>
<ol start="5">
<li><strong>热数据的缓存策略</strong>：</li>
</ol>
<ul>
<li>如果某个数据集被认为是“热数据”（即在多个操作中频繁访问），并且显式地调用了<code>cache()</code>，Spark会将其缓存到内存中，以减少重新计算的开销。这种情况下的缓存通常是用户明确指定的。</li>
</ul>
<ol start="6">
<li><strong>内存溢出避免</strong>：</li>
</ol>
<ul>
<li>当Spark的数据量超过内存容量时，它会自动将溢出的数据存储到磁盘上，而不是主动缓存。这个过程称为“spill”。虽然这不属于主动缓存，但它显示了Spark在资源管理中的智能行为。</li>
</ul>
<p>注意事项：</p>
<p>Spark不会自动缓存数据集，除非用户明确要求。缓存操作虽然能提升性能，但也会占用内存资源，因此需要根据具体情况选择性地进行缓存，以免导致内存溢出或不必要的资源消耗。</p>
<p>缓存数据可以加速重复计算，但并不是所有场景都适合缓存，使用时要权衡内存资源和计算成本。</p>
<p>RDD通过Cache或者Persist方法将前面的计算结果缓存，默认情况下会把数据以序列化的形式缓存在JVM的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action算子时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p>
<p>cache的缓存级别有12个等级，其中RDDcache默认的缓存采用MEMORY_ONLY（只缓存再内存中），DataFrame的cache默认采用MEMORY_AND_DISK（先缓存在内存中，空间如果不够再缓存在磁盘中）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mapRDD.cache();<span class="comment">//延时生效</span></span><br><span class="line">mapRDD.persist(StorageLevel.MEMORY_ONLY());<span class="comment">//延时生效</span></span><br><span class="line">mapRDD.unpersist();<span class="comment">//立即生效，一般将缓存回收语句放在程序最末尾</span></span><br></pre></td></tr></table></figure>

<h3 id="29-⭐说一说RDD的CheckPoint（检查点）机制（错误容忍机制-）"><a href="#29-⭐说一说RDD的CheckPoint（检查点）机制（错误容忍机制-）" class="headerlink" title="29. ⭐说一说RDD的CheckPoint（检查点）机制（错误容忍机制 ）"></a>29. ⭐说一说RDD的CheckPoint（检查点）机制（错误容忍机制 ）</h3><p>检查点的核心思想是将计算过程中的某些重要数据（数据依赖关系比较复杂，重新计算代价较高）进行持久化，这样再次执行时可以从检查点执行，从而减少重复计算的开销。检查点的数据通常存储在HDFS中，存储格式为二进制文件。</p>
<p>对RDD进行Checkpoint操作并不会马上被执行，必须执行Action操作才能触发。当前job结束后会另外启动专门的job去完成checkpoint，需要checkpoint的RDD会被<strong>计算两次</strong>。为了节省开销，建议将需要被checkpoint的rdd先进行缓存，这样启动专门的job只需要将缓存数据进行checkpoint即可，不需要重新计算RDD。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">jsc.setCheckpointDir(<span class="string">&quot;cp&quot;</span>);</span><br><span class="line">...</span><br><span class="line">mapRDD.cache();</span><br><span class="line">mapRDD.checkpoint();</span><br></pre></td></tr></table></figure>

<h3 id="30-⭐checkpoint和缓存的区别"><a href="#30-⭐checkpoint和缓存的区别" class="headerlink" title="30. ⭐checkpoint和缓存的区别"></a>30. ⭐checkpoint和缓存的区别</h3><p>（1）<strong>目的不同</strong>：数据缓存是为了加速后续运行的job，而检查点的目的是在job运行失败能够<strong>快速恢复</strong>，及加速当前需要重新运行的job</p>
<p>（2）<strong>存储性质和位置不同</strong>：数据缓存是为了读写速度快，所以主要使用<strong>内存</strong>，偶尔使用磁盘。而检查点是为了能够可靠读写，因此主要使用分布式文件系统如<strong>HDFS</strong></p>
<p>（3）<strong>写入速度和规则不同</strong>：数据缓存速度较快，在job<strong>运行时</strong>进行缓存；检查点写入速度慢，额外启动专门的job进行持久化。</p>
<p>（4）<strong>对血缘影响不同</strong>：对某个RDD进行缓存后，RDD的血缘没有影响。而对某个RDD进行checkpoint后，会切断RDD的血缘。</p>
<p>（5）<strong>应用场景不同</strong>：数据缓存适合于被多次读取，占用空间不是非常大的RDD，而checkpoint适用于数据依赖关系比较复杂、重新计算代价较高的RDD。</p>
<h3 id="31-⭐Hive-on-Spark与Spark-on-Hive的区别"><a href="#31-⭐Hive-on-Spark与Spark-on-Hive的区别" class="headerlink" title="31. ⭐Hive on Spark与Spark on Hive的区别"></a>31. ⭐Hive on Spark与Spark on Hive的区别</h3><ul>
<li>Spark on Hive：使用Spark SQL语法操作Hive表，Hive只作为存储元数据，Spark负责SQL解析优化，Spark底层采用dataset执行</li>
<li>Hive on Spark：使用HiveSQL语法操作Hive表，Hive既作为存储元数据又负责SQL解析优化，执行引擎为Spark，底层采用RDD执行</li>
</ul>
<h3 id="32-Spark内存消耗来自于哪几方面？"><a href="#32-Spark内存消耗来自于哪几方面？" class="headerlink" title="32. Spark内存消耗来自于哪几方面？"></a>32. Spark内存消耗来自于哪几方面？</h3><p>（1）<strong>用户代码消耗内存</strong>，比如算子中定义的一些func，消耗内存量难以预估</p>
<p>（2）<strong>Shuffle机制中产生的中间数据消耗内存</strong>，在Shuffle Write阶段，聚合（使用HashMap）和排序（使用类似Array）需要消耗内存。在Shuffle Read阶段，分配buffer，聚合（HashMap）和排序（类似Array）都可能需要消耗内存</p>
<img src="111111111111111111111111111-172545400944852.png" alt="111111111111111111111111111" style="zoom: 50%;">

<img src="Snipaste_2024-07-31_15-07-38-172545400944853.png" alt="Snipaste_2024-07-31_15-07-38" style="zoom:50%;">

<p>（3）<strong>数据缓存消耗内存</strong></p>
<h3 id="33-⭐Spark内存管理模型"><a href="#33-⭐Spark内存管理模型" class="headerlink" title="33. ⭐Spark内存管理模型"></a>33. ⭐Spark内存管理模型</h3><p>在spark1.6之前的版本，使用<strong>静态内存管理</strong>，将内存划分为数据缓存空间：框架执行空间：用户代码空间&#x3D;6：2：2，用户也可以根据自身程序情况进性内存比例设置。但是对复杂程序这个比例很难确定，并不存在一个最优的静态比例，容易造成资源浪费、内存溢出等问题。</p>
<p>在spark1.6版本之后，采用<strong>统一内存管理模型</strong>，仍然将内存划分为三份：<strong>数据缓存空间</strong>、<strong>框架执行空间</strong>、<strong>用户代码空间</strong>。其中用户代码空间，将其设定为固定大小；数据缓存空间和框架执行空间共同组成（共享）一个固定空间——<strong>Framework memory</strong>，这个空间为<strong>缓存空间</strong>和<strong>框架执行空间</strong>设定了初始比例，在运行中可以<strong>动态调整</strong>，并且比例也有<strong>上下界</strong>。</p>
<p>除此之外，当<strong>框架执行空间不足</strong>的时候，Spark也会将shuffle数据<strong>溢写到磁盘</strong>。当<strong>数据缓存空间不足</strong>的时候，Spark会进行<strong>缓存替换、移除缓存</strong>等操作。而且Spark为<strong>每一个task的内存</strong>使用也进行了限制。</p>
<h3 id="34-Executor-JVM的整个内存空间划分"><a href="#34-Executor-JVM的整个内存空间划分" class="headerlink" title="34. Executor JVM的整个内存空间划分"></a>34. Executor JVM的整个内存空间划分</h3><img src="Snipaste_2024-07-31_15-47-26-172545400944854.png" alt="Snipaste_2024-07-31_15-47-26" style="zoom:67%;">

<p>整体上分为堆内内存和堆外内存，堆内内存分为<strong>系统保留空间、用户代码空间、框架内存空间</strong>，框架内存空间又分为<strong>框架执行空间和数据缓存空间</strong>。</p>
<p>为了<strong>减少CG开销</strong>，Spark的统一内存管理机制也允许使用堆外内存，该空间<strong>不受JVM垃圾回收管理机制</strong>，在结束使用时需要<strong>手动释放空间</strong>。堆外内存主要存储<strong>序列化对象数据</strong>，只包括框架执行空间和数据缓存空间，两者大小比例与堆内内存相同。</p>
<p><strong>何时用到堆外内存？</strong> SerializedShuffle可以利用堆外内存来进行Shuffle Write；用户使用rdd.persist(OFF_HEAP)可以将rdd存储在堆外的数据缓存空间。</p>
<h3 id="35-说一说广播变量"><a href="#35-说一说广播变量" class="headerlink" title="35. 说一说广播变量"></a>35. 说一说广播变量</h3><p>广播变量是一种分布式只读变量，具体操作是将一个较大的对象（要广播的数据一般预先存储在Driver端，Spark在Driver端将要广播的数据划分为数据块）广播给各个Executor，Executor接收到数据块后，将其放到堆内的<strong>数据缓存空间</strong>里，然后每个task从Executor中读取使用。</p>
<img src="Snipaste_2024-07-31_17-37-04-172545400944855.png" alt="Snipaste_2024-07-31_17-37-04" style="zoom:50%;">

<h3 id="36-SparkSQL是如何将数据写到Hive表的？"><a href="#36-SparkSQL是如何将数据写到Hive表的？" class="headerlink" title="36. SparkSQL是如何将数据写到Hive表的？"></a>36. SparkSQL是如何将数据写到Hive表的？</h3><p>利用 Spark SQL 将获取的数据 RDD 转换成 DataSet，再将DataSet 写成视图表，最后利用 Spark SQL 直接插入 hive 表中。</p>
<h3 id="37-⭐用spark手写wordcount"><a href="#37-⭐用spark手写wordcount" class="headerlink" title="37. ⭐用spark手写wordcount"></a>37. ⭐用spark手写wordcount</h3><p>src&#x2F;main&#x2F;data&#x2F;word.txt</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hadoop Hive Flume Spark Flink</span><br><span class="line">Hadoop Hive Flume Spark</span><br><span class="line">Hadoop Hive Flume</span><br><span class="line">Hadoop Hive</span><br><span class="line">Hadoop</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Spark01_Operate_Transform_wordCount</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建配置对象</span></span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkCore&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 创建sparkContext</span></span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">jsc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 编写代码</span></span><br><span class="line">        <span class="comment">//TODO 分组聚合</span></span><br><span class="line">        <span class="comment">//   1.读取文件</span></span><br><span class="line">        JavaRDD&lt;String&gt; lineRDD = jsc.textFile(<span class="string">&quot;src/main/data/word.txt&quot;</span>);</span><br><span class="line">        <span class="comment">//   2.将文件数据进行分解（扁平化）</span></span><br><span class="line">        JavaRDD&lt;String&gt; wordRDD = lineRDD.flatMap(line -&gt; Arrays.asList(line.split(<span class="string">&quot; &quot;</span>)).iterator());</span><br><span class="line">        <span class="comment">//   3.将每一个单词转换为kKV格式，key为单词，Value均为1，代表每一个单词计数为1</span></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; kv = wordRDD.mapToPair(v1 -&gt; <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(v1, <span class="number">1</span>));</span><br><span class="line">        <span class="comment">//   4.计算每个单词出现的次数</span></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; result = kv.reduceByKey((v1, v2) -&gt; v1 + v2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  依次输出</span></span><br><span class="line">        result.collect().forEach(System.out::println);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 关闭sc</span></span><br><span class="line">        jsc.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(Flink,1)</span><br><span class="line">(Hive,4)</span><br><span class="line">(Spark,2)</span><br><span class="line">(Flume,3)</span><br><span class="line">(Hadoop,5)</span><br></pre></td></tr></table></figure>

<h3 id="38-⭐Spark数据倾斜的解决办法（Spark特有的方法）"><a href="#38-⭐Spark数据倾斜的解决办法（Spark特有的方法）" class="headerlink" title="38. ⭐Spark数据倾斜的解决办法（Spark特有的方法）"></a>38. ⭐Spark数据倾斜的解决办法（Spark特有的方法）</h3><p>（1）使用Hive ETL预处理数据</p>
<p>通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行joi），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</p>
<p>（2）提高shuffle操作的并行度</p>
<p>spark.sql.shuffle.partitions（shuffle read task的并行度），让更多的task分担数据倾斜的任务。</p>
<p>（3）两阶段聚合</p>
<p>适用于聚合类shuffle算子，第一次是局部聚合，先给每个key都打上一个随机数，接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，然后将各个key的前缀给去掉，再次进行全局聚合操作，就可以得到最终结果了。</p>
<p>（4）。。。</p>
<h3 id="39-⭐Spark-SQL的性能优化（CBO，AQE）"><a href="#39-⭐Spark-SQL的性能优化（CBO，AQE）" class="headerlink" title="39. ⭐Spark SQL的性能优化（CBO，AQE）"></a>39. ⭐Spark SQL的性能优化（CBO，AQE）</h3><p>一般而言，在Hive on Spark中的sql层面的优化基本在spark sql上也使用，这里说一些spark sql独有的优化方法。</p>
<p>（1）RBO与CBO优化策略</p>
<ul>
<li>启发式优化RBO：应用一些列规则重写执行计划，可以对连接操作、聚合操作等进行优化。</li>
<li>基于成本的CBO：基于<strong>代价模型</strong>（数据读取、数据传输、计算操作）来评估执行计划的执行成本，并选择最优秀的执行计划<code>spark.sql.cbo.enabled=true</code></li>
</ul>
<p>（2）AQE优化</p>
<p><code>spark.sql.adaptive.enabled=true</code></p>
<ul>
<li>AQE的自动分区合并：AQE自动检测和合并具有相同结构的分区，避免重复处理数据，减少数据移动和计算操作。</li>
<li>AQE的自动倾斜处理：①spark通过采样和统计确定哪些key发生倾斜。②spark自动对倾斜的key进行动态重分区，将倾斜的key均匀分布到多个task中。③复制倾斜数据到多个task进一步增加并行度。④spark将更多资源分配给倾斜数据的task，加速处理速度。</li>
<li>AQE的Join策略调整：根据数据和查询特征动态选择最佳的join策略</li>
</ul>
<h3 id="40-简单介绍一下sparkstreaming"><a href="#40-简单介绍一下sparkstreaming" class="headerlink" title="40. 简单介绍一下sparkstreaming"></a>40. 简单介绍一下sparkstreaming</h3><img src="image-20240821104943448.png" alt="image-20240821104943448" style="zoom:67%;">

<img src="image-20240821104958955.png" alt="image-20240821104958955" style="zoom:67%;">

<h3 id="41-简单介绍sparkStreaming"><a href="#41-简单介绍sparkStreaming" class="headerlink" title="41. 简单介绍sparkStreaming"></a>41. 简单介绍sparkStreaming</h3><img src="image-20240821105638224.png" alt="image-20240821105638224" style="zoom: 67%;">

<h3 id="42-⭐⭐sparksql的三种join实现"><a href="#42-⭐⭐sparksql的三种join实现" class="headerlink" title="42. ⭐⭐sparksql的三种join实现"></a>42. ⭐⭐sparksql的三种join实现</h3><img src="image-20240821111636731.png" alt="image-20240821111636731" style="zoom:67%;">

<h3 id="43-累加器和广播变量（除了RDD外的其他数据结构）"><a href="#43-累加器和广播变量（除了RDD外的其他数据结构）" class="headerlink" title="43. 累加器和广播变量（除了RDD外的其他数据结构）"></a>43. 累加器和广播变量（除了RDD外的其他数据结构）</h3><img src="image-20240821113047898.png" alt="image-20240821113047898" style="zoom:67%;">

<h3 id="44-⭐⭐⭐Spark调优（全）"><a href="#44-⭐⭐⭐Spark调优（全）" class="headerlink" title="44. ⭐⭐⭐Spark调优（全）"></a>44. ⭐⭐⭐Spark调优（全）</h3><img src="image-20240826132121527.png" alt="image-20240826132121527" style="zoom:50%;">

<img src="image-20240826132143848.png" alt="image-20240826132143848" style="zoom:50%;">

<img src="image-20240826132215211.png" alt="image-20240826132215211" style="zoom:50%;">

<img src="image-20240826132239407.png" alt="image-20240826132239407" style="zoom:50%;">

<img src="image-20240826132303873.png" alt="image-20240826132303873" style="zoom:50%;">

<img src="image-20240826132326159.png" alt="image-20240826132326159" style="zoom:50%;">

<h2 id="十二、Flink"><a href="#十二、Flink" class="headerlink" title="十二、Flink"></a>十二、Flink</h2><h3 id="1-简单介绍一下Flink"><a href="#1-简单介绍一下Flink" class="headerlink" title="1. 简单介绍一下Flink"></a>1. 简单介绍一下Flink</h3><img src="image-20240826193659002.png" alt="image-20240826193659002" style="zoom: 67%;">

<h3 id="2-Flink与spark-Streaming的区别"><a href="#2-Flink与spark-Streaming的区别" class="headerlink" title="2. Flink与spark Streaming的区别"></a>2. Flink与spark Streaming的区别</h3><p><img src="image-20240826193747819.png" alt="image-20240826193747819"></p>
<h2 id="十三、Doris"><a href="#十三、Doris" class="headerlink" title="十三、Doris"></a>十三、Doris</h2><h3 id="1-简单介绍Doris"><a href="#1-简单介绍Doris" class="headerlink" title="1. 简单介绍Doris"></a>1. 简单介绍Doris</h3><p>Apache Doris 是一个现代化的 MPP（Massively Parallel Processing）数据库系统，旨在提供高性能、高吞吐量和易用的分析型查询服务。它最早由百度开发，后来成为 Apache 的顶级项目。</p>
<p>Doris 的主要特点：</p>
<ol>
<li><strong>高性能</strong>：<ul>
<li><strong>列式存储</strong>：Doris 使用列式存储，使得在分析查询中只需扫描所需列的数据，大大减少了 I&#x2F;O 开销。</li>
<li><strong>向量化执行</strong>：通过向量化执行技术，可以更有效地利用 CPU，提高查询速度。</li>
<li><strong>索引机制</strong>：Doris 支持多种索引类型，如前缀索引、倒排索引等，优化查询性能。</li>
</ul>
</li>
<li><strong>易用性</strong>：<ul>
<li><strong>MySQL 兼容性</strong>：Doris 的 SQL 语法兼容 MySQL，支持用户通过 MySQL 客户端工具连接和操作，非常适合数据分析场景下的快速上手。</li>
<li><strong>自动化运维</strong>：Doris 提供了简便的集群管理和运维工具，支持自动化的容错和数据分布管理。</li>
</ul>
</li>
<li><strong>高可扩展性</strong>：<ul>
<li><strong>弹性扩展</strong>：Doris 可以根据业务需求轻松进行节点的扩展或缩减，适应不同的负载。</li>
<li><strong>MPP 架构</strong>：Doris 的 MPP 架构允许多个节点并行处理数据，能够处理大规模数据集和复杂查询。</li>
</ul>
</li>
<li><strong>实时分析能力</strong>：<ul>
<li><strong>高吞吐量的导入</strong>：支持实时数据的高效导入，适用于实时分析场景。</li>
<li><strong>流式数据处理</strong>：Doris 能够处理实时流数据，提供低延迟的实时分析服务。</li>
</ul>
</li>
</ol>
<p>适用场景：</p>
<ul>
<li><strong>数据分析</strong>：由于 Doris 提供了高性能的列式存储和查询优化机制，非常适合处理复杂的分析型查询。</li>
<li><strong>实时 BI</strong>：Doris 支持高吞吐量的数据导入和低延迟查询，非常适合构建实时商业智能系统。</li>
<li><strong>大数据处理</strong>：凭借 MPP 架构和高可扩展性，Doris 可以处理大规模的数据集。</li>
</ul>
<h3 id="2-Doris与clickhouse的区别和应用场景"><a href="#2-Doris与clickhouse的区别和应用场景" class="headerlink" title="2. Doris与clickhouse的区别和应用场景"></a>2. Doris与clickhouse的区别和应用场景</h3><p><img src="image-20240826195057580.png" alt="image-20240826195057580"></p>
<p><img src="image-20240826195117998.png" alt="image-20240826195117998"></p>
<h2 id="十九、clickhouse"><a href="#十九、clickhouse" class="headerlink" title="十九、clickhouse"></a>十九、clickhouse</h2><h3 id="1-clickhouse的概念和优缺点"><a href="#1-clickhouse的概念和优缺点" class="headerlink" title="1. clickhouse的概念和优缺点"></a>1. clickhouse的概念和优缺点</h3><img src="image-20240821113158665.png" alt="image-20240821113158665" style="zoom:67%;">

<h3 id="⭐⭐列式存储和行式存储有什么区别？"><a href="#⭐⭐列式存储和行式存储有什么区别？" class="headerlink" title="⭐⭐列式存储和行式存储有什么区别？"></a>⭐⭐列式存储和行式存储有什么区别？</h3><p>行式存储：</p>
<p> 1、数据是按行存储的</p>
<p> 2、没有建立索引的查询消耗很大的IO</p>
<p> 3、建立索引和视图花费一定的物理空间和时间资源</p>
<p> 4、面对大量的查询，复杂的复杂的数据库必须使用大量性能才能满足</p>
<p> 列式存储：</p>
<p> 1、数据按列存储，每一列单独存放</p>
<p> 2、只访问查询设计的列，大量降低系统的IO</p>
<p> 3、数据类型一致，数据特征相似就可以高效的压缩</p>
<p> 优势：</p>
<p> 分析场景中往往需要读大量行但是少数几个列。在行式存模式下，数据按行连续存储，所有列的数据都存储在一个block中，不参与计算的列在IO时也要全部读出，读取操作被严重放大。而列存模式下，只需要读取参与计算的列即可，极大的减低了IO cost，加速了查询。</p>
<p> 同一列中的数据属于同一类型，压缩效果显著。列存往往有着高达十倍甚至更高的压缩比，节省了大量的存储空间，降低了存储成本。</p>
<p> 更高的压缩比意味着更小的 data size，从磁盘中读取相应数据耗时更短。</p>
<p> 自由的压缩算法选择。不同列的数据具有不同的数据类型，适用的压缩算法也就不尽相同。可以针对不同列类型，选择最合适的压缩算法。</p>
<p> 高压缩比，意味着同等大小的内存能够存放更多数据，系统cache效果更好。</p>
<h3 id="2-⭐clickhouse写入和读取为什么快？"><a href="#2-⭐clickhouse写入和读取为什么快？" class="headerlink" title="2. ⭐clickhouse写入和读取为什么快？"></a>2. ⭐clickhouse写入和读取为什么快？</h3><img src="image-20240821204439841.png" alt="image-20240821204439841">

<img src="image-20240903195043969.png" alt="image-20240903195043969" style="zoom:50%;">

<h3 id="ck中的数据类型"><a href="#ck中的数据类型" class="headerlink" title="ck中的数据类型"></a>ck中的数据类型</h3><img src="image-20240903194133495.png" alt="image-20240903194133495" style="zoom: 67%;">

<h3 id="ck中的表引擎"><a href="#ck中的表引擎" class="headerlink" title="ck中的表引擎"></a>ck中的表引擎</h3><img src="image-20240903194215458.png" alt="image-20240903194215458" style="zoom:67%;">

<p><img src="image-20240903194734946.png" alt="image-20240903194734946"></p>
<p><img src="image-20240903194756358.png" alt="image-20240903194756358"></p>
<p><img src="image-20240903194816272.png" alt="image-20240903194816272"></p>
<p><img src="image-20240903194835900.png" alt="image-20240903194835900"></p>
<h3 id="物化视图"><a href="#物化视图" class="headerlink" title="物化视图"></a>物化视图</h3><p><img src="image-20240903194950544.png" alt="image-20240903194950544"></p>
<h3 id="3-clickhouse相对于hive有什么优势？"><a href="#3-clickhouse相对于hive有什么优势？" class="headerlink" title="3. clickhouse相对于hive有什么优势？"></a>3. clickhouse相对于hive有什么优势？</h3><img src="image-20240831111006957.png" alt="image-20240831111006957" style="zoom: 67%;">

<img src="image-20240831111029363.png" alt="image-20240831111029363" style="zoom:67%;">

<h3 id="4-clickshouse的高可用有了解吗？"><a href="#4-clickshouse的高可用有了解吗？" class="headerlink" title="4. clickshouse的高可用有了解吗？"></a>4. clickshouse的高可用有了解吗？</h3><p>ClickHouse的高可用性主要通过<strong>集群部署、数据副本以及分布式处理等</strong>方式实现。这些方法共同确保了在大规模数据处理和查询的情况下，即使单节点出现故障，整个系统仍能稳定运行，从而提供持续的数据服务。下面将详细介绍ClickHouse的高可用性特点及实现方式：</p>
<ol>
<li><strong>集群部署</strong><ul>
<li><strong>多节点分片</strong>：ClickHouse支持多节点分片部署，这意味着可以将数据分散存储在多个节点上。例如，一个四节点的集群可以分为两个分片，每个分片有两个副本[^1^]。这种架构能够有效平衡各节点的存储和查询压力，提高系统的可扩展性和容错能力。</li>
<li><strong>负载均衡</strong>：通过负载均衡技术，ClickHouse集群可以均匀分配客户端请求到各个节点，避免单点过载。一旦某个节点发生故障，负载均衡器会自动将请求转发到其他健康的节点上[^5^]。</li>
</ul>
</li>
<li><strong>数据副本</strong><ul>
<li><strong>本地表代理</strong>：在每个节点上创建ReplicatedMergeTree表，同时建立一个分布式总表来代理写入、查询和分发操作。这样，所有数据的写入和查询操作都会经过分布式总表进行路由[^1^]。这种结构有助于保证数据的一致性和完整性。</li>
<li><strong>ZooKeeper同步</strong>：由于ReplicatedMergeTree引擎依赖于ZooKeeper，因此需要安装并配置ZooKeeper服务。ZooKeeper在有数据写入或修改时，会借助其分布式协调能力实现多个副本间的同步[^1^]。</li>
</ul>
</li>
<li><strong>分布式处理</strong><ul>
<li><strong>Distributed引擎</strong>：ClickHouse使用Distributed引擎支持分布式查询。该引擎可以在多个节点上并行执行查询，汇总结果后返回给客户端。这种分布式计算模式显著提升了大数据量下的查询性能[^4^]。</li>
<li><strong>多维度分片</strong>：数据分片允许多台服务器并行执行查询，实现分布式并行计算。不同服务器存储同一张表的不同部分，缓解单节点的压力，增强系统的可扩展性[^5^]。</li>
</ul>
</li>
<li><strong>故障转移</strong><ul>
<li><strong>自动故障转移</strong>：当某个节点出现故障时，由于数据的多副本存储和分布式架构，系统能够自动将读写请求转移到其他健康的节点上继续提供服务[^3^]。</li>
<li><strong>定期健康检查</strong>：使用chproxy代理时，它具备灵活的配置和健康检查机制。通过定期检查各节点的健康状态，确保在节点故障时能够及时进行故障转移[^5^]。</li>
</ul>
</li>
<li><strong>高并发支持</strong><ul>
<li><strong>面向OLAP优化</strong>：ClickHouse主要用于在线分析处理查询（OLAP），不特别关注数据的事务性处理，而是专注于对大量数据进行多维度、复杂的分析，这使得其在高并发场景下表现尤为出色[^2^]。</li>
<li><strong>MPP架构</strong>：采用大规模并行处理（MPP）架构，任务被分散到多个服务器和节点上并行执行，最终汇总结果，大幅提升系统的并发处理能力和吞吐量[^2^]。</li>
</ul>
</li>
<li><strong>灵活配置</strong><ul>
<li><strong>配置文件管理</strong>：通过配置文件，用户可以灵活定义每个节点的角色、分片信息和副本设置。例如，通过metrika.xml文件配置ZooKeeper的节点信息和集群的分片、副本信息[^1^]。</li>
<li><strong>多实例部署</strong>：在单个物理机上可以部署多个ClickHouse实例，以充分利用硬件资源[^3^]。例如，通过在不同的端口运行多个ClickHouse服务，可以有效扩展系统的处理能力。</li>
</ul>
</li>
</ol>
<p>综上所述，ClickHouse的高可用性体现在多个方面，从集群部署、数据副本、分布式处理、故障转移、高并发支持到灵活配置，每一个环节都为系统的稳健运行提供了坚实的保障。这些特性使得ClickHouse在面对大规模数据处理和实时分析需求时，能够提供高效、可靠的服务。在实际应用场景中，用户应根据具体的业务需求、数据量和预算等因素，合理配置和优化ClickHouse集群，以充分发挥其高可用性的优势。</p>
<h3 id="5-clickhouse和Doris有什么区别？分别适用在什么场景下？"><a href="#5-clickhouse和Doris有什么区别？分别适用在什么场景下？" class="headerlink" title="5. clickhouse和Doris有什么区别？分别适用在什么场景下？"></a>5. clickhouse和Doris有什么区别？分别适用在什么场景下？</h3><p>ClickHouse和Doris都是现代的、高性能的列式存储数据库，主要用于大数据实时分析。它们在<strong>架构设计、查询性能以及数据模型</strong>等方面存在区别。以下是具体分析：</p>
<ol>
<li><strong>架构设计</strong><ul>
<li><strong>ClickHouse</strong>：采用高效的列式存储格式，优化了大规模数据的聚合查询和分析。其向量化执行引擎能够高效处理批量数据操作，支持水平扩展，通过分片和复制实现高可用性和大容量存储[^1^]。</li>
<li><strong>Doris</strong>：基于大规模并行处理（MPP）架构，每个节点独立处理查询任务。自动将数据分布在集群中，根据用户配置可以实现基于哈希或范围的分区策略[^1^]。</li>
</ul>
</li>
<li><strong>查询性能</strong><ul>
<li><strong>ClickHouse</strong>：对实时插入和查询具有较好的支持，特别适合批处理或近实时分析。提供强大的SQL查询能力，支持复杂的聚合、窗口函数以及其他高级分析功能[^1^]。</li>
<li><strong>Doris</strong>：强调即时数据分析能力，支持高并发的低延迟查询，特别适合即席查询和BI分析场景。同时对于小范围内的数据更新也提供了支持[^1^]。</li>
</ul>
</li>
<li><strong>数据模型</strong><ul>
<li><strong>ClickHouse</strong>：数据模型相对简单，主要依赖于表和分布式表的代理。提供多种数据压缩选项以减少存储空间需求[^1^]。</li>
<li><strong>Doris</strong>：支持更灵活的表模型，如稀疏矩阵存储，允许用户定义维度和指标列，便于进行多维分析[^1^]。</li>
</ul>
</li>
<li><strong>事务支持</strong><ul>
<li><strong>ClickHouse</strong>：对事务支持有限，主要面向批处理和分析任务[^1^]。</li>
<li><strong>Doris</strong>：同样支持实时写入，但对小范围数据更新有更好的支持，适用于需要一定事务支持的场景[^1^]。</li>
</ul>
</li>
<li><strong>易用性</strong><ul>
<li><strong>ClickHouse</strong>：安装和配置较复杂，但性能调优空间大，适合有技术背景的团队使用[^1^]。</li>
<li><strong>Doris</strong>：强调易用性和低延迟查询，对BI工具的良好集成使其更适合业务分析师和数据科学家[^1^]。</li>
</ul>
</li>
<li><strong>扩展性</strong><ul>
<li><strong>ClickHouse</strong>：支持通过分片和复制实现的水平扩展，适应大规模数据仓库需求[^1^]。</li>
<li><strong>Doris</strong>：自动分区与数据分布特性使其具备良好的垂直和水平扩展能力[^1^]。</li>
</ul>
</li>
<li><strong>典型应用场景</strong><ul>
<li><strong>ClickHouse</strong>：适用于大数据分析、日志分析、时序数据处理、实时报表和数据仓库等场景，特别适合需要高性能聚合查询的业务[^3^]。</li>
<li><strong>Doris</strong>：适用于大规模实时数据分析、即席查询、BI分析及需要频繁数据更新的业务场景，如招联金融的实时数仓升级案例所示[^5^]。</li>
</ul>
</li>
</ol>
<p>综合来看，在选择ClickHouse和Doris时，需要根据具体的业务需求、数据规模、实时性要求及技术栈适配情况来做决策。ClickHouse更适合极致查询性能需求和大数据量下的复杂分析，而Doris则在保证查询性能的同时，更加注重易用性、实时查询体验以及针对BI工具的良好集成。</p>
<h3 id="6-clickhouse有会小文件的问题吗？"><a href="#6-clickhouse有会小文件的问题吗？" class="headerlink" title="6. clickhouse有会小文件的问题吗？"></a>6. clickhouse有会小文件的问题吗？</h3><p><strong>ClickHouse存在小文件问题</strong>。</p>
<p>在大数据和实时分析领域，ClickHouse因其高性能和易扩展性而广受欢迎。然而，与许多其他数据库系统一样，ClickHouse也存在小文件问题，这会影响其性能和管理效率。这个问题主要源于ClickHouse的MergeTree存储引擎的设计机制。以下将详细探讨该问题的具体表现、原因及解决方法：</p>
<ol>
<li><strong>小文件问题的表现形式</strong><ul>
<li><strong>生成过多小parts文件</strong>：ClickHouse使用MergeTree作为主要的存储引擎，每当有数据写入表时，都会生成新的parts文件[^2^]。在高并发写入的情况下，短时间内会生成大量小文件，导致文件系统压力增大。</li>
<li><strong>查询性能下降</strong>：随着小文件数量的增加，查询时需要扫描的文件数增多，导致查询性能显著下降。因为每个part都带有一个索引，更多的文件意味着更多的索引需要被处理[^4^]。</li>
<li><strong>合并速度跟不上插入速度</strong>：ClickHouse后台会自动进行合并操作，但默认情况下，如果一次合并的文件数超过300个，就会触发异常[^5^]。当数据插入速度过快时，合并线程无法及时处理这些小文件，导致问题加重。</li>
</ul>
</li>
<li><strong>产生小文件问题的原因</strong><ul>
<li><strong>高并发写入</strong>：当客户端以高并发方式向ClickHouse写入数据时，每条数据都会生成一个新的小文件。例如，并发数为200时，一批写入就会产生200个文件[^2^]。这些小文件会迅速积累，导致性能问题。</li>
<li><strong>分区键选择不当</strong>：选择具有高基数的分区键会导致生成过多的目录，分散在各个目录下的小文件难以被合并[^4^]。例如，若使用timestamp作为分区键，将会创建大量独特的时间戳目录，阻碍文件合并。</li>
<li><strong>物化视图过多</strong>：物化视图会在每次数据插入时触发聚合计算并生成新的parts，从而增加小文件的数量[^4^]。过多的物化视图会显著提高小文件生成的速度，加剧问题。</li>
</ul>
</li>
<li><strong>解决小文件问题的方法</strong><ul>
<li><strong>调整写入并发数</strong>：降低写入数据的并发数，增大批处理的数据量，从而减少生成的文件数。例如，将并发数从200调整到50，将批次大小从1万条调整到5万条数据[^5^]。</li>
<li><strong>修改配置参数</strong>：通过修改<code>&lt;merge_tree&gt;</code>部分的配置参数，如<code>parts_to_delay_insert</code>和<code>parts_to_throw_insert</code>，可以增加合并文件的阈值，减缓插入速度与合并速度之间的矛盾[^5^]。</li>
<li><strong>选择合适的分区键</strong>：选择一个基数适中的分区键，避免使用高基数的列如时间戳。合理分区能够显著减少文件碎片，提高合并效率[^4^]。</li>
<li><strong>定期手动合并</strong>：可以通过<code>OPTIMIZE TABLE</code>语句定期对表进行手动合并操作，强制合并小文件，减少文件数量。这虽然会增加CPU负担，但能显著改善查询性能[^2^]。</li>
<li><strong>使用合并策略</strong>：ClickHouse提供了多种数据合并策略，如<code>merge_as_needed</code>（按需合并）、<code>merge_if_not_inserted</code>（未插入数据则合并）等。根据实际应用场景选择合适的合并策略，能有效控制文件数量[^1^]。</li>
</ul>
</li>
<li><strong>小文件问题的影响</strong><ul>
<li><strong>影响系统性能</strong>：大量的小文件会占用大量的文件描述符，使得系统在处理文件描述符上消耗更多的资源，从而影响整体性能[^3^]。</li>
<li><strong>增加管理难度</strong>：海量的小文件会增大数据管理的复杂度，不仅备份和恢复耗时增加，还可能在文件系统中引发inode耗尽的问题[^4^]。</li>
<li><strong>降低查询效率</strong>：每个part文件都有自己的索引，文件数量越多，查询时需要扫描的索引越多，导致查询效率低下[^4^]。</li>
</ul>
</li>
<li><strong>小文件问题的深层原因</strong><ul>
<li><strong>存储引擎机制</strong>：MergeTree存储引擎的设计决定了数据写入和查询的方式。这种机制在支持高效查询的同时，也带来了小文件管理的挑战[^1^]。</li>
<li><strong>缺乏自动合并资源</strong>：ClickHouse的自动合并依赖于系统资源（如CPU和I&#x2F;O），当资源不足以支撑及时合并时，就会出现小文件累积[^2^]。</li>
<li><strong>用户行为</strong>：用户的使用习惯和对ClickHouse理解程度也会影响小文件问题。例如，频繁的小批量插入、不恰当的分区键选择等都会加剧问题[^4^]。</li>
</ul>
</li>
<li><strong>小文件问题的实践案例</strong><ul>
<li><strong>案例一：电商数据分析</strong>：某电商平台使用ClickHouse进行实时数据分析。初期由于高并发订单记录写入，产生了大量小文件，导致查询性能急剧下降。通过调整写入并发数并选择合适的分区键（如按小时分区），显著减少了小文件数量，提高了查询响应速度[^2^]。</li>
<li><strong>案例二：日志系统</strong>：一个基于ClickHouse的日志系统原本使用timestamp作为分区键，导致每个日志条目都独立成一个文件。改为每日分区后，文件数量大幅下降，查询性能提升明显[^4^]。</li>
</ul>
</li>
</ol>
<p>综上所述，可以看到，尽管ClickHouse在处理大规模数据分析任务中表现出色，但小文件问题确实是一个需要关注和解决的问题。通过调整写入策略、优化配置参数及选择合适的分区键等方法，可以有效缓解这一问题，从而发挥ClickHouse的最大性能。</p>
<h3 id="7-介绍一下在hive中没有的但在clickhouse中有的函数"><a href="#7-介绍一下在hive中没有的但在clickhouse中有的函数" class="headerlink" title="7. 介绍一下在hive中没有的但在clickhouse中有的函数"></a>7. 介绍一下在hive中没有的但在clickhouse中有的函数</h3><p><strong>ClickHouse中有一些特定函数是Hive中没有的</strong>。</p>
<p>这些函数在数据分析和处理方面，特别是针对实时查询和高并发场景，展示了ClickHouse的独特优势。以下是具体的函数及其用途：</p>
<ol>
<li><strong>聚合函数</strong><ul>
<li><strong>argMax和argMin</strong>：这两个函数用于从字符串中提取最大或最小值所在的索引。例如，<code>argMax(column)</code>会返回该列中最大值的索引[^2^]。</li>
<li><strong>honeycodeLike</strong>：这个函数用于实现类似Honeycode的模糊匹配功能。它允许用户使用通配符进行模糊匹配，从而灵活地搜索数据[^2^]。</li>
</ul>
</li>
<li><strong>数组和复杂类型函数</strong><ul>
<li><strong>arrayFilter和arrayMap</strong>：<code>arrayFilter</code>函数用于过滤数组中的特定元素，而<code>arrayMap</code>则能够对数组中的每个元素应用一个函数。这两个函数极大地增强了对数组类型的操作能力[^2^]。</li>
<li><strong>groupArray</strong>：这个函数用于将多个列组合成一个数组，使得数据结构化更加方便。这在需要将多个相关列合并为单个数组字段时特别有用[^2^]。</li>
</ul>
</li>
<li><strong>日期和时间函数</strong><ul>
<li><strong>toDate和toDateTime</strong>：<code>toDate</code>函数用于将字符串转换为日期格式，而<code>toDateTime</code>则能将字符串转换为日期和时间格式。这使得对日期数据的处理更加灵活[^2^]。</li>
<li><strong>dateDiff</strong>：这个函数用于计算两个日期之间的差异，可以精确到天、月甚至年，为日期差值计算提供了便利[^2^]。</li>
</ul>
</li>
<li><strong>字符串和路径函数</strong><ul>
<li><strong>hasPrefix和hasSuffix</strong>：这两个函数分别用于检查一个字符串是否以指定的前缀或后缀开始或结束。它们在处理文件路径和URL时非常有用[^2^]。</li>
<li><strong>splitByCharPreserve</strong>：该函数用于按照指定字符分割字符串，但与常规分割函数不同，它保留了分隔符。这在解析经过特定字符分割的数据时非常有用[^2^]。</li>
</ul>
</li>
<li><strong>数学和统计函数</strong><ul>
<li><strong>hypot</strong>：这个函数用于计算直角三角形的斜边长度，即给定两个直角边的长度，它能够返回斜边的长度。这是基本的数学函数之一[^2^]。</li>
<li><strong>mode</strong>：该函数用于计算一组数值中的众数，即出现频率最高的数值。这在统计分析中十分常见[^2^]。</li>
</ul>
</li>
<li><strong>高级分析函数</strong><ul>
<li><strong>lead和lag</strong>：<code>lead</code>函数用于获取当前行的下一行数据，而<code>lag</code>则用于获取上一行数据。这两个函数在数据分析中非常实用，特别是在需要进行跨行比较的场景中[^2^]。</li>
<li><strong>firstValue和nthValue</strong>：这些函数分别用于获取窗口函数中的第一行和指定行的值，增强了对数据集的窗口分析能力[^2^]。</li>
</ul>
</li>
<li><strong>地理和位置函数</strong><ul>
<li><strong>st_astext和st_geomfromtext</strong>：这两个函数用于将地理位置数据从一种格式转换为另一种格式。<code>st_astext</code>将地理空间数据对象转换为文本表示，而<code>st_geomfromtext</code>则将文本表示转换为地理空间数据对象[^2^]。</li>
</ul>
</li>
</ol>
<p>综上所述，ClickHouse提供了多种独特的函数，这些函数在数据处理、分析和实时查询方面具有显著优势。对于希望从Hive迁移到ClickHouse的用户来说，理解和充分利用这些函数将大大提升数据处理能力和效率。</p>
<h2 id="二十、数据湖"><a href="#二十、数据湖" class="headerlink" title="二十、数据湖"></a>二十、数据湖</h2><h3 id="1-什么是数据湖？数据湖和数仓的区别"><a href="#1-什么是数据湖？数据湖和数仓的区别" class="headerlink" title="1. 什么是数据湖？数据湖和数仓的区别"></a>1. 什么是数据湖？数据湖和数仓的区别</h3><p>数仓的缺点：</p>
<img src="image-20240821114617159.png" alt="image-20240821114617159" style="zoom:67%;">

<p>数据湖是一个集中式存储库，允许存储所有结构化和非结构化的数据</p>
<p><img src="image-20240821113535631.png" alt="image-20240821113535631"></p>
<h3 id="2-数据湖的产品"><a href="#2-数据湖的产品" class="headerlink" title="2. 数据湖的产品"></a>2. 数据湖的产品</h3><p>Delta Lake，Apache Iceberg，Apache Hudi</p>
<h3 id="3-数据湖可以代替数仓吗？什么是湖仓一体？"><a href="#3-数据湖可以代替数仓吗？什么是湖仓一体？" class="headerlink" title="3. 数据湖可以代替数仓吗？什么是湖仓一体？"></a>3. 数据湖可以代替数仓吗？什么是湖仓一体？</h3><p>数据湖和数仓的关系，是一种相互补充的关系，生产落地的场景数据湖会作为上游存在，然后接入数仓，即为湖仓一体。</p>
<p><img src="image-20240821113859870.png" alt="image-20240821113859870"> </p>
<h2 id="二十、Java八股文"><a href="#二十、Java八股文" class="headerlink" title="二十、Java八股文"></a>二十、Java八股文</h2><h3 id="1-JVM、JRE、JDK三者关系？"><a href="#1-JVM、JRE、JDK三者关系？" class="headerlink" title="1. JVM、JRE、JDK三者关系？"></a>1. JVM、JRE、JDK三者关系？</h3><p><img src="Snipaste_2024-08-02_12-49-53-172545400944856.png" alt="Snipaste_2024-08-02_12-49-53"></p>
<h3 id="2-⭐Java的八种基本数据类型"><a href="#2-⭐Java的八种基本数据类型" class="headerlink" title="2. ⭐Java的八种基本数据类型"></a>2. ⭐Java的八种基本数据类型</h3><p>java支持基本数据类型和引用数据类型。基本数据类型有八种，</p>
<p><img src="Snipaste_2024-08-02_12-58-33-172545400944857.png" alt="Snipaste_2024-08-02_12-58-33"></p>
<p>java八种基本数据类型的字节数:1字节(byte、boolean)、 2字节(short、char)、4字节(int、float)、8字节（long、double）</p>
<h3 id="3-数据类型的转换方式"><a href="#3-数据类型的转换方式" class="headerlink" title="3. 数据类型的转换方式"></a>3. 数据类型的转换方式</h3><p><img src="Snipaste_2024-08-02_13-01-41-172545400944858.png" alt="Snipaste_2024-08-02_13-01-41"></p>
<h3 id="4-装箱和拆箱是什么？"><a href="#4-装箱和拆箱是什么？" class="headerlink" title="4. 装箱和拆箱是什么？"></a>4. 装箱和拆箱是什么？</h3><p>是将基本数据类型和对应的包装类之间进行转换的过程。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//手动装箱</span></span><br><span class="line"><span class="type">Integer</span> <span class="variable">integer</span> <span class="operator">=</span> Integer.valueOf(<span class="number">100</span>);</span><br><span class="line"><span class="comment">//手动拆箱</span></span><br><span class="line"><span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> integer.intValue();</span><br><span class="line"></span><br><span class="line"><span class="comment">//自动装箱</span></span><br><span class="line"><span class="type">Integer</span> <span class="variable">integer1</span> <span class="operator">=</span> <span class="number">100</span>;</span><br><span class="line"><span class="comment">//自动拆箱</span></span><br><span class="line"><span class="type">int</span> <span class="variable">value</span> <span class="operator">=</span> integer1;</span><br></pre></td></tr></table></figure>

<p>基本数据类型和String类型之间的转换</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//基本数据类型转换为String类型</span></span><br><span class="line"><span class="type">int</span> <span class="variable">i1</span> <span class="operator">=</span> <span class="number">2</span>;</span><br><span class="line"><span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> i1 + <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//String类型转换为基本数据类型</span></span><br><span class="line"><span class="type">String</span> <span class="variable">s1</span> <span class="operator">=</span> <span class="string">&quot;321&quot;</span>;</span><br><span class="line"><span class="type">int</span> <span class="variable">i2</span> <span class="operator">=</span> Integer.parseInt(s1);</span><br></pre></td></tr></table></figure>

<h3 id="5-⭐java面向对象的三大特征"><a href="#5-⭐java面向对象的三大特征" class="headerlink" title="5. ⭐java面向对象的三大特征"></a>5. ⭐java面向对象的三大特征</h3><p><img src="Snipaste_2024-08-02_13-38-56-172545400944859.png" alt="Snipaste_2024-08-02_13-38-56"></p>
<p><strong>封装</strong>：主要指对象的属性隐藏在对象内部，不允许外部程序直接访问对象内部属性，而是通过该类提供的方法来实现对内部信息的操作和访问。</p>
<p><strong>继承</strong>：继承就是子类extend父类，可以提高代码的复用性，父类中可以体现所有子类的共同特征，可以从父类中派生出更多的子类，并且在父类的基础上进行扩展</p>
<p><strong>多态</strong>：父类的变量指向子类的对象的形式被称为多态引用，当编译时类型和运行时类型不一致，就会出现多态性。</p>
<h3 id="6-⭐方法的重载和重写有什么区别？"><a href="#6-⭐方法的重载和重写有什么区别？" class="headerlink" title="6. ⭐方法的重载和重写有什么区别？"></a>6. ⭐方法的重载和重写有什么区别？</h3><p>重载（Overload）：在同一个类中，拥有两个或多个<strong>方法名相同</strong>但是<strong>参数列表不同</strong>的方法，参数列表不同指的是<strong>参数类型不同</strong>或者<strong>参数个数不同</strong>。</p>
<p>重写（Override）：指的是子类重写父类的方法，方法名、参数列表和返回值类型必须与父类中的方法一致。</p>
<h3 id="7-说一说final关键字"><a href="#7-说一说final关键字" class="headerlink" title="7. 说一说final关键字"></a>7. 说一说final关键字</h3><ul>
<li>final修饰<strong>基本数据类型</strong>的变量，表示该变量的<strong>数据值</strong>不能被修改，</li>
<li>final修饰<strong>引用数据类型</strong>的变量，表示该变量的<strong>地址值</strong>不能被修改，即该变量不能再指向新的对象，</li>
<li>final修饰<strong>方法</strong>，表示该方法不能被重写</li>
<li>final修饰<strong>类</strong>，表示该类不能被继承</li>
</ul>
<h3 id="8-抽象方法和抽象类"><a href="#8-抽象方法和抽象类" class="headerlink" title="8. 抽象方法和抽象类"></a>8. 抽象方法和抽象类</h3><ul>
<li>抽象方法：没有方法体实现的方法</li>
<li>抽象类：包含抽象方法的抽象类，或者创建实例对象没有意义的父类也应该设计为抽象类</li>
</ul>
<h3 id="9-抽象类和普通类的区别"><a href="#9-抽象类和普通类的区别" class="headerlink" title="9. 抽象类和普通类的区别"></a>9. 抽象类和普通类的区别</h3><ul>
<li>实例化：普通类可以直接实例化对象，而抽象类不能被实例化，只能被继承</li>
<li>final修饰：抽象类不能被final修饰，普通类可以</li>
<li>方法重写：子类继承抽象类之后，如果子类不再是抽象类，那么子类必须重写抽象父类的所有抽象方法</li>
</ul>
<h3 id="10-抽象类和接口的区别"><a href="#10-抽象类和接口的区别" class="headerlink" title="10. 抽象类和接口的区别"></a>10. 抽象类和接口的区别</h3><p>抽象类（abstract class）：用于描述类的共同特征和行为，具有抽象性。可以有属性，方法和构造器。</p>
<p>接口（interface）：用于定义行为规范，只能有静态常量和抽象方法，接口可以被多实现</p>
<p>区别：</p>
<ul>
<li>实现方式：抽象类本质上也是类，类和类之间是单继承（class A extends 类B），接口和接口之间是多继承（interface A extends 接口B，接口C），类和接口之间是多实现（class A implements 接口B，接口C）</li>
<li>方法方式：接口中的方法都是抽象方法，抽象类中的方法可以是抽象方法也可以不是抽象方法</li>
<li>属性方式：接口中的属性只能是静态变量，抽象类中的属性则不是</li>
</ul>
<h3 id="11-非静态内部类和静态内部类的区别？"><a href="#11-非静态内部类和静态内部类的区别？" class="headerlink" title="11. 非静态内部类和静态内部类的区别？"></a>11. 非静态内部类和静态内部类的区别？</h3><p>略</p>
<p><img src="Snipaste_2024-08-02_22-23-59-172545400944860.png" alt="Snipaste_2024-08-02_22-23-59"></p>
<h3 id="12-⭐深拷贝和浅拷贝的区别？"><a href="#12-⭐深拷贝和浅拷贝的区别？" class="headerlink" title="12. ⭐深拷贝和浅拷贝的区别？"></a>12. ⭐深拷贝和浅拷贝的区别？</h3><img src="1720683675376-c5af6668-4538-479f-84e8-42d4143ab101.png" alt="1720683675376-c5af6668-4538-479f-84e8-42d4143ab101" style="zoom:50%;">

<img src="image-20240820152643032.png" alt="image-20240820152643032" style="zoom: 80%;">

<h3 id="13-什么是泛型？"><a href="#13-什么是泛型？" class="headerlink" title="13. 什么是泛型？"></a>13. 什么是泛型？</h3><p>泛型允许类、接口和方法在定义时使用一个或多个类型参数，这些类型参数在使用时可以被指定为具体的类型，主要目的是在编译时提供更强的类型检查，并且在编译后能够保留类型信息。</p>
<h3 id="14-⭐-x3D-x3D-与equals有什么区别？"><a href="#14-⭐-x3D-x3D-与equals有什么区别？" class="headerlink" title="14. ⭐&#x3D;&#x3D;与equals有什么区别？"></a>14. ⭐&#x3D;&#x3D;与equals有什么区别？</h3><p>”&#x3D;&#x3D;“比较的是变量中存储的值是否相等，可以用于判断基本数据类型变量（比较的就是变量值），也可以用于判断引用数据类型（比较的就是对象的首地址）。</p>
<p>equals方法用于引用数据类型（两个对象）比较两者的内容是否相等，不重写equals方法的话，默认实现的equals方法和”&#x3D;&#x3D;“的效果一样。重写equals方法希望判断的是两个对象的属性内容是否相等。</p>
<p><strong>注意：</strong>Java中比较两个字符串是否相等还可以用String类中的compareTo()方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">str1</span> <span class="operator">=</span> <span class="string">&quot;hello&quot;</span>;</span><br><span class="line"><span class="type">String</span> <span class="variable">str2</span> <span class="operator">=</span> <span class="string">&quot;hello&quot;</span>;</span><br><span class="line"><span class="type">int</span> <span class="variable">result</span> <span class="operator">=</span> str1.compareTo(str2);<span class="comment">//当两个字符串的值相同时，返回值为0；当第一个字符串小于第二个字符串时，返回值为负数；当第一个字符串大于第二个字符串的时候，返回值为正数。</span></span><br></pre></td></tr></table></figure>

<h3 id="15-重写equals方法为什么必须重写hashCode方法"><a href="#15-重写equals方法为什么必须重写hashCode方法" class="headerlink" title="15. 重写equals方法为什么必须重写hashCode方法"></a>15. 重写equals方法为什么必须重写hashCode方法</h3><p>如果两个对象调用equals方法返回true，那么要求这两个对象的hashCode值一定是相等的，</p>
<p>如果两个对象的hashCode值不相等，那么这两个对象调用equals方法一定是false</p>
<p>如果两个对象的hashCode值相等，那么这两个对象调用equals方法可能是true，也可能是false</p>
<p><strong>所以，如果重写equals方法但没有重写hashCode方法，会导致equals方法判断的两个对象，hashCode值却不相等。</strong></p>
<h3 id="16-⭐StringBuffer和StringBuilder区别是什么？"><a href="#16-⭐StringBuffer和StringBuilder区别是什么？" class="headerlink" title="16. ⭐StringBuffer和StringBuilder区别是什么？"></a>16. ⭐StringBuffer和StringBuilder区别是什么？</h3><p>String类对象是不可变的，一旦修改就会产生新对象，如果涉及频繁修改或拼接效率会很低。</p>
<p>StringBuffer和StringBuilder是可变字符序列，提供了append方法，可以将字符串添加到已有序列的末位或指定位置。</p>
<p>StringBuffer是线程安全的，StringBuilder是线程不安全的。</p>
<p><strong>操作少量的数据使用String，</strong></p>
<p><strong>单线程操作大量数据使用StringBuilder，</strong></p>
<p><strong>多线程操作大量数据使用StringBuffer</strong></p>
<h3 id="17-⭐java中有哪些容器？"><a href="#17-⭐java中有哪些容器？" class="headerlink" title="17. ⭐java中有哪些容器？"></a>17. ⭐java中有哪些容器？</h3><p><img src="Snipaste_2024-08-02_23-39-55-172545400944867.png" alt="Snipaste_2024-08-02_23-39-55"></p>
<h3 id="18-数组与集合区别？"><a href="#18-数组与集合区别？" class="headerlink" title="18. 数组与集合区别？"></a>18. 数组与集合区别？</h3><p><img src="Snipaste_2024-08-02_23-43-40-172545400944862.png" alt="Snipaste_2024-08-02_23-43-40"></p>
<h3 id="19-Arraylist和LinkedList的区别，哪个集合是线程安全的？"><a href="#19-Arraylist和LinkedList的区别，哪个集合是线程安全的？" class="headerlink" title="19. Arraylist和LinkedList的区别，哪个集合是线程安全的？"></a>19. Arraylist和LinkedList的区别，哪个集合是线程安全的？</h3><p><img src="Snipaste_2024-08-02_23-46-37-172545400944863.png" alt="Snipaste_2024-08-02_23-46-37"></p>
<h3 id="20-ArrayList的扩容机制说一下？"><a href="#20-ArrayList的扩容机制说一下？" class="headerlink" title="20. ArrayList的扩容机制说一下？"></a>20. ArrayList的扩容机制说一下？</h3><p><img src="Snipaste_2024-08-02_23-48-46-172545400944864.png" alt="Snipaste_2024-08-02_23-48-46"></p>
<h3 id="21-Set集合的特点？如何实现元素去重？"><a href="#21-Set集合的特点？如何实现元素去重？" class="headerlink" title="21. Set集合的特点？如何实现元素去重？"></a>21. Set集合的特点？如何实现元素去重？</h3><p><img src="Snipaste_2024-08-02_23-25-01-172545400944865.png" alt="Snipaste_2024-08-02_23-25-01"></p>
<h3 id="22-HashSet、LinkedHashSet和TreeSet的异同？"><a href="#22-HashSet、LinkedHashSet和TreeSet的异同？" class="headerlink" title="22. HashSet、LinkedHashSet和TreeSet的异同？"></a>22. HashSet、LinkedHashSet和TreeSet的异同？</h3><p>三者都是Set接口的实现类，都能保证元素唯一，并且都不是线程安全的。</p>
<ul>
<li>HashSet：底层数据结构是哈希表，不保证元素的添加顺序</li>
<li>LinkedHashSet：底层数据结构是链表和哈希表，保证元素的添加顺序（FIFO）</li>
<li>TreeSet：底层数据结构是红黑树，支持元素自然排序和定制排序</li>
</ul>
<h3 id="23-⭐Hashtable和HashMap的区别？"><a href="#23-⭐Hashtable和HashMap的区别？" class="headerlink" title="23. ⭐Hashtable和HashMap的区别？"></a>23. ⭐Hashtable和HashMap的区别？</h3><table>
<thead>
<tr>
<th></th>
<th>HashMap</th>
<th>Hsahtable</th>
</tr>
</thead>
<tbody><tr>
<td>数据结构</td>
<td>数组+单链表+红黑树</td>
<td>数组+单链表</td>
</tr>
<tr>
<td>线程是否安全</td>
<td>线程不安全</td>
<td>线程安全</td>
</tr>
<tr>
<td>效率</td>
<td>效率高</td>
<td>效率低，已经被淘汰</td>
</tr>
<tr>
<td>是否能存null key或null value</td>
<td>可以</td>
<td>不可以</td>
</tr>
<tr>
<td>初始容量及扩容大小</td>
<td>初始16，之后每次扩为原来的2倍</td>
<td>初始11，之后每次扩为原来的2n+1</td>
</tr>
</tbody></table>
<h3 id="24-⭐HashMap和TreeMap区别？"><a href="#24-⭐HashMap和TreeMap区别？" class="headerlink" title="24. ⭐HashMap和TreeMap区别？"></a>24. ⭐HashMap和TreeMap区别？</h3><table>
<thead>
<tr>
<th></th>
<th>HashMap</th>
<th>TreeMap</th>
</tr>
</thead>
<tbody><tr>
<td>数据结构</td>
<td>数组+单链表+红黑树</td>
<td>基于红黑树实现</td>
</tr>
<tr>
<td>性能</td>
<td>高，get，put等基本操作都是O(1)的性能</td>
<td>低，大多数操作add()，remove()都是O(logn)的性能</td>
</tr>
<tr>
<td>线程是否安全</td>
<td>线程不安全</td>
<td>线程不安全</td>
</tr>
<tr>
<td>元素顺序</td>
<td>无序</td>
<td>有序（默认自然升序）</td>
</tr>
<tr>
<td>应用场景</td>
<td>适合用于在Map中插入、删除和定位元素</td>
<td>适合于按自然顺序或自定义顺序遍历键的场景</td>
</tr>
</tbody></table>
<h3 id="25-⭐HashMap和HashSet的区别"><a href="#25-⭐HashMap和HashSet的区别" class="headerlink" title="25. ⭐HashMap和HashSet的区别"></a>25. ⭐HashMap和HashSet的区别</h3><p><img src="Snipaste_2024-08-14_16-12-04-172545400944866.png" alt="Snipaste_2024-08-14_16-12-04"></p>
<h3 id="26-⭐⭐HashMap的实现原理？"><a href="#26-⭐⭐HashMap的实现原理？" class="headerlink" title="26. ⭐⭐HashMap的实现原理？"></a>26. ⭐⭐HashMap的实现原理？</h3><p>创建HashMap实例以后，不马上初始化数组，当首次添加（key，value）的时候，底层会初始化数组<code>Node[] table = new Node[16]</code>，数组默认的初始容量为16，默认加载因为为0.75，当插入元素个数达到<code>数组长度*加载因子</code>且插入位置的数据不为null（也就是插入该数据会插在链表上），就扩容为原来的两倍。</p>
<p>当调用put方法将(key1,value1)添加到当前的map中的时候：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">首先，需要调用key1所在类的hashCode()方法，计算key1对应的key1_hashcode1，key1_hashcode1经过某种算法(hash())之后，得到key1_hashcode2。</span><br><span class="line">key1_hashcode2再经过某种算法(indexFor())之后，就确定了(key1,value1)在数组table中的索引位置i。</span><br><span class="line">  1.1 如果此索引位置i的数组上没有元素，则(key1,value1)添加成功。  ----&gt;情况1</span><br><span class="line">  1.2 如果此索引位置i的数组上有元素(key2,value2),则需要继续比较key1和key2的哈希值2  ---&gt;哈希冲突</span><br><span class="line">         2.1 如果key1的哈希值2与key2的哈希值2不相同，则(key1,value1)添加成功【链表尾插法添加】。   ----&gt;情况2</span><br><span class="line">         2.2 如果key1的哈希值2与key2的哈希值2相同，则需要继续比较key1和key2的equals()。要调用key1所在类的equals(),将key2作为参数传递进去。</span><br><span class="line">               3.1 调用equals()，返回false: 则(key1,value1)添加成功【链表尾插法添加】。   ----&gt;情况3</span><br><span class="line">               3.2 调用equals()，返回true: 则认为key1和key2是相同的。默认情况下，value1替换原有的value2。</span><br><span class="line"></span><br><span class="line">说明：情况1：将(key1,value1)存放到数组的索引i的位置</span><br><span class="line">     情况2,情况3：(key1,value1)元素与现有的(key2,value2)构成单向链表结构，(key2,value2)指向(key1,value1)</span><br><span class="line"></span><br><span class="line">什么时候会使用单向链表变为红黑树：如果数组索引i位置上的元素的个数达到8，并且数组的长度达到64时，我们就将此索引i位置上</span><br><span class="line">                            的多个元素改为使用红黑树的结构进行存储。（为什么修改呢？红黑树进行put()/get()/remove()操作的时间复杂度为O(logn)，比单向链表的时间复杂度O(n)的好。性能更高。</span><br><span class="line"></span><br><span class="line">什么时候会使用红黑树变为单向链表：当使用红黑树的索引i位置上的元素的个数低于6的时候，就会将红黑树结构退化为单向链表。（为什                             么退化？因为红黑树占用空间比链表大）</span><br></pre></td></tr></table></figure>

<h3 id="27-HashMap和LinkedHashMap"><a href="#27-HashMap和LinkedHashMap" class="headerlink" title="27. HashMap和LinkedHashMap"></a>27. HashMap和LinkedHashMap</h3><p>HashMap是Map的主要实现类，线程不安全，效率高，可以添加null的key和value值，底层使用<strong>数组+单链表+红黑树</strong>结构存储。</p>
<p>LinkedHashMap是HashMap的子类，在HashMap的基础上，增加了一对双向链表，用于记录添加的元素的先后顺序。</p>
<h3 id="28-线程和进程的定义、关系、区别及优缺点？java线程和操作系统的线程有什么区别？"><a href="#28-线程和进程的定义、关系、区别及优缺点？java线程和操作系统的线程有什么区别？" class="headerlink" title="28. 线程和进程的定义、关系、区别及优缺点？java线程和操作系统的线程有什么区别？"></a>28. 线程和进程的定义、关系、区别及优缺点？java线程和操作系统的线程有什么区别？</h3><p>进程：进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。在 Java 中，当我们启动 main 函数时其实就是启动了一个 JVM 的进程，<strong>而 main 函数所在的线程就是这个进程中的一个线程，也称主线程。</strong></p>
<p>线程：比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。同类的多个线程共享进程的<strong>堆</strong>和<strong>方法区</strong>资源，但每个线程有自己的<strong>程序计数器</strong>、<strong>虚拟机栈</strong>和<strong>本地方法栈</strong>，所以系统在产生一个线程，或是在各个线程之间做切换工作时，负担要比进程小得多。例如Spark允许在Yarn上，<strong>APPMaster</strong>向另外一台<strong>NM</strong>发送启动ExecutorBackend命令，ExectorBackend启动后通过通信模块向<strong>APPMaster</strong>请求注册Executor，注册成功后，ExectorBackend创建<strong>Exector计算对象（进程）</strong>，在Executor进程中有许多task线程（spark中最小计算单元），这些线程共享Executor的内存和资源。</p>
<p>本质上java创建成雪的线程和操作系统的线程是一样的，是一对一的关系</p>
<h3 id="29-并行和并发有什么区别？"><a href="#29-并行和并发有什么区别？" class="headerlink" title="29. 并行和并发有什么区别？"></a>29. 并行和并发有什么区别？</h3><p><img src="Snipaste_2024-08-03_11-10-49-172545400944868.png" alt="Snipaste_2024-08-03_11-10-49"></p>
<h3 id="30-同步和异步的区别？"><a href="#30-同步和异步的区别？" class="headerlink" title="30. 同步和异步的区别？"></a>30. 同步和异步的区别？</h3><p><img src="Snipaste_2024-08-03_11-11-30-172545400944869.png" alt="Snipaste_2024-08-03_11-11-30"></p>
<p>消息队列的一个应用场景就是异步通信：<strong>允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们</strong></p>
<h2 id="二十一、计算机基础知识"><a href="#二十一、计算机基础知识" class="headerlink" title="二十一、计算机基础知识"></a>二十一、计算机基础知识</h2><h3 id="21-1-数据结构"><a href="#21-1-数据结构" class="headerlink" title="21.1 数据结构"></a>21.1 数据结构</h3><h4 id="21-1-1-各种数据结构介绍"><a href="#21-1-1-各种数据结构介绍" class="headerlink" title="21.1.1 各种数据结构介绍"></a>21.1.1 各种数据结构介绍</h4><h4 id="（1）数组"><a href="#（1）数组" class="headerlink" title="（1）数组"></a>（1）数组</h4><p>由相同类型的元素组成，并且用一块连续的内存来存储。可以随机访问</p>
<ul>
<li>静态数组</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//静态数组（必须指定数组的长度）</span></span><br><span class="line"><span class="type">int</span>[] ints = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">6</span>];</span><br><span class="line"><span class="type">int</span>[] a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>&#125;;</span><br><span class="line">System.out.println(a.length);<span class="comment">//6</span></span><br><span class="line">a[<span class="number">2</span>] = <span class="number">300</span>;<span class="comment">//更改数据的某个元素</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i : a) &#123;</span><br><span class="line">    System.out.print(i + <span class="string">&quot;,&quot;</span>);</span><br><span class="line">&#125;<span class="comment">//1,2,300,4,5,6</span></span><br></pre></td></tr></table></figure>

<p>Arrays工具类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span>[] arr1 = <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line"><span class="type">int</span>[] arr2 = <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">boolean</span> <span class="variable">isEquals</span> <span class="operator">=</span> Arrays.equals(arr1,arr2);<span class="comment">//比较数组中的内容是否相等</span></span><br><span class="line">System.out.println(isEquals);<span class="comment">//ture</span></span><br><span class="line"></span><br><span class="line">System.out.println(Arrays.toString(arr1));<span class="comment">//[1, 2, 3, 4, 5],不用自己再写for循环了</span></span><br><span class="line"></span><br><span class="line">Arrays.fill(arr1,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>);<span class="comment">//指定范围或位置替换（左闭右开）</span></span><br><span class="line">System.out.println(Arrays.toString(arr1));<span class="comment">//[1, 1, 10, 10, 10]</span></span><br><span class="line"></span><br><span class="line">Arrays.sort(arr1);<span class="comment">//使用快速排序实现排序，直接操作后，原arr1就变成有序的数组了</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//使用前提：当前数组必须是有序的</span></span><br><span class="line"><span class="type">int</span> <span class="variable">index</span> <span class="operator">=</span> Arrays.binarySearch(arr3,<span class="number">5</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>动态数据</li>
</ul>
<p>指的是，<strong>List</strong>接口下面的<strong>ArrayList类</strong>和<strong>Vector类</strong>（线程安全，效率低，现在已经不用了），ArrayList在初始化容量为长度为0的空数组，在添加第一个元素时再创建一个长度为10的数组，后续再添加一个新元素时，如果现有数组容量不够，则会将数组长度扩容为原来的1.5倍</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">List</span> <span class="variable">list</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ArrayList</span>();<span class="comment">//初始化，此时集合长度为0，等待插入元素</span></span><br><span class="line"><span class="type">List</span> <span class="variable">list1</span> <span class="operator">=</span> Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">9</span>,<span class="number">10</span>);<span class="comment">//初始化，规定元素内容，且自动装箱</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//增-单个元素</span></span><br><span class="line"><span class="comment">//add(Object obj)</span></span><br><span class="line">list.add(<span class="string">&quot;AA&quot;</span>);</span><br><span class="line">list.add(<span class="number">123</span>);<span class="comment">//自动装箱</span></span><br><span class="line">list.add(<span class="string">&quot;BB&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//add(int index, Object ele)</span></span><br><span class="line">list.add(<span class="number">2</span>,<span class="string">&quot;CC&quot;</span>);<span class="comment">//在索引2处插入“CC”</span></span><br><span class="line">System.out.println(list);<span class="comment">//[AA, 123, CC, BB]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//增-增加整个list</span></span><br><span class="line"><span class="type">List</span> <span class="variable">list1</span> <span class="operator">=</span> Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">list.addAll(list1);</span><br><span class="line"></span><br><span class="line"><span class="comment">//删</span></span><br><span class="line">list.remove(<span class="number">2</span>);<span class="comment">//删除索引2对应的内容</span></span><br><span class="line">list.remove(Integer.valueOf(<span class="number">2</span>));<span class="comment">//删除数据2的包装类对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//改</span></span><br><span class="line">list.set(<span class="number">2</span>,<span class="string">&quot;AAA&quot;</span>);<span class="comment">//将索引2处的元素改为&quot;AAA&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//查</span></span><br><span class="line">list.get(<span class="number">2</span>);<span class="comment">//获取索引2处的元素</span></span><br><span class="line">list.subList(<span class="number">0</span>,<span class="number">2</span>);<span class="comment">//获取从索引0到索引2的子集合</span></span><br><span class="line">list.index(<span class="string">&quot;AAA&quot;</span>);<span class="comment">//获取元素&quot;AAA&quot;第一次在集合中出现的位置索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//长度</span></span><br><span class="line">list.size();</span><br><span class="line"></span><br><span class="line"><span class="comment">//遍历三种方式</span></span><br><span class="line"><span class="comment">//一般for循环</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; list.size(); i++) &#123;</span><br><span class="line">    System.out.println(list.get(i));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//增强for循环</span></span><br><span class="line"><span class="keyword">for</span> (Object obj : list)&#123;</span><br><span class="line">    System.out.println(obj);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//使用迭代器</span></span><br><span class="line"><span class="type">Iterator</span> <span class="variable">iterator</span> <span class="operator">=</span> list.iterator();</span><br><span class="line"><span class="keyword">while</span> (iterator.hasNext())&#123;</span><br><span class="line">    System.out.println(iterator.next());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Collections工具类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">List</span> <span class="variable">list</span> <span class="operator">=</span> Arrays.asList(<span class="number">23</span>, <span class="number">23</span>, <span class="number">56</span>, <span class="number">78</span>, <span class="number">92</span>, <span class="number">13</span>, <span class="number">56</span>, <span class="number">20</span>, <span class="number">17</span>);</span><br><span class="line"><span class="comment">//反转元素</span></span><br><span class="line">Collections.reverse(list);</span><br><span class="line"></span><br><span class="line"><span class="comment">//对集合元素进行随机排序（洗牌）</span></span><br><span class="line">Collections.shuffle(list);</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据元素的自然顺序对指定 List 集合元素按升序排序</span></span><br><span class="line">Collections.sort(list);</span><br><span class="line"></span><br><span class="line"><span class="comment">//Object max(Collection)：根据元素的自然顺序，返回给定集合中的最大元素</span></span><br><span class="line"><span class="type">Comparable</span> <span class="variable">max</span> <span class="operator">=</span> Collections.max(list);</span><br><span class="line"></span><br><span class="line"><span class="comment">//int binarySearch(List list,T key)在List集合中查找某个元素的下标，但是List的元素必须是T或T的子类对象，而且必须是可比较大小的，即支持自然排序的。而且集合也事先必须是有序的，否则结果不确定。</span></span><br><span class="line"><span class="type">int</span> <span class="variable">a</span> <span class="operator">=</span> Collections.binarySearch(list,<span class="number">56</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//int frequency(Collection c，Object o)：返回指定集合中指定元素的出现次数</span></span><br><span class="line"><span class="type">int</span> <span class="variable">frequency</span> <span class="operator">=</span> Collections.frequency(list, <span class="number">23</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//void copy(List dest,List src)：将src中的内容复制到dest中</span></span><br><span class="line"><span class="type">List</span> <span class="variable">dest</span> <span class="operator">=</span> Arrays.asList(<span class="keyword">new</span> <span class="title class_">Object</span>[list.size()]);</span><br><span class="line">Collections.copy(dest,list);</span><br></pre></td></tr></table></figure>

<h4 id="（2）链表"><a href="#（2）链表" class="headerlink" title="（2）链表"></a>（2）链表</h4><p>LinkedList，方法与List接口方法类似</p>
<p>使用不连续的空间存储数据，插入和删除操作O(1)，查找和访问O(n)</p>
<ul>
<li>单链表：只有一个方向，节点只有一个后继指针next指向下一个节点，有一个不保存任何值的head节点（头节点），通过头节点可以遍历整个链表，尾节点通常指向null</li>
<li>循环链表：特殊的单链表，循环链表的尾节点不指向努力了，而是指向链表的头节点</li>
<li>双向链表：包含两个指针，一个prev指向前一个节点，一个next指向后一个节点</li>
<li>双向循环链表：最后一个节点的next指向head，而head的prev指向最后一个节点，构成一个环</li>
</ul>
<h4 id="（3）栈"><a href="#（3）栈" class="headerlink" title="（3）栈"></a>（3）栈</h4><p>先进后出，后进先出。使用Stack类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Stack&lt;Integer&gt; stack = <span class="keyword">new</span> <span class="title class_">Stack</span>&lt;&gt;();</span><br><span class="line"><span class="comment">//入栈push</span></span><br><span class="line">stack.push(<span class="number">1</span>);</span><br><span class="line">stack.push(<span class="number">2</span>);</span><br><span class="line">stack.push(<span class="number">3</span>);</span><br><span class="line"><span class="comment">//返回栈顶元素但不移除</span></span><br><span class="line"><span class="type">Integer</span> <span class="variable">peek</span> <span class="operator">=</span> stack.peek();</span><br><span class="line"><span class="comment">//返回栈顶元素且移除</span></span><br><span class="line"><span class="type">Integer</span> <span class="variable">pop</span> <span class="operator">=</span> stack.pop();</span><br><span class="line"><span class="comment">//判断栈是否为空</span></span><br><span class="line"><span class="type">boolean</span> <span class="variable">empty</span> <span class="operator">=</span> stack.isEmpty();</span><br><span class="line"><span class="comment">//元素到栈顶的位置</span></span><br><span class="line"><span class="type">int</span> <span class="variable">search</span> <span class="operator">=</span> stack.search(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<h4 id="（4）队列（使用LinkedList实现）"><a href="#（4）队列（使用LinkedList实现）" class="headerlink" title="（4）队列（使用LinkedList实现）"></a>（4）队列（使用LinkedList实现）</h4><p>先进先出，在后端进行插入即入队，在前端进行删除即出队</p>
<ul>
<li>单队列：顺序队列（数组实现），链式队列（链表实现）</li>
<li>循环队列</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用LinkedList实现队列</span></span><br><span class="line">Queue&lt;String&gt; queue = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;String&gt;();</span><br><span class="line">queue.offer(<span class="string">&quot;as&quot;</span>);<span class="comment">//使用offer方法在队尾插入元素</span></span><br><span class="line">queue.offer(<span class="string">&quot;123&quot;</span>);</span><br><span class="line">queue.offer(<span class="string">&quot;ddd&quot;</span>);</span><br><span class="line">System.out.println(queue);<span class="comment">//[as, 123, ddd]</span></span><br><span class="line"></span><br><span class="line"><span class="type">String</span> <span class="variable">peek</span> <span class="operator">=</span> queue.peek();<span class="comment">//返回队头元素，但不删除</span></span><br><span class="line">System.out.println(peek);<span class="comment">//as</span></span><br><span class="line">System.out.println(queue);<span class="comment">//[as, 123, ddd]</span></span><br><span class="line"></span><br><span class="line"><span class="type">String</span> <span class="variable">poll</span> <span class="operator">=</span> queue.poll();<span class="comment">//返回队头元素，并删除</span></span><br><span class="line">System.out.println(poll);<span class="comment">//as</span></span><br><span class="line">System.out.println(queue);<span class="comment">//[123, ddd]</span></span><br></pre></td></tr></table></figure>

<ul>
<li>双端队列：在队列的两端都可以进行插入和删除操作的队列</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用LinkedList实现双端队列</span></span><br><span class="line">Deque&lt;String &gt; deque = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;String&gt;();</span><br><span class="line">deque.offerFirst(<span class="string">&quot;sdf&quot;</span>);<span class="comment">//使用offerFirst方法在队头插入元素</span></span><br><span class="line">deque.offerFirst(<span class="string">&quot;kkk&quot;</span>);</span><br><span class="line">deque.offerLast(<span class="string">&quot;ddd&quot;</span>);<span class="comment">//使用offerLast方法在队尾插入元素</span></span><br><span class="line">deque.offerLast(<span class="string">&quot;nmn&quot;</span>);</span><br><span class="line">System.out.println(deque);<span class="comment">//[kkk, sdf, ddd, nmn]</span></span><br><span class="line"></span><br><span class="line"><span class="type">String</span> <span class="variable">first</span> <span class="operator">=</span> deque.peekFirst();<span class="comment">//返回队头元素，但不删除</span></span><br><span class="line">System.out.println(first);<span class="comment">//kkk</span></span><br><span class="line">System.out.println(deque);<span class="comment">//[kkk, sdf, ddd, nmn]</span></span><br><span class="line"></span><br><span class="line"><span class="type">String</span> <span class="variable">last</span> <span class="operator">=</span> deque.peekLast();<span class="comment">//返回队尾元素，但不删除</span></span><br><span class="line">System.out.println(last);<span class="comment">//nmn</span></span><br><span class="line">System.out.println(deque);<span class="comment">//[kkk, sdf, ddd, nmn]</span></span><br><span class="line"></span><br><span class="line"><span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> deque.pollFirst();<span class="comment">//返回队头元素，并且删除</span></span><br><span class="line">System.out.println(s);<span class="comment">//kkk</span></span><br><span class="line">System.out.println(deque);<span class="comment">//[sdf, ddd, nmn]</span></span><br><span class="line"></span><br><span class="line"><span class="type">String</span> <span class="variable">s1</span> <span class="operator">=</span> deque.pollLast();<span class="comment">//返回队尾元素，并且删除</span></span><br><span class="line">System.out.println(s1);<span class="comment">//nmn</span></span><br><span class="line">System.out.println(deque);<span class="comment">//[sdf, ddd]</span></span><br></pre></td></tr></table></figure>

<ul>
<li>优先队列：由堆来实现，在每个元素入队时，优先队列会将元素插入堆中并调整堆。在队列出队时，优先队列会返回堆顶元素并调整堆。（具体java实现看堆）</li>
</ul>
<h4 id="（5）堆"><a href="#（5）堆" class="headerlink" title="（5）堆"></a>（5）堆</h4><p>堆是一棵完全二叉树，堆的每一个节点值都大于等于（小于等于）子树中所有的节点的值</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">PriorityQueue_Exercise</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        PriorityQueue&lt;Integer&gt; priorityQueue = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;&gt;();<span class="comment">//默认小根堆</span></span><br><span class="line">        System.out.println(priorityQueue.size());<span class="comment">//0</span></span><br><span class="line">        priorityQueue.offer(<span class="number">2</span>);</span><br><span class="line">        priorityQueue.offer(<span class="number">3</span>);</span><br><span class="line">        priorityQueue.offer(<span class="number">4</span>);</span><br><span class="line">        priorityQueue.offer(<span class="number">5</span>);</span><br><span class="line">        priorityQueue.offer(<span class="number">1</span>);</span><br><span class="line">        priorityQueue.offer(<span class="number">0</span>);</span><br><span class="line">        System.out.println(priorityQueue);<span class="comment">//[0, 2, 1, 5, 3, 4],注意，返回的数组元素没有特定的顺序</span></span><br><span class="line">        System.out.println(priorityQueue.size());<span class="comment">//6</span></span><br><span class="line"></span><br><span class="line">        System.out.println(priorityQueue.poll());<span class="comment">//0出队</span></span><br><span class="line">        System.out.println(priorityQueue.poll());<span class="comment">//1出队</span></span><br><span class="line">        System.out.println(priorityQueue.poll());<span class="comment">//2出队</span></span><br><span class="line">        System.out.println(priorityQueue.poll());<span class="comment">//3出队</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        PriorityQueue&lt;Integer&gt; priorityQueue1 = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;&gt;(((o1, o2) -&gt; o2-o1));<span class="comment">//改为大根堆</span></span><br><span class="line">        System.out.println(priorityQueue1.size());<span class="comment">//0</span></span><br><span class="line">        priorityQueue1.offer(<span class="number">2</span>);</span><br><span class="line">        priorityQueue1.offer(<span class="number">3</span>);</span><br><span class="line">        priorityQueue1.offer(<span class="number">4</span>);</span><br><span class="line">        priorityQueue1.offer(<span class="number">5</span>);</span><br><span class="line">        priorityQueue1.offer(<span class="number">1</span>);</span><br><span class="line">        priorityQueue1.offer(<span class="number">0</span>);</span><br><span class="line">        System.out.println(priorityQueue1);<span class="comment">//[5, 4, 3, 2, 1, 0],注意，返回的数组元素没有特定的顺序</span></span><br><span class="line">        System.out.println(priorityQueue1.size());<span class="comment">//6</span></span><br><span class="line"></span><br><span class="line">        System.out.println(priorityQueue1.poll());<span class="comment">//5出队</span></span><br><span class="line">        System.out.println(priorityQueue1.poll());<span class="comment">//4出队</span></span><br><span class="line">        System.out.println(priorityQueue1.poll());<span class="comment">//3出队</span></span><br><span class="line">        System.out.println(priorityQueue1.poll());<span class="comment">//2出队</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="（6）树"><a href="#（6）树" class="headerlink" title="（6）树"></a>（6）树</h4><p>满二叉树：如果每一个层的节点树都达到最大值，即为满二叉树</p>
<p>完全二叉树：除最后一层外，其余层都是满的，并且最后一层是满的或者在右边缺少连续若干几点</p>
<p>平衡二叉树：可以是空树；或者它的左右两个子树的高度差绝对不超过1，并且左右两个子树都是一棵平衡二叉树。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TreeNode</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> val;</span><br><span class="line">    <span class="keyword">public</span> TreeNode left;</span><br><span class="line">    <span class="keyword">public</span> TreeNode right;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">TreeNode</span><span class="params">(<span class="type">int</span> val)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.val = val;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">TreeNode</span><span class="params">(TreeNode left, <span class="type">int</span> val, TreeNode right)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.left = left;</span><br><span class="line">        <span class="built_in">this</span>.val = val;</span><br><span class="line">        <span class="built_in">this</span>.right = right;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="21-1-2-八股文"><a href="#21-1-2-八股文" class="headerlink" title="21.1.2 八股文"></a>21.1.2 八股文</h4><h4 id="（1）数组和链表的区别？"><a href="#（1）数组和链表的区别？" class="headerlink" title="（1）数组和链表的区别？"></a>（1）数组和链表的区别？</h4><p><img src="Snipaste_2024-08-03_10-45-28-172545400944870.png" alt="Snipaste_2024-08-03_10-45-28"></p>
<h4 id="（2）队列和栈的区别？"><a href="#（2）队列和栈的区别？" class="headerlink" title="（2）队列和栈的区别？"></a>（2）队列和栈的区别？</h4><p><img src="Snipaste_2024-08-03_10-46-03-172545400944871.png" alt="Snipaste_2024-08-03_10-46-03"></p>
<h4 id="（3）如何使用两个栈实现队列？"><a href="#（3）如何使用两个栈实现队列？" class="headerlink" title="（3）如何使用两个栈实现队列？"></a>（3）如何使用两个栈实现队列？</h4><p><img src="Snipaste_2024-08-03_10-51-30-172545400944872.png" alt="Snipaste_2024-08-03_10-51-30"></p>
<h4 id="（4）排序算法原理及其时间空间复杂度"><a href="#（4）排序算法原理及其时间空间复杂度" class="headerlink" title="（4）排序算法原理及其时间空间复杂度"></a>（4）排序算法原理及其时间空间复杂度</h4><ul>
<li>冒泡排序：通过相邻的元素的比较和交换，每次将最大（或最小）的元素逐步冒泡到最后。最好O(n)，最坏O(n^2)，稳定</li>
<li>快速排序：通过选择一个基准元素，将数组划分为两个子数组，使得左子数组的元素都小于等于基准元素，右子数组的元素都大于等于基准数组，然后对子数组进行递归排序。最好情况O(nlogn)，不稳定</li>
<li>归并排序：将数组不断分割为更小的子数组，然后将子数组进行合并，合并过程中进行排序，时间复杂度：最好情况下O(nlogn)，最坏情况下O(nlogn)，平均情况下O(nlogn），稳定</li>
<li>堆排序：通过将待排序元素构建成一个最大堆（或最小堆），然后将堆顶元素与末尾元素交换，再重新调整堆，重复该过程直到排序完成。时间复杂度：最好情况下O(nlogn)，最坏情况下O(nlogn)，不稳定</li>
</ul>
<h3 id="21-2-计算机网络"><a href="#21-2-计算机网络" class="headerlink" title="21.2 计算机网络"></a>21.2 计算机网络</h3><p>暂时略</p>
<h3 id="21-3-操作系统"><a href="#21-3-操作系统" class="headerlink" title="21.3 操作系统"></a>21.3 操作系统</h3><p>暂时略</p>
<hr>
<p>第二章第三章暂时略去</p>
<hr>
<h1 id="第四章-HR面"><a href="#第四章-HR面" class="headerlink" title="第四章 HR面"></a>第四章 HR面</h1><h2 id="⭐1-自我介绍"><a href="#⭐1-自我介绍" class="headerlink" title="⭐1. 自我介绍"></a>⭐1. 自我介绍</h2><p>面试官你好，我叫王宇涵，26岁。本科就读于哈尔滨工程大学，硕士就读于大连理工大学，计算机技术专业，2025届应届生，秋招打算求职一份数仓开发或大数据开发的工作。</p>
<p>在研究生阶段一次偶然的机会阅读了Google大数据开山的三篇论文（File-System，BigTable，MapReduce），再加上我的研究方向是量化交易相关的，需要和大量股票数据和因子数据打交道，在此期间接触到了一些数仓理论和大数据计算引擎，从而对大数据行业产生浓厚的兴趣，结合研究生的课程和自学学习了很多大数据的技术组件和数仓建模理论，也做了一个离线数仓的项目。通过对项目也锻炼了我动手实操和理解业务的能力。</p>
<p>在今年三月入职快手数据平台部，担任数据研发实习生，主要工作内容是参与商业化广告流量相关业务的数仓建设，包括模型设计、ETL开发，数据同步、维度指标建设等，参与模型的性能优化，内容治理和日常维护的工作。参与了一些商业化广告流量项目的建设，比如生态体验-用户行为指标建设和CPM-广告竞价链路指标建设，还有横向的数仓模型设计（将主站的三张DWD表进行数据提炼汇总，汇总原则是数据收口在商业化范围内，供下游商业化广告数仓使用），涉及的技术栈有Hadoop（Mapreduce、HDFS、Yarn）、Spark、Hive、Clickhouse、维度建模等。</p>
<p>Hello, interviewer. My name is Wang Yuhan. I am 26 years old. I studied at Harbin Engineering University for my undergraduate degree. I am a 2025 graduate student majoring in computer technology at Dalian University of Technology. I plan to seek a job in data warehouse development or big data development <del>in the autumn recruitment</del>. During my graduate studies, I happened to read three seminal papers on Google’s big data (File-System, BigTable, MapReduce). Coupled with the fact that my research direction is related to quantitative trading and I need to deal with a large amount of stock data and factor data. During this period, I was exposed to some data warehouse theories and big data computing engines, which generated a strong interest in the big data industry. Combining graduate courses and self-study, I have learned many technical components of big data and data warehouse modeling theories. I also completed an offline data warehouse project. In March this year, I joined the Data Platform Department of Kuaishou as a data research and development intern. My main job responsibilities include participating in the construction of the data warehouse for commercial advertising traffic-related businesses, including model design, ETL development, dimension and indicator construction, etc. I also participated in performance optimization of models, content governance, and daily maintenance work. I participated in the construction of some commercial advertising traffic projects. The technical stacks involved include Hadoop (MapReduce, HDFS, Yarn), Spark, Hive, Clickhouse, dimensional modeling, etc.</p>
<p>我的大数据学习路线是：视频（快速了解技术概况和开发操作）+权威书籍（对不懂的知识点进行查缺补漏）+官方文档（把控组件最新发展和bug说明，同时作为工具书在开发过程中查阅）+个人技术博客总结（总结课程笔记和读书笔记和相关知识点，浓缩精华，记录开发过程中遇到的问题和解决办法，供他人分享）</p>
<h2 id="⭐2-介绍自己的兴趣爱好（坚持的最长的一件事情是什么？）"><a href="#⭐2-介绍自己的兴趣爱好（坚持的最长的一件事情是什么？）" class="headerlink" title="⭐2. 介绍自己的兴趣爱好（坚持的最长的一件事情是什么？）"></a>⭐2. 介绍自己的兴趣爱好（坚持的最长的一件事情是什么？）</h2><p><strong>兴趣爱好</strong></p>
<p>喜欢看书，比如数据开发类的技术书籍（《Hadoop权威指南》，《Spark权威指南》，《数据仓库工具箱》），人文社科类书籍（吴晓波《激荡四十年》，吴军《硅谷之谜》），当代史类书籍（《邓小平时代》，《他改变了中国-江泽民传》）</p>
<p>《数据仓库工具箱》这本书汇集了很多维度建模的理论，还覆盖了整个数据仓库生命周期，可以给数据仓库团队的每个成员提供有用的指导。</p>
<p>运动，乒乓球，羽毛球，或者足球类运动（体会团队合作的乐趣，有加入到学院的足球俱乐部，成员之间的配合和集体荣誉感）为什么：一方面是竞技体育的刺激性和强身健体，另一方面就是培养团队协作能力，当看到前锋进球了，即使这个球不是自己进到，但是其实是整个团队共同参与配合的成果，内心就会非常激动</p>
<p><strong>坚持的最长的一件事情</strong></p>
<p>看书，至今为止保持着每日睡前阅读的好习惯，睡前会阅读一些人文社科类书籍。闭眼后，在脑海里把今天做过的事情做一个复盘，会有所收获，不虚度光阴。</p>
<p>从读研开始周坚持打2~3次乒乒球，强身健体。</p>
<h2 id="⭐3-个人优点缺点，性格（给我一个应聘你的理由）"><a href="#⭐3-个人优点缺点，性格（给我一个应聘你的理由）" class="headerlink" title="⭐3. 个人优点缺点，性格（给我一个应聘你的理由）"></a>⭐3. 个人优点缺点，性格（给我一个应聘你的理由）</h2><p>应聘理由：具有扎实的技术基础和离线开发的方法论（方法论很重要可以提升工作效率），善于将技术和业务结合（深知技术服务于业务）</p>
<p>优点：</p>
<ul>
<li>注重细节（比如尽量搞懂每一行代码的含义，尽量搞懂每个框架的底层细节）；</li>
<li>比较勤奋，自学能力强，抗压能力强，积极乐观</li>
<li>对大数据行业热爱，对数据敏感</li>
<li>良好的团队协作能力，团队很重要，1+1 &gt; 2，乐于助人，团队合作强于单打独斗</li>
</ul>
<p>缺点：</p>
<p>对事情比较执着，我有时候会对自己的工作过于苛求，追求完美可能会花费过多的时间和精力，一个bug解决不掉可能会睡不着觉。另外，我也意识到我在某些时候过于关注细节，这可能会影响我的工作效率。</p>
<p>性格：</p>
<ul>
<li>认真负责，注重工作细节；</li>
<li>善于沟通，和下游数据分析做对接，了解业务；</li>
<li>乐于学习，保持乐观开放的心态，善于情绪把控和调节</li>
</ul>
<h2 id="4-⭐朋友对你的评价"><a href="#4-⭐朋友对你的评价" class="headerlink" title="4. ⭐朋友对你的评价"></a>4. ⭐朋友对你的评价</h2><p>身边的室友，同学，师兄师姐，导师</p>
<ol>
<li>乐于助人，开源精神，分享成果（自己有写博客）</li>
<li>乐观向上，面对困难不抱怨，善于与他人合作，善于找到问题的解决办法（实验室可能在项目方面会遇到困难，比如制作一个港资持仓信息数据挖掘的项目，我们会齐心协力搞好分工，有的搞爬虫，有的搞数据清洗，有的搞公众号开发）</li>
<li>东北人，热情，讲义气，答应别人的事情尽力做到</li>
</ol>
<h2 id="5-学习中让你成长最快的一件事，或者最难忘的（影响最大的）一件事"><a href="#5-学习中让你成长最快的一件事，或者最难忘的（影响最大的）一件事" class="headerlink" title="5. 学习中让你成长最快的一件事，或者最难忘的（影响最大的）一件事"></a>5. 学习中让你成长最快的一件事，或者最难忘的（影响最大的）一件事</h2><p>我本科刚开始接触编程时遇到了很大的困难。我对这门新语言一无所知，看着复杂的代码和错误信息，我感到很沮丧。但我没有放弃，我找了很多在线教程和书籍，不断尝试、实践。慢慢地，我开始理解了这门语言的逻辑。通过这个过程，我不仅学会了编程，更重要的是，我学会了面对困难时要有耐心和坚持。这种态度在我日后的学习和工作中都起到了很大的帮助。</p>
<h2 id="6-什么时候能到岗？"><a href="#6-什么时候能到岗？" class="headerlink" title="6. 什么时候能到岗？"></a>6. 什么时候能到岗？</h2><p>看公司安排，在当前城市随时到岗，不在当前城市还需要租房最快一周之内到岗</p>
<h2 id="⭐7-对我们公司有了解吗？为什么来我们公司工作？"><a href="#⭐7-对我们公司有了解吗？为什么来我们公司工作？" class="headerlink" title="⭐7. 对我们公司有了解吗？为什么来我们公司工作？"></a>⭐7. 对我们公司有了解吗？为什么来我们公司工作？</h2><h3 id="（1）快手"><a href="#（1）快手" class="headerlink" title="（1）快手"></a>（1）快手</h3><p>了解。</p>
<p><strong>先简单介绍公司</strong>：快手公司是一家技术驱动的互联网大厂，快手app更是短视频领域的开创者和引领者，在快手成立的12年时间内，将业务从短视频直播延申到直播电商、本地生活、企业服务、AIGC等众多领域。是一家技术领先，温暖多元的高科技互联网企业。</p>
<p><strong>为什么选择快手</strong>：</p>
<p>①<strong>技术氛围</strong>：作为技术岗位的求职者，我在牛客，脉脉等各大网站论坛上了解到了咱们快手的技术氛围非常好，会有导师带队因材施教，针对不同的岗位都开设了相应的新人训练营，各个部门也会有各种各样的新人培养项目，公开课、大讲堂等等，有近距离跟业务大佬面对面交流的机会，对于新人来说这一点非常吸引我。</p>
<p>②<strong>企业文化</strong>：首先我是快手app的老用户了，2017年就注册了。在快手中，我能看到很多小人物的感人故事，在平凡的生活中感受不平凡的自己，不仅给我带来的欢乐也带来了人间温情。就像快手的理念一样，快手服务于普通人的记录与分享，平等互惠是快手的核心价值观，每个人都值得被记录，看到更大的世界，也被更大的世界看到。这种理念我无比赞同和欣赏。</p>
<p>③<strong>职业发展</strong>：对于我要应聘的大数据研发工作而言，我觉得目前团队的技术栈和我的技术栈十分契合和匹配，之前的面试官也和我说了咱们商业化团队现在主要做广告的离线数仓，和我目前的知识储备还有求职意愿是相一致的。并且我了解到咱们快手的全新的E单轨的职级标准，以“业绩战功、能力水平、认知水平”3个维度衡量同学的成长，对于想在快手长期从事大数据研发工作的我来讲是十分向往的。</p>
<p>④<strong>公司潜力</strong>：快手公司在2023年的表现非常出色。不单单是个人感觉，这应该是业内公认的。根据查询快手电商发布的战报，2023年双11购物狂欢节期间，快手商城的GMV较大促前增长了12倍，泛货架场景GMV同比增长了340%，搜索GMV同比增长了559%。这些数据显示出快手在电商消电家居行业的强劲增长势头。而且根据财报显示，2023年三季度快手总营收同比增长20.8%至279.5亿元，经调整净利润31.7亿元；三季度作为电商淡季，快手依旧实现了营收利润的双向增长，盈利能力可见一斑。所以在广告和电商两大核心支柱的持续发力下，快手的盈利能力也在持续释放潜力，公司发展也会越来越好。</p>
<p>所以根据以上四点多维度的考察，我都认为快手是非常适合我的公司，我也相信我能为公司带来更大的价值。</p>
<p><strong>补充问题：如何看待快手的产品？如何看待竞品抖音？对比一下抖音和快手。快手未来的改进等</strong></p>
<p>如果面试官问你如何看待快手和竞品抖音，可以从以下几个角度来回答：</p>
<p>（1）. <strong>产品定位与用户群体</strong></p>
<ul>
<li><strong>快手</strong>：快手一直专注于下沉市场，拥有大量的三四线及以下城市用户，并通过真实、接地气的内容吸引了广泛的用户。快手的社区文化强调“普惠”和“老铁经济”，让用户感受到较强的归属感。</li>
<li><strong>抖音</strong>：抖音更多定位于一二线城市，内容偏向时尚、潮流，吸引了大量年轻用户和高消费能力群体。其算法精准推送和强大的流量分发能力，使得内容质量和传播速度具有极大优势。</li>
</ul>
<p>   <strong>改进点</strong>：快手可以在保持现有下沉市场优势的同时，加强对高线城市用户的渗透，推出更多适合年轻群体和都市用户的内容，提高内容的时尚性和潮流感，进一步提升品牌影响力。</p>
<p>（2）. <strong>商业化模式</strong></p>
<ul>
<li><strong>快手</strong>：快手的商业化模式比较多样化，主要依靠广告、电商带货、直播打赏等。特别是在电商方面，快手通过直播带货形成了强大的私域流量闭环，但整体用户消费力较抖音略低。</li>
<li><strong>抖音</strong>：抖音的广告收入和电商带货能力表现强劲，尤其是广告精准推送和短视频带货能力突出，其高消费群体使得商品单价和广告单价都更高。</li>
</ul>
<p>   <strong>改进点</strong>：快手可以进一步提升广告和电商的精细化运营，通过强化大数据和AI技术，提升广告投放的精准度，结合用户画像实现更高效的转化。同时可以针对高价值商品、电商消费进行精准推送，提升用户的客单价。</p>
<p>（3）. <strong>电商生态</strong></p>
<ul>
<li><strong>快手</strong>：快手的电商生态以“老铁经济”为核心，强调信任关系，通过主播与粉丝的深度互动形成稳定的带货渠道。这种模式在下沉市场非常成功，适合大宗和日常消费品的推广。</li>
<li><strong>抖音</strong>：抖音依托庞大的流量池和算法推荐，电商流量更强大，偏向打造爆款，但用户与商家之间的信任度较低，更多依靠短期流量实现销量增长。</li>
</ul>
<p>   <strong>改进点</strong>：快手可以加强其电商平台的基础设施建设，特别是商品审核、物流、售后等服务体验，以提升整体购物体验。同时，进一步推广快手小店等自有电商渠道，降低平台对第三方电商平台的依赖。</p>
<p>（4）. <strong>社区与社交属性</strong></p>
<ul>
<li><strong>快手</strong>：快手的社交性较强，用户之间的互动较为频繁，社区氛围较好，用户忠诚度高。</li>
<li><strong>抖音</strong>：抖音则更加注重内容消费，用户互动较弱，社交属性相对较弱。</li>
</ul>
<p>   <strong>改进点</strong>：快手可以在现有社交基础上，增强内容社交化，将内容消费与用户社交进一步结合，比如加强内容分享、互动评论等功能，提升用户活跃度。</p>
<p>总结：</p>
<p>快手有很强的社区和社交属性，在电商和商业化上也有不错的发展潜力，但可以通过精准化广告、电商升级和增强时尚潮流内容来扩大高价值用户群体的覆盖。在与抖音的竞争中，快手可以发挥社区粘性和私域流量优势，同时加强在高线城市的影响力，以便在多个维度上进一步提升竞争力。</p>
<h3 id="（2）小度"><a href="#（2）小度" class="headerlink" title="（2）小度"></a>（2）小度</h3><p><strong>对百度小度的了解（可以结合智慧酒店这个场景）</strong></p>
<p>关于小度旗下产品和小度公司，可以从以下几个方面进行概括：</p>
<p><strong>对小度旗下产品的了解</strong></p>
<p>小度是百度旗下基于人工智能（AI）技术的智能语音助手品牌，主要产品涵盖以下几个方面：</p>
<ol>
<li><p><strong>智能音箱与屏幕</strong>：</p>
<ul>
<li>小度智能音箱和带屏幕的“小度在家”智能设备，是小度系列的核心硬件产品。通过语音交互，它们可以实现智能家居控制、娱乐功能（如播放音乐、视频）、信息查询（如天气、新闻），并支持儿童教育功能。</li>
</ul>
</li>
<li><p><strong>智能家居生态</strong>：</p>
<ul>
<li>小度产品与其他智能家居设备深度整合，通过语音控制灯光、家电、空调等，打造智能家庭场景。这些产品依赖百度强大的AI能力，实现了自然语言处理、语音识别等智能功能。</li>
</ul>
</li>
<li><p><strong>行业解决方案</strong>：</p>
<ul>
<li>小度的AI技术不仅应用于家庭场景，还在多个行业中提供解决方案，如智慧酒店、智慧零售、智慧城市等。例如，智慧酒店通过小度设备和云平台提供语音控制、个性化服务和运营优化。</li>
</ul>
</li>
<li><p><strong>生态开放性</strong>：</p>
<ul>
<li>小度还与第三方开发者合作，通过开放API接口，构建丰富的应用生态，让外部开发者可以开发基于小度平台的应用，进一步丰富用户体验。</li>
</ul>
</li>
</ol>
<p><strong>对小度公司的了解</strong></p>
<ol>
<li><p><strong>背景与技术实力</strong>：<br>小度是百度AI技术的重要输出窗口，依托百度强大的人工智能和云计算能力，拥有全球领先的语音识别、自然语言处理、知识图谱等核心技术。小度在语音助手领域拥有广泛的用户基础，是中国领先的智能语音助手品牌之一。</p>
</li>
<li><p><strong>创新与用户体验</strong>：<br>小度公司始终致力于通过技术创新来提升用户体验，产品强调智能、便捷、个性化，覆盖了智能家居、语音交互、智能硬件等多个领域。小度不断通过升级语音识别算法和数据处理能力，来优化其语音交互的流畅度和准确性。</p>
</li>
<li><p><strong>商业化与生态发展</strong>：<br>小度不仅仅是消费级产品的提供者，还不断探索B2B市场，通过与各行业的结合推动商业化应用，比如智慧酒店、智慧教育等场景。这体现了小度公司将AI技术应用于现实问题解决的能力，并逐步形成了智能硬件、软件服务与行业解决方案三位一体的商业化模式。</p>
</li>
</ol>
<p>通过对小度产品和公司的了解，可以看出小度依靠其技术积累和生态布局，正在AI与物联网（IoT）领域中开辟更广泛的市场前景。</p>
<hr>
<p><strong>小度的产品及其应用场景：智慧酒店</strong></p>
<p>小度是百度旗下的智能语音助手，它的核心技术基于百度强大的人工智能和大数据平台，依托语音识别、自然语言处理、深度学习等技术。小度广泛应用于多个场景，如家庭智能设备、智慧城市、智慧酒店等，为用户提供更加便捷的智能体验。</p>
<p><strong>智慧酒店场景中的应用</strong></p>
<p>在智慧酒店中，小度提供了多种智能服务，极大提升了客户的入住体验以及酒店的运营效率。以下是一些典型应用场景：</p>
<ol>
<li><p><strong>智能客房体验</strong>：</p>
<ul>
<li>住客可以通过房间内的小度智能音箱与酒店设施进行交互。例如，通过语音指令控制房间的灯光、空调、窗帘等设备，完全解放了双手。这种方式不仅提升了入住的便捷性，还为用户提供了极具科技感的体验。</li>
<li>小度还可以通过语音查询酒店服务，快速安排送餐、打扫等服务。这种基于自然语言处理的语音交互，减少了传统电话沟通的等待时间。</li>
</ul>
</li>
<li><p><strong>大数据与个性化服务</strong>：</p>
<ul>
<li>通过小度的大数据分析，酒店可以根据住客的历史行为和偏好，提供个性化的服务。例如，住客上次入住时选择的房间温度、喜欢的灯光亮度、以及常点的餐饮服务都可以被记录并在下一次入住时自动应用，从而极大提高客户满意度。</li>
<li>此外，小度云平台可以分析酒店的客流量和使用习惯，为酒店管理者提供运营优化建议。例如，基于大数据分析，酒店可以调整清洁安排、优化人力资源配置、并通过动态定价策略提高入住率。</li>
</ul>
</li>
<li><p><strong>智能服务集成</strong>：</p>
<ul>
<li>小度的生态系统能够整合第三方服务，如在线支付、景区门票预定、打车服务等。这种一站式服务为住客提供了更全面的入住体验，并通过大数据实时分析，帮助酒店提升增值服务的转化率。</li>
</ul>
</li>
<li><p><strong>智慧酒店的运营管理</strong>：</p>
<ul>
<li>小度云服务平台不仅能为住客提供便利，还为酒店管理方提供智能化管理工具。通过大数据监控，酒店可以实时监测能源使用情况、设施故障情况等，实现智能化的设备管理和维护，降低运营成本。</li>
<li>同时，酒店的前台、餐饮、娱乐等部门的运营数据也可以通过小度云平台汇集并分析，为管理者提供决策支持，提升酒店整体运营效率。</li>
</ul>
</li>
</ol>
<p><strong>总结</strong>：<br>百度小度在智慧酒店场景中，通过智能语音交互和大数据分析，不仅为住客提供了便捷和个性化的服务，还为酒店管理带来了运营上的优化和成本节约。其背后的技术支持，如语音识别、大数据处理和云计算平台，充分展示了小度产品的智能化和商业价值。</p>
<p>在大数据开发工程师的岗位上，我将利用自己的技术能力，深入参与小度云平台的数据开发与分析，进一步推动智慧酒店等场景的落地应用，并为提升产品的智能化做出贡献。</p>
<h3 id="（3）科大讯飞"><a href="#（3）科大讯飞" class="headerlink" title="（3）科大讯飞"></a>（3）科大讯飞</h3><p>科大讯飞（iFLYTEK）是中国领先的人工智能企业，总部位于安徽省合肥市。公司成立于1999年，致力于人工智能核心技术的研发，特别是在语音识别、自然语言处理、智能语音合成等领域具有显著的技术优势。</p>
<p>主要产品与技术</p>
<ol>
<li><p><strong>语音识别</strong>：<br>科大讯飞的语音识别技术在中文语音识别领域具有领先地位。其语音识别系统能够准确识别多种方言和口音，并且在噪声环境下也能保持较高的识别准确率。科大讯飞的语音识别技术广泛应用于智能语音助手、语音输入法等产品中。</p>
</li>
<li><p><strong>语音合成</strong>：<br>科大讯飞提供高质量的语音合成服务，可以将文字转换成自然流畅的语音。其合成技术不仅支持中文，还支持多种外语，能够生成多种风格和音色的语音。</p>
</li>
<li><p><strong>自然语言处理（NLP）</strong>：<br>公司的自然语言处理技术涵盖了文本分析、情感分析、机器翻译等多个方面。其NLP技术在理解和生成自然语言方面表现出色，应用于智能客服、智能问答等场景中。</p>
</li>
<li><p><strong>翻译产品</strong>：<br>科大讯飞的翻译产品支持多种语言的即时翻译，包括文本翻译和语音翻译，适用于旅行、商务沟通等场景。</p>
</li>
<li><p><strong>教育产品</strong>：<br>科大讯飞在教育领域也有丰富的产品线，包括智能学习机、智能评测系统、个性化学习平台等。通过语音识别和自然语言处理技术，提供智能化的学习辅导和评估服务。</p>
</li>
<li><p><strong>智能硬件</strong>：<br>科大讯飞还推出了一系列智能硬件产品，如语音助手、智能音箱等，这些产品集成了公司的语音识别和语音合成技术，为用户提供便捷的语音交互体验。</p>
</li>
</ol>
<p>行业领先地位</p>
<ol>
<li><p><strong>技术领先</strong>：<br>科大讯飞在语音识别和语音合成领域的技术能力是全球领先的，尤其是在中文语音处理方面。公司拥有多项自主研发的核心技术和专利，技术水平在国内外具有广泛认可。</p>
</li>
<li><p><strong>市场占有率</strong>：<br>科大讯飞在国内语音识别市场占有率较高，且在国际市场也有一定的影响力。公司产品和技术被广泛应用于各类商业、教育、医疗等场景中。</p>
</li>
<li><p><strong>研发投入</strong>：<br>科大讯飞持续投入大量资源用于技术研发和创新，不断推动人工智能技术的发展。公司设有多个研发中心，并与多所高校和科研机构合作，保持技术的前沿地位。</p>
</li>
<li><p><strong>合作伙伴</strong>：<br>科大讯飞与许多知名企业和机构建立了合作关系，包括微软、华为、阿里巴巴等。通过合作，科大讯飞不断拓展其技术应用范围和市场影响力。</p>
</li>
</ol>
<p>总体来说，科大讯飞凭借其在语音技术和自然语言处理领域的优势，已经在人工智能行业中占据了领先地位。</p>
<h3 id="（4）58同城"><a href="#（4）58同城" class="headerlink" title="（4）58同城"></a>（4）58同城</h3><p>58同城（58.com）是一家中国领先的分类信息平台，成立于2005年，总部位于北京。它为用户提供多种生活服务和信息，包括招聘、二手交易、房产、汽车、生活服务等。58同城的商业模式主要基于广告和增值服务，帮助用户和企业通过平台进行交易和信息匹配。</p>
<ul>
<li><p><strong>业务范围</strong>：涵盖生活服务、房地产、汽车、招聘、二手物品、黄页等领域，几乎涵盖了生活中的各个方面。招聘信息和房产服务是58同城的重要业务板块，企业和个人可以发布需求，用户可以通过平台获取信息。</p>
</li>
<li><p><strong>技术背景</strong>：58同城拥有庞大的用户数据和信息量，依赖大数据分析、云计算、人工智能等技术来优化用户体验，并为客户提供定制化服务。大数据在业务中的应用，尤其体现在推荐系统、用户画像、广告投放精准性等方面。</p>
</li>
<li><p><strong>公司发展</strong>：58同城已在美国纽交所上市，2015年与赶集网合并，形成了一个庞大的分类信息平台。之后，58同城进一步扩展其业务，包括58到家、58同城金融、58同城房产等领域，持续创新并丰富其产品和服务。</p>
</li>
</ul>
<p>在58同城的大数据开发岗位，你可能会参与到平台海量数据的处理、数据建模、优化广告推荐算法等工作，帮助提升业务效率和用户体验。</p>
<p>如果你有具体的技术问题或想要准备面试的方向，我也可以进一步为你提供帮助。</p>
<p>58同城的优势业务主要集中在以下几个方面：</p>
<p>（1）. <strong>招聘与求职</strong></p>
<p>招聘业务是58同城的核心业务之一，平台汇集了大量的招聘信息，涵盖了从蓝领岗位到白领、管理岗位的广泛职位需求。58同城通过简历投递、面试提醒、招聘推荐等功能，为企业和求职者提供方便快捷的服务，尤其在中低端招聘市场具有显著的竞争优势。</p>
<p>（2）. <strong>房地产</strong></p>
<p>58同城在房地产信息服务方面表现突出，提供新房、二手房、租房、商铺写字楼等多种房产信息。通过与房产中介、开发商合作，平台整合了丰富的房源信息，方便用户查找房产，同时也为房产经纪公司提供了宣传和广告服务的渠道。</p>
<p>（3）. <strong>二手交易</strong></p>
<p>58同城的二手物品交易市场是平台的一大亮点。用户可以通过平台发布和查找二手手机、家具、家电、汽车等物品的信息，直接进行交易。这类业务对个人用户和小型商家都具有吸引力，因为它提供了低成本处理闲置物品和获取便宜商品的途径。</p>
<p>（4）. <strong>本地生活服务</strong></p>
<p>平台提供全面的本地生活服务信息，包括搬家、家政、维修、教育培训等各类日常生活服务。58同城通过精准的地理位置和分类筛选功能，帮助用户快速找到附近的服务提供者，并为商家提供本地化营销和推广渠道。</p>
<p>（5）. <strong>汽车服务</strong></p>
<p>58同城提供丰富的汽车信息服务，包括新车、二手车买卖、租车、以及汽车保养、维修服务。通过与汽车经销商、二手车商家的合作，58同城整合了大量汽车交易资源，帮助用户完成购车决策。</p>
<p>（6）. <strong>广告与增值服务</strong></p>
<p>58同城通过广告和增值服务获取收入，尤其是在招聘、房产、二手交易等领域为商家提供付费推广服务。利用平台的大数据分析和推荐算法，58同城能够为商家提供精准的广告投放，提升转化率。</p>
<p>这些业务让58同城在分类信息市场中占据重要位置，特别是在招聘、房地产和本地生活服务领域的市场优势非常明显。</p>
<h3 id="（5）作业帮"><a href="#（5）作业帮" class="headerlink" title="（5）作业帮"></a>（5）作业帮</h3><p>公司发展</p>
<p>作业帮产品</p>
<p>作业帮直播课</p>
<h2 id="7-⭐期望的工作城市（为什么选择这座城市）"><a href="#7-⭐期望的工作城市（为什么选择这座城市）" class="headerlink" title="7. ⭐期望的工作城市（为什么选择这座城市）"></a>7. ⭐期望的工作城市（为什么选择这座城市）</h2><h3 id="（1）北京"><a href="#（1）北京" class="headerlink" title="（1）北京"></a>（1）北京</h3><p>作为大数据开发工程师，选择在北京工作有多个明显的优势，这些优势使得北京成为大数据行业人才的热门选择。以下是一些主要的优点：</p>
<ol>
<li><strong>行业集聚</strong></li>
</ol>
<p>北京是中国的大数据产业中心之一，汇集了大量的科技公司、数据公司和互联网巨头。这里有很多领先的大数据企业和创新型公司，如百度、阿里巴巴、京东等。这些企业的集中提供了丰富的职业机会和多样的工作环境。</p>
<ol start="2">
<li><strong>丰富的资源</strong></li>
</ol>
<p>北京拥有丰富的技术资源和支持平台，包括大数据技术社区、行业研讨会和技术交流活动。作为大数据开发工程师，你可以轻松接触到最新的技术趋势和行业动态，提升自己的技术水平和职业竞争力。</p>
<ol start="3">
<li><strong>创新氛围</strong></li>
</ol>
<p>北京作为国家的科技创新中心，拥有大量的科技创新资源和高水平的研发机构。许多前沿科技项目和创新研究都在这里进行，这为大数据开发工程师提供了良好的创新环境和合作机会。</p>
<ol start="4">
<li><strong>优质的教育资源</strong></li>
</ol>
<p>北京拥有众多顶尖的高等院校和研究机构，如清华大学、北京大学、北京航空航天大学等。这些机构提供了良好的学术支持和人才培养基地，也为大数据领域的研究和发展提供了强大的学术支持。</p>
<ol start="5">
<li><strong>职业发展机会</strong></li>
</ol>
<p>在北京，作为大数据开发工程师，你可以接触到更多的高端项目和复杂的数据处理需求。这个城市的职场环境也有更多的晋升和发展机会，适合那些希望在职业生涯中不断成长和挑战自我的工程师。</p>
<ol start="6">
<li><strong>高生活质量</strong></li>
</ol>
<p>北京作为中国的首都，拥有丰富的文化活动和高水平的生活配套设施。你可以享受多样化的文化娱乐活动、丰富的美食选择和良好的生活条件，这些都为职业生活提供了更多的舒适和便利。</p>
<ol start="7">
<li><strong>政策支持</strong></li>
</ol>
<p>北京作为国家的政治、经济和科技中心，享受着政策支持和政府资源的优待。对于从事大数据领域的企业和个人，政策的支持有助于促进技术研发和项目实施。</p>
<ol start="8">
<li><strong>国际化氛围</strong></li>
</ol>
<p>北京是一个国际化的大都市，拥有多元化的工作和生活环境。对于希望在国际化的环境中工作的工程师来说，北京提供了良好的平台和机会。</p>
<p>总之，北京的科技资源丰富、行业氛围浓厚、职业发展机会多样，是大数据开发工程师理想的工作地点。</p>
<h3 id="（2）合肥"><a href="#（2）合肥" class="headerlink" title="（2）合肥"></a>（2）合肥</h3><p>作为大数据开发工程师，选择在合肥工作也有很多独特的优势。以下是一些主要的优点：</p>
<ol>
<li><strong>快速发展的科技城市</strong></li>
</ol>
<p>合肥近年来在科技和创新领域取得了显著进展，特别是在人工智能、大数据和半导体等领域。合肥拥有多个高新技术开发区和科技园区，为大数据相关的企业和项目提供了良好的发展环境。</p>
<ol start="2">
<li><strong>有力的政府支持</strong></li>
</ol>
<p>合肥市政府积极支持科技创新和产业发展，推出了多项优惠政策和扶持措施。对于大数据企业和从业人员，政府的政策支持可以提供财政补贴、税收优惠以及创业和技术研发的资助。</p>
<ol start="3">
<li><strong>丰厚的教育资源</strong></li>
</ol>
<p>合肥拥有一些知名高校，如合肥工业大学、安徽大学等，这些学校在计算机科学、人工智能和数据科学领域具有强大的科研和教育能力。高水平的教育资源为大数据行业提供了源源不断的人才支持。</p>
<ol start="4">
<li><strong>蓬勃发展的产业生态</strong></li>
</ol>
<p>合肥的产业生态系统日益完善，尤其是在大数据和人工智能领域。许多知名科技企业和创新型公司在合肥设立了研发中心或分支机构，形成了良好的产业链条和合作网络。</p>
<ol start="5">
<li><strong>相对较低的生活成本</strong></li>
</ol>
<p>相比于北上广深等一线城市，合肥的生活成本较低。这意味着你可以在享受较高薪资的同时，拥有更为实惠的生活条件，这对于追求生活质量和经济平衡的工程师来说非常有吸引力。</p>
<ol start="6">
<li><strong>良好的工作环境</strong></li>
</ol>
<p>合肥市内拥有较为清新的自然环境和优美的城市风景，适合工作之余放松心情。城市的生活节奏相对较慢，为工作和生活提供了一个平衡的环境。</p>
<ol start="7">
<li><strong>积极的创业氛围</strong></li>
</ol>
<p>合肥市的创业环境不断改善，支持创业的政策和资金逐渐到位。对于希望创业或参与创业项目的大数据开发工程师，合肥提供了良好的支持平台和机会。</p>
<ol start="8">
<li><strong>发展潜力大</strong></li>
</ol>
<p>合肥作为新兴的科技城市，未来发展潜力巨大。作为早期进入这个市场的大数据工程师，你可以参与到这座城市的科技发展和创新过程中，并在行业发展中获得更多的成长和机会。</p>
<ol start="9">
<li><strong>交通便利</strong></li>
</ol>
<p>合肥的交通网络逐渐完善，地铁、公交、以及高铁等交通设施的建设提高了城市的交通便利性。这为日常的通勤和出行提供了便利，节省了时间和成本。</p>
<p>总的来说，合肥作为大数据开发工程师的工作地，结合了政府支持、教育资源、较低的生活成本和不断发展的产业生态等优点，是一个充满机遇和潜力的城市。</p>
<h3 id="（3）上海"><a href="#（3）上海" class="headerlink" title="（3）上海"></a>（3）上海</h3><h2 id="⭐8-想要在我们公司学到什么技术？你能为公司带来什么？"><a href="#⭐8-想要在我们公司学到什么技术？你能为公司带来什么？" class="headerlink" title="⭐8. 想要在我们公司学到什么技术？你能为公司带来什么？"></a>⭐8. 想要在我们公司学到什么技术？你能为公司带来什么？</h2><p><strong>想学什么：</strong>首先就是我本职岗位大数据研发相关的数仓建模理论，大数据组件在实际生产中的运用，编码能力；其次还有真实项目的实战能力；最后就是良好的沟通能力和团队协作能力。</p>
<p><strong>能为公司带来什么：</strong></p>
<p>我在之前的学习和实践中，积累了丰富的数仓开发经验，我相信这些经验能够帮助我快速融入公司并适应工作。同时，我非常欣赏贵公司的企业文化和价值观，我确信我的能力和经验可以为贵公司带来更多的价值。如果我有幸加入贵公司，我会继续努力学习，为公司创造更大的价值。利用我学习到的知识协助商业化广告团队完成部分工作，提高团队效率。</p>
<h2 id="9-⭐你是如何学习的？如何了解先进的技术趋势？学习新技术的倾向"><a href="#9-⭐你是如何学习的？如何了解先进的技术趋势？学习新技术的倾向" class="headerlink" title="9. ⭐你是如何学习的？如何了解先进的技术趋势？学习新技术的倾向"></a>9. ⭐你是如何学习的？如何了解先进的技术趋势？学习新技术的倾向</h2><p>我的大数据学习路线是：视频（快速了解技术概况和开发操作）+权威书籍（对不懂的知识点进行查缺补漏）+官方文档（把控组件最新发展和bug说明，同时作为工具书在开发过程中查阅）+个人技术博客总结（总结课程笔记和读书笔记和相关知识点，浓缩精华，记录开发过程中遇到的问题和解决办法，供他人分享）</p>
<p>关注DataFunTalk公众号，Flink中文社区。湖仓一体（很火的Paimon，实时数据湖存储格式，），Flink流批一体化等等</p>
<h2 id="⭐10-前面了哪些公司，是否有其他offer，其他公司是否有正在走的流程？"><a href="#⭐10-前面了哪些公司，是否有其他offer，其他公司是否有正在走的流程？" class="headerlink" title="⭐10. 前面了哪些公司，是否有其他offer，其他公司是否有正在走的流程？"></a>⭐10. 前面了哪些公司，是否有其他offer，其他公司是否有正在走的流程？</h2><p>其他的公司在走流程或者只停留在技术面阶段，贵公司的流程目前是最快，也是我内心最中意的，和我的技术栈是最匹配的，base地点我也很满意。在我所有的面试中，贵公司给我的面试体验是最好的，没有之一，无论是面试专业度还是我个人的技术匹配度都是最高的。</p>
<h2 id="⭐11-为什么跨专业读研？"><a href="#⭐11-为什么跨专业读研？" class="headerlink" title="⭐11. 为什么跨专业读研？"></a>⭐11. 为什么跨专业读研？</h2><p>我本科是船舶与海洋工程，这个专业需要接触一些流体力学分析软件，还需要对大量的船舶型线数据进行分析，所以在此期间我学习了一些编程语言和数据分析方法，有时候常常为了解决一个bug熬夜寻找解决办法，从而对计算机产生了兴趣，期间也旁听了计算机学院的课程，和老师进行沟通，建议我如果对计算机方面的知识感兴趣可以跨专业读研，以后从事计算机编程方面的工作，将兴趣和工作完美结合，所以我选择了跨专业读研。</p>
<h2 id="⭐⭐12-读研期间或学习（工作，项目）期间遇到的最困难的事是什么？如何解决的？（解决方法要明确自己刻苦钻研-团队合作）"><a href="#⭐⭐12-读研期间或学习（工作，项目）期间遇到的最困难的事是什么？如何解决的？（解决方法要明确自己刻苦钻研-团队合作）" class="headerlink" title="⭐⭐12. 读研期间或学习（工作，项目）期间遇到的最困难的事是什么？如何解决的？（解决方法要明确自己刻苦钻研+团队合作）"></a>⭐⭐12. 读研期间或学习（工作，项目）期间遇到的最困难的事是什么？如何解决的？（解决方法要明确自己刻苦钻研+团队合作）</h2><p><strong>读研期间</strong>：在研一的时候，导师给我们组一个任务，通过爬取历史港资持仓交易数据制作一个港资交易数据分析平台，这其实不太容易，因为历史港资持仓交易在披露网上只能查到2017年至今港资整体持仓变化，对于每一只股票按时间顺序港资持仓变化以及不同机构对股票增减持的变化这种数据整理起来就十分麻烦，我们尝试了很多种办法，但是获取的数据要不就是不准确，要不就是不及时。某天晚上，我建议组内的成员进行层级分工，一部分人负责并行爬虫程序的编写，一部分人负责数据的整理和可视化，一部分人负责网页的设计和公众号的开发，终于在一个多月的共同努力下，成功上线了微信公众号，并且提供了准确及时的港资持仓数据，据我所知，这些数据目前在量化交易行业内也属于稀缺数据（东方财富的专业人士确认过）。通过这件事，我学到了两点：一是科研工作的严谨性，二是团队合作的重要性，直到现在，这两条品质都深深影响着我。</p>
<p><strong>工作（实习）期间</strong>：<br>整体上：刚接触业务工作的时候，可能会有一些不适应的点，对平台的工具不太熟悉，对业务理解不上去，编码不规范等等。解决办法分三方面：首先基于技术问题要主动去找mentor或者同事取经，之前哪个同事做过类似的业务，他对模型的设计表的设计是什么样的去参考，在开发中遇到了什么坑，可优化的点和资源瓶颈在哪里？有没有沉淀下来技术文档或者数据资产白皮书；其次对于业务问题，要主动和业务方（数据分析）去沟通，了解业务底层逻辑，进而在寻求评审会（DE，DA，DPM碰头）上进行有效提问和方案探讨；最后要时刻保持自驱学习的意识，在开发过程中，学习文档，多看代码，精通优化，总结经验，提炼离线数仓开发的方法论</p>
<p>具体需求上：举生态体验广告重复度和用户行为指标设计的例子。从需求梳理-到任务开发，Hive任务创建-DQC质量保障-提测-任务优化-上线运维-数据治理全流程的离线数仓方法论。其中遇到了比如SQL逻辑实现的难点，我和mentor商议解决方案（使用UDF进行逻辑简化，减少代码开发，便于业务后期迭代），标准数据集创建流程的卡点（和同事沟通工具的使用方法），业务上的不理解（和DA联系生态体验的白皮书，后续的实验细节和数据）。在整个任务提测上线结束后，需要整理数据方案文档，记录开发过程中的卡点和解决办法，并进行任务事后监控和治理工具。</p>
<p><strong>自学期间</strong>：</p>
<p>做项目中遇到的BUG</p>
<p>在搭建数仓工作流程调度器Dolphinscheduler时，发现登录UI页面输入用户名和密码后不能登录</p>
<p>（1）首先查看海豚调度器各项服务的状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 dolphinscheduler]# ./bin/status-all.sh</span><br></pre></td></tr></table></figure>

<p>发现alert-server是STOP的状态</p>
<p>（2）此时我们去查看海豚调度器的日志</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 dolphinscheduler]# cd logs/</span><br><span class="line">[root@hadoop102 logs]# ll</span><br><span class="line">总用量 42116</span><br><span class="line">-rw-r--r-- 1 root root        0 11月 23 16:57 dolphinscheduler-alert.log</span><br><span class="line">-rw-r--r-- 1 root root    15330 11月 23 22:24 dolphinscheduler-alert-server-hadoop102.out</span><br><span class="line">-rw-r--r-- 1 root root    25303 11月 23 16:57 dolphinscheduler-api.2023-11-23_16.0.log</span><br><span class="line">-rw-r--r-- 1 root root    38903 11月 23 17:51 dolphinscheduler-api.2023-11-23_17.0.log</span><br><span class="line">-rw-r--r-- 1 root root    28699 11月 23 20:57 dolphinscheduler-api.2023-11-23_20.0.log</span><br><span class="line">-rw-r--r-- 1 root root   578481 11月 23 21:41 dolphinscheduler-api.2023-11-23_21.0.log</span><br><span class="line">-rw-r--r-- 1 root root    29513 11月 23 22:25 dolphinscheduler-api.log</span><br><span class="line">-rw-r--r-- 1 root root      823 11月 23 22:24 dolphinscheduler-api-server-hadoop102.out</span><br><span class="line">-rw-r--r-- 1 root root    16938 11月 23 22:24 dolphinscheduler-logger-server-hadoop102.out</span><br><span class="line">-rw-r--r-- 1 root root   144357 11月 23 16:59 dolphinscheduler-master.2023-11-23_16.0.log</span><br><span class="line">-rw-r--r-- 1 root root   425153 11月 23 17:50 dolphinscheduler-master.2023-11-23_17.0.log</span><br><span class="line">-rw-r--r-- 1 root root    97497 11月 23 20:59 dolphinscheduler-master.2023-11-23_20.0.log</span><br><span class="line">-rw-r--r-- 1 root root 32347547 11月 23 21:59 dolphinscheduler-master.2023-11-23_21.0.log</span><br><span class="line">-rw-r--r-- 1 root root  8564475 11月 23 22:28 dolphinscheduler-master.log</span><br><span class="line">-rw-r--r-- 1 root root      823 11月 23 22:24 dolphinscheduler-master-server-hadoop102.out</span><br><span class="line">-rw-r--r-- 1 root root    73998 11月 23 16:59 dolphinscheduler-worker.2023-11-23_16.0.log</span><br><span class="line">-rw-r--r-- 1 root root    10999 11月 23 17:50 dolphinscheduler-worker.2023-11-23_17.0.log</span><br><span class="line">-rw-r--r-- 1 root root    73685 11月 23 20:59 dolphinscheduler-worker.2023-11-23_20.0.log</span><br><span class="line">-rw-r--r-- 1 root root   322045 11月 23 21:57 dolphinscheduler-worker.2023-11-23_21.0.log</span><br><span class="line">-rw-r--r-- 1 root root    91550 11月 23 22:25 dolphinscheduler-worker.log</span><br><span class="line">-rw-r--r-- 1 root root      823 11月 23 22:25 dolphinscheduler-worker-server-hadoop102.out</span><br><span class="line">-rw-r--r-- 1 root root     1581 11月 23 22:25 gc.log</span><br><span class="line">[root@hadoop102 logs]# cat dolphinscheduler-alert-server-hadoop102.out</span><br></pre></td></tr></table></figure>

<p>发现其中的错误为java.net.ConnectException Connection refusedconnect，也就是hadoop102节点的MySQL服务器连接失败。</p>
<p>（3）此时我们再去查看mysql服务器的状态是failed的，登录时发现：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# mysql -uroot -p&quot;wyhdhr19980418&quot;</span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">ERROR 2002 (HY000): Can&#x27;t connect to local MySQL server through socket &#x27;/var/lib/mysql/mysql.sock&#x27; (111)</span><br></pre></td></tr></table></figure>

<p>（4）再去查看mysql日志</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# tail -n 20 /var/log/mysqld.log </span><br><span class="line">2023-11-23T14:14:58.093935Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier</span><br><span class="line">2023-11-23T14:14:58.093937Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11</span><br><span class="line">2023-11-23T14:14:58.093939Z 0 [Note] InnoDB: Using Linux native AIO</span><br><span class="line">2023-11-23T14:14:58.094088Z 0 [Note] InnoDB: Number of pools: 1</span><br><span class="line">2023-11-23T14:14:58.094150Z 0 [Note] InnoDB: Using CPU crc32 instructions</span><br><span class="line">2023-11-23T14:14:58.094993Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk</span><br><span class="line">2023-11-23T14:14:58.099334Z 0 [Note] InnoDB: Completed initialization of buffer pool</span><br><span class="line">2023-11-23T14:14:58.100680Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread panged. See the man page of setpriority().</span><br><span class="line">2023-11-23T14:14:58.112744Z 0 [Note] InnoDB: Highest supported file format is Barracuda.</span><br><span class="line">2023-11-23T14:14:58.113537Z 0 [ERROR] InnoDB: Ignoring the redo log due to missing MLOG_CHECKPOINT between the8141 and the end 362697728.</span><br><span class="line">2023-11-23T14:14:58.113557Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error</span><br><span class="line">2023-11-23T14:14:58.716506Z 0 [ERROR] Plugin &#x27;InnoDB&#x27; init function returned error.</span><br><span class="line">2023-11-23T14:14:58.716589Z 0 [ERROR] Plugin &#x27;InnoDB&#x27; registration as a STORAGE ENGINE failed.</span><br><span class="line">2023-11-23T14:14:58.716611Z 0 [ERROR] Failed to initialize builtin plugins.</span><br><span class="line">2023-11-23T14:14:58.716613Z 0 [ERROR] Aborting</span><br><span class="line"></span><br><span class="line">2023-11-23T14:14:58.716659Z 0 [Note] Binlog end</span><br><span class="line">2023-11-23T14:14:58.716768Z 0 [Note] Shutting down plugin &#x27;CSV&#x27;</span><br><span class="line">2023-11-23T14:14:58.717111Z 0 [Note] /usr/sbin/mysqld: Shutdown complete</span><br></pre></td></tr></table></figure>

<p>可以看到：MySQL 容器报错 Ignoring the redo log due to missing MLOG_CHECKPOINT between the checkpoint xxxxxxxxx and the end xxxxxxxxx.\n</p>
<p>查网络得知，当有一些用户的机器突然断电关机或者不正常关机之后，DataEase 服务中的 MySQL 容器无法启动，之前确实因为在操作海豚调度器的时候电脑蓝屏重启了。报错原因，一般是服务器断电或者 MySQL 服务不正常结束导致的，和 MySQL redo log 有关。因为ib_logfile文件中记录些innodb引擎非常有用的信息比如说默认的innodb默认的配置信息，在未正常关闭server情况下，重启后的server不支持innodb引擎。</p>
<p>（5）解决办法：</p>
<p>进入如下目录，将ib_logfile0和ib_logfile1文件重命名（变为备份文件）或者删除</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /var/lib/mysql</span><br><span class="line">[root@hadoop102 mysql]# ll</span><br><span class="line">总用量 245616</span><br><span class="line">-rw-r-----. 1 mysql mysql       56 10月  7 15:42 auto.cnf</span><br><span class="line">-rw-------. 1 mysql mysql     1676 10月  7 15:42 ca-key.pem</span><br><span class="line">-rw-r--r--. 1 mysql mysql     1112 10月  7 15:42 ca.pem</span><br><span class="line">-rw-r--r--. 1 mysql mysql     1112 10月  7 15:42 client-cert.pem</span><br><span class="line">-rw-------. 1 mysql mysql     1676 10月  7 15:42 client-key.pem</span><br><span class="line">drwxr-x---  2 mysql mysql     4096 11月 23 16:54 dolphinscheduler</span><br><span class="line">drwxr-x---  2 mysql mysql     4096 11月 13 13:23 edu</span><br><span class="line">drwxr-x---  2 mysql mysql     4096 11月 23 13:28 edu_report</span><br><span class="line">-rw-r-----  1 mysql mysql     1204 11月 22 22:07 ib_buffer_pool</span><br><span class="line">-rw-r-----. 1 mysql mysql 79691776 11月 23 21:04 ibdata1</span><br><span class="line">-rw-r-----. 1 mysql mysql 50331648 11月 23 21:04 ib_logfile0</span><br><span class="line">-rw-r-----. 1 mysql mysql 50331648 11月 23 21:04 ib_logfile1</span><br><span class="line">-rw-r-----  1 mysql mysql 12582912 11月 23 17:48 ibtmp1</span><br><span class="line">drwxr-x---  2 mysql mysql     4096 10月 30 15:35 ke</span><br><span class="line">drwxr-x---  2 mysql mysql      302 11月 13 15:46 maxwell</span><br><span class="line">drwxr-x---. 2 mysql mysql     8192 10月  7 16:16 metastore</span><br><span class="line">drwxr-x---. 2 mysql mysql     4096 10月  7 15:42 mysql</span><br><span class="line">-rw-r-----  1 mysql mysql  2188508 11月 13 22:01 mysql-bin.000001</span><br><span class="line">-rw-r-----  1 mysql mysql 39952457 11月 14 21:46 mysql-bin.000002</span><br><span class="line">-rw-r-----  1 mysql mysql      177 11月 15 21:06 mysql-bin.000003</span><br><span class="line">-rw-r-----  1 mysql mysql      154 11月 16 19:12 mysql-bin.000004</span><br><span class="line">-rw-r-----  1 mysql mysql      177 11月 16 19:12 mysql-bin.000005</span><br><span class="line">-rw-r-----  1 mysql mysql 16285621 11月 16 22:15 mysql-bin.000006</span><br><span class="line">-rw-r-----  1 mysql mysql      177 11月 17 21:45 mysql-bin.000007</span><br><span class="line">-rw-r-----  1 mysql mysql      177 11月 21 22:24 mysql-bin.000008</span><br><span class="line">-rw-r-----  1 mysql mysql      177 11月 22 22:07 mysql-bin.000009</span><br><span class="line">-rw-r-----  1 mysql mysql      794 11月 23 16:51 mysql-bin.000010</span><br><span class="line">-rw-r-----  1 mysql mysql      190 11月 23 12:34 mysql-bin.index</span><br><span class="line">srwxrwxrwx  1 mysql mysql        0 11月 23 12:34 mysql.sock</span><br><span class="line">-rw-------  1 mysql mysql        5 11月 23 12:34 mysql.sock.lock</span><br><span class="line">drwxr-x---. 2 mysql mysql     8192 10月  7 15:42 performance_schema</span><br><span class="line">-rw-------. 1 mysql mysql     1676 10月  7 15:42 private_key.pem</span><br><span class="line">-rw-r--r--. 1 mysql mysql      452 10月  7 15:42 public_key.pem</span><br><span class="line">-rw-r--r--. 1 mysql mysql     1112 10月  7 15:42 server-cert.pem</span><br><span class="line">-rw-------. 1 mysql mysql     1680 10月  7 15:42 server-key.pem</span><br><span class="line">drwxr-x---. 2 mysql mysql     8192 10月  7 15:42 sys</span><br><span class="line">[root@hadoop102 mysql]# mv ib_logfile0 ib_logfile0.bak</span><br><span class="line">[root@hadoop102 mysql]# mv ib_logfile1 ib_logfile1.bak</span><br></pre></td></tr></table></figure>

<p>（6）启动mysql服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql]# systemctl start mysqld</span><br></pre></td></tr></table></figure>

<p>（7）此时mysql服务为active状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql]# systemctl status mysqld</span><br><span class="line">● mysqld.service - MySQL Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 四 2023-11-23 22:20:41 CST; 15min ago</span><br><span class="line">     Docs: man:mysqld(8)</span><br><span class="line">           http://dev.mysql.com/doc/refman/en/using-systemd.html</span><br><span class="line">  Process: 2837 ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid $MYSQLD_OPTS (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 2813 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 2839 (mysqld)</span><br><span class="line">    Tasks: 57</span><br><span class="line">   CGroup: /system.slice/mysqld.service</span><br><span class="line">           └─2839 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid</span><br><span class="line"></span><br><span class="line">11月 23 22:20:40 hadoop102 systemd[1]: Starting MySQL Server...</span><br><span class="line">11月 23 22:20:41 hadoop102 systemd[1]: Started MySQL Server.</span><br></pre></td></tr></table></figure>

<p>（8）接着启动并查看海豚调度器各项服务，都是RUNNING状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 dolphinscheduler]# ./bin/status-all.sh </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">====================== dolphinscheduler server config =============================</span><br><span class="line">1.dolphinscheduler server node config hosts:[  hadoop102,hadoop103,hadoop104  ]</span><br><span class="line">2.master server node config hosts:[  hadoop102  ]</span><br><span class="line">3.worker server node config hosts:[  hadoop102:default,hadoop103:default,hadoop104:default  ]</span><br><span class="line">4.alert server node config hosts:[  hadoop102  ]</span><br><span class="line">5.api server node config hosts:[  hadoop102  ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">====================== dolphinscheduler server status =============================</span><br><span class="line">node server state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hadoop102  Begin status master-server......</span><br><span class="line">master-server  [  RUNNING  ]</span><br><span class="line">End status master-server.</span><br><span class="line">hadoop102  Begin status worker-server......</span><br><span class="line">worker-server  [  RUNNING  ]</span><br><span class="line">End status worker-server.</span><br><span class="line">hadoop102  Begin status logger-server......</span><br><span class="line">logger-server  [  RUNNING  ]</span><br><span class="line">End status logger-server.</span><br><span class="line">hadoop103  Begin status worker-server......</span><br><span class="line">worker-server  [  RUNNING  ]</span><br><span class="line">End status worker-server.</span><br><span class="line">hadoop103  Begin status logger-server......</span><br><span class="line">logger-server  [  RUNNING  ]</span><br><span class="line">End status logger-server.</span><br><span class="line">hadoop104  Begin status worker-server......</span><br><span class="line">worker-server  [  RUNNING  ]</span><br><span class="line">End status worker-server.</span><br><span class="line">hadoop104  Begin status logger-server......</span><br><span class="line">logger-server  [  RUNNING  ]</span><br><span class="line">End status logger-server.</span><br><span class="line">hadoop102  Begin status alert-server......</span><br><span class="line">alert-server  [  RUNNING  ]</span><br><span class="line">End status alert-server.</span><br><span class="line">hadoop102  Begin status api-server......</span><br><span class="line">api-server  [  RUNNING  ]</span><br><span class="line">End status api-server.</span><br></pre></td></tr></table></figure>



<h2 id="13-⭐⭐最有成就感和最有挫败感的事情"><a href="#13-⭐⭐最有成就感和最有挫败感的事情" class="headerlink" title="13. ⭐⭐最有成就感和最有挫败感的事情"></a>13. ⭐⭐最有成就感和最有挫败感的事情</h2><p>学校期间最有成就感的事情：学习期间的分工做港资公众号的例子；</p>
<p>工作期间最有成就感的事情：独立完成生态体验数仓项目的建设;</p>
<p>学校期间最有<strong>挫败感</strong>的事情：</p>
<p>在读研期间，最让我感到挫败的一次经历是我努力想要在一个学期内获得一等奖学金。当时我非常努力，花了大量时间学习、做科研、发表论文，还申请了专利。为了拿到这个奖学金，我几乎每天都在实验室加班，晚上也在赶进度，希望能在学术和科研上都有好的表现。当时我感觉自己已经拼尽全力，学业和科研都取得了一些成果，所以对能评上一等奖学金充满期待。然而，最终结果出来时，我却因为差了1.2分没能评上。这让我感到非常沮丧和挫败，因为我投入了那么多时间和精力。虽然没能拿到一等奖学金让我失望，但这段经历让我收获了许多。在过程中，我不仅在学术研究上打下了更加扎实的基础，还提升了自己在时间管理和压力应对方面的能力。通过撰写论文和申请专利，我学会了如何将理论应用于实践，提升了科研思维和创新能力。同时，这段经历让我更加清楚地认识到，成功不仅仅取决于结果，过程中的坚持、努力和积累同样重要。这些收获对我未来的学习和工作都有深远的影响。</p>
<h2 id="14-对我们团队有什么想了解的？"><a href="#14-对我们团队有什么想了解的？" class="headerlink" title="14. 对我们团队有什么想了解的？"></a>14. 对我们团队有什么想了解的？</h2><p>团队主要负责公司的什么业务？成员每天的工作任务？员工的业余生活？（健身，乒乒球馆羽毛球馆？）团队的工作氛围如何？</p>
<h2 id="15-与主管（monter）意见有分歧，你如何解决？"><a href="#15-与主管（monter）意见有分歧，你如何解决？" class="headerlink" title="15. 与主管（monter）意见有分歧，你如何解决？"></a>15. 与主管（monter）意见有分歧，你如何解决？</h2><p>首先我会尽可能避免这种情况在我身上发生。如果一旦发生，我会服从主管的命令，在公共场合或者会议中服从monter的安排和要求是员工的天职，如果迫不得已有一些问题需要请教，我会在事后和monter进行礼貌的沟通和交流。</p>
<h2 id="15-⭐期望和什么样的上级一起工作"><a href="#15-⭐期望和什么样的上级一起工作" class="headerlink" title="15. ⭐期望和什么样的上级一起工作"></a>15. ⭐期望和什么样的上级一起工作</h2><img src="image-20240912182716666.png" alt="image-20240912182716666" style="zoom:67%;">

<h2 id="16-⭐如何处理工作中的压力"><a href="#16-⭐如何处理工作中的压力" class="headerlink" title="16. ⭐如何处理工作中的压力"></a>16. ⭐如何处理工作中的压力</h2><img src="image-20240912182812010.png" alt="image-20240912182812010" style="zoom:67%;">

<h2 id="17-⭐让你带领一个团队完成项目，你会怎么做？"><a href="#17-⭐让你带领一个团队完成项目，你会怎么做？" class="headerlink" title="17. ⭐让你带领一个团队完成项目，你会怎么做？"></a>17. ⭐让你带领一个团队完成项目，你会怎么做？</h2><p>如果让我带领一个团队完成项目，我会从以下几个关键步骤入手，以确保项目顺利进行并达成预期目标：</p>
<p>（1）. <strong>明确目标与分工</strong></p>
<ul>
<li><strong>确定项目目标</strong>：首先，我会确保团队对项目的整体目标有清晰的认识，了解我们要实现的最终结果。设定具体的、可衡量的目标，并且与团队成员达成共识。</li>
<li><strong>分工明确</strong>：根据每个团队成员的专长和技能，将任务合理分配。确保每个人都清楚自己的职责和预期交付物。同时，我会鼓励团队成员发挥自己的优势，承担自己擅长的任务。</li>
</ul>
<p>（2）. <strong>制定详细的项目计划</strong></p>
<ul>
<li><strong>时间表与里程碑</strong>：我会制定一个详细的项目计划，明确每个阶段的任务和时间表，设定里程碑，确保进展符合预期。</li>
<li><strong>任务管理与资源分配</strong>：根据项目的优先级，合理调配资源，并建立任务的跟踪机制，确保每个任务都能按时完成。对于可能的瓶颈或风险，提前识别并制定应对策略。</li>
</ul>
<p>（3）. <strong>团队沟通与协作</strong></p>
<ul>
<li><strong>建立透明的沟通机制</strong>：定期召开团队会议，跟踪项目进展，确保每个人都有机会分享自己的工作进度和遇到的问题。我会鼓励开放的沟通，确保团队内外的信息传递高效且透明。</li>
<li><strong>跨团队协作</strong>：在需要与其他团队合作时，我会主动与相关部门建立联系，协调资源，确保各方协作顺畅。</li>
</ul>
<p>（4）. <strong>解决问题与管理风险</strong></p>
<ul>
<li><strong>主动识别和管理风险</strong>：在项目开始时，我会评估潜在的风险，并针对高风险点制定应急预案。如果遇到问题，我会迅速处理，并与团队一起寻找解决方案，防止问题扩大。</li>
<li><strong>灵活调整计划</strong>：当遇到不可预见的变化或挑战时，我会根据实际情况灵活调整项目计划，确保项目的顺利推进。</li>
</ul>
<p>（5）. <strong>激励与支持团队</strong></p>
<ul>
<li><strong>提供支持和反馈</strong>：作为团队领导，我会在每个阶段提供支持，及时为团队成员解决困难，确保他们在工作中保持高效和积极的状态。同时，我也会给予团队成员及时的反馈，认可他们的努力。</li>
<li><strong>激励与发展</strong>：我会关注团队成员的成长，给予他们机会提升技能，帮助他们在项目中学习和进步。同时，我也会通过设定阶段性目标来激励团队，为他们提供清晰的成就感。</li>
</ul>
<p>（6）. <strong>项目总结与优化</strong></p>
<ul>
<li><strong>项目复盘</strong>：项目完成后，我会带领团队进行一次全面的复盘，分析项目的成功经验和不足之处。通过总结教训和经验，不断优化团队的工作流程，为未来的项目做好准备。</li>
</ul>
<h2 id="⭐16-对应聘岗位的认识和理解（数据开发岗位）（对大数据的理解）"><a href="#⭐16-对应聘岗位的认识和理解（数据开发岗位）（对大数据的理解）" class="headerlink" title="⭐16. 对应聘岗位的认识和理解（数据开发岗位）（对大数据的理解）"></a>⭐16. 对应聘岗位的认识和理解（数据开发岗位）（对大数据的理解）</h2><img src="image-20240905111333083.png" alt="image-20240905111333083" style="zoom: 80%;">

<img src="image-20240905111401581.png" alt="image-20240905111401581" style="zoom:80%;">

<p>①利用Hive，Spark来处理千亿级的数据，支持商业化业务的数据需求</p>
<p>②基于onedata建模思路进行商业化数仓的建模实践</p>
<p>③日常报表数据的开发和维护</p>
<p>④利用OLAP技术建设快速查询和分析平台，支持商业客户的数据需求</p>
<p>以上这些和我的技术栈十分匹配</p>
<h2 id="17-在学校的成绩，排名前百分之几你们的专业有多少人？"><a href="#17-在学校的成绩，排名前百分之几你们的专业有多少人？" class="headerlink" title="17. 在学校的成绩，排名前百分之几你们的专业有多少人？"></a>17. 在学校的成绩，排名前百分之几你们的专业有多少人？</h2><p>38&#x2F;150</p>
<h2 id="18-属于哪类学生？（勤奋or聪明）"><a href="#18-属于哪类学生？（勤奋or聪明）" class="headerlink" title="18. 属于哪类学生？（勤奋or聪明）"></a>18. 属于哪类学生？（勤奋or聪明）</h2><p>我认为自己是一个比较勤奋的学生。在学习过程中，我总是努力深入探究，不满足于浅尝辄止。我坚信通过不断地努力和实践，可以不断提高自己的技能和能力。当然，我也明白聪明在学习中很重要，但我更倾向于认为勤奋、毅力和正确的学习方法同样关键。在学习中，我曾遇到过一些困难，但我通过勤奋和努力成功地解决了它们。我希望能够在贵公司发挥自己的潜力，与团队共同成长。</p>
<h2 id="⭐⭐19-实习锻炼了你的什么能力？学到了什么？"><a href="#⭐⭐19-实习锻炼了你的什么能力？学到了什么？" class="headerlink" title="⭐⭐19. 实习锻炼了你的什么能力？学到了什么？"></a>⭐⭐19. 实习锻炼了你的什么能力？学到了什么？</h2><p><img src="image-20240905105140718.png" alt="image-20240905105140718"></p>
<img src="image-20240901215259456.png" alt="image-20240901215259456" style="zoom:67%;">

<h2 id="⭐⭐20-你自己的职业规划是什么？"><a href="#⭐⭐20-你自己的职业规划是什么？" class="headerlink" title="⭐⭐20. 你自己的职业规划是什么？"></a>⭐⭐20. 你自己的职业规划是什么？</h2><p>还是在互联网和计算机领域发展，大数据开发工程师这个方向，近期目标和远期目标</p>
<img src="image-20240912181146720.png" alt="image-20240912181146720" style="zoom: 67%;">

<p><img src="image-20240912181214191.png" alt="image-20240912181214191"></p>
<p>（1）夯实基础，提升核心技术能力</p>
<p><img src="image-20240905111937712.png" alt="image-20240905111937712"></p>
<p>（2）拓宽技术栈，深入大数据处理</p>
<p><img src="image-20240905112011161.png" alt="image-20240905112011161"></p>
<p>（3）参与更加复杂的项目，提升业务理解</p>
<p><img src="image-20240905112050898.png" alt="image-20240905112050898"></p>
<p>（4）持续学习，保持学习热情，了解技术前沿</p>
<p><img src="image-20240905112149782.png" alt="image-20240905112149782"></p>
<h2 id="⭐21-找工作更看重哪些方面？"><a href="#⭐21-找工作更看重哪些方面？" class="headerlink" title="⭐21. 找工作更看重哪些方面？"></a>⭐21. 找工作更看重哪些方面？</h2><p>①<strong>技术氛围</strong>：作为技术岗位的求职者，我在牛客，脉脉等各大网站论坛上了解到了咱们快手的技术氛围非常好，会有导师带队因材施教，针对不同的岗位都开设了相应的新人训练营，各个部门也会有各种各样的新人培养项目，公开课、大讲堂等等，有近距离跟业务大佬面对面交流的机会，对于新人来说这一点非常吸引我。</p>
<p>②<strong>企业文化</strong>：首先我是快手app的老用户了，2017年就注册了。在快手中，我能看到很多小人物的感人故事，在平凡的生活中感受不平凡的自己，不仅给我带来的欢乐也带来了人间温情。就像快手的理念一样，快手服务于普通人的记录与分享，平等互惠是快手的核心价值观，每个人都值得被记录，看到更大的世界，也被更大的世界看到。这种理念我无比赞同和欣赏。</p>
<p>③<strong>职业发展</strong>：对于我要应聘的大数据研发工作而言，我觉得目前团队的技术栈和我的技术栈十分契合和匹配，之前的面试官也和我说了咱们商业化团队现在主要做广告的离线数仓，和我目前的知识储备还有求职意愿是相一致的。并且我了解到咱们快手的全新的E单轨的职级标准，以“业绩战功、能力水平、认知水平”3个维度衡量同学的成长，对于想在快手长期从事大数据研发工作的我来讲是十分向往的。</p>
<p>④<strong>公司潜力</strong>：快手公司在2023年的表现非常出色。不单单是个人感觉，这应该是业内公认的。根据查询快手电商发布的战报，2023年双11购物狂欢节期间，快手商城的GMV较大促前增长了12倍，泛货架场景GMV同比增长了340%，搜索GMV同比增长了559%。这些数据显示出快手在电商消电家居行业的强劲增长势头。而且根据财报显示，2023年三季度快手总营收同比增长20.8%至279.5亿元，经调整净利润31.7亿元；三季度作为电商淡季，快手依旧实现了营收利润的双向增长，盈利能力可见一斑。所以在广告和电商两大核心支柱的持续发力下，快手的盈利能力也在持续释放潜力，公司发展也会越来越好。</p>
<h2 id="22-是否接受加班？"><a href="#22-是否接受加班？" class="headerlink" title="22. 是否接受加班？"></a>22. 是否接受加班？</h2><p>我理解加班是职场中常见的情况，但我的工作态度是在正常工作时间内尽可能高效地完成任务，并保证工作质量。当有特殊情况需要加班时，我会与团队成员积极沟通，共同协作完成。</p>
<p>我注重工作与生活的平衡，加班不会是常态，但我有能力在必要时高效地完成任务。请问加班的具体情境和频率是怎样的？</p>
<h2 id="⭐23-反问面试官？"><a href="#⭐23-反问面试官？" class="headerlink" title="⭐23. 反问面试官？"></a>⭐23. 反问面试官？</h2><p><strong>面试的话，除了技术以外，还需要候选人具备什么样的能力和素质？</strong></p>
<p><strong>咱们部门有多少人？都负责哪几类业务，离线和实时是分团队开发吗？我入职之后的具体工作是什么？</strong></p>
<p><strong>您对于我这个职位的期望是什么？我想参考确定未来努力的目标和方向</strong></p>
<p><strong>作为校招生，咱们公司新人培养方案</strong></p>
<p><strong>我大概什么时候能得到贵公司的回复？</strong></p>
<hr>
<p><strong>您对我本次面试的表现如何？有哪些不足？希望您给我一些建议。</strong></p>
<p>公司数仓业务线的技术栈是什么？</p>
<p><strong>公司希望我入职后能给公司解决什么样的问题？</strong></p>
<p><strong>我入职公司后具体的工作任务是什么？</strong></p>
<p><strong>入职之后会有培训吗？</strong></p>
<p><strong>一周工作几天，几点上下班，是否双休？是否加班？上下班是否打卡？</strong></p>
<p><strong>团队主要负责公司的什么业务？成员每天的工作任务？员工的业余生活？（健身，乒乒球馆羽毛球馆？）团队的工作氛围如何？</strong></p>
<p>实习是否提供转正机会？</p>
<p><strong>薪资问题（薪资构成，是否有补助）</strong>【听到之后，接着回答：“了解了解，那还是不错的，和我期望的一致”】</p>
<p>加hr微信，方便后续沟通</p>
<h2 id="⭐24-最后的彩虹屁"><a href="#⭐24-最后的彩虹屁" class="headerlink" title="⭐24.最后的彩虹屁"></a>⭐24.最后的彩虹屁</h2><p>最后：谢谢，我大概什么时候能够得到贵公司的回复。</p>
<p>感谢快手给我的面试机会，很期待早日成为公司的一员。</p>
<h1 id="第六章-常考SQL"><a href="#第六章-常考SQL" class="headerlink" title="第六章 常考SQL"></a>第六章 常考SQL</h1><p>一切一切的前提：熟悉Hive Sql语法，常用函数，join，开窗函数，炸裂函数等等，在此基础上要见各种常见必考题型，起码也要会思想，就算不会写，描述也得描述出来</p>
<h2 id="1-distinct多字段去重，只写到第一个字段前面"><a href="#1-distinct多字段去重，只写到第一个字段前面" class="headerlink" title="1. distinct多字段去重，只写到第一个字段前面"></a>1. distinct多字段去重，只写到第一个字段前面</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select distinct a, b</span><br><span class="line">from temp;</span><br></pre></td></tr></table></figure>

<h2 id="2-去重三种方法"><a href="#2-去重三种方法" class="headerlink" title="2. 去重三种方法"></a>2. 去重三种方法</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">-- 直接用distinct</span><br><span class="line">select distinct a,b</span><br><span class="line">from temp;</span><br><span class="line">-- 使用group by</span><br><span class="line">select a,b</span><br><span class="line">from temp</span><br><span class="line">group by a,b;</span><br><span class="line">-- 使用窗口函数</span><br><span class="line">with t1 as (</span><br><span class="line">	select a,b,</span><br><span class="line">           row_number() over(partition by a,b order by c) rn</span><br><span class="line">    from temp</span><br><span class="line">)</span><br><span class="line">select a,b</span><br><span class="line">from temp</span><br><span class="line">where rn = 1;</span><br></pre></td></tr></table></figure>

<h2 id="3-关于聚合函数"><a href="#3-关于聚合函数" class="headerlink" title="3. 关于聚合函数"></a>3. 关于聚合函数</h2><p>count(*)，表示统计所有行数，包含null值，对表计数；同count(1)</p>
<p>count(某列)，表示该列一共有多少行，不包含null值，是对列计数，涉及字段的筛选，以及数据序列化和反序列化；</p>
<p>max()，求最大值，不包含null，除非所有值都是null；</p>
<p>min()，求最小值，不包含null，除非所有值都是null；</p>
<p>sum()，求和，不包含null。</p>
<p>avg()，求平均值，不包含null。</p>
<h2 id="4-查找重复数据"><a href="#4-查找重复数据" class="headerlink" title="4. 查找重复数据"></a>4. 查找重复数据</h2><p>题目：查找所有重复的学生姓名：思路按姓名分组，并count()，计数大于1即为重复的姓名</p>
<img src="Snipaste_2023-12-08_13-29-23.png" alt="Snipaste_2023-12-08_13-29-23" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> `姓名`</span><br><span class="line"><span class="keyword">from</span> `查找重复数据`</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> `姓名`</span><br><span class="line"><span class="keyword">having</span> <span class="built_in">count</span>(`姓名`) <span class="operator">&gt;</span> <span class="number">1</span>;</span><br><span class="line"># 万能模板</span><br><span class="line"><span class="keyword">select</span> 列名</span><br><span class="line"><span class="keyword">from</span> 表名</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> 列名</span><br><span class="line"><span class="keyword">having</span> <span class="built_in">count</span>(列名) <span class="operator">&gt;</span> <span class="number">1</span>; # 查找重复出现N次 <span class="keyword">having</span> <span class="built_in">count</span>(列名) <span class="operator">=</span> N;</span><br></pre></td></tr></table></figure>

<h2 id="5-SQL语句的执行顺序"><a href="#5-SQL语句的执行顺序" class="headerlink" title="5. SQL语句的执行顺序"></a>5. SQL语句的执行顺序</h2><ul>
<li>from</li>
<li>where</li>
<li>group by </li>
<li>having</li>
<li>select</li>
<li>order by </li>
<li>limit</li>
</ul>
<h2 id="6-快递量区间分布（case，when）"><a href="#6-快递量区间分布（case，when）" class="headerlink" title="6. 快递量区间分布（case，when）"></a>6. 快递量区间分布（case，when）</h2><img src="Snipaste_2023-12-08_13-58-32.png" alt="Snipaste_2023-12-08_13-58-32" style="zoom:33%;">

<p>现在查询运单号创建在5月、不同单量区间的客户分布</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 步骤<span class="number">1</span>：<span class="number">5</span>月份每个客户的单量</span><br><span class="line"><span class="keyword">select</span> `客户id`,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">distinct</span> `运单号`) `单量`</span><br><span class="line"><span class="keyword">from</span> `<span class="number">4</span>_6`</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">month</span>(`创建时间`) <span class="operator">=</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> `客户id`;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_14-15-17.png" alt="Snipaste_2023-12-08_14-15-17" style="zoom: 33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 步骤<span class="number">2</span>：对单量进行区间分组</span><br><span class="line"># <span class="keyword">case</span> when...then...end ,首先注意就是<span class="keyword">end</span>不要忘写，整个语句结束之后要给该字段起一个名称</span><br><span class="line"><span class="keyword">select</span> `客户id`,`单量`,</span><br><span class="line">       (</span><br><span class="line">           <span class="keyword">case</span> <span class="keyword">when</span> `单量` <span class="operator">&lt;=</span> <span class="number">5</span> <span class="keyword">then</span> <span class="string">&#x27;0-5&#x27;</span></span><br><span class="line">           <span class="keyword">when</span> `单量` <span class="operator">&gt;=</span> <span class="number">6</span> <span class="keyword">and</span> `单量` <span class="operator">&lt;=</span> <span class="number">20</span> <span class="keyword">then</span> <span class="string">&#x27;11-20&#x27;</span></span><br><span class="line">           <span class="keyword">when</span> `单量` <span class="operator">&gt;=</span> <span class="number">11</span> <span class="keyword">and</span> `单量` <span class="operator">&lt;=</span> <span class="number">20</span> <span class="keyword">then</span> <span class="string">&#x27;11-20&#x27;</span></span><br><span class="line">           <span class="keyword">else</span> <span class="string">&#x27;20以上&#x27;</span></span><br><span class="line">           <span class="keyword">end</span></span><br><span class="line">           ) `单量区间`</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> `客户id`,</span><br><span class="line">             <span class="built_in">count</span>(<span class="keyword">distinct</span> `运单号`) `单量`</span><br><span class="line">      <span class="keyword">from</span> `<span class="number">4</span>_6`</span><br><span class="line">      <span class="keyword">where</span> <span class="keyword">month</span>(`创建时间`) <span class="operator">=</span> <span class="number">5</span></span><br><span class="line">      <span class="keyword">group</span> <span class="keyword">by</span> `客户id`) t1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_14-26-45.png" alt="Snipaste_2023-12-08_14-26-45" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 步骤<span class="number">3</span>：利用胆量区间分组，对客户id计数</span><br><span class="line"><span class="keyword">select</span> `单量区间`,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">distinct</span> `客户id`) `客户数`</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> `客户id`,</span><br><span class="line">                `单量`,</span><br><span class="line">                (</span><br><span class="line">                    <span class="keyword">case</span></span><br><span class="line">                        <span class="keyword">when</span> `单量` <span class="operator">&lt;=</span> <span class="number">5</span> <span class="keyword">then</span> <span class="string">&#x27;0-5&#x27;</span></span><br><span class="line">                        <span class="keyword">when</span> `单量` <span class="operator">&gt;=</span> <span class="number">6</span> <span class="keyword">and</span> `单量` <span class="operator">&lt;=</span> <span class="number">20</span> <span class="keyword">then</span> <span class="string">&#x27;11-20&#x27;</span></span><br><span class="line">                        <span class="keyword">when</span> `单量` <span class="operator">&gt;=</span> <span class="number">11</span> <span class="keyword">and</span> `单量` <span class="operator">&lt;=</span> <span class="number">20</span> <span class="keyword">then</span> <span class="string">&#x27;11-20&#x27;</span></span><br><span class="line">                        <span class="keyword">else</span> <span class="string">&#x27;20以上&#x27;</span></span><br><span class="line">                        <span class="keyword">end</span></span><br><span class="line">                    ) `单量区间`</span><br><span class="line"></span><br><span class="line">         <span class="keyword">from</span> (<span class="keyword">select</span> `客户id`,</span><br><span class="line">                      <span class="built_in">count</span>(<span class="keyword">distinct</span> `运单号`) `单量`</span><br><span class="line">               <span class="keyword">from</span> `<span class="number">4</span>_6`</span><br><span class="line">               <span class="keyword">where</span> <span class="keyword">month</span>(`创建时间`) <span class="operator">=</span> <span class="number">5</span></span><br><span class="line">               <span class="keyword">group</span> <span class="keyword">by</span> `客户id`) t1</span><br><span class="line">     ) t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> `单量区间`;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_14-30-05.png" alt="Snipaste_2023-12-08_14-30-05" style="zoom:33%;">

<h2 id="7-⭐行列互换"><a href="#7-⭐行列互换" class="headerlink" title="7. ⭐行列互换"></a>7. ⭐行列互换</h2><h3 id="group-by-sum-if"><a href="#group-by-sum-if" class="headerlink" title="group by+sum(if())"></a>group by+sum(if())</h3><p>常规做法是，group by+sum(if())</p>
<p>原始表：</p>
<img src="Snipaste_2023-12-08_14-39-12.png" alt="Snipaste_2023-12-08_14-39-12" style="zoom:33%;">

<p>行转列：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select year,                             --写case when ..then.. else..end也行</span><br><span class="line">       sum(if(month = 1, amount, 0)) m1, --sum(if(字段=值，1或对应的同行的值，0))，</span><br><span class="line">       sum(if(month = 2, amount, 0)) m2,--其中，字段为行专列后表中的除了分组字段的另外字段，值也就是它所有可能的取值</span><br><span class="line">       sum(if(month = 3, amount, 0)) m3,--这里是月份，可能取1，2，3，4</span><br><span class="line">       sum(if(month = 4, amount, 0)) m4</span><br><span class="line">from table2</span><br><span class="line">group by year;</span><br><span class="line">--group by的是行列变换后不变的那个字段</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_14-40-38.png" alt="Snipaste_2023-12-08_14-40-38" style="zoom:33%;">

<hr>
<p>原始表：</p>
<img src="Snipaste_2023-12-08_14-41-21.png" alt="Snipaste_2023-12-08_14-41-21" style="zoom:33%;">

<p>行转列：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select DDate,</span><br><span class="line">       sum(`if`(shengfu = &#x27;胜&#x27;, 1, 0)) `胜`,--这里的字段是’shengfu‘,列出所有可能取值，也就是&#x27;胜&#x27;,&#x27;负&#x27;</span><br><span class="line">       sum(`if`(shengfu = &#x27;负&#x27;, 1, 0)) `负`--由于shengfu字段后面没有再同行对应的值了，直接取1</span><br><span class="line">from table1</span><br><span class="line">group by DDate;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_14-42-14.png" alt="Snipaste_2023-12-08_14-42-14" style="zoom:33%;">

<hr>
<h3 id="聚合与炸裂"><a href="#聚合与炸裂" class="headerlink" title="聚合与炸裂"></a>聚合与炸裂</h3><p>原始表A：</p>
<img src="Snipaste_2023-12-08_14-42-55.png" alt="Snipaste_2023-12-08_14-42-55" style="zoom:33%;">

<p>原始表B：</p>
<img src="Snipaste_2023-12-08_14-43-42.png" alt="Snipaste_2023-12-08_14-43-42" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">--将tableA输出为tableB的格式；</span><br><span class="line">select qq,</span><br><span class="line">       concat_ws(&#x27;_&#x27;, collect_list(game)) game --collect_list()变数组，concat_ws()让容器中的东西用某个分隔符分割</span><br><span class="line">from tableA</span><br><span class="line">group by qq;</span><br><span class="line">--将tableB输出为tableA的格式;（使用炸裂函数，了解）</span><br><span class="line">select qq,</span><br><span class="line">       tmp.game</span><br><span class="line">from tableB lateral view explode(split(game, &#x27;_&#x27;)) tmp as game;--注意：explode只能炸裂数组字段，所以要使用split使其变为数组字段</span><br></pre></td></tr></table></figure>

<h2 id="8-⭐组内排名万能模板"><a href="#8-⭐组内排名万能模板" class="headerlink" title="8. ⭐组内排名万能模板"></a>8. ⭐组内排名万能模板</h2><table>
<thead>
<tr>
<th>成绩</th>
<th>rank()</th>
<th>dense_rank()</th>
<th>row_number()</th>
</tr>
</thead>
<tbody><tr>
<td>100</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>100</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>100</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>99</td>
<td>4</td>
<td>2</td>
<td>4</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select *,</span><br><span class="line">	  row_number() over(partition by &lt;要分组的列名&gt; order by &lt;要排序的列名&gt;) as rn</span><br><span class="line">from 表名;</span><br></pre></td></tr></table></figure>

<h3 id="去除最大值，最小值后求平均值："><a href="#去除最大值，最小值后求平均值：" class="headerlink" title="去除最大值，最小值后求平均值："></a>去除最大值，最小值后求平均值：</h3><img src="Snipaste_2023-12-08_15-53-53.png" alt="Snipaste_2023-12-08_15-53-53" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 通过正序排序和倒序排序两个字段确定最大值和最小值</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dep_id <span class="keyword">order</span> <span class="keyword">by</span> salary <span class="keyword">desc</span> ) <span class="keyword">as</span> rn_1,</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dep_id <span class="keyword">order</span> <span class="keyword">by</span> salary <span class="keyword">asc</span> ) <span class="keyword">as</span> rn_2</span><br><span class="line"><span class="keyword">from</span> window_1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_15-57-17.png" alt="Snipaste_2023-12-08_15-57-17" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 去除最大值和最小值，然后再计算各部门的平均工资</span><br><span class="line"><span class="keyword">select</span> dep_id,</span><br><span class="line">       <span class="built_in">avg</span>(salary) <span class="keyword">as</span> avg_salary</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">                <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dep_id <span class="keyword">order</span> <span class="keyword">by</span> salary <span class="keyword">desc</span> ) <span class="keyword">as</span> rn_1,</span><br><span class="line">                <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dep_id <span class="keyword">order</span> <span class="keyword">by</span> salary <span class="keyword">asc</span> )  <span class="keyword">as</span> rn_2</span><br><span class="line">         <span class="keyword">from</span> window_1</span><br><span class="line">     ) t1</span><br><span class="line"><span class="keyword">where</span> rn_1 <span class="operator">&gt;</span><span class="number">1</span> <span class="keyword">and</span> rn_2 <span class="operator">&gt;</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> dep_id;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_16-00-32.png" alt="Snipaste_2023-12-08_16-00-32" style="zoom:33%;">



<hr>
<h3 id="员工薪水中位数："><a href="#员工薪水中位数：" class="headerlink" title="员工薪水中位数："></a>员工薪水中位数：</h3><p>查询每个公司的薪水中位数，不使用任何内置函数</p>
<img src="Snipaste_2023-12-08_19-58-39.png" alt="Snipaste_2023-12-08_19-58-39" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select Id, Company, Salary</span><br><span class="line">from (</span><br><span class="line">         select Id,</span><br><span class="line">                Company,</span><br><span class="line">                Salary,</span><br><span class="line">                ROW_NUMBER() over (partition by Company order by Salary) rk, # 按照公司分组，给工资排名</span><br><span class="line">                count(*) over (partition by Company)                     cnt # 计算每个公司的员工数</span><br><span class="line">         from employee</span><br><span class="line">     ) t1</span><br><span class="line">where rk IN (FLOOR((cnt + 1) / 2), FLOOR((cnt + 2) / 2)); # 中位数，奇数个最中间的是中位数，偶数个，中间的两个都是中位数</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-15-09.png" alt="Snipaste_2023-12-08_20-15-09" style="zoom:33%;">

<hr>
<p><strong>牛客SQL217</strong> <strong>对所有员工的薪水按照salary降序进行1-N的排名</strong></p>
<p>有一个薪水表salaries简况如下:</p>
<img src="Snipaste_2023-12-20_21-30-56.png" alt="Snipaste_2023-12-20_21-30-56" style="zoom: 50%;">

<p>对所有员工的薪水按照salary降序先进行1-N的排名，如果salary相同，再按照emp_no升序排列：</p>
<img src="Snipaste_2023-12-20_21-31-27.png" alt="Snipaste_2023-12-20_21-31-27" style="zoom:50%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 本题注意点是t_rank是确定的，在窗口中不能写两个排序字段，</span><br><span class="line"># 在窗口中排序后，要再将继续排序的字段写在外面</span><br><span class="line"><span class="keyword">select</span> emp_no,</span><br><span class="line">       salary,</span><br><span class="line">       <span class="built_in">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> salary <span class="keyword">desc</span>) t_rank</span><br><span class="line"><span class="keyword">from</span> salaries</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> t_rank, emp_no;</span><br></pre></td></tr></table></figure>



<h2 id="9-⭐TopN问题"><a href="#9-⭐TopN问题" class="headerlink" title="9. ⭐TopN问题"></a>9. ⭐TopN问题</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 万能模板</span><br><span class="line"><span class="keyword">select</span><span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">          <span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="operator">&lt;</span>要分组的列名<span class="operator">&gt;</span> <span class="keyword">order</span> <span class="keyword">by</span> <span class="operator">&lt;</span>要排序的列名<span class="operator">&gt;</span>) <span class="keyword">as</span> rn</span><br><span class="line">    <span class="keyword">from</span> 表名;</span><br><span class="line">) t1</span><br><span class="line"><span class="keyword">where</span> rn <span class="operator">&lt;=</span> N</span><br></pre></td></tr></table></figure>

<p>emp1表，求出每个部门工资最高的前三名员工，并计算这些员工的工资占所属部门总工资的百分比。</p>
<img src="Snipaste_2023-12-08_20-51-29.png" alt="Snipaste_2023-12-08_20-51-29" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">--先计算每个部门工资排名和每个部门的总工资</span><br><span class="line">select *,</span><br><span class="line">       row_number() over (partition by deptno order by sal desc ) rn,--每个部门工资排名</span><br><span class="line">       sum(sal) over(partition by deptno) sum_sal  --每个部门的总工资</span><br><span class="line">from emp1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_21-05-07.png" alt="Snipaste_2023-12-08_21-05-07" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">--再取sal前三和这些员工工资占部门总工资百分比</span><br><span class="line">select empno,</span><br><span class="line">       sal,</span><br><span class="line">       deptno,</span><br><span class="line">       rn,</span><br><span class="line">       sum_sal,</span><br><span class="line">       round(sal/sum_sal,2) rate</span><br><span class="line">from (</span><br><span class="line">         select *,</span><br><span class="line">                row_number() over (partition by deptno order by sal desc ) rn,--每个部门工资排名</span><br><span class="line">                sum(sal) over (partition by deptno)                        sum_sal --每个部门的总工资</span><br><span class="line">         from emp1</span><br><span class="line">     ) t1</span><br><span class="line">where rn &lt;= 3;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_21-05-51.png" alt="Snipaste_2023-12-08_21-05-51" style="zoom:33%;">

<hr>
<p>问题：2020-12-01至今每日订单量top3的城市及其订单量(订单量对order_id去重)(在线写)</p>
<img src="Snipaste_2023-12-08_21-06-51.png" alt="Snipaste_2023-12-08_21-06-51" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--首先按照日期和城市分组得到不同从2020-12-01开始到现在的不同城市的订单量（注意是两个分组字段，日期和城市）</span><br><span class="line">select c_date,</span><br><span class="line">       city_id,</span><br><span class="line">       count(distinct order_id) cnt</span><br><span class="line">from t_order</span><br><span class="line">where c_date &gt;= &#x27;2020-12-01&#x27; and c_date &lt;= `current_date`()</span><br><span class="line">group by c_date, city_id;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_21-08-58.png" alt="Snipaste_2023-12-08_21-08-58" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">--然后按照上表进行按日期分组，对cnt进行排序</span><br><span class="line">select *,</span><br><span class="line">       row_number() over (partition by c_date order by cnt desc ) rn</span><br><span class="line">from (</span><br><span class="line">         select c_date,</span><br><span class="line">                city_id,</span><br><span class="line">                count(distinct order_id) cnt</span><br><span class="line">         from t_order</span><br><span class="line">         where c_date &gt;= &#x27;2020-12-01&#x27;</span><br><span class="line">           and c_date &lt;= `current_date`()</span><br><span class="line">         group by c_date, city_id</span><br><span class="line">     ) t1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_21-10-15.png" alt="Snipaste_2023-12-08_21-10-15" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">--筛选出top3城市</span><br><span class="line">select c_date,</span><br><span class="line">       city_id,</span><br><span class="line">       cnt,</span><br><span class="line">       rn</span><br><span class="line">from (</span><br><span class="line">         select *,</span><br><span class="line">                row_number() over (partition by c_date order by cnt desc ) rn</span><br><span class="line">         from (</span><br><span class="line">                  select c_date,</span><br><span class="line">                         city_id,</span><br><span class="line">                         count(distinct order_id) cnt</span><br><span class="line">                  from t_order</span><br><span class="line">                  where c_date &gt;= &#x27;2020-12-01&#x27;</span><br><span class="line">                    and c_date &lt;= `current_date`()</span><br><span class="line">                  group by c_date, city_id</span><br><span class="line">              ) t1</span><br><span class="line">     ) t2</span><br><span class="line">where rn &lt;= 3;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_21-11-19.png" alt="Snipaste_2023-12-08_21-11-19" style="zoom:33%;">

<h2 id="10-⭐组内比较问题"><a href="#10-⭐组内比较问题" class="headerlink" title="10. ⭐组内比较问题"></a>10. ⭐组内比较问题</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 万能模板</span><br><span class="line"><span class="keyword">select</span><span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">          函数名(列名) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="operator">&lt;</span>要分组的列名<span class="operator">&gt;</span>) <span class="keyword">as</span> 组内比较值</span><br><span class="line">    <span class="keyword">from</span> 表名;</span><br><span class="line">) t1</span><br><span class="line"><span class="keyword">where</span> 列名 <span class="operator">&gt;</span> 组内比较值</span><br></pre></td></tr></table></figure>

<p>求：每个部门低于平均薪水的雇员</p>
<img src="Snipaste_2023-12-08_15-53-53.png" alt="Snipaste_2023-12-08_15-53-53" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select emp_id,</span><br><span class="line">       dep_id,</span><br><span class="line">       salary</span><br><span class="line">from (</span><br><span class="line">         select *,</span><br><span class="line">                avg(salary) over (partition by dep_id) as dep_avg_salary</span><br><span class="line">         from window_1</span><br><span class="line">     ) t1</span><br><span class="line">where salary &lt; dep_avg_salary;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-15-52.png" alt="Snipaste_2023-12-08_20-15-52" style="zoom:33%;">

<hr>
<p>在员工表的基础上，统计每年入职总数及截至本年累积入职总人数;</p>
<img src="Snipaste_2023-12-08_20-35-50.png" alt="Snipaste_2023-12-08_20-35-50" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">--先求最简单的两列，年度，各年度入职人数</span><br><span class="line">select year(hiredate) years,</span><br><span class="line">       count(*) cnt</span><br><span class="line">from emp1</span><br><span class="line">group by year(hiredate);</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-38-01.png" alt="Snipaste_2023-12-08_20-38-01" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">--再利用窗口函数求截至各个年份的入职人数</span><br><span class="line">select *,</span><br><span class="line">       sum(cnt) over(order by years rows between unbounded preceding and current row ) cnt2--基于值和基于行都行</span><br><span class="line">from (</span><br><span class="line">         select year(hiredate) years,</span><br><span class="line">                count(*)       cnt</span><br><span class="line">         from emp1</span><br><span class="line">         group by year(hiredate)</span><br><span class="line">     ) t1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-38-28.png" alt="Snipaste_2023-12-08_20-38-28" style="zoom:33%;">

<hr>
<h3 id="percent-rank"><a href="#percent-rank" class="headerlink" title="percent_rank()"></a>percent_rank()</h3><p>根据商品大类goods_type对商品金额price从小到大排序，前30%为低挡，30%~85%为中档，高于85%为高档，打上标签。</p>
<img src="Snipaste_2023-12-08_20-41-08.png" alt="Snipaste_2023-12-08_20-41-08" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">--首先按照商品类别，对每个分类的商品计数，商品价格排序</span><br><span class="line">select *,</span><br><span class="line">       row_number() over (partition by goods_type order by price) rn,</span><br><span class="line">       count(*) over (partition by goods_type) cnt</span><br><span class="line">from goods;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-41-41.png" alt="Snipaste_2023-12-08_20-41-41" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">--打上标签</span><br><span class="line">select goods_type,</span><br><span class="line">       goods_name,</span><br><span class="line">       price,</span><br><span class="line">       rn,</span><br><span class="line">       case</span><br><span class="line">           when rn / cnt &lt; 0.3 then &#x27;低档&#x27;</span><br><span class="line">           when rn / cnt &gt;= 0.3 and rn / cnt &lt; 0.85 then &#x27;中档&#x27;</span><br><span class="line">           when rn / cnt &gt;= 0.85 then &#x27;高档&#x27;</span><br><span class="line">           end `等级`</span><br><span class="line">from (</span><br><span class="line">         select *,</span><br><span class="line">                row_number() over (partition by goods_type order by price) rn,</span><br><span class="line">                count(*) over (partition by goods_type)                    cnt</span><br><span class="line">         from goods</span><br><span class="line">     ) t1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-42-23.png" alt="Snipaste_2023-12-08_20-42-23" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">--或者直接一步到位用percent_rank()函数</span><br><span class="line">select goods_type,</span><br><span class="line">       goods_name,</span><br><span class="line">       price,</span><br><span class="line">       rn,</span><br><span class="line">       case</span><br><span class="line">            when rn &lt; 0.3 then &#x27;低档&#x27;</span><br><span class="line">            when rn &gt;= 0.3 and rn &lt; 0.85 then &#x27;中档&#x27;</span><br><span class="line">            when rn &gt;= 0.85 then &#x27;高档&#x27;</span><br><span class="line">       end `等级`</span><br><span class="line">from (</span><br><span class="line">         select *,</span><br><span class="line">                percent_rank() over (partition by goods_type order by price) rn</span><br><span class="line">         from goods</span><br><span class="line">     ) t1;</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>牛客SQL281</strong> <strong>最差是第几名(一)</strong></p>
<p>现在有班级成绩表(class_grade)如下:</p>
<img src="Snipaste_2023-12-20_21-34-44.png" alt="Snipaste_2023-12-20_21-34-44" style="zoom:50%;">

<p>第1行表示成绩为A的学生有2个</p>
<p>…….</p>
<p>最后1行表示成绩为B的学生有2个</p>
<p>请你写出一个SQL查询，如果一个学生知道了自己综合成绩以后，最差是排第几名? 结果按照grade升序排序，以上例子查询如下:</p>
<img src="Snipaste_2023-12-20_21-35-14.png" alt="Snipaste_2023-12-20_21-35-14" style="zoom:50%;">

<p>实际上就是一个窗口函数按窗口范围累加的问题</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> grade, </span><br><span class="line"><span class="built_in">sum</span>(number) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> unbounded preceding <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span>)<span class="keyword">as</span> t_rank</span><br><span class="line"><span class="keyword">from</span> class_grade;</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>牛客SQL267</strong> <strong>考试分数(二)</strong></p>
<p>牛客每次考试完，都会有一个成绩表(grade)，如下:</p>
<img src="Snipaste_2023-12-20_21-47-37.png" alt="Snipaste_2023-12-20_21-47-37" style="zoom: 50%;">

<p>请你写一个sql语句查询用户分数大于其所在工作(job)分数的平均分的所有grade的属性，并且以id的升序排序，如下:</p>
<img src="Snipaste_2023-12-20_21-48-12.png" alt="Snipaste_2023-12-20_21-48-12" style="zoom:50%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       job,</span><br><span class="line">       score</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> id,</span><br><span class="line">       job,</span><br><span class="line">       score,</span><br><span class="line">       <span class="built_in">avg</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> job) avg_score # 注意此处的写法</span><br><span class="line">    <span class="keyword">from</span> grade</span><br><span class="line">) t1 </span><br><span class="line"><span class="keyword">where</span> score <span class="operator">&gt;</span> avg_score</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> id;</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>牛客SQL248</strong> <strong>平均工资</strong></p>
<p>查找排除在职(to_date &#x3D; ‘9999-01-01’ )员工的最大、最小salary之后，其他的在职员工的平均工资avg_salary。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">avg</span>(salary)</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> salary) r1,</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> salary <span class="keyword">desc</span>) r2</span><br><span class="line">    <span class="keyword">from</span> salaries</span><br><span class="line">    <span class="keyword">where</span> to_date <span class="operator">=</span> <span class="string">&#x27;9999-01-01&#x27;</span></span><br><span class="line">) t1</span><br><span class="line"><span class="keyword">where</span> t1.r1 <span class="operator">&lt;&gt;</span> <span class="number">1</span> <span class="keyword">and</span> t1.r2 <span class="operator">&lt;&gt;</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure>



<h2 id="11-⭐连续问题"><a href="#11-⭐连续问题" class="headerlink" title="11. ⭐连续问题"></a>11. ⭐连续问题</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">-- 模板</span><br><span class="line">select distinct 列</span><br><span class="line">from (</span><br><span class="line">         select 列,</span><br><span class="line">                lead(列, 1) over (partition by 分组的列 order by 排序的列) 列1, </span><br><span class="line">                lead(列, 2) over (partition by 分组的列 order by 排序的列) 列2,</span><br><span class="line">    		   ...</span><br><span class="line">    		   lead(列, n-1) over (partition by 分组的列 order by 排序的列) 列n-1</span><br><span class="line">         from 表名</span><br><span class="line">     ) t1</span><br><span class="line">where (列 = 列1 and ... and 列 = 列n-1);</span><br></pre></td></tr></table></figure>

<p>查找所有至少’连续’出现三次的数字：</p>
<img src="Snipaste_2023-12-08_16-32-32.png" alt="Snipaste_2023-12-08_16-32-32" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span> Num ConsecutiveNums</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> Num,</span><br><span class="line">                <span class="built_in">lead</span>(Num, <span class="number">1</span>) <span class="keyword">over</span> () n2, # 将Num字段整体向上移动一格</span><br><span class="line">                <span class="built_in">lead</span>(Num, <span class="number">2</span>) <span class="keyword">over</span> () n3  # 将Num字段整体向上移动两格</span><br><span class="line">         <span class="keyword">from</span> `<span class="number">1</span>`</span><br><span class="line">     ) t1</span><br><span class="line"><span class="keyword">where</span> Num <span class="operator">=</span> n2 <span class="keyword">and</span> Num <span class="operator">=</span> n3;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-16-57.png" alt="Snipaste_2023-12-08_20-16-57" style="zoom:33%;">

<hr>
<p>当用户连续访问同一个页面时，只保留一次访问记录：</p>
<img src="Snipaste_2023-12-08_16-42-05.png" alt="Snipaste_2023-12-08_16-42-05" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> user_id,</span><br><span class="line">       page,</span><br><span class="line">       visit_time</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">                <span class="built_in">lag</span>(page, <span class="number">1</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> visit_time) last_visit_page</span><br><span class="line">         <span class="keyword">from</span> window_2) t1</span><br><span class="line"></span><br><span class="line"><span class="keyword">where</span> last_visit_page <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">or</span> page <span class="operator">!=</span> last_visit_page;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_16-45-59.png" alt="Snipaste_2023-12-08_16-45-59" style="zoom:33%;">

<h2 id="12-⭐日期和时间函数"><a href="#12-⭐日期和时间函数" class="headerlink" title="12. ⭐日期和时间函数"></a>12. ⭐日期和时间函数</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">--当前日期</span><br><span class="line">current_date</span><br><span class="line">--当前时间</span><br><span class="line">current_timestamp</span><br><span class="line">--获取年，月，日</span><br><span class="line">year(),month(),day()</span><br><span class="line">--两个日期相差天数</span><br><span class="line">datediff(&#x27;2023-08-08&#x27;,&#x27;2023-04-04&#x27;)</span><br><span class="line">--日期加天数</span><br><span class="line">date_add(&quot;2023-04-18&quot;,100)</span><br><span class="line">--日期减天数</span><br><span class="line">date_sub(&quot;2023-04-18&quot;,100)</span><br><span class="line">--日期格式化</span><br><span class="line">date_format(&#x27;xxx&#x27;,&#x27;%Y-%m-%d&#x27;)</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>SQL187</strong> <strong>牛客直播各科目平均观看时长</strong></p>
<img src="Snipaste_2023-12-20_22-16-19.png" alt="Snipaste_2023-12-20_22-16-19" style="zoom:50%;">

<img src="Snipaste_2023-12-20_22-16-40.png" alt="Snipaste_2023-12-20_22-16-40" style="zoom:50%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 主要是timestampdiff()函数的使用</span><br><span class="line"><span class="keyword">select</span> course_name,</span><br><span class="line">       round(<span class="built_in">avg</span>(timestampdiff(<span class="keyword">minute</span>,in_datetime,out_datetime)),<span class="number">2</span>) avg_len</span><br><span class="line"><span class="keyword">from</span> attend_tb a <span class="keyword">left</span> <span class="keyword">join</span> course_tb b</span><br><span class="line"><span class="keyword">on</span> a.course_id <span class="operator">=</span> b.course_id</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> course_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> avg_len <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>SQL186</strong> <strong>牛客直播开始时各直播间在线人数</strong></p>
<p>表还是上面的两张表</p>
<img src="Snipaste_2023-12-20_22-19-41.png" alt="Snipaste_2023-12-20_22-19-41" style="zoom:50%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) online_num</span><br><span class="line"><span class="keyword">from</span> attend_tb a <span class="keyword">left</span> <span class="keyword">join</span> course_tb b </span><br><span class="line"><span class="keyword">on</span> a.course_id <span class="operator">=</span> b.course_id</span><br><span class="line"><span class="keyword">where</span> <span class="type">time</span>(in_datetime) <span class="operator">&lt;=</span> <span class="string">&#x27;19:00:00&#x27;</span> <span class="keyword">and</span> <span class="type">time</span>(out_datetime) <span class="operator">&gt;=</span> <span class="string">&#x27;19:00:00&#x27;</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.course_id,course_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> a.course_id</span><br></pre></td></tr></table></figure>

<h2 id="13-⭐连续N天登录（最长连续登录天数）"><a href="#13-⭐连续N天登录（最长连续登录天数）" class="headerlink" title="13. ⭐连续N天登录（最长连续登录天数）"></a>13. ⭐连续N天登录（最长连续登录天数）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-- 思路（通法）：按照分组进行组内排序，将每个用户的登录日期减去自己的对应排序得到date2，统计相同的用户且相同的date2的个数，</span><br><span class="line">--找出相同个数大于等于3的，即为连续三天登录</span><br><span class="line">--公式：</span><br><span class="line">-- distinct</span><br><span class="line">-- row_number</span><br><span class="line">-- date_sub(dt,rn) as dt2</span><br><span class="line">-- group by dt2,name</span><br><span class="line">-- having count(*)&gt;=N天</span><br><span class="line">-- distinct name</span><br><span class="line">-- count(name)</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-08-01.png" alt="Snipaste_2023-12-08_20-08-01" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">--连续N天登录模板</span><br><span class="line">with t1 as (select distinct name, `date` from game),</span><br><span class="line">     t2 as (select *,</span><br><span class="line">                   row_number() over (partition by name order by `date`) rn</span><br><span class="line">            from t1),</span><br><span class="line">     t3 as (select *, date_sub(`date`, rn) date2 from t2),</span><br><span class="line">     t4 as (select name, date2</span><br><span class="line">            from t3</span><br><span class="line">            group by name, date2</span><br><span class="line">            having count(*) &gt;= N)</span><br><span class="line">select distinct name</span><br><span class="line">from t4;</span><br><span class="line"></span><br><span class="line">--方法2：</span><br><span class="line">select distinct name</span><br><span class="line">from (select name,</span><br><span class="line">             `date`,</span><br><span class="line">             lead(`date`, 1, &#x27;9999-12-31&#x27;) over (partition by name order by `date`) r1,</span><br><span class="line">             lead(`date`, 2, &#x27;9999-12-31&#x27;) over (partition by name order by `date`) r2</span><br><span class="line">      from (select distinct name, `date` from game) t1) t2</span><br><span class="line">where datediff(r2, r1) = 1</span><br><span class="line">  and datediff(r1, `date`) = 1;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">--最长连续登录天数模板</span><br><span class="line">with t1 as (select distinct name, `date` from game),</span><br><span class="line">     t2 as (select *,</span><br><span class="line">                   row_number() over (partition by name order by `date`) rn</span><br><span class="line">            from t1),</span><br><span class="line">     t3 as (select *, date_sub(`date`, rn) date2 from t2),</span><br><span class="line">     t4 as (select name, date2,count(*) as conse_days</span><br><span class="line">            from t3</span><br><span class="line">            group by name, date2)</span><br><span class="line">select name, max(conse_days) as max_conse_days</span><br><span class="line">from t4</span><br><span class="line">group by name;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-18-39.png" alt="Snipaste_2023-12-08_20-18-39" style="zoom:33%;">

<hr>
<p>在过去一个月内，曾连续两天活跃的用户：</p>
<img src="Snipaste_2023-12-08_20-19-44.png" alt="Snipaste_2023-12-08_20-19-44" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">with t1 as (select distinct d, uid from dau),</span><br><span class="line">     t2 as (select *,</span><br><span class="line">                   row_number() over (partition by uid order by d) rn</span><br><span class="line">            from t1</span><br><span class="line">            where d &lt;= `current_date`()</span><br><span class="line">              and d &gt;= date_sub(`current_date`(), 30)), --限定时间：过去一个月内</span><br><span class="line">     t3 as (select *,</span><br><span class="line">                   date_sub(d, rn) d2</span><br><span class="line">            from t2),</span><br><span class="line">     t4 as (select uid, d2</span><br><span class="line">            from t3</span><br><span class="line">            group by uid, d2</span><br><span class="line">            having count(*) &gt;= 2)</span><br><span class="line">select distinct uid</span><br><span class="line">from t4;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-20-46.png" alt="Snipaste_2023-12-08_20-20-46" style="zoom:33%;">

<hr>
<p><strong>牛客SQL194</strong> <strong>某乎问答最大连续回答问题天数大于等于3天的用户及其对应等级</strong></p>
<p>现有某乎问答创作者信息表author_tb如下(其中author_id表示创作者编号、author_level表示创作者级别，共1-6六个级别、sex表示创作者性别)：</p>
<img src="Snipaste_2023-12-20_16-51-53.png" alt="Snipaste_2023-12-20_16-51-53" style="zoom:33%;">

<p>创作者回答情况表answer_tb如下（其中answer_date表示创作日期、author_id指创作者编号、issue_id指回答问题编号、char_len表示回答字数）：</p>
<img src="Snipaste_2023-12-20_16-52-20.png" alt="Snipaste_2023-12-20_16-52-20" style="zoom:33%;">

<p>请你统计最大连续回答问题的天数大于等于3天的用户及其等级（若有多条符合条件的数据，按author_id升序排序）</p>
<img src="Snipaste_2023-12-20_17-26-34.png" alt="Snipaste_2023-12-20_17-26-34" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t4.author_id,</span><br><span class="line">       author_level,</span><br><span class="line">       day_cnt</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> author_id,</span><br><span class="line">                date2,</span><br><span class="line">                <span class="built_in">count</span>(<span class="operator">*</span>) day_cnt</span><br><span class="line">         <span class="keyword">from</span> (</span><br><span class="line">                  <span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">                         date_sub(answer_date, <span class="type">INTERVAL</span> rk <span class="keyword">day</span>) date2 # 这里和Hive不一样，这是Mysql的语法要求</span><br><span class="line">                  <span class="keyword">from</span> (</span><br><span class="line">                           <span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">                                  <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> author_id <span class="keyword">order</span> <span class="keyword">by</span> answer_date) rk</span><br><span class="line">                           <span class="keyword">from</span> (</span><br><span class="line">                                    <span class="keyword">select</span> <span class="keyword">distinct</span> author_id, answer_date</span><br><span class="line">                                    <span class="keyword">from</span> answer_tb</span><br><span class="line">                                ) t1</span><br><span class="line">                       ) t2</span><br><span class="line">              ) t3</span><br><span class="line">         <span class="keyword">group</span> <span class="keyword">by</span> author_id, date2</span><br><span class="line">         <span class="keyword">having</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="operator">&gt;=</span> <span class="number">3</span></span><br><span class="line">     ) t4 <span class="keyword">join</span> author_tb t5 <span class="keyword">on</span> t4.author_id <span class="operator">=</span> t5.author_id;</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>牛客SQL184</strong> <strong>某宝店铺连续2天及以上购物的用户及其对应的天数</strong></p>
<p>11月结束后，小牛同学需要对其在某宝的网店就11月份用户交易情况和产品情况进行分析以更好的经营小店。</p>
<p>11月份销售数据表sales_tb如下（其中，sales_date表示销售日期，user_id指用户编号，item_id指货号，sales_num表示销售数量，sales_price表示结算金额）：</p>
<img src="Snipaste_2023-12-20_17-24-37.png" alt="Snipaste_2023-12-20_17-24-37" style="zoom:33%;">

<p>请你统计连续2天及以上在该店铺购物的用户及其对应的次数（若有多个用户，按user_id升序排序）：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> t1 <span class="keyword">as</span> (<span class="keyword">select</span> <span class="keyword">distinct</span> user_id,sales_date <span class="keyword">from</span> sales_tb),</span><br><span class="line">     t2 <span class="keyword">as</span> (</span><br><span class="line">        <span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">               <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> sales_date) rk</span><br><span class="line">        <span class="keyword">from</span> t1</span><br><span class="line">     ),</span><br><span class="line">     t3 <span class="keyword">as</span> (<span class="keyword">select</span> <span class="operator">*</span>,date_sub(sales_date,<span class="type">INTERVAL</span> rk <span class="keyword">day</span>) date2 <span class="keyword">from</span> t2),</span><br><span class="line">     t4 <span class="keyword">as</span> (</span><br><span class="line">        <span class="keyword">select</span> user_id,date2,<span class="built_in">count</span>(<span class="operator">*</span>) days_count</span><br><span class="line">        <span class="keyword">from</span> t3</span><br><span class="line">        <span class="keyword">group</span> <span class="keyword">by</span> user_id,date2</span><br><span class="line">        <span class="keyword">having</span> days_count <span class="operator">&gt;=</span> <span class="number">2</span></span><br><span class="line">     )</span><br><span class="line"><span class="keyword">select</span> user_id,days_count <span class="keyword">from</span> t4;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-20_17-25-31.png" alt="Snipaste_2023-12-20_17-25-31" style="zoom:33%;">

<h2 id="14-⭐N日存留率"><a href="#14-⭐N日存留率" class="headerlink" title="14. ⭐N日存留率"></a>14. ⭐N日存留率</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">--方案1：公式</span><br><span class="line">-- -&gt; where 日期 in (首日,1天后,7天后)</span><br><span class="line">-- -&gt; group by 用户</span><br><span class="line">-- -&gt;sum(if(日期=首日,1,0))  as cnt</span><br><span class="line">--   sum(if(日期=1天后,1,0)) as cnt2</span><br><span class="line">--   sum(if(日期=7天后,1,0)) as cnt8</span><br><span class="line">-- -&gt;having cnt&gt;0</span><br><span class="line">-- -&gt;sum(cnt) as 首日总数</span><br><span class="line">--   sum(cnt2) as 次日留存数</span><br><span class="line">--   sum(cnt8) as 7日留存数</span><br><span class="line">-- -&gt;sum(cnt2)/sum(cnt) as 次日留存率</span><br><span class="line">--   sum(cnt8)/sum(cnt) as 7日留存率</span><br><span class="line"></span><br><span class="line">--方案2：三个表left join，分别（首日登录用户id）left join （此日登录用户id）left join （7日后登录用户id）</span><br></pre></td></tr></table></figure>

<h3 id="指定日期的留存率"><a href="#指定日期的留存率" class="headerlink" title="指定日期的留存率"></a>指定日期的留存率</h3><p>求2020-03-01的ios设备用户活跃的次日存留率是多少？</p>
<img src="Snipaste_2023-12-08_20-24-01.png" alt="Snipaste_2023-12-08_20-24-01" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-- 第一步：</span><br><span class="line">select user_id,</span><br><span class="line">       sum(`if`(ds = &#x27;2020-03-01&#x27;, 1, 0)) cnt1,</span><br><span class="line">       sum(`if`(ds = &#x27;2020-03-02&#x27;, 1, 0)) cnt2</span><br><span class="line">from tablegame</span><br><span class="line">where device = &#x27;ios&#x27;</span><br><span class="line">  and is_active = 1</span><br><span class="line">  and ds in (&#x27;2020-03-01&#x27;, &#x27;2020-03-02&#x27;)</span><br><span class="line">group by user_id</span><br><span class="line">having cnt1 &gt; 0;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-25-09.png" alt="Snipaste_2023-12-08_20-25-09" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">--第二步：</span><br><span class="line">select sum(cnt2) / sum(cnt1) `次日用户留存率`</span><br><span class="line">from (</span><br><span class="line">         select user_id,</span><br><span class="line">                sum(`if`(ds = &#x27;2020-03-01&#x27;, 1, 0)) cnt1,</span><br><span class="line">                sum(`if`(ds = &#x27;2020-03-02&#x27;, 1, 0)) cnt2</span><br><span class="line">         from tablegame</span><br><span class="line">         where device = &#x27;ios&#x27;</span><br><span class="line">           and is_active = 1</span><br><span class="line">           and ds in (&#x27;2020-03-01&#x27;, &#x27;2020-03-02&#x27;)</span><br><span class="line">         group by user_id</span><br><span class="line">         having cnt1 &gt; 0</span><br><span class="line">     ) t1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-28-34.png" alt="Snipaste_2023-12-08_20-28-34" style="zoom:33%;">

<hr>
<p>写出用户表 tb_cuid_1d的 20200401 的次日、次7日留存的具体HQL：</p>
<img src="Snipaste_2023-12-08_20-29-38.png" alt="Snipaste_2023-12-08_20-29-38" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">--第一步：</span><br><span class="line">select cuid,</span><br><span class="line">       sum(`if`(event_day = &#x27;2020-04-01&#x27;, 1, 0)) cnt_4_1, --这实际就相当于一个行转列</span><br><span class="line">       sum(`if`(event_day = &#x27;2020-04-02&#x27;, 1, 0)) cnt_4_2,</span><br><span class="line">       sum(`if`(event_day = &#x27;2020-04-08&#x27;, 1, 0)) cnt_4_8</span><br><span class="line">from tb_cuid_1d</span><br><span class="line">where event_day in (&#x27;2020-04-01&#x27;, &#x27;2020-04-02&#x27;, &#x27;2020-04-08&#x27;) --求次日存留率和次7日存留率，只需当日，次日和次七日的数据</span><br><span class="line">group by cuid</span><br><span class="line">having cnt_4_1 &gt; 0;-- 2020-04-01必须登录，剔除掉2020-04-01没登录的</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-30-55.png" alt="Snipaste_2023-12-08_20-30-55" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">select sum(cnt_4_2) / sum(cnt_4_1) `次日留存率`,</span><br><span class="line">       sum(cnt_4_8) / sum(cnt_4_1) `7日留存率`</span><br><span class="line">from (</span><br><span class="line">         select cuid,</span><br><span class="line">                sum(`if`(event_day = &#x27;2020-04-01&#x27;, 1, 0)) cnt_4_1,</span><br><span class="line">                sum(`if`(event_day = &#x27;2020-04-02&#x27;, 1, 0)) cnt_4_2,</span><br><span class="line">                sum(`if`(event_day = &#x27;2020-04-08&#x27;, 1, 0)) cnt_4_8</span><br><span class="line">         from tb_cuid_1d</span><br><span class="line">         where event_day in (&#x27;2020-04-01&#x27;, &#x27;2020-04-02&#x27;, &#x27;2020-04-08&#x27;)</span><br><span class="line">         group by cuid</span><br><span class="line">         having cnt_4_1 &gt; 0</span><br><span class="line">     ) t1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_20-31-50.png" alt="Snipaste_2023-12-08_20-31-50" style="zoom:33%;">

<hr>
<h3 id="每天活跃的新用户个数"><a href="#每天活跃的新用户个数" class="headerlink" title="每天活跃的新用户个数"></a>每天活跃的新用户个数</h3><p><strong>SQL263</strong> <strong>牛客每个人最近的登录日期(四)（查询每天登录的新用户的个数）</strong></p>
<img src="Snipaste_2023-12-21_11-19-08.png" alt="Snipaste_2023-12-21_11-19-08" style="zoom: 50%;">

<img src="Snipaste_2023-12-21_11-19-41.png" alt="Snipaste_2023-12-21_11-19-41" style="zoom:50%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="type">date</span>,</span><br><span class="line">       <span class="built_in">sum</span>(if(<span class="type">date</span><span class="operator">=</span>new_date,<span class="number">1</span>,<span class="number">0</span>)) <span class="keyword">new</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> user_id,</span><br><span class="line">           <span class="type">date</span>,</span><br><span class="line">           <span class="built_in">min</span>(<span class="type">date</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> <span class="type">date</span>) new_date</span><br><span class="line">    <span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">distinct</span> user_id,<span class="type">date</span> <span class="keyword">from</span> login)t1</span><br><span class="line">) t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="type">date</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="type">date</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="新用户次日留存（日期散乱）"><a href="#新用户次日留存（日期散乱）" class="headerlink" title="新用户次日留存（日期散乱）"></a>新用户次日留存（日期散乱）</h3><p><strong>牛客SQL164</strong> <strong>2021年11月每天新用户的次日留存率（新用户）</strong></p>
<p>用户行为日志表tb_user_log，（uid-用户ID, artical_id-文章ID, in_time-进入时间, out_time-离开时间, sign_in-是否签到）</p>
<p><strong>问题</strong>：统计2021年11月每天新用户的次日留存率（保留2位小数）</p>
<p><strong>注</strong>：</p>
<ul>
<li>次日留存率为当天新增的用户数中第二天又活跃了的用户数占比。</li>
<li>如果<strong>in_time-进入时间</strong>和<strong>out_time-离开时间</strong>跨天了，在两天里都记为该用户活跃过，结果按日期升序。</li>
</ul>
<p><img src="Snipaste_2023-12-20_10-54-16.png" alt="Snipaste_2023-12-20_10-54-16"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 本题难点是，如果in_time<span class="operator">-</span>进入时间和out_time<span class="operator">-</span>离开时间跨天了，在两天里都记为该用户活跃过，</span><br><span class="line"># 这里可以理解为把out_time也算作是活跃日</span><br><span class="line"># 第一步：用<span class="keyword">union</span>把in_time和out_time并联起来，对uid和<span class="type">date</span>去重活获得一张用户活跃表</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> uid,<span class="type">DATE</span>(in_time) <span class="keyword">AS</span> dt <span class="keyword">FROM</span> tb_user_log</span><br><span class="line"><span class="keyword">UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> uid,<span class="type">DATE</span>(out_time) <span class="keyword">AS</span> dt <span class="keyword">FROM</span> tb_user_log;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-20_10-59-21.png" alt="Snipaste_2023-12-20_10-59-21" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 第二步：找出次活日期和首次登录日期，并判断新用户和次留新用户。</span><br><span class="line"><span class="keyword">SELECT</span> uid,dt,</span><br><span class="line">   <span class="built_in">MIN</span>(dt) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid <span class="keyword">order</span> <span class="keyword">by</span> dt) <span class="keyword">AS</span> new_dt,</span><br><span class="line">    <span class="built_in">LEAD</span>(dt,<span class="number">1</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid <span class="keyword">ORDER</span> <span class="keyword">BY</span> dt) <span class="keyword">AS</span> next_dt</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span>(</span><br><span class="line">   <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> uid,<span class="type">DATE</span>(in_time) <span class="keyword">AS</span> dt <span class="keyword">FROM</span> tb_user_log</span><br><span class="line">   <span class="keyword">UNION</span></span><br><span class="line">   <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> uid,<span class="type">DATE</span>(out_time) <span class="keyword">AS</span> dt <span class="keyword">FROM</span> tb_user_log</span><br><span class="line">)act_table;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-20_11-01-48.png" alt="Snipaste_2023-12-20_11-01-48" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 第三步：如果dt<span class="operator">=</span>new_dt那这个用户为新用户，如果dt<span class="operator">=</span>new_dt且next_dt和new_dt的日期差为<span class="number">1</span>则这个用户为次留新用户</span><br><span class="line"><span class="keyword">WITH</span> t1 <span class="keyword">AS</span>(</span><br><span class="line"><span class="keyword">SELECT</span> uid,dt,</span><br><span class="line">   <span class="built_in">MIN</span>(dt) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid <span class="keyword">order</span> <span class="keyword">by</span> dt) <span class="keyword">AS</span> new_dt,</span><br><span class="line">    <span class="built_in">LEAD</span>(dt,<span class="number">1</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid <span class="keyword">ORDER</span> <span class="keyword">BY</span> dt) <span class="keyword">AS</span> next_dt</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span>(</span><br><span class="line">   <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> uid,<span class="type">DATE</span>(in_time) <span class="keyword">AS</span> dt <span class="keyword">FROM</span> tb_user_log</span><br><span class="line">   <span class="keyword">UNION</span></span><br><span class="line">   <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> uid,<span class="type">DATE</span>(out_time) <span class="keyword">AS</span> dt <span class="keyword">FROM</span> tb_user_log</span><br><span class="line">    ) act_table</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> dt,</span><br><span class="line">   <span class="keyword">CASE</span> <span class="keyword">WHEN</span> dt<span class="operator">=</span>new_dt <span class="keyword">AND</span> datediff(next_dt,new_dt)<span class="operator">=</span><span class="number">1</span> <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">ELSE</span> <span class="number">0</span> <span class="keyword">END</span> is_next_day,</span><br><span class="line">    <span class="keyword">CASE</span> <span class="keyword">WHEN</span> dt<span class="operator">=</span>new_dt <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">ELSE</span> <span class="number">0</span> <span class="keyword">END</span> is_new_day</span><br><span class="line"><span class="keyword">FROM</span> t1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-20_11-03-09.png" alt="Snipaste_2023-12-20_11-03-09" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 计算新用户次留率</span><br><span class="line"><span class="keyword">WITH</span> t1 <span class="keyword">AS</span>(</span><br><span class="line"><span class="keyword">SELECT</span> uid,dt,</span><br><span class="line">   <span class="built_in">MIN</span>(dt) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid <span class="keyword">order</span> <span class="keyword">by</span> dt) <span class="keyword">AS</span> new_dt,</span><br><span class="line">    <span class="built_in">LEAD</span>(dt,<span class="number">1</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid <span class="keyword">ORDER</span> <span class="keyword">BY</span> dt) <span class="keyword">AS</span> next_dt</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span>(</span><br><span class="line">   <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> uid,<span class="type">DATE</span>(in_time) <span class="keyword">AS</span> dt <span class="keyword">FROM</span> tb_user_log</span><br><span class="line">   <span class="keyword">UNION</span></span><br><span class="line">   <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> uid,<span class="type">DATE</span>(out_time) <span class="keyword">AS</span> dt <span class="keyword">FROM</span> tb_user_log</span><br><span class="line">   ) act_table</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> dt,</span><br><span class="line">   ROUND(<span class="built_in">SUM</span>(<span class="keyword">CASE</span> <span class="keyword">WHEN</span> dt<span class="operator">=</span>new_dt <span class="keyword">AND</span> datediff(next_dt,new_dt)<span class="operator">=</span><span class="number">1</span> <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">ELSE</span> <span class="number">0</span> <span class="keyword">END</span>)</span><br><span class="line">         <span class="operator">/</span></span><br><span class="line">         <span class="built_in">SUM</span>(<span class="keyword">CASE</span> <span class="keyword">WHEN</span> dt<span class="operator">=</span>new_dt <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">ELSE</span> <span class="number">0</span> <span class="keyword">END</span>),<span class="number">2</span>) uv_left_rate</span><br><span class="line"><span class="keyword">FROM</span> t1</span><br><span class="line"><span class="keyword">WHERE</span> DATE_FORMAT(new_dt,<span class="string">&#x27;%Y%m&#x27;</span>)<span class="operator">=</span><span class="string">&#x27;202111&#x27;</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> dt</span><br><span class="line"><span class="keyword">HAVING</span> uv_left_rate <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> dt;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-20_11-06-07.png" alt="Snipaste_2023-12-20_11-06-07" style="zoom:33%;">

<hr>
<h3 id="每天活跃的新用户占比"><a href="#每天活跃的新用户占比" class="headerlink" title="每天活跃的新用户占比"></a>每天活跃的新用户占比</h3><p><strong>牛客SQL166</strong> <strong>每天的日活数及新用户占比（和上一题很类似）</strong></p>
<img src="Snipaste_2023-12-21_10-25-52.png" alt="Snipaste_2023-12-21_10-25-52" style="zoom: 50%;">

<img src="Snipaste_2023-12-21_10-26-32.png" alt="Snipaste_2023-12-21_10-26-32" style="zoom: 50%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># <span class="number">1</span>）老方法，先建立一张拥有基本信息的用户活跃基础表</span><br><span class="line"># 这张表要包含用户id，活跃日，成为新用户的日期。因为用户可能<span class="number">1</span>天活跃N次，所以要做去重处理。</span><br><span class="line"># 活跃日直接并联in_time和out_time</span><br><span class="line"># 成为新用户日期，用窗口函数来取：<span class="built_in">MIN</span>(<span class="type">DATE</span>(in_time))<span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">order</span> <span class="keyword">by</span> dt uid)  <span class="keyword">AS</span> new_dt</span><br><span class="line"><span class="keyword">select</span> uid,</span><br><span class="line">       dt,</span><br><span class="line">       <span class="built_in">min</span>(dt) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> uid <span class="keyword">order</span> <span class="keyword">by</span> dt) new_dt</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> <span class="keyword">distinct</span> uid, <span class="type">date</span>(in_time) dt</span><br><span class="line">         <span class="keyword">from</span> tb_user_log</span><br><span class="line">         <span class="keyword">union</span></span><br><span class="line">         <span class="keyword">select</span> <span class="keyword">distinct</span> uid, <span class="type">date</span>(out_time) dt</span><br><span class="line">         <span class="keyword">from</span> tb_user_log</span><br><span class="line">     ) t1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-21_10-45-23.png" alt="Snipaste_2023-12-21_10-45-23" style="zoom: 33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># <span class="number">2</span>）定义新用户</span><br><span class="line"># 如果dt<span class="operator">=</span>new_dt那这天就是用户首次登录成为新用户的日子啦<span class="operator">~</span></span><br><span class="line"><span class="keyword">with</span> t2 <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">select</span> uid,</span><br><span class="line">           dt,</span><br><span class="line">           <span class="built_in">min</span>(dt) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> uid <span class="keyword">order</span> <span class="keyword">by</span> dt) new_dt</span><br><span class="line">    <span class="keyword">from</span> (</span><br><span class="line">             <span class="keyword">select</span> <span class="keyword">distinct</span> uid, <span class="type">date</span>(in_time) dt</span><br><span class="line">             <span class="keyword">from</span> tb_user_log</span><br><span class="line">             <span class="keyword">union</span></span><br><span class="line">             <span class="keyword">select</span> <span class="keyword">distinct</span> uid, <span class="type">date</span>(out_time) dt</span><br><span class="line">             <span class="keyword">from</span> tb_user_log</span><br><span class="line">         ) t1</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> uid,dt,if(dt <span class="operator">=</span> new_dt,<span class="number">1</span>,<span class="number">0</span>) is_new # <span class="number">1</span>为新用户，<span class="number">0</span>不是新用户</span><br><span class="line"><span class="keyword">from</span> t2;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-21_10-46-16.png" alt="Snipaste_2023-12-21_10-46-16" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># <span class="number">3</span>）计算新用户占比，结果按照日期升序，输出结果。</span><br><span class="line"># 日活：<span class="built_in">COUNT</span>(<span class="number">1</span>)</span><br><span class="line"># 新用户数：SUM（是否为新用户）</span><br><span class="line"># 新用户占比：ROUND(<span class="built_in">SUM</span>(新用户)<span class="operator">/</span><span class="built_in">COUNT</span>(<span class="number">1</span>),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">with</span> t2 <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">select</span> uid,</span><br><span class="line">           dt,</span><br><span class="line">           <span class="built_in">min</span>(dt) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> uid <span class="keyword">order</span> <span class="keyword">by</span> dt) new_dt</span><br><span class="line">    <span class="keyword">from</span> (</span><br><span class="line">             <span class="keyword">select</span> <span class="keyword">distinct</span> uid, <span class="type">date</span>(in_time) dt</span><br><span class="line">             <span class="keyword">from</span> tb_user_log</span><br><span class="line">             <span class="keyword">union</span></span><br><span class="line">             <span class="keyword">select</span> <span class="keyword">distinct</span> uid, <span class="type">date</span>(out_time) dt</span><br><span class="line">             <span class="keyword">from</span> tb_user_log</span><br><span class="line">         ) t1</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> dt,</span><br><span class="line">       <span class="built_in">count</span>(<span class="operator">*</span>) dau, # 日活用户</span><br><span class="line">       round(<span class="built_in">sum</span>(if(dt <span class="operator">=</span> new_dt,<span class="number">1</span>,<span class="number">0</span>))<span class="operator">/</span><span class="built_in">count</span>(<span class="operator">*</span>),<span class="number">2</span>) uv_new_ratio  # 新用户占比</span><br><span class="line"><span class="keyword">from</span> t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> dt <span class="keyword">order</span> <span class="keyword">by</span> dt;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-21_10-46-57.png" alt="Snipaste_2023-12-21_10-46-57" style="zoom:50%;">

<hr>
<p><strong>牛客SQL262</strong> <strong>牛客每个人最近的登录日期(三)（新用户）</strong></p>
<p>牛客每天有很多人登录，请你统计一下牛客新登录用户的次日成功的留存率，<br>有一个登录(login)记录表，简况如下:</p>
<img src="Snipaste_2023-12-20_21-19-55.png" alt="Snipaste_2023-12-20_21-19-55" style="zoom: 50%;">

<p>请你写出一个sql语句查询新登录用户次日成功的留存率，即第1天登陆之后，第2天再次登陆的概率,保存小数点后面3位(3位之后的四舍五入)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 第一步：找出次活日期和首次登录日期，并判断新用户和次留新用户。</span><br><span class="line"><span class="keyword">SELECT</span> user_id,<span class="type">date</span>,</span><br><span class="line">   <span class="built_in">MIN</span>(<span class="type">date</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id) <span class="keyword">AS</span> new_dt,</span><br><span class="line">   <span class="built_in">LEAD</span>(<span class="type">date</span>,<span class="number">1</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="type">date</span>) <span class="keyword">AS</span> next_dt</span><br><span class="line"><span class="keyword">FROM</span> login</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-20_21-22-17.png" alt="Snipaste_2023-12-20_21-22-17" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 第二步：如果dt<span class="operator">=</span>new_dt那这个用户为新用户，如果dt<span class="operator">=</span>new_dt且next_dt和new_dt的日期差为<span class="number">1</span>则这个用户为次留新用户</span><br><span class="line"><span class="keyword">WITH</span> t1 <span class="keyword">AS</span>(</span><br><span class="line"><span class="keyword">SELECT</span> user_id,<span class="type">date</span>,</span><br><span class="line">   <span class="built_in">MIN</span>(<span class="type">date</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id) <span class="keyword">AS</span> new_dt,</span><br><span class="line">    <span class="built_in">LEAD</span>(<span class="type">date</span>,<span class="number">1</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="type">date</span>) <span class="keyword">AS</span> next_dt</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> login</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> ROUND(<span class="built_in">SUM</span>(<span class="keyword">CASE</span> <span class="keyword">WHEN</span> <span class="type">date</span><span class="operator">=</span>new_dt <span class="keyword">AND</span> datediff(next_dt,new_dt)<span class="operator">=</span><span class="number">1</span> <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">ELSE</span> <span class="number">0</span> <span class="keyword">END</span>)</span><br><span class="line">         <span class="operator">/</span></span><br><span class="line">         <span class="built_in">SUM</span>(<span class="keyword">CASE</span> <span class="keyword">WHEN</span> <span class="type">date</span><span class="operator">=</span>new_dt <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">ELSE</span> <span class="number">0</span> <span class="keyword">END</span>),<span class="number">3</span>) p</span><br><span class="line"><span class="keyword">FROM</span> t1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-20_21-24-23.png" alt="Snipaste_2023-12-20_21-24-23" style="zoom:33%;">

<hr>
<h3 id="用户次日存留（日期散乱）"><a href="#用户次日存留（日期散乱）" class="headerlink" title="用户次日存留（日期散乱）"></a>用户次日存留（日期散乱）</h3><p><strong>牛客SQL29</strong> <strong>计算用户的平均次日留存率</strong></p>
<p>题目：现在运营想要查看用户在某天刷题后第二天还会再来刷题的平均概率。请你取出相应数据</p>
<img src="Snipaste_2023-12-20_11-15-40.png" alt="Snipaste_2023-12-20_11-15-40" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 步骤<span class="number">1</span>：首先查询一下所有用户每天登录情况，用<span class="keyword">distinct</span>是为了防止有用户同一天登录多次，这对我们没有用，我们只考虑某天登录后第二天仍登录的情况</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span> device_id,<span class="type">date</span></span><br><span class="line"><span class="keyword">from</span> question_practice_detail;</span><br><span class="line"># 步骤<span class="number">2</span>：使用<span class="built_in">lead</span>()窗口函数，将同一用户连续两天的记录拼接起来</span><br><span class="line"><span class="keyword">select</span> device_id,</span><br><span class="line">       <span class="type">date</span>,</span><br><span class="line">       <span class="built_in">lead</span>(<span class="type">date</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> device_id <span class="keyword">order</span> <span class="keyword">by</span> <span class="type">date</span>) <span class="keyword">as</span> date2</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">distinct</span> device_id,<span class="type">date</span></span><br><span class="line">    <span class="keyword">from</span> question_practice_detail</span><br><span class="line">) t1;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-20_11-17-24.png" alt="Snipaste_2023-12-20_11-17-24" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 步骤<span class="number">3</span>：检查date2和date1的日期差是不是为<span class="number">1</span>，是则为<span class="number">1</span>（次日留存了），否则为<span class="number">0</span>（次日未留存），取avg即可得平均概率。</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="built_in">avg</span>(if (datediff (date2, <span class="type">date</span>) <span class="operator">=</span> <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)) <span class="keyword">as</span> avg_ret</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">            device_id,</span><br><span class="line">            <span class="type">date</span>,</span><br><span class="line">            <span class="built_in">lead</span> (<span class="type">date</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> device_id <span class="keyword">order</span> <span class="keyword">by</span> <span class="type">date</span>) <span class="keyword">as</span> date2</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">            (</span><br><span class="line">                <span class="keyword">select</span> <span class="keyword">distinct</span> device_id,<span class="type">date</span></span><br><span class="line">                <span class="keyword">from</span> question_practice_detail</span><br><span class="line">            ) t1</span><br><span class="line">    ) t2;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-20_11-18-02.png" alt="Snipaste_2023-12-20_11-18-02" style="zoom:33%;">

<img src="image-20240821204018662.png" alt="image-20240821204018662" style="zoom: 67%;">

<h2 id="15-⭐爆炸函数"><a href="#15-⭐爆炸函数" class="headerlink" title="15. ⭐爆炸函数"></a>15. ⭐爆炸函数</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-- 公式：用【lateral view】+【explode】将大字段拆解成小字段：</span><br><span class="line">--  select  主表名.旧字段,视图名.新字段 from 主表名 lateral view explode(数组字段) 视图名 as 新字段名</span><br></pre></td></tr></table></figure>

<p>常驻人数top10的城市，男女比例的人数分布情况</p>
<img src="Snipaste_2023-12-08_21-18-25.png" alt="Snipaste_2023-12-08_21-18-25" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">--第一步：不管三七二十一直接炸裂数组字段</span><br><span class="line">select uid,</span><br><span class="line">       gender,</span><br><span class="line">       view1.addr</span><br><span class="line">from user_info lateral view explode(addr_list) view1 as addr;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_21-19-41.png" alt="Snipaste_2023-12-08_21-19-41" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">--从表1中统计不同城市的男性人数和女性人数（行转列思想）</span><br><span class="line">select addr,</span><br><span class="line">       count(uid) cnt,</span><br><span class="line">       sum(`if`(gender = &#x27;male&#x27;,1,0)) cnt_male,</span><br><span class="line">       sum(`if`(gender = &#x27;female&#x27;,1,0)) cnt_female</span><br><span class="line">from (</span><br><span class="line">         select uid,</span><br><span class="line">                gender,</span><br><span class="line">                view1.addr</span><br><span class="line">         from user_info lateral view explode(addr_list) view1 as addr</span><br><span class="line">     ) t1</span><br><span class="line">group by addr;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_21-21-12.png" alt="Snipaste_2023-12-08_21-21-12" style="zoom:33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">--从上表中进行开窗排名</span><br><span class="line">select addr,</span><br><span class="line">       row_number() over (order by cnt desc ) rn,</span><br><span class="line">       cnt_male,</span><br><span class="line">       cnt_female</span><br><span class="line">from (</span><br><span class="line">         select addr,</span><br><span class="line">                count(uid)                         cnt,</span><br><span class="line">                sum(`if`(gender = &#x27;male&#x27;, 1, 0))   cnt_male,</span><br><span class="line">                sum(`if`(gender = &#x27;female&#x27;, 1, 0)) cnt_female</span><br><span class="line">         from (</span><br><span class="line">                  select uid,</span><br><span class="line">                         gender,</span><br><span class="line">                         view1.addr</span><br><span class="line">                  from user_info lateral view explode(addr_list) view1 as addr</span><br><span class="line">              ) t1</span><br><span class="line">         group by addr</span><br><span class="line">     ) t2</span><br><span class="line">limit 10;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-08_21-22-31.png" alt="Snipaste_2023-12-08_21-22-31" style="zoom:33%;">

<hr>
<img src="Snipaste_2023-10-18_16-10-33.png" alt="Snipaste_2023-10-18_16-10-33" style="zoom:50%;">

<p>根据上述电影信息表，统计各分类的电影数量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select movie, split(category, &#x27;,&#x27;) cates</span><br><span class="line">from movie_info;</span><br></pre></td></tr></table></figure>

<img src="1111_2023-10-18_16-21-36.png" alt="1111_2023-10-18_16-21-36" style="zoom:50%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select movie, cate</span><br><span class="line">from (</span><br><span class="line">         select movie, split(category, &#x27;,&#x27;) cates</span><br><span class="line">         from movie_info</span><br><span class="line">    ) t1 lateral view explode(cates) tmp as cate;</span><br></pre></td></tr></table></figure>

<img src="1123-10-18_16-22-27.png" alt="1123-10-18_16-22-27" style="zoom:50%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select cate, count(*)</span><br><span class="line">from (</span><br><span class="line">         select movie, cate</span><br><span class="line">         from (</span><br><span class="line">                  select movie, split(category, &#x27;,&#x27;) cates</span><br><span class="line">                  from movie_info</span><br><span class="line">              ) t1 lateral view explode(cates) tmp as cate</span><br><span class="line">     ) t2</span><br><span class="line">group by cate;</span><br></pre></td></tr></table></figure>

<img src="111112paste_2023-10-18_16-24-10.png" alt="111112paste_2023-10-18_16-24-10" style="zoom:50%;">

<hr>
<p>爆炸函数补充1：</p>
<img src="微信图片_20231221100120.png" alt="微信图片_20231221100120" style="zoom:50%;">

<img src="微信图片_20231221100133.png" alt="微信图片_20231221100133" style="zoom:50%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select label,</span><br><span class="line">       sum(show) show,</span><br><span class="line">       sum(click) click</span><br><span class="line">from (</span><br><span class="line">         select a.label,</span><br><span class="line">                show,</span><br><span class="line">                click</span><br><span class="line">         from zhalie1 lateral view explode(labels) a as label</span><br><span class="line">     ) t1</span><br><span class="line">group by label;</span><br></pre></td></tr></table></figure>

<hr>
<p>爆炸函数补充2：</p>
<img src="微信图片_20231221100346.png" alt="微信图片_20231221100346" style="zoom: 33%;">

<img src="微信图片_20231221100349.png" alt="微信图片_20231221100349" style="zoom: 33%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">select user_id,</span><br><span class="line">       avg(pr_explode) price</span><br><span class="line">from (</span><br><span class="line">         select user_id,</span><br><span class="line">                tmp.pr_explode</span><br><span class="line">         from (</span><br><span class="line">                  select user_id,</span><br><span class="line">                         split(price_list, &#x27;,&#x27;) pr</span><br><span class="line">                  from zhalie2</span><br><span class="line">              ) t1 lateral view explode(pr) tmp as pr_explode</span><br><span class="line">     ) t2</span><br><span class="line">group by user_id;</span><br></pre></td></tr></table></figure>

<h2 id="16-⭐同一时刻最大在线人数问题（套路模板）"><a href="#16-⭐同一时刻最大在线人数问题（套路模板）" class="headerlink" title="16. ⭐同一时刻最大在线人数问题（套路模板）"></a>16. ⭐同一时刻最大在线人数问题（套路模板）</h2><p><strong>SQL163</strong> <strong>每篇文章同一时刻最大在看人数</strong></p>
<img src="Snipaste_2023-12-20_22-22-10.png" alt="Snipaste_2023-12-20_22-22-10" style="zoom:50%;">

<img src="Snipaste_2023-12-20_22-22-30.png" alt="Snipaste_2023-12-20_22-22-30" style="zoom:50%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># ①将用户的进入时间单独拎出来，同时记为<span class="number">1</span>；离开时间单独拎出来，同时记为<span class="number">-1</span>，这样就聚合这两个表，按照时间排序，意思就是：进去一个加<span class="number">1</span>，离开一个减<span class="number">1</span>。</span><br><span class="line"><span class="keyword">select</span> artical_id, in_time dt, <span class="number">1</span> num</span><br><span class="line"><span class="keyword">from</span> tb_user_log</span><br><span class="line"><span class="keyword">where</span> artical_id <span class="operator">!=</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> artical_id, out_time dt, <span class="number">-1</span> num</span><br><span class="line"><span class="keyword">from</span> tb_user_log</span><br><span class="line"><span class="keyword">where</span> artical_id <span class="operator">!=</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-20_22-31-12.png" alt="Snipaste_2023-12-20_22-31-12" style="zoom: 33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># ②然后利用窗口函数对计数（<span class="number">1</span>或者<span class="number">-1</span>）求累计和，因为题目规定：同一时间有就有出的话先算进来的后算出去的，所以排序的时候就要看好了先按时间排序，再按计数排序！</span><br><span class="line"><span class="keyword">select</span> artical_id,</span><br><span class="line">       <span class="built_in">sum</span>(num) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> artical_id <span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">asc</span>,num <span class="keyword">desc</span>) <span class="keyword">as</span> cnt</span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> artical_id, in_time dt, <span class="number">1</span> num</span><br><span class="line">      <span class="keyword">from</span> tb_user_log</span><br><span class="line">      <span class="keyword">where</span> artical_id <span class="operator">!=</span> <span class="number">0</span></span><br><span class="line">      <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">      <span class="keyword">select</span> artical_id, out_time dt, <span class="number">-1</span> num</span><br><span class="line">      <span class="keyword">from</span> tb_user_log</span><br><span class="line">      <span class="keyword">where</span> artical_id <span class="operator">!=</span> <span class="number">0</span></span><br><span class="line">     ) <span class="keyword">as</span> a</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-12-20_22-33-20.png" alt="Snipaste_2023-12-20_22-33-20" style="zoom:33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># ③然后再在每个分组里面去求最大的累积和就是最多同时在线的人数了！</span><br><span class="line"><span class="keyword">select</span> artical_id, <span class="built_in">max</span>(cnt) max_uv</span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> artical_id,</span><br><span class="line">             <span class="built_in">sum</span>(num) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> artical_id <span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">asc</span>,num <span class="keyword">desc</span>) <span class="keyword">as</span> cnt</span><br><span class="line">      <span class="keyword">from</span> (<span class="keyword">select</span> artical_id, in_time dt, <span class="number">1</span> num</span><br><span class="line">            <span class="keyword">from</span> tb_user_log</span><br><span class="line">            <span class="keyword">where</span> artical_id <span class="operator">!=</span> <span class="number">0</span></span><br><span class="line">            <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">            <span class="keyword">select</span> artical_id, out_time dt, <span class="number">-1</span> num</span><br><span class="line">            <span class="keyword">from</span> tb_user_log</span><br><span class="line">            <span class="keyword">where</span> artical_id <span class="operator">!=</span> <span class="number">0</span>) <span class="keyword">as</span> a</span><br><span class="line">    ) <span class="keyword">as</span> b</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> artical_id</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> max_uv <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>SQL189</strong> <strong>牛客直播各科目同时在线人数</strong></p>
<p><img src="Snipaste_2023-12-20_22-36-05.png" alt="Snipaste_2023-12-20_22-36-05"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> course_id,course_name,<span class="built_in">max</span>(uv_cnt)max_num</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">   <span class="keyword">select</span> t1.course_id,course_name,<span class="built_in">sum</span>(uv) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> t1.course_id <span class="keyword">order</span> <span class="keyword">by</span> dt,uv <span class="keyword">desc</span> ) uv_cnt</span><br><span class="line">   <span class="keyword">from</span> (<span class="keyword">select</span> course_id,user_id,in_datetime dt,<span class="number">1</span> <span class="keyword">as</span> uv <span class="keyword">from</span> attend_tb</span><br><span class="line">        <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">        <span class="keyword">select</span> course_id,user_id,out_datetime dt,<span class="number">-1</span> <span class="keyword">as</span> uv <span class="keyword">from</span> attend_tb</span><br><span class="line">       ) t1</span><br><span class="line">   <span class="keyword">join</span> course_tb t2 <span class="keyword">on</span> t1.course_id <span class="operator">=</span> t2.course_id</span><br><span class="line">)t3 <span class="keyword">group</span> <span class="keyword">by</span> course_id,course_name <span class="keyword">order</span> <span class="keyword">by</span> course_id;</span><br></pre></td></tr></table></figure>

<h2 id="17-一些小众函数的使用"><a href="#17-一些小众函数的使用" class="headerlink" title="17. 一些小众函数的使用"></a>17. 一些小众函数的使用</h2><p><strong>牛客SQL246</strong> <strong>获取employees中的first_name</strong></p>
<p>现有employees表如下：</p>
<img src="Snipaste_2023-12-20_21-51-57.png" alt="Snipaste_2023-12-20_21-51-57" style="zoom:50%;">

<p>请你将employees中的first_name，并按照first_name最后两个字母升序进行输出。</p>
<p>以上示例数据的输出如下：</p>
<img src="Snipaste_2023-12-20_21-52-25.png" alt="Snipaste_2023-12-20_21-52-25" style="zoom: 33%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> first_name</span><br><span class="line"><span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">substring</span>(first_name, (length(first_name)<span class="number">-1</span>), <span class="number">2</span>) <span class="keyword">asc</span> # 注意：字符串函数的字符下标从<span class="number">1</span>开始，limit函数的行数索引从<span class="number">0</span>开始</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>牛客SQL245</strong> <strong>查找字符串中逗号出现的次数</strong></p>
<img src="Snipaste_2023-12-20_21-54-59.png" alt="Snipaste_2023-12-20_21-54-59" style="zoom:50%;">

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    id,</span><br><span class="line">    length (string) <span class="operator">-</span> length (replace (string, &quot;,&quot;, &quot;&quot;))</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    strings;</span><br></pre></td></tr></table></figure>

<h2 id="18-⭐三出二广告重复率"><a href="#18-⭐三出二广告重复率" class="headerlink" title="18. ⭐三出二广告重复率"></a>18. ⭐三出二广告重复率</h2><p>写一个三出二的重复度sql</p>
<p>ta表（id(key), user_id, live_id, ad_id,create_time），id是主键，user_id代表用户_id，live_id是视频id，ad_id是广告id（如果为null代表该条视频为自然流量，不为null代表该条视频是广告，重复的广告拥有相同的ad_id），create_time是时间，格式为”2024-09-09 13:45:34”</p>
<p>求每一个用户每一天每观看三个视频出现两条重复的广告概率</p>
<p>示例：</p>
<p>首先对于ta表进行ad_id不为null的过滤，即where ad_id is not null，剩下的都是用户观看视频广告的记录</p>
<p>然后再对来源表ta中的user_id，create_time的年月日部分进行窗口划分，然后在每一个窗口内按照create_time的时间部分进行正序排序，得到每一个用户每一天各个时间点观看广告视频的情况，如下表：</p>
<table>
<thead>
<tr>
<th>id</th>
<th>user_id</th>
<th>create_time</th>
<th>live_id</th>
<th>ad_id</th>
</tr>
</thead>
<tbody><tr>
<td>2</td>
<td>1001</td>
<td>2024-09-09 14:45:34</td>
<td>101</td>
<td>22</td>
</tr>
<tr>
<td>3</td>
<td>1001</td>
<td>2024-09-09 14:50:34</td>
<td>102</td>
<td>22</td>
</tr>
<tr>
<td>4</td>
<td>1001</td>
<td>2024-09-09 15:45:34</td>
<td>103</td>
<td>11</td>
</tr>
<tr>
<td>5</td>
<td>1001</td>
<td>2024-09-09 16:45:34</td>
<td>104</td>
<td>11</td>
</tr>
</tbody></table>
<p>对于ad_id而言有4行数据，其中[22,22,11],[22,11,11]组成两个滑动窗口，即重复率的分母为3，在第一个窗口中存在两个连续重复出现的22，记为重复一次，在第二个窗口中存在两个连续重复出现的11，记为重复1次，总共重复两次，重复率的分子为2，重复率计算得到2&#x2F;2*100% &#x3D; 100%.</p>
<p>按照上面的逻辑，求每一个用户每一天每观看三个视频出现两条重复的广告概率</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">WITH filtered_ads AS (</span><br><span class="line">    SELECT</span><br><span class="line">        user_id,</span><br><span class="line">        DATE(create_time) AS view_date,</span><br><span class="line">        ad_id,</span><br><span class="line">        create_time,</span><br><span class="line">        count(*) OVER (PARTITION BY user_id, DATE(create_time) ORDER BY create_time) as live_cnt,</span><br><span class="line">        LEAD(ad_id, 1) OVER (PARTITION BY user_id, DATE(create_time) ORDER BY create_time) AS next_ad_id1</span><br><span class="line">    FROM</span><br><span class="line">        ks_ad_dw_dev.test_test_2029999999</span><br><span class="line">    WHERE</span><br><span class="line">        ad_id IS NOT NULL</span><br><span class="line">),</span><br><span class="line">tmp_1 as (</span><br><span class="line">    SELECT</span><br><span class="line">        user_id,</span><br><span class="line">        view_date,</span><br><span class="line">        ad_id,</span><br><span class="line">        next_ad_id1,</span><br><span class="line">        live_cnt as dup_total,</span><br><span class="line">        SUM(CASE WHEN ad_id = next_ad_id1 THEN 1 ELSE 0 END) OVER (PARTITION BY user_id, DATE(create_time) ORDER BY create_time rows between current row and 2 following) as dup_cnt</span><br><span class="line">    FROM</span><br><span class="line">        filtered_ads</span><br><span class="line">    WHERE</span><br><span class="line">        live_cnt &gt;= 3</span><br><span class="line">)</span><br><span class="line">select user_id,</span><br><span class="line">       view_date,</span><br><span class="line">       sum(dup_cnt/(dup_total-2)) as dup_rate</span><br><span class="line">from tmp_1</span><br><span class="line">group by user_id,</span><br><span class="line">       view_date</span><br></pre></td></tr></table></figure>



<h1 id="第七章-大数据场景题"><a href="#第七章-大数据场景题" class="headerlink" title="第七章 大数据场景题"></a>第七章 大数据场景题</h1><h2 id="1-1亿整数中找出最大的10000个数"><a href="#1-1亿整数中找出最大的10000个数" class="headerlink" title="1. 1亿整数中找出最大的10000个数"></a>1. 1亿整数中找出最大的10000个数</h2><p><img src="image-20240826204722032.png" alt="image-20240826204722032"></p>
<h2 id="2-有16T的数据，其中包含大量的重复数据，求解找到重复次数前k多的数，说出思路，如何用Spark-RDD实现？需要用到哪些spark算子"><a href="#2-有16T的数据，其中包含大量的重复数据，求解找到重复次数前k多的数，说出思路，如何用Spark-RDD实现？需要用到哪些spark算子" class="headerlink" title="2. 有16T的数据，其中包含大量的重复数据，求解找到重复次数前k多的数，说出思路，如何用Spark-RDD实现？需要用到哪些spark算子"></a>2. 有16T的数据，其中包含大量的重复数据，求解找到重复次数前k多的数，说出思路，如何用Spark-RDD实现？需要用到哪些spark算子</h2><p>要处理16TB的数据并找到重复次数前k多的数字，你可以使用Spark-RDD来进行分布式数据处理。下面是一个实现思路和所需的Spark算子的简要说明：</p>
<p>思路</p>
<ol>
<li><strong>数据加载</strong>：首先将16TB的数据加载到一个RDD中。</li>
<li>**映射操作 (mapToPair)**：将每个数字映射为键值对 <code>(数字, 1)</code>，这样可以为每个数字打上标记。</li>
<li>**归约操作 (reduceByKey)**：对相同的数字进行累加，得到每个数字的出现次数 <code>(数字, 出现次数)</code>。</li>
<li>**转换操作 (mapToPair)**：转换为 <code>(出现次数, 数字)</code> 的键值对，这样可以方便地按照出现次数进行排序。</li>
<li>**全局排序 (sortByKey)**：对所有数据按照出现次数进行降序排序。</li>
<li>**取前k个 (take)**：获取出现次数前k多的数字及其对应的出现次数。</li>
</ol>
<p>需要用到的Spark算子</p>
<ol>
<li><strong>mapToPair</strong>：用于将每个元素转换成键值对。</li>
<li><strong>reduceByKey</strong>：用于对相同的键（即数字）进行累加。</li>
<li><strong>mapToPair</strong>：用于交换键和值的位置，以便排序。</li>
<li><strong>sortByKey</strong>：用于对RDD按照键（即出现次数）进行排序。</li>
<li><strong>take</strong>：用于获取排序后的前k个元素。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将每个数字映射为 (number, 1)</span></span><br><span class="line">JavaPairRDD&lt;Integer, Integer&gt; pairs = dataRDD.mapToPair(number -&gt; <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(number, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对相同的数字进行累加 (number, count)</span></span><br><span class="line">JavaPairRDD&lt;Integer, Integer&gt; counts = pairs.reduceByKey((a, b) -&gt; a + b);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 转换为 (count, number) 键值对</span></span><br><span class="line">JavaPairRDD&lt;Integer, Integer&gt; swapped = counts.mapToPair(tuple -&gt; <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(tuple._2, tuple._1));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 按照count进行降序排序</span></span><br><span class="line">JavaPairRDD&lt;Integer, Integer&gt; sorted = swapped.sortByKey(<span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取前k个出现次数最多的数字</span></span><br><span class="line">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; topK = sorted.take(k);</span><br><span class="line"></span><br><span class="line">topK.forEach(tuple -&gt; System.out.println(<span class="string">&quot;Number: &quot;</span> + tuple._2 + <span class="string">&quot; Count: &quot;</span> + tuple._1));</span><br></pre></td></tr></table></figure>

<h2 id="3-一张付费表，一张退货表，数据同步不一致，一张表已经显示退货了，另一张表还没有显示付费，这样在依赖这些表做指标计算时就会出问题。如何处理这种因为数据同步不一致导致的问题？"><a href="#3-一张付费表，一张退货表，数据同步不一致，一张表已经显示退货了，另一张表还没有显示付费，这样在依赖这些表做指标计算时就会出问题。如何处理这种因为数据同步不一致导致的问题？" class="headerlink" title="3. 一张付费表，一张退货表，数据同步不一致，一张表已经显示退货了，另一张表还没有显示付费，这样在依赖这些表做指标计算时就会出问题。如何处理这种因为数据同步不一致导致的问题？"></a>3. 一张付费表，一张退货表，数据同步不一致，一张表已经显示退货了，另一张表还没有显示付费，这样在依赖这些表做指标计算时就会出问题。如何处理这种因为数据同步不一致导致的问题？</h2><p>处理因为数据同步不一致导致的问题，可以从多个层面采取措施，以确保在依赖这些表做指标计算时能够避免或最小化不一致带来的影响。以下是一些解决方案：</p>
<p>（1）<strong>数据同步的事务性保障</strong></p>
<ul>
<li><strong>事务管理</strong>: 确保付费和退货操作在同一事务中处理，保证数据的一致性。在事务内，只有当所有操作都成功时，数据才会被提交，否则进行回滚，避免一个表更新而另一个表未更新的情况。</li>
<li><strong>两阶段提交 (Two-phase commit)</strong>: 在分布式系统中，可以使用两阶段提交协议确保多个表或多个系统之间的数据一致性。第一阶段准备数据变更，第二阶段统一提交变更，确保所有表的更新操作要么全部成功，要么全部失败。</li>
</ul>
<p>（2） <strong>延迟计算</strong></p>
<ul>
<li><strong>延迟窗口</strong>: 在指标计算中引入延迟窗口，即不直接使用实时数据，而是延迟一定时间后再进行计算，以确保数据同步完成。例如，延迟 5 分钟或 10 分钟，等待付费和退货数据都同步到位后再进行指标计算。</li>
<li><strong>数据检查点</strong>: 设置数据检查点，只有当付费表和退货表的数据都达到某一时间点或条件后，才进行后续的指标计算，以避免数据不一致。</li>
</ul>
<p>（3） <strong>数据一致性校验</strong></p>
<ul>
<li><strong>对账机制</strong>: 建立对账机制，定期对付费表和退货表的数据进行比对，找出不一致的数据并进行人工或自动修正。例如，每日或每小时对两张表的数据进行比对，确保它们之间的数据一致性。</li>
<li><strong>一致性检查工具</strong>: 开发或使用现有的一致性检查工具，在数据同步完成后自动检查两张表的数据是否一致。如果发现不一致，则触发报警或自动纠正。</li>
</ul>
<p>（4） <strong>幂等性处理</strong></p>
<ul>
<li><strong>幂等操作</strong>: 设计幂等操作，确保即使同一笔付费或退货操作被多次处理，也不会导致数据不一致。例如，可以为每一笔交易分配唯一的交易 ID，并基于此 ID 确保每个操作只被处理一次。</li>
<li><strong>补偿机制</strong>: 当发现数据不一致时，使用补偿机制重新执行不一致的数据操作，以确保两张表中的数据同步一致。</li>
</ul>
<p>（5） <strong>指标计算的容错处理</strong></p>
<ul>
<li><strong>宽松的匹配规则</strong>: 在计算指标时，允许一定的容错范围。例如，允许在一定时间窗口内付费和退货数据不完全同步的情况下进行计算，但在报告生成或关键指标计算时，要求严格的数据一致性。</li>
<li><strong>数据标记和过滤</strong>: 在付费表和退货表中增加状态字段，标记哪些记录已经完成同步，哪些未完成。在进行指标计算时，只考虑已经完成同步的数据，过滤掉尚未同步的数据。</li>
</ul>
<p>（6） <strong>数据重试与回退</strong></p>
<ul>
<li><strong>重试机制</strong>: 在数据同步失败或不一致时，设置自动重试机制，定期重试未完成的同步操作，直到数据一致为止。</li>
<li><strong>回退策略</strong>: 在指标计算过程中，如果检测到数据不一致，可以暂时回退到上一版本的计算结果，直到数据同步完成并通过一致性校验为止。</li>
</ul>
<p>（7） <strong>数据同步的监控与报警</strong></p>
<ul>
<li><strong>实时监控</strong>: 实时监控数据同步的进度和状态，及时发现和处理数据同步延迟或失败的问题。</li>
<li><strong>异常报警</strong>: 当付费表和退货表的数据不同步时，自动触发报警，提醒相关人员及时处理。</li>
</ul>
<p>（8） <strong>最终一致性原则</strong></p>
<ul>
<li><strong>最终一致性</strong>: 在实际操作中，有时可以接受一定程度的短期数据不一致，只要系统能够在合理的时间内达到最终一致性。例如，允许短期内付费表和退货表不同步，但最终会通过补偿和校验机制来确保它们在一定时间窗口内达到一致。</li>
</ul>
<p>总结</p>
<p>要处理因数据同步不一致导致的问题，关键是要加强数据同步的事务性保障，引入延迟计算、数据一致性校验、幂等处理、监控与报警等机制。通过这些方法，可以确保在依赖这些表进行指标计算时，能够有效避免或处理因数据不一致带来的风险，从而保证数据质量和指标的准确性。</p>
<h2 id="4-如果数仓回溯数据从一年变为两年，但是我们的存储空间有限，有什么好的解决办法？"><a href="#4-如果数仓回溯数据从一年变为两年，但是我们的存储空间有限，有什么好的解决办法？" class="headerlink" title="4. 如果数仓回溯数据从一年变为两年，但是我们的存储空间有限，有什么好的解决办法？"></a>4. 如果数仓回溯数据从一年变为两年，但是我们的存储空间有限，有什么好的解决办法？</h2><p>当数据仓库需要回溯的数据从一年增加到两年，而存储空间有限时，面临的挑战是如何在有限的存储资源下有效管理和存储这些数据。以下是一些解决方案，可以帮助你在有限的存储空间内实现对两年数据的回溯：</p>
<p>（1） <strong>数据压缩</strong></p>
<ul>
<li>使用更高效的压缩算法<ul>
<li>检查当前使用的压缩算法是否最优。例如，如果目前使用的是Snappy，可以考虑使用Zstandard（ZSTD）等压缩比更高的算法，同时保持良好的压缩速度。</li>
<li>确保所有数据层（如DWD、DWS、ADS）都应用了压缩，以最大化节省存储空间。</li>
</ul>
</li>
<li>列式存储格式<ul>
<li>如果尚未使用列式存储格式（如Parquet或ORC），可以考虑将数据存储格式转换为这些格式。列式存储不仅能提高查询性能，还能显著减少存储空间占用。</li>
</ul>
</li>
</ul>
<p>（2） <strong>分区管理和数据生命周期管理</strong></p>
<ul>
<li>分区策略优化<ul>
<li>重新评估现有的分区策略，根据业务查询频率和数据分布情况优化分区策略。可以将较旧的分区合并为更大的分区，从而减少存储空间占用。</li>
</ul>
</li>
<li>冷热数据分离<ul>
<li>实施冷热数据分离策略，将访问频率较低的历史数据（例如一年前的数据）存储在更便宜、更高效的存储介质上，例如云存储或冷存储服务中，而将最近的数据保存在高性能存储中。</li>
</ul>
</li>
<li>数据清理<ul>
<li>定期清理或归档不再需要的历史数据，或仅保留关键字段和指标，将详细数据做聚合处理后存储，减少空间占用。</li>
</ul>
</li>
<li>增量存储<ul>
<li>如果业务允许，可以只保存数据的增量变化，而不是完整的每日快照。使用增量数据和前期基线数据结合的方法来恢复完整数据。</li>
</ul>
</li>
</ul>
<p>（3） <strong>层级数据下推</strong></p>
<ul>
<li>层级数据下推（数据降级存储）<ul>
<li>将DWD层、DWS层的部分数据下推到更低的存储层级。例如，将DWD层的原始数据经过清洗和聚合后，定期（如每月或每季度）下推到更紧凑的DWS或归档层中，从而减少DWD层的存储量。</li>
<li>这种方式可以通过在DWS层保存更长时间范围的聚合数据来降低存储需求，而DWD层则只保留近期数据。</li>
</ul>
</li>
</ul>
<p>（4） <strong>存储优化与架构调整</strong></p>
<ul>
<li>使用外部存储服务<ul>
<li>考虑使用云存储或对象存储服务，如Amazon S3、Azure Blob Storage等，将不常访问的历史数据存储在云端。这些存储服务通常提供更低的存储成本和弹性扩展能力。</li>
</ul>
</li>
<li>数据分片和分布式存储<ul>
<li>如果数仓架构允许，可以考虑使用分布式存储系统，如Hadoop HDFS或分布式文件系统，将数据分片并分布到多台机器上，缓解单机存储压力。</li>
</ul>
</li>
<li>垂直拆分<ul>
<li>根据业务需求，将存储压力较大的数据表或字段进行拆分处理。例如，将宽表拆分为多个细表，按需进行数据查询和拼接，从而减少每个表的存储空间占用。</li>
</ul>
</li>
</ul>
<p>（5） <strong>数据去重与优化</strong></p>
<ul>
<li>数据去重<ul>
<li>通过检查并去除冗余数据，减少重复存储的数据量。特别是在日志数据、用户行为数据等场景中，重复数据可能占用大量存储。</li>
</ul>
</li>
<li>数据精简<ul>
<li>根据业务需求，精简存储的数据字段，仅保留必要的核心数据。例如，对于较早的数据，可以考虑丢弃详细的操作日志，只保留关键的聚合数据或指标。</li>
</ul>
</li>
</ul>
<p>（6） <strong>动态存储策略</strong></p>
<ul>
<li>基于时间的动态存储策略<ul>
<li>实施基于时间的存储策略，比如只对最近一年的数据保留高精度数据，而将更早的数据以较低的精度存储，例如只保留周或月的汇总数据。</li>
</ul>
</li>
<li>滚动存储策略<ul>
<li>使用滚动存储策略，当存储空间接近限制时，自动将最早的数据进行压缩或移至次级存储，从而始终为新数据保留足够的存储空间。</li>
</ul>
</li>
</ul>
<p>通过结合这些方法，能够有效应对存储空间有限的情况下数据量的增长，确保数据仓库能够继续支持更长时间跨度的回溯分析，而不会对系统性能和成本造成过大压力。</p>
<h2 id="5-归并排序和快速排序适用场景（以MR-shuffle举例）"><a href="#5-归并排序和快速排序适用场景（以MR-shuffle举例）" class="headerlink" title="5. 归并排序和快速排序适用场景（以MR shuffle举例）"></a>5. 归并排序和快速排序适用场景（以MR shuffle举例）</h2><p>在MapReduce框架中，shuffle阶段是一个关键步骤，涉及数据的分发、排序和合并。选择合适的排序算法对shuffle性能的影响很大。在此背景下，归并排序（Merge Sort）和快速排序（Quick Sort）各有其优势，适用于不同的场景。</p>
<p>归并排序（Merge Sort）</p>
<p>适用场景：</p>
<ul>
<li><strong>大规模数据排序</strong>：归并排序的稳定性和较低的最坏情况下的时间复杂度（O(n log n)）使其适合处理非常大的数据集，尤其是在MapReduce中，当数据量超过单个节点的内存限制时。</li>
<li><strong>外部排序</strong>：当数据量很大，无法全部加载到内存中时，归并排序表现更好。它可以将数据分割成小块排序后再合并，适合在磁盘上进行多路归并操作。</li>
<li><strong>数据已经部分有序</strong>：归并排序在处理部分有序的数据时依然保持O(n log n)的时间复杂度，没有任何性能退化，这对于MapReduce处理分布式数据时，节点上数据的局部有序性可能较高的情况非常适用。</li>
<li><strong>稳定性要求高</strong>：如果MapReduce任务对排序的稳定性有要求（即相等的元素必须保持原来的顺序），归并排序是更好的选择，因为它是稳定排序算法。</li>
</ul>
<p>MapReduce Shuffle中的应用：</p>
<ul>
<li>在MapReduce中，归并排序常用于Reducer阶段的合并排序。Mapper输出的数据首先在内存中使用快速排序排序，当内存满时，将排序后的数据块写入磁盘（生成中间文件）。在Reduce阶段，会使用归并排序将这些中间文件进行多路合并，从而得到有序的最终结果。</li>
</ul>
<p>快速排序（Quick Sort）</p>
<p>适用场景：</p>
<ul>
<li><strong>内存排序</strong>：快速排序在内存中排序时非常高效，平均时间复杂度为O(n log n)，而且它的常数因子较小，适合用于内存能够容纳全部数据的场景。</li>
<li><strong>数据量适中且无需外部排序</strong>：当数据量较小或者适中，且能完全放入内存时，快速排序的高效性使其成为首选。这种情况常见于MapReduce的Mapper阶段或在Reducer合并前的数据排序。</li>
<li><strong>无稳定性要求</strong>：如果MapReduce任务对排序的稳定性没有要求，快速排序更为适合。虽然它在最坏情况下（O(n^2)）表现不佳，但这种情况可以通过随机化或其他优化方法（如三向切分快速排序）进行有效缓解。</li>
</ul>
<p>MapReduce Shuffle中的应用：</p>
<ul>
<li>在MapReduce的Mapper阶段，快速排序常用于对Mapper输出的数据进行初步排序。由于Mapper的输出通常需要在内存中进行排序（在溢写到磁盘之前），快速排序因为其高效的平均性能而被广泛使用。</li>
</ul>
<p>总结</p>
<ul>
<li><strong>归并排序</strong>：适用于需要稳定排序、数据量巨大且超出内存限制的场景，尤其是在MapReduce Shuffle阶段，Reducer端的合并排序。</li>
<li><strong>快速排序</strong>：适用于数据量适中、能够在内存中完成排序且不需要稳定性的场景，特别是在MapReduce的Mapper阶段进行初步排序时。</li>
</ul>
<p>两者在MapReduce中常常结合使用，以发挥各自的优势：快速排序用于内存中的快速排序，而归并排序则用于处理和合并大的、分块的数据集。</p>
<h2 id="6-关于不同spark-shuffle产生的文件个数"><a href="#6-关于不同spark-shuffle产生的文件个数" class="headerlink" title="6. 关于不同spark shuffle产生的文件个数"></a>6. 关于不同spark shuffle产生的文件个数</h2><p>有10个Map Task，2个Reduce Task，2个Executor，每个Executor有两个core，问Hash shuffle产生文件的个数，优化的Hash shuffle产生的文件个数，sortshuffle产生的文件个数</p>
<img src="image-20240902232354451.png" alt="image-20240902232354451" style="zoom:50%;">

<img src="image-20240902232442758.png" alt="image-20240902232442758" style="zoom:50%;">

<h2 id="7-split切分文本文件的时候，不一定正好切分在行末尾，可能切分在一行的中间，hadoop是如何保证多个切片处理的都是完整的行"><a href="#7-split切分文本文件的时候，不一定正好切分在行末尾，可能切分在一行的中间，hadoop是如何保证多个切片处理的都是完整的行" class="headerlink" title="7. split切分文本文件的时候，不一定正好切分在行末尾，可能切分在一行的中间，hadoop是如何保证多个切片处理的都是完整的行"></a>7. split切分文本文件的时候，不一定正好切分在行末尾，可能切分在一行的中间，hadoop是如何保证多个切片处理的都是完整的行</h2><p>在Hadoop中，当处理文本文件时，<strong>Split</strong> 可能不会精确地在行末尾进行切分，可能会发生在一行的中间。为了确保在这种情况下，每个Split处理的都是完整的行，Hadoop通过 <code>RecordReader</code> 类（通常是 <code>LineRecordReader</code>）来保证每个Map任务只处理完整的行。</p>
<p>详细机制解释：</p>
<ol>
<li><p><strong>Split的生成</strong>：</p>
<ul>
<li><code>InputFormat</code> 会将输入文件分割成多个Splits，每个Split有一个起始字节偏移量和长度。</li>
<li>这些Splits可能不会严格按照行的边界进行划分。例如，一个128MB的Split可能会从文件的第100MB字节开始，并在第228MB字节结束。这可能会导致第一个Split的最后一行和下一个Split的第一行都是不完整的。</li>
</ul>
</li>
<li><p><strong>RecordReader的作用</strong>：</p>
<ul>
<li>Hadoop使用 <code>RecordReader</code> 来处理每个Split。对于文本文件，默认的 <code>RecordReader</code> 是 <code>LineRecordReader</code>。</li>
<li><code>LineRecordReader</code> 会从Split的起始位置开始读取数据，直到找到行结束符（如 <code>\n</code> 或 <code>\r\n</code>）。如果Split的起始位置不在一行的开头，它会向前跳过不完整的行，从下一行的开头开始读取。这确保了Map任务不会处理部分行。</li>
</ul>
</li>
<li><p><strong>处理第一个Split</strong>：</p>
<ul>
<li>第一个Split从文件的开头开始读取，因此它不会有不完整的行，直接从文件的第一个字节开始处理。</li>
</ul>
</li>
<li><p><strong>处理中间的Splits</strong>：</p>
<ul>
<li>对于不是第一个的Splits， <code>LineRecordReader</code> 首先会跳过起始字节位置处可能不完整的行，然后从下一行的开头开始读取。</li>
<li>同样地，当<code>LineRecordReader</code>读取到Split的末尾时，它不会立即停止，而是会继续读取到行结束符为止。这意味着它可能会读取一些属于下一个Split的数据，但这部分数据在下一个Split中会被跳过（因为那里的<code>LineRecordReader</code>会跳过这一行的开头）。</li>
</ul>
</li>
<li><p><strong>保证行的完整性</strong>：</p>
<ul>
<li>&#96;&#96;&#96;<br>LineRecordReader<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">      的这种机制确保了：</span><br><span class="line"></span><br><span class="line">     - 每个Map任务只处理完整的行。</span><br><span class="line">     - 没有任何一行被多个Map任务重复处理。</span><br><span class="line"></span><br><span class="line">例子</span><br><span class="line"></span><br><span class="line">假设有一个文本文件，其内容如下：</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<p>plaintext复制代码line1<br>line2<br>line3<br>line4<br>line5</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">文件大小为50字节，每行10字节。如果Hadoop将其分成两个Splits：</span><br><span class="line"></span><br><span class="line">- 第一个Split从0字节开始，长度为25字节（可能在`line2`的中间结束）。</span><br><span class="line">- 第二个Split从25字节开始，长度为25字节（可能从`line2`的中间开始）。</span><br><span class="line"></span><br><span class="line">**处理过程**：</span><br><span class="line"></span><br><span class="line">- **Split 1**:</span><br><span class="line">  - `LineRecordReader` 从0字节开始，读取 `line1`。</span><br><span class="line">  - 继续读取 `line2`，直到完整的 `line2` 被读取完。</span><br><span class="line">  - 第一个Split的任务结束时，它会读取完整的 `line2`。</span><br><span class="line">- **Split 2**:</span><br><span class="line">  - `LineRecordReader` 从25字节开始，但发现这是 `line2` 的中间。</span><br><span class="line">  - 它会跳过不完整的 `line2`，从 `line3` 开始读取。</span><br><span class="line">  - 继续读取 `line4` 和 `line5`。</span><br><span class="line"></span><br><span class="line">最终的结果是：</span><br><span class="line"></span><br><span class="line">- Split 1 处理了 `line1` 和 `line2`。</span><br><span class="line">- Split 2 处理了 `line3`，`line4` 和 `line5`。</span><br><span class="line"></span><br><span class="line">这就确保了行的完整性，没有行被重复处理或部分丢失。</span><br><span class="line"></span><br><span class="line">总结</span><br><span class="line"></span><br><span class="line">Hadoop通过 `LineRecordReader` 来确保每个Map任务处理的都是完整的行，即使Splits本身并不严格按照行边界划分。 `LineRecordReader` 会在处理Split时，跳过不完整的行开头，并在必要时读取多于Split长度的数据，以确保整个行被正确处理。这样，Hadoop能够在大规模分布式环境中高效、准确地处理文本数据。</span><br><span class="line"></span><br><span class="line">## 8. 有一张表的数据，一个字段为时间戳（时间序列），如果某个用户对应下的时间序列特别长（比如某个用户在一天疯狂刷了100000条视频，每条视频都对应一个时间戳），这样就会产生数据倾斜，如何解决这种时间序列下的数据倾斜</span><br><span class="line"></span><br><span class="line">将时间戳进行切分</span><br><span class="line"></span><br><span class="line">（1） **固定时间窗口切分**</span><br><span class="line"></span><br><span class="line">- **方法描述**：按照固定的时间窗口对时间序列进行切分。例如，可以将时间序列按天、小时、分钟等时间间隔进行切分。</span><br><span class="line">- 实现方式</span><br><span class="line">  - 按照时间字段划分，提取时间戳的特定部分，如按天划分 `DATE_TRUNC(&#x27;day&#x27;, timestamp)`，或者按小时划分 `DATE_TRUNC(&#x27;hour&#x27;, timestamp)`。</span><br><span class="line">  - 切分后，每段时间窗口的数据就会形成独立的块，避免长时间序列的数据集中在一个分片上。</span><br><span class="line"></span><br><span class="line">**示例**：将某用户的一天的刷视频记录按小时切分成24个小时段：</span><br><span class="line"></span><br><span class="line">```sql</span><br><span class="line">SELECT user_id, video_id, timestamp, </span><br><span class="line">       DATE_TRUNC(&#x27;hour&#x27;, timestamp) AS hour_segment</span><br><span class="line">FROM user_video_log</span><br></pre></td></tr></table></figure>

<p>这样，用户一天内的数据会被切分成小时为单位的不同时间段。</p>
<p>（2） <strong>滑动时间窗口切分</strong></p>
<ul>
<li><strong>方法描述</strong>：使用滑动窗口技术，每个窗口覆盖一段时间，并且窗口之间有重叠。通过滑动窗口可以平滑过渡切分数据，特别适合需要连续时间段分析的场景。</li>
<li>实现方式<ul>
<li>定义一个时间窗口长度（如5分钟、10分钟等）以及滑动步长（如每分钟滑动一次）。</li>
<li>使用窗口函数或数据处理框架（如Flink、Spark）进行窗口划分，并将长时间序列分割成多段。</li>
</ul>
</li>
</ul>
<p><strong>示例</strong>：将时间序列按照5分钟窗口，每分钟滑动一次：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> user_id, video_id, <span class="type">timestamp</span>, </span><br><span class="line">       <span class="built_in">FLOOR</span>(<span class="built_in">EXTRACT</span>(epoch <span class="keyword">FROM</span> <span class="type">timestamp</span>) <span class="operator">/</span> <span class="number">300</span>) <span class="keyword">AS</span> window_segment</span><br><span class="line"><span class="keyword">FROM</span> user_video_log</span><br></pre></td></tr></table></figure>

<p>这里<code>300</code>秒表示5分钟，生成的<code>window_segment</code>字段会将时间戳按5分钟为单位进行分割。</p>
<p>（3） <strong>按行为数量切分</strong></p>
<ul>
<li><strong>方法描述</strong>：根据用户行为的数量而非时间来进行切分。每当某个用户的行为数据达到一定量（例如5000条记录），将其分为一段。</li>
<li>实现方式<ul>
<li>使用窗口函数或者分组技术，将每个用户的行为按数量划分为多个组。每组数据中记录数达到指定的数量时切分到下一组。</li>
<li>这样可以在数据量较大时进行等量分割，避免某个用户的行为数据量过大导致负载不均衡。</li>
</ul>
</li>
</ul>
<p><strong>示例</strong>：将每5000条记录切分为一段：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> user_id, video_id, <span class="type">timestamp</span>, </span><br><span class="line">       <span class="built_in">NTILE</span>(<span class="built_in">CEIL</span>(<span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id) <span class="operator">/</span> <span class="number">5000</span>)) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="type">timestamp</span>) <span class="keyword">AS</span> segment</span><br><span class="line"><span class="keyword">FROM</span> user_video_log</span><br></pre></td></tr></table></figure>

<p><code>NTILE</code>函数可以根据指定的数量对用户的行为进行切分，每5000条记录为一个组。</p>
<p>（4）<strong>自定义时间段切分</strong></p>
<ul>
<li><strong>方法描述</strong>：根据业务需求，设定特定的时间段进行切分。例如，可以根据视频播放的高峰期和低峰期，自定义将某些时间段的数据进行划分，切分成早、中、晚等时间段。</li>
<li>实现方式<ul>
<li>使用<code>CASE</code>或<code>IF</code>语句将特定时间段进行划分。</li>
</ul>
</li>
</ul>
<p><strong>示例</strong>：将一天按早（00:00-08:00）、中（08:00-16:00）、晚（16:00-24:00）三个时段切分：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> user_id, video_id, <span class="type">timestamp</span>, </span><br><span class="line">       <span class="keyword">CASE</span> </span><br><span class="line">         <span class="keyword">WHEN</span> <span class="built_in">EXTRACT</span>(<span class="keyword">hour</span> <span class="keyword">FROM</span> <span class="type">timestamp</span>) <span class="keyword">BETWEEN</span> <span class="number">0</span> <span class="keyword">AND</span> <span class="number">7</span> <span class="keyword">THEN</span> <span class="string">&#x27;morning&#x27;</span></span><br><span class="line">         <span class="keyword">WHEN</span> <span class="built_in">EXTRACT</span>(<span class="keyword">hour</span> <span class="keyword">FROM</span> <span class="type">timestamp</span>) <span class="keyword">BETWEEN</span> <span class="number">8</span> <span class="keyword">AND</span> <span class="number">15</span> <span class="keyword">THEN</span> <span class="string">&#x27;afternoon&#x27;</span></span><br><span class="line">         <span class="keyword">ELSE</span> <span class="string">&#x27;evening&#x27;</span></span><br><span class="line">       <span class="keyword">END</span> <span class="keyword">AS</span> time_segment</span><br><span class="line"><span class="keyword">FROM</span> user_video_log</span><br></pre></td></tr></table></figure>

<p>这样可以根据业务逻辑划分时间段，便于分析用户在不同时间段的行为。</p>
<p>（5） <strong>动态分区切分</strong></p>
<ul>
<li><strong>方法描述</strong>：针对极端的长时间序列行为，可以动态调整分区策略，对高频用户的数据进行进一步切分。</li>
<li>实现方式<ul>
<li>先对时间序列进行分析，识别出高频用户，然后针对这些用户单独设定切分策略，确保其数据不会集中在某个节点或分片上。</li>
<li>使用分布式计算框架（如Flink、Spark）中的动态分区策略，可以自动检测数据负载，并动态分配更多资源来处理这些倾斜数据。</li>
</ul>
</li>
</ul>
<p>总结：</p>
<p>通过时间窗口、滑动窗口、行为数量、时间段自定义等方法，可以有效将长时间序列切分成多个小段，减少数据倾斜带来的问题。具体选择哪种方法，取决于业务需求和数据的特性，通常可以结合多种方法进行切分以达到最佳效果。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">面试</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/11/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-Hive调优及源码编译" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/27/Hive%E8%B0%83%E4%BC%98%E5%8F%8A%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/">Hive调优及源码编译</a>
    </h1>
  

        
        <a href="/2023/11/27/Hive%E8%B0%83%E4%BC%98%E5%8F%8A%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/" class="archive-article-date">
  	<time datetime="2023-11-27T04:52:43.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-11-27</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="目标：搭建Hive-on-Spark环境"><a href="#目标：搭建Hive-on-Spark环境" class="headerlink" title="目标：搭建Hive on Spark环境"></a>目标：搭建Hive on Spark环境</h1><h1 id="第一章-部署Hadoop和Hive"><a href="#第一章-部署Hadoop和Hive" class="headerlink" title="第一章 部署Hadoop和Hive"></a>第一章 部署Hadoop和Hive</h1><h2 id="1-1-版本选择"><a href="#1-1-版本选择" class="headerlink" title="1.1 版本选择"></a>1.1 版本选择</h2><p>Hive版本：3.1.3</p>
<p>Hadoop版本：3.1.3</p>
<h2 id="1-2-Hadoop部署"><a href="#1-2-Hadoop部署" class="headerlink" title="1.2 Hadoop部署"></a>1.2 Hadoop部署</h2><p>集群规划</p>
<img src="Snipaste_2023-11-27_13-22-33.png" alt="Snipaste_2023-11-27_13-22-33" style="zoom:33%;">

<h2 id="1-3-Hive部署"><a href="#1-3-Hive部署" class="headerlink" title="1.3 Hive部署"></a>1.3 Hive部署</h2><p>集群规划</p>
<img src="Snipaste_2023-11-27_13-23-50.png" alt="Snipaste_2023-11-27_13-23-50" style="zoom:43%;">

<h2 id="1-4-测试"><a href="#1-4-测试" class="headerlink" title="1.4 测试"></a>1.4 测试</h2><p>启动hadoop集群，再启动hive客户端（前提是Hadoop3.1.3和Hive3.1.3安装包都是官网上下载的）</p>
<p>此时控制台会报错：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError:com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V</span><br></pre></td></tr></table></figure>

<h1 id="第二章-Hadoop和Hive的兼容性问题"><a href="#第二章-Hadoop和Hive的兼容性问题" class="headerlink" title="第二章 Hadoop和Hive的兼容性问题"></a>第二章 Hadoop和Hive的兼容性问题</h1><h2 id="2-1-问题原因"><a href="#2-1-问题原因" class="headerlink" title="2.1 问题原因"></a>2.1 问题原因</h2><p>上述问题是由 Hadoop3.1.3 版本所依赖的 guava-27.0-jre 和 Hive-3.1.3 版本所依赖的guava-19.0 不兼容所致。我们猜测27.0版本的guava中有checkArgument()这个方法，而19.0版本的guava中没有checkArgument()这个方法同名方法或者参数不对应，以至于我们启动Hive客户端时调用checkArgument()方法时找不到</p>
<p>Hive所依赖的：</p>
<p><img src="Snipaste_2023-11-27_13-35-08.png" alt="Snipaste_2023-11-27_13-35-08"></p>
<p>hadoop所依赖的：</p>
<p><img src="Snipaste_2023-11-27_13-37-09.png" alt="Snipaste_2023-11-27_13-37-09"></p>
<p>guava27.0文档：</p>
<p><img src="Snipaste_2023-11-27_13-48-59.png" alt="Snipaste_2023-11-27_13-48-59"></p>
<p>guava19.0文档：</p>
<p><img src="Snipaste_2023-11-27_13-51-56.png" alt="Snipaste_2023-11-27_13-51-56"></p>
<h2 id="2-2-解决思路"><a href="#2-2-解决思路" class="headerlink" title="2.2 解决思路"></a>2.2 解决思路</h2><p>（1）更换Hadoop版本</p>
<p>经过观察发现，Hadoop-3.1.0，Hadoop-3.1.1，Hadoop-3.1.2 版本的 guava 依赖均为 guava-11.0.2，而到了 Hadoop-3.1.3 版本，guava 依赖的版本突然升级到了 guava-27.0-jre。</p>
<p>Hive-3 的所有发行版本的 guava 依赖均为 guava-19.0。而 guava-19.0 和 guava-11.0.2 版本是兼容的，所以理论上降低 Hadoop 版本，这个问题就能得到有效的解决。</p>
<p>（2）升级Hive-3.1.3中的版本的guava依赖版本，并重新编译Hive</p>
<p>若将 Hive-3.1.3 中的 guava 依赖版本升级到 guava-27.0-jre，这样就能避免不同版本的guava 依赖冲突，上述问题同样能得到解决。</p>
<h2 id="2-3-解决实操"><a href="#2-3-解决实操" class="headerlink" title="2.3 解决实操"></a>2.3 解决实操</h2><p>此处，我们选择升级 Hive-3.1.3 中的 guava 依赖版本，所以我们需要拉取 Hive 源码进行修改并重新编译。由于 Hive 源码的编译工作需要在** Linux 系统**中进行，同时我们需要修改 Hive 的源码，所以我们需要一个合适的操作环境。</p>
<h3 id="2-3-1-搭建编译环境"><a href="#2-3-1-搭建编译环境" class="headerlink" title="2.3.1 搭建编译环境"></a>2.3.1 搭建编译环境</h3><p>（1）虚拟机准备</p>
<p>（2）安装jdk并解压，配置jdk环境变量，source一下使变量生效。安装成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# java -version</span><br><span class="line">java version &quot;1.8.0_212&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_212-b10)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)</span><br></pre></td></tr></table></figure>

<p>（3）安装Maven</p>
<p>①将Maven安装包上传到虚拟机&#x2F;opt&#x2F;software目录</p>
<p>②解压Maven到&#x2F;opt&#x2F;module目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>③配置Maven环境变量</p>
<p>编辑&#x2F;etc&#x2F;profile.d&#x2F;my_env.sh 文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">MAVEN_HOME</span></span><br><span class="line">export MAVEN_HOME=/opt/module/apache-maven-3.6.3</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# source /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>④监测Maven是否安装成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# mvn -version</span><br><span class="line">Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)</span><br><span class="line">Maven home: /opt/module/apache-maven-3.6.3</span><br><span class="line">Java version: 1.8.0_212, vendor: Oracle Corporation, runtime: /opt/module/jdk1.8.0_212/jre</span><br><span class="line">Default locale: zh_CN, platform encoding: UTF-8</span><br><span class="line">OS name: &quot;linux&quot;, version: &quot;3.10.0-1160.el7.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;</span><br></pre></td></tr></table></figure>

<p>⑤配置仓库镜像</p>
<p>修改Maven配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# vim /opt/module/apache-maven-3.6.3/conf/settings.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">id</span>&gt;</span>aliyunmaven<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>阿里云公共仓库<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://maven.aliyun.com/repository/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（4）安装Git</p>
<p>①安装第三方库</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# yum install https://repo.ius.io/ius-release-el7.rpm https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br></pre></td></tr></table></figure>

<p>②安装Git</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# yum install -y git236</span><br></pre></td></tr></table></figure>

<p>（5）安装IDEA</p>
<p>①解压IDEA安装包到&#x2F;opt&#x2F;software目录</p>
<p>②解压IDEA到&#x2F;opt&#x2F;module目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf ideaIU-2021.1.3.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>③启动IDEA</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# cd /opt/module/idea-IU-211.7628.21/</span><br><span class="line">[root@hadoop102 idea-IU-211.7628.21]# bin/idea.sh </span><br></pre></td></tr></table></figure>

<p>④配置Maven（输入激活码后）</p>
<img src="Snipaste_2023-11-27_14-52-03.png" alt="Snipaste_2023-11-27_14-52-03" style="zoom:43%;">

<img src="Snipaste_2023-11-27_14-54-10.png" alt="Snipaste_2023-11-27_14-54-10" style="zoom:43%;">

<h3 id="2-3-2-修改并编译Hive源码"><a href="#2-3-2-修改并编译Hive源码" class="headerlink" title="2.3.2 修改并编译Hive源码"></a>2.3.2 修改并编译Hive源码</h3><p>（1）在IDEA中新建项目拉取Hive源码</p>
<p>Hive 源码的远程仓库地址：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/apache/hive.git">https://github.com/apache/hive.git</a></p>
<p>国内镜像地址：</p>
<p><a target="_blank" rel="noopener" href="https://gitee.com/apache/hive.git">https://gitee.com/apache/hive.git</a></p>
<img src="Snipaste_2023-11-27_14-57-52.png" alt="Snipaste_2023-11-27_14-57-52" style="zoom:43%;">


      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">Linux</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/11/27/Hive%E8%B0%83%E4%BC%98%E5%8F%8A%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-HBase框架学习笔记" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/26/HBase%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">HBase框架学习笔记</a>
    </h1>
  

        
        <a href="/2023/11/26/HBase%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="archive-article-date">
  	<time datetime="2023-11-26T10:32:47.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-11-26</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第一章-HBase简介"><a href="#第一章-HBase简介" class="headerlink" title="第一章 HBase简介"></a>第一章 HBase简介</h1>
      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">Linux</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/11/26/HBase%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-基于阿里云DataWorks-MaxCompute构建电商离线数仓" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/24/%E5%9F%BA%E4%BA%8E%E9%98%BF%E9%87%8C%E4%BA%91DataWorks-MaxCompute%E6%9E%84%E5%BB%BA%E7%94%B5%E5%95%86%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/">基于阿里云DataWorks+MaxCompute构建电商离线数仓</a>
    </h1>
  

        
        <a href="/2023/11/24/%E5%9F%BA%E4%BA%8E%E9%98%BF%E9%87%8C%E4%BA%91DataWorks-MaxCompute%E6%9E%84%E5%BB%BA%E7%94%B5%E5%95%86%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/" class="archive-article-date">
  	<time datetime="2023-11-24T08:30:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-11-24</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="前置介绍"><a href="#前置介绍" class="headerlink" title="前置介绍"></a>前置介绍</h1><h2 id="阿里云大数据开发治理平台-DataWorks"><a href="#阿里云大数据开发治理平台-DataWorks" class="headerlink" title="阿里云大数据开发治理平台-DataWorks"></a>阿里云大数据开发治理平台-DataWorks</h2><p>将所有大数据组件汇聚在一起成为一个大数据平台，支持多种大数据引擎（比如下面介绍的MaxCompute）绑定，全图形化界面，SQL为主</p>
<h3 id="阿里云大数据计算服务-MaxCompute"><a href="#阿里云大数据计算服务-MaxCompute" class="headerlink" title="阿里云大数据计算服务-MaxCompute"></a>阿里云大数据计算服务-MaxCompute</h3><p>MaxCompute自动部署好了hadoop、Hive、HBase等体系</p>
<p><img src="Snipaste_2023-11-24_20-07-55.png" alt="Snipaste_2023-11-24_20-07-55"></p>
<h1 id="第一章-数据仓库概念"><a href="#第一章-数据仓库概念" class="headerlink" title="第一章 数据仓库概念"></a>第一章 数据仓库概念</h1><p>数据仓库定义（ Data Warehouse ），是为企业所有决策制定过程，提供所有系统数据支持的战略集合。</p>
<p>数据仓库 VS 传统数据库存储三大优势：（1） 体量大，效率高，数据量在超过一定的阈值后，MySQL的效率会急剧下降，所以MySQL并不适合海量数据的分析计算（2） 历史追查，时光回溯，传统数据库中的数据一旦修改删除就很难找回原有的模样，而数据仓库将不同时间段的数据存储在不同的分区表中（3） 数据可用性强</p>
<img src="Snipaste_2023-11-24_20-14-39.png" alt="Snipaste_2023-11-24_20-14-39" style="zoom:43%;">

<h1 id="第二章-项目需求及架构设计"><a href="#第二章-项目需求及架构设计" class="headerlink" title="第二章 项目需求及架构设计"></a>第二章 项目需求及架构设计</h1><h2 id="2-1-项目需求分析"><a href="#2-1-项目需求分析" class="headerlink" title="2.1 项目需求分析"></a>2.1 项目需求分析</h2><img src="Snipaste_2023-11-24_20-24-05.png" alt="Snipaste_2023-11-24_20-24-05" style="zoom:43%;">

<h2 id="2-2-项目框架"><a href="#2-2-项目框架" class="headerlink" title="2.2 项目框架"></a>2.2 项目框架</h2><img src="Snipaste_2023-11-24_20-24-33.png" alt="Snipaste_2023-11-24_20-24-33" style="zoom:50%;">

<h3 id="2-2-1-技术选型"><a href="#2-2-1-技术选型" class="headerlink" title="2.2.1 技术选型"></a>2.2.1 技术选型</h3><img src="Snipaste_2023-11-24_20-31-31.png" alt="Snipaste_2023-11-24_20-31-31" style="zoom:50%;">

<h3 id="2-2-2-系统数据流程设计"><a href="#2-2-2-系统数据流程设计" class="headerlink" title="2.2.2 系统数据流程设计"></a>2.2.2 系统数据流程设计</h3><img src="Snipaste_2023-11-24_20-37-27.png" alt="Snipaste_2023-11-24_20-37-27" style="zoom:50%;">

<h3 id="2-2-3-DataWorks和MaxCompute"><a href="#2-2-3-DataWorks和MaxCompute" class="headerlink" title="2.2.3 DataWorks和MaxCompute"></a>2.2.3 DataWorks和MaxCompute</h3><p>DataWorks基于阿里云MaxCompute&#x2F;Hologres&#x2F;EMR&#x2F;CDP等大数据引擎，为数据仓库&#x2F;数据湖&#x2F;湖仓一体等解决方案提供统一的全链路大数据开发治理平台。作为阿里巴巴数据中台的建设者，DataWorks从2009年起不断沉淀阿里巴巴大数据建设方法论，同时与数万名政务&#x2F;金融&#x2F;零售&#x2F;互联网&#x2F;能源&#x2F;制造等客户携手，助力产业数字化升级。</p>
<p>MaxCompute 是面向分析的企业级 SaaS 模式云数据仓库，以 Serverless 架构提供快速、全托管的在线数据仓库服务，消除了传统数据平台在资源扩展性和弹性方面的限制，最小化用户运维投入，使您可以经济并高效的分析处理海量数据。数以万计的企业正基于 MaxCompute 进行数据计算与分析，将数据高效转换为业务洞察。</p>
<img src="Snipaste_2023-11-24_20-44-45.png" alt="Snipaste_2023-11-24_20-44-45" style="zoom:50%;">

<p>盘古：相当于Hadoop中的HDFS</p>
<p>伏羲：相当于Hadoop中的YARN</p>
<p>MaxCompute Engine：相当于MR、Tez等计算引擎</p>
<p>MaxCompute和DataWorks一起向用户提供完善的ETL和数仓管理能力，以及SQL、MR、Graph等多种经典的分布式计算模型，能够更快速地解决用户海量数据计算问题，有效降低企业成本，保障数据安全。</p>
<h3 id="2-2-4-创建工作空间"><a href="#2-2-4-创建工作空间" class="headerlink" title="2.2.4 创建工作空间"></a>2.2.4 创建工作空间</h3><p>控制台入口：<a target="_blank" rel="noopener" href="https://workbench.data.aliyun.com/consolenew">https://workbench.data.aliyun.com/consolenew</a></p>
<p>（1）购买DataWorks</p>
<p><img src="Snipaste_2023-11-24_20-52-28.png" alt="Snipaste_2023-11-24_20-52-28"></p>
<p>（2）添加数据建模模块</p>
<p><img src="Snipaste_2023-11-24_20-53-09.png" alt="Snipaste_2023-11-24_20-53-09"></p>
<p>（3）同意协议并下单</p>
<p><img src="Snipaste_2023-11-24_20-53-50.png" alt="Snipaste_2023-11-24_20-53-50"></p>
<p>（4）进入控制台，创建工作空间</p>
<p><a target="_blank" rel="noopener" href="https://dataworks.console.aliyun.com/overview">DataWorks 管控台 (aliyun.com)</a></p>
<p><img src="Snipaste_2023-11-24_20-57-16.png" alt="Snipaste_2023-11-24_20-57-16"></p>
<img src="Snipaste_2023-11-24_20-59-51.png" alt="Snipaste_2023-11-24_20-59-51" style="zoom:43%;">

<img src="Snipaste_2023-11-24_21-00-17.png" alt="Snipaste_2023-11-24_21-00-17" style="zoom:43%;">

<p>（5）绑定引擎，MaxCompute</p>
<p>进入MaxCompute管控台</p>
<p><a target="_blank" rel="noopener" href="https://maxcompute.console.aliyun.com/cn-beijing/project-list?spm=5176.28008736.free-tier.3.c1863e4dsHFPSQ">MaxCompute管理控制台 (aliyun.com)</a></p>
<img src="Snipaste_2023-11-24_21-32-58.png" alt="Snipaste_2023-11-24_21-32-58" style="zoom:50%;">

<p>这里要注意，阿里云MaxCompute做了升级，通过DataWorks创建项目并绑定MaxCompute需要首先创建数据源，后面再看。</p>
<h1 id="第三章业务表格"><a href="#第三章业务表格" class="headerlink" title="第三章业务表格"></a>第三章业务表格</h1><h2 id="3-1-需要的业务表（仅下单支付业务）"><a href="#3-1-需要的业务表（仅下单支付业务）" class="headerlink" title="3.1 需要的业务表（仅下单支付业务）"></a>3.1 需要的业务表（仅下单支付业务）</h2><p><img src="%E5%9B%BE%E7%89%871.png" alt="图片1"></p>
<h3 id="3-1-1-省份表（base-province）"><a href="#3-1-1-省份表（base-province）" class="headerlink" title="3.1.1 省份表（base_province）"></a>3.1.1 省份表（base_province）</h3><img src="Snipaste_2023-11-25_17-54-06.png" alt="Snipaste_2023-11-25_17-54-06" style="zoom:43%;">

<h3 id="3-1-2-订单表（order-info）"><a href="#3-1-2-订单表（order-info）" class="headerlink" title="3.1.2 订单表（order_info）"></a>3.1.2 订单表（order_info）</h3><img src="Snipaste_2023-11-25_17-55-12.png" alt="Snipaste_2023-11-25_17-55-12" style="zoom:43%;">

<h3 id="3-1-3-订单状态流水表（order-status-log）"><a href="#3-1-3-订单状态流水表（order-status-log）" class="headerlink" title="3.1.3 订单状态流水表（order_status_log）"></a>3.1.3 订单状态流水表（order_status_log）</h3><img src="Snipaste_2023-11-25_17-56-55.png" alt="Snipaste_2023-11-25_17-56-55" style="zoom:43%;">

<h3 id="3-1-4-支付表（payment-info）"><a href="#3-1-4-支付表（payment-info）" class="headerlink" title="3.1.4 支付表（payment_info）"></a>3.1.4 支付表（payment_info）</h3><img src="Snipaste_2023-11-25_17-57-44.png" alt="Snipaste_2023-11-25_17-57-44" style="zoom:43%;">

<h3 id="3-1-5-订单明细表（order-detail）"><a href="#3-1-5-订单明细表（order-detail）" class="headerlink" title="3.1.5 订单明细表（order_detail）"></a>3.1.5 订单明细表（order_detail）</h3><img src="Snipaste_2023-11-25_17-58-42.png" alt="Snipaste_2023-11-25_17-58-42" style="zoom:43%;">

<h3 id="3-1-6-SKU信息表（sku-info）"><a href="#3-1-6-SKU信息表（sku-info）" class="headerlink" title="3.1.6 SKU信息表（sku_info）"></a>3.1.6 SKU信息表（sku_info）</h3><img src="Snipaste_2023-11-25_17-59-29.png" alt="Snipaste_2023-11-25_17-59-29" style="zoom:43%;">

<h3 id="3-1-7-用户信息表（user-info）"><a href="#3-1-7-用户信息表（user-info）" class="headerlink" title="3.1.7 用户信息表（user_info）"></a>3.1.7 用户信息表（user_info）</h3><img src="Snipaste_2023-11-25_18-00-21.png" alt="Snipaste_2023-11-25_18-00-21" style="zoom:43%;">

<h3 id="3-1-8-字典表（base-dic）"><a href="#3-1-8-字典表（base-dic）" class="headerlink" title="3.1.8 字典表（base_dic）"></a>3.1.8 字典表（base_dic）</h3><img src="Snipaste_2023-11-25_18-00-51.png" alt="Snipaste_2023-11-25_18-00-51" style="zoom:43%;">

<h2 id="3-2-同步策略"><a href="#3-2-同步策略" class="headerlink" title="3.2 同步策略"></a>3.2 同步策略</h2><p>数据同步策略的类型包括：全量同步、增量同步</p>
<ul>
<li>全量同步：每天同步完整的数据。</li>
<li>增量同步：每天同步新增加的数据。</li>
</ul>
<img src="Snipaste_2023-11-25_18-11-07.png" alt="Snipaste_2023-11-25_18-11-07" style="zoom:43%;">

<p>根据上述对比，可以得出以下结论：</p>
<p>通常情况，业务表数据量比较大，优先考虑增量，数据量比较小，优先考虑全量。</p>
<img src="Snipaste_2023-11-25_18-11-37.png" alt="Snipaste_2023-11-25_18-11-37" style="zoom:43%;">

<h1 id="第四章-数仓智能建模工具"><a href="#第四章-数仓智能建模工具" class="headerlink" title="第四章 数仓智能建模工具"></a>第四章 数仓智能建模工具</h1><h2 id="4-1-整体流程"><a href="#4-1-整体流程" class="headerlink" title="4.1 整体流程"></a>4.1 整体流程</h2><p>（1）数仓规划</p>
<p>（2）定义分层、数据域、业务过程</p>
<p>（3）创建维度层：定义维度、维度表</p>
<p>（4）创建明细层：定义明细层表</p>
<p>（5）创建汇总层：包括定义原子指标、派生指标、定义汇总层数据表</p>
<p>（6）创建应用层：包括创建数据集市、主题域、定义应用层数据表</p>
<h2 id="4-2-数仓规划"><a href="#4-2-数仓规划" class="headerlink" title="4.2 数仓规划"></a>4.2 数仓规划</h2><p>点击智能数据建模</p>
<p><img src="Snipaste_2023-11-24_21-40-40.png" alt="Snipaste_2023-11-24_21-40-40"></p>
<h3 id="4-2-1-业务分类"><a href="#4-2-1-业务分类" class="headerlink" title="4.2.1 业务分类"></a>4.2.1 业务分类</h3><p>业务分类：业务板块是某一大类的业务的指标和维度的集合，如电商，直播。</p>
<p><img src="Snipaste_2023-11-24_21-49-28.png" alt="Snipaste_2023-11-24_21-49-28"></p>
<h3 id="4-2-2-数仓分层"><a href="#4-2-2-数仓分层" class="headerlink" title="4.2.2 数仓分层"></a>4.2.2 数仓分层</h3><p>系统内置的数仓分层基本与本项目规划的分层一致，可以保持不做修改</p>
<p><img src="Snipaste_2023-11-24_21-46-13.png" alt="Snipaste_2023-11-24_21-46-13"></p>
<h4 id="4-2-2-1-维度层检查器"><a href="#4-2-2-1-维度层检查器" class="headerlink" title="4.2.2.1 维度层检查器"></a>4.2.2.1 维度层检查器</h4><p>点击维度层DIM，新增两个模型规则</p>
<p><img src="Snipaste_2023-11-24_21-54-24.png" alt="Snipaste_2023-11-24_21-54-24"></p>
<h4 id="4-2-2-2-明细数据层检查器"><a href="#4-2-2-2-明细数据层检查器" class="headerlink" title="4.2.2.2 明细数据层检查器"></a>4.2.2.2 明细数据层检查器</h4><p><img src="Snipaste_2023-11-24_21-56-45.png" alt="Snipaste_2023-11-24_21-56-45"></p>
<h4 id="4-2-2-3-汇总层检查器"><a href="#4-2-2-3-汇总层检查器" class="headerlink" title="4.2.2.3 汇总层检查器"></a>4.2.2.3 汇总层检查器</h4><p>增加模型规则：</p>
<p><img src="Snipaste_2023-11-24_21-58-55.png" alt="Snipaste_2023-11-24_21-58-55"></p>
<p>增加指标规则：</p>
<p><img src="Snipaste_2023-11-24_22-00-39.png" alt="Snipaste_2023-11-24_22-00-39"></p>
<h4 id="4-2-2-4-应用层检查器"><a href="#4-2-2-4-应用层检查器" class="headerlink" title="4.2.2.4 应用层检查器"></a>4.2.2.4 应用层检查器</h4><p><img src="Snipaste_2023-11-24_22-03-01.png" alt="Snipaste_2023-11-24_22-03-01"></p>
<h3 id="4-2-3-定义数据域"><a href="#4-2-3-定义数据域" class="headerlink" title="4.2.3 定义数据域"></a>4.2.3 定义数据域</h3><p>数据仓库模型设计除横向的分层外，通常也需要根据业务情况进行纵向划分数据域。</p>
<p>划分数据域的意义是便于数据的管理和应用。</p>
<p>通常可以根据业务过程或者部门进行划分，本项目根据业务过程进行划分，需要注意的是一个业务过程只能属于一个数据域。</p>
<p>下面是本项目所有业务过程及数据域划分详情。</p>
<img src="Snipaste_2023-11-24_22-04-05.png" alt="Snipaste_2023-11-24_22-04-05" style="zoom:43%;">

<p>创建数据域:</p>
<img src="Snipaste_2023-11-24_22-05-15.png" alt="Snipaste_2023-11-24_22-05-15" style="zoom:43%;">

<img src="Snipaste_2023-11-24_22-07-05.png" alt="Snipaste_2023-11-24_22-07-05" style="zoom:43%;">

<h3 id="4-2-4-定义业务过程"><a href="#4-2-4-定义业务过程" class="headerlink" title="4.2.4 定义业务过程"></a>4.2.4 定义业务过程</h3><p>新建业务过程</p>
<img src="Snipaste_2023-11-24_22-07-57.png" alt="Snipaste_2023-11-24_22-07-57" style="zoom:43%;">

<img src="Snipaste_2023-11-24_22-12-04.png" alt="Snipaste_2023-11-24_22-12-04" style="zoom:43%;">

<h1 id="第五章-业务数据准备"><a href="#第五章-业务数据准备" class="headerlink" title="第五章 业务数据准备"></a>第五章 业务数据准备</h1><h2 id="5-1-业务数仓架构图"><a href="#5-1-业务数仓架构图" class="headerlink" title="5.1 业务数仓架构图"></a>5.1 业务数仓架构图</h2><p>略</p>
<h2 id="5-2-RDS服务器准备"><a href="#5-2-RDS服务器准备" class="headerlink" title="5.2 RDS服务器准备"></a>5.2 RDS服务器准备</h2><h3 id="5-2-1-RDS服务器购买"><a href="#5-2-1-RDS服务器购买" class="headerlink" title="5.2.1 RDS服务器购买"></a>5.2.1 RDS服务器购买</h3><p>阿里云关系型数据库（Relational Database Service，简称RDS）是一种稳定可靠、可弹性伸缩的在线数据库服务。</p>
<p>购买RDS for MySQL服务器：<a target="_blank" rel="noopener" href="https://www.aliyun.com/product/rds/mysql">https://www.aliyun.com/product/rds/mysql</a></p>
<p>购买之后登录以下网站：</p>
<p><a target="_blank" rel="noopener" href="https://rdsnext.console.aliyun.com/dashboard/cn-hangzhou?spm=a2c6h.13066369.question.6.10367cacZrs44S">RDS管理控制台 (aliyun.com)</a></p>
<p>可以看到RDS实例正在运行中：</p>
<p><img src="Snipaste_2023-11-25_18-42-05.png" alt="Snipaste_2023-11-25_18-42-05"></p>
<h3 id="5-2-2-RDS服务器配置"><a href="#5-2-2-RDS服务器配置" class="headerlink" title="5.2.2 RDS服务器配置"></a>5.2.2 RDS服务器配置</h3><p>服务建立好以后，首先要建立连接通道，可以让用户远程操控RDS服务器。</p>
<p>（1）点击rm-cn-pe33hv29r000gz进入实例基本信息，再点击查看连接详情</p>
<img src="Snipaste_2023-11-25_18-50-42.png" alt="Snipaste_2023-11-25_18-50-42" style="zoom: 33%;">

<p>（2）点击配置白名单</p>
<p>白名单：白名单上的IP地址，是可以访问该RDS服务器，其他IP地址都拒绝。</p>
<img src="图片11.png" alt="图片11" style="zoom:43%;">

<p>修改白名单分组：</p>
<img src="Snipaste_2023-11-25_18-56-56.png" alt="Snipaste_2023-11-25_18-56-56" style="zoom:43%;">

<p>（3）再在基本信息业中点击开通外网地址</p>
<img src="图片12.png" alt="图片12" style="zoom:43%;">

<img src="图片13.png" alt="图片13" style="zoom:43%;">

<img src="Snipaste_2023-11-25_19-00-59.png" alt="Snipaste_2023-11-25_19-00-59" style="zoom:43%;">

<p>（4）点击rm-cn-pe33hv29r000gz进入实例基本信息，再点击账号管理，创建账号，用作远程对其连接：</p>
<p><img src="Snipaste_2023-11-25_18-44-23.png" alt="Snipaste_2023-11-25_18-44-23"></p>
<p>账号：root，密码：Wyhdhr19980418</p>
<p><img src="Snipaste_2023-11-25_18-46-42.png" alt="Snipaste_2023-11-25_18-46-42"></p>
<p><img src="Snipaste_2023-11-25_19-01-55.png" alt="Snipaste_2023-11-25_19-01-55"></p>
<h3 id="5-2-3-RDS服务器连接"><a href="#5-2-3-RDS服务器连接" class="headerlink" title="5.2.3 RDS服务器连接"></a>5.2.3 RDS服务器连接</h3><p>略</p>
<h2 id="5-3-创建业务数据库及表"><a href="#5-3-创建业务数据库及表" class="headerlink" title="5.3 创建业务数据库及表"></a>5.3 创建业务数据库及表</h2><p>创建并使用业务数据库gmall</p>
<img src="Snipaste_2023-11-25_19-41-29.png" alt="Snipaste_2023-11-25_19-41-29" style="zoom:33%;">

<p>导入数据库脚本</p>
<p><img src="Snipaste_2023-11-25_19-43-24.png" alt="Snipaste_2023-11-25_19-43-24"></p>
<p>导入成功：</p>
<img src="Snipaste_2023-11-25_19-45-13.png" alt="Snipaste_2023-11-25_19-45-13" style="zoom:43%;">

<h1 id="第六章-业务数仓搭建"><a href="#第六章-业务数仓搭建" class="headerlink" title="第六章 业务数仓搭建"></a>第六章 业务数仓搭建</h1><h2 id="6-1-业务数仓架构图"><a href="#6-1-业务数仓架构图" class="headerlink" title="6.1 业务数仓架构图"></a>6.1 业务数仓架构图</h2><p>略</p>
<h2 id="6-2-数据开发结构设计"><a href="#6-2-数据开发结构设计" class="headerlink" title="6.2 数据开发结构设计"></a>6.2 数据开发结构设计</h2><p>进入Dataworks控制台，进入工作空间列表，点击大数据开发-DataStudio</p>
<img src="Snipaste_2023-11-25_20-03-57.png" alt="Snipaste_2023-11-25_20-03-57" style="zoom:33%;">

<p>点击设置-表管理</p>
<img src="Snipaste_2023-11-25_20-06-44.png" alt="Snipaste_2023-11-25_20-06-44" style="zoom:33%;">

<p>添加用户主题、交易主题和基础主题</p>
<img src="Snipaste_2023-11-25_20-08-51.png" alt="Snipaste_2023-11-25_20-08-51" style="zoom:43%;">

<p>添加数仓层级</p>
<img src="Snipaste_2023-11-25_20-11-10.png" alt="Snipaste_2023-11-25_20-11-10" style="zoom:43%;">

<h2 id="6-3-数据同步"><a href="#6-3-数据同步" class="headerlink" title="6.3 数据同步"></a>6.3 数据同步</h2><p>ODS层存储的是原始数据，即不需要对数据进行任何处理，所以直接使用DataWorks的同步工具将数据导入到MaxComputer中即可。</p>
<p>（1）进入dataworks的DataStudio，点击左侧数据开发，新建-新建业务流程</p>
<img src="Snipaste_2023-11-25_20-32-19.png" alt="Snipaste_2023-11-25_20-32-19" style="zoom: 33%;">

<p>（2）新建文件夹</p>
<img src="Snipaste_2023-11-25_20-35-09.png" alt="Snipaste_2023-11-25_20-35-09" style="zoom:43%;">

<img src="Snipaste_2023-11-25_20-35-42.png" alt="Snipaste_2023-11-25_20-35-42" style="zoom:43%;">

<p>（3）新建离线同步节点</p>
<img src="Snipaste_2023-11-25_20-36-32.png" alt="Snipaste_2023-11-25_20-36-32" style="zoom:43%;">

<img src="Snipaste_2023-11-25_20-37-15.png" alt="Snipaste_2023-11-25_20-37-15" style="zoom:43%;">

<p>（4）新建数据源</p>
<img src="Snipaste_2023-11-25_20-40-11.png" alt="Snipaste_2023-11-25_20-40-11" style="zoom: 33%;">

<p>连接成功：</p>
<img src="Snipaste_2023-11-25_20-41-06.png" alt="Snipaste_2023-11-25_20-41-06" style="zoom: 33%;">

<p>（5）选择公共资源组</p>
<p>首先解决之前的MaxCompute引擎问题：</p>
<p>（1）进入Dataworks控制台，点击管理中心进入</p>
<img src="Snipaste_2023-11-25_20-16-43.png" alt="Snipaste_2023-11-25_20-16-43" style="zoom: 33%;">

<p>（2）点击计算引擎信息，选择MaxCompute，点击新增实例</p>
<img src="Snipaste_2023-11-25_20-17-29.png" alt="Snipaste_2023-11-25_20-17-29" style="zoom:33%;">

<p>（3）点击前往创建数据源-新增数据源，选择MySQL</p>
<img src="Snipaste_2023-11-25_20-18-43.png" alt="Snipaste_2023-11-25_20-18-43" style="zoom:43%;">

<p>（4）输入相应信息</p>
<img src="Snipaste_2023-11-25_20-21-54.png" alt="Snipaste_2023-11-25_20-21-54" style="zoom:33%;">






      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">项目</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/11/24/%E5%9F%BA%E4%BA%8E%E9%98%BF%E9%87%8C%E4%BA%91DataWorks-MaxCompute%E6%9E%84%E5%BB%BA%E7%94%B5%E5%95%86%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-大数据项目——在线教育" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/07/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE%E2%80%94%E2%80%94%E5%9C%A8%E7%BA%BF%E6%95%99%E8%82%B2/">大数据项目——在线教育</a>
    </h1>
  

        
        <a href="/2023/11/07/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE%E2%80%94%E2%80%94%E5%9C%A8%E7%BA%BF%E6%95%99%E8%82%B2/" class="archive-article-date">
  	<time datetime="2023-11-07T06:40:11.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-11-07</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="在线教育数仓中台立项方案"><a href="#在线教育数仓中台立项方案" class="headerlink" title="在线教育数仓中台立项方案"></a>在线教育数仓中台立项方案</h1><h2 id="背景分析"><a href="#背景分析" class="headerlink" title="背景分析"></a>背景分析</h2><p>企业制定决策，提供数据支持：构建数据仓库和BI看板，提高项目报告效率，提高客户满意度，沉淀项目数据资产，解决目前人工统计报表，没有直观数据产品的痛点。</p>
<p><strong>业务数据</strong>：就是各行业在处理事务过程中产生的数据。比如用户在电商网站中登录、下单、支付等过程中，需要和网站后台数据库进行增删改查交互，产生的数据就是业务数据。业务数据通常存储在MySQL、Oracle等数据库中。</p>
<p><strong>用户行为数据</strong>：用户在使用产品过程中，通过埋点收集与客户端产品交互过程中产生的数据，并发往日志服务器进行保存。比如页面浏览、点击、停留、评论、点赞、收藏等。</p>
<h2 id="项目目标"><a href="#项目目标" class="headerlink" title="项目目标"></a>项目目标</h2><img src="Snipaste_2023-11-07_15-04-20.png" alt="Snipaste_2023-11-07_15-04-20" style="zoom: 50%;">

<h2 id="面向人群及作用"><a href="#面向人群及作用" class="headerlink" title="面向人群及作用"></a>面向人群及作用</h2><p>面向人群：本产品赋能产品团队，可根据自身需求进行数据看板规划及落地数据 展示，其中包括日报、周报、月报、专项报告、数据看板；</p>
<p>优势及作用：通过标准数据仓库建设，各业务线统一使用平台能力实现降本增效；</p>
<p>价值体现：辅助团队提高撰写报告的效率 释放时间专注更高层的数据场景 解决临时数据查询需求。实现多业务线运营数据报告 辅助运营丰富关键指标。</p>
<h2 id="项目架构"><a href="#项目架构" class="headerlink" title="项目架构"></a>项目架构</h2><p><img src="Snipaste_2023-11-07_15-06-53.png" alt="Snipaste_2023-11-07_15-06-53"></p>
<h1 id="第一部分-采集平台-用户行为采集平台"><a href="#第一部分-采集平台-用户行为采集平台" class="headerlink" title="第一部分 采集平台-用户行为采集平台"></a>第一部分 采集平台-用户行为采集平台</h1><h2 id="第一章-数据仓库概念"><a href="#第一章-数据仓库概念" class="headerlink" title="第一章 数据仓库概念"></a>第一章 数据仓库概念</h2><p>数据仓库（ Data Warehouse ），是为企业制定决策，提供数据支持的。可以帮助企业改进业务流程、提高产品质量等。采集+存储+计算+分析。</p>
<p>数据仓库的输入数据通常包括：<strong>业务数据、用户行为数据和爬虫数据</strong>等</p>
<p><strong>业务数据：</strong>就是各行业在处理事务过程中产生的数据。比如用户在网站中登录、下单、支付等过程中，需要和网站后台数据库进行增删改查交互，产生的数据就是业务数据。<strong>业务数据通常存储在MySQL、Oracle等数据库中</strong>。</p>
<p><strong>用户行为数据</strong>：用户在使用产品过程中，通过<strong>埋点</strong>收集与客户端产品交互过程中产生的数据，并发往日志服务器进行保存。比如页面浏览、点击、停留、评论、点赞、收藏等。<strong>用户行为数据通常存储在日志文件中</strong>。</p>
<p><strong>爬虫数据</strong>：通常是通过技术手段获取其他公司网站的数据。不建议同学们这样去做。</p>
<img src="Snipaste_2023-11-07_15-45-44.png" alt="Snipaste_2023-11-07_15-45-44" style="zoom:43%;">

<p><img src="Snipaste_2023-11-07_15-49-46.png" alt="Snipaste_2023-11-07_15-49-46"></p>
<p> <img src="%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6(4).png" alt="未命名文件(4)"></p>
<h2 id="第二章-项目需求及架构设计"><a href="#第二章-项目需求及架构设计" class="headerlink" title="第二章 项目需求及架构设计"></a>第二章 项目需求及架构设计</h2><h3 id="2-1-项目需求分析"><a href="#2-1-项目需求分析" class="headerlink" title="2.1 项目需求分析"></a>2.1 项目需求分析</h3><p>（1）采集平台</p>
<ul>
<li>用户行为数据采集平台搭建</li>
<li>业务数据采集平台搭建</li>
</ul>
<img src="Snipaste_2023-11-07_16-11-43.png" alt="Snipaste_2023-11-07_16-11-43" style="zoom:43%;">

<h3 id="2-2-项目框架"><a href="#2-2-项目框架" class="headerlink" title="2.2 项目框架"></a>2.2 项目框架</h3><h4 id="2-2-1-技术选型"><a href="#2-2-1-技术选型" class="headerlink" title="2.2.1 技术选型"></a>2.2.1 技术选型</h4><img src="Snipaste_2023-11-07_16-17-14.png" alt="Snipaste_2023-11-07_16-17-14" style="zoom:50%;">

<h4 id="2-2-2-系统数据流程设计"><a href="#2-2-2-系统数据流程设计" class="headerlink" title="2.2.2 系统数据流程设计"></a>2.2.2 系统数据流程设计</h4><p><img src="Snipaste_2023-11-07_16-24-18.png" alt="Snipaste_2023-11-07_16-24-18"></p>
<h4 id="2-2-3-框架版本选择"><a href="#2-2-3-框架版本选择" class="headerlink" title="2.2.3 框架版本选择"></a>2.2.3 框架版本选择</h4><img src="Snipaste_2023-11-07_20-54-29.png" alt="Snipaste_2023-11-07_20-54-29" style="zoom:43%;">

<img src="Snipaste_2023-11-07_16-28-48.png" alt="Snipaste_2023-11-07_16-28-48" style="zoom:43%;">

<h4 id="2-2-4-服务器选型"><a href="#2-2-4-服务器选型" class="headerlink" title="2.2.4 服务器选型"></a>2.2.4 服务器选型</h4><img src="Snipaste_2023-11-07_20-54-52.png" alt="Snipaste_2023-11-07_20-54-52" style="zoom:43%;">

<h4 id="2-2-5-集群规模"><a href="#2-2-5-集群规模" class="headerlink" title="2.2.5 集群规模"></a>2.2.5 集群规模</h4><img src="Snipaste_2023-11-07_21-07-40.png" alt="Snipaste_2023-11-07_21-07-40" style="zoom:50%;">

<h4 id="2-2-6-集群资源规划设计"><a href="#2-2-6-集群资源规划设计" class="headerlink" title="2.2.6 集群资源规划设计"></a>2.2.6 集群资源规划设计</h4><p>在企业中通常会搭建一套生产集群和一套测试集群。生产集群运行生产任务，测试集群用于上线前代码编写和测试。</p>
<p>（1）生产集群</p>
<ul>
<li>消耗内存的分开（NN和rm通常部署在不同的服务器上）</li>
<li>数据传输数据比较紧密的放在一起（如kafka和ZooKeeper部署在同一台服务器上）</li>
<li>客户端尽量放在一到两台服务器上，方便外部访问（hive，mysql，spark，Azkaban等客户端）</li>
<li>有依赖关系的尽量放到同一台服务器上（如hive和Azkaban Executor）</li>
</ul>
<img src="Snipaste_2023-11-07_21-27-01.png" alt="Snipaste_2023-11-07_21-27-01" style="zoom:50%;">

<p>（2）测试集群</p>
<img src="Snipaste_2023-11-07_21-28-37.png" alt="Snipaste_2023-11-07_21-28-37" style="zoom:50%;">

<h2 id="第三章-数据生成模块"><a href="#第三章-数据生成模块" class="headerlink" title="第三章 数据生成模块"></a>第三章 数据生成模块</h2><h3 id="3-1-目标数据"><a href="#3-1-目标数据" class="headerlink" title="3.1 目标数据"></a>3.1 目标数据</h3><p>我们要收集和分析的数据主要包括<strong>页面数据、事件数据、曝光数据、启动数据、播放数据和错误数据</strong>。</p>
<h4 id="3-1-1-页面（page）"><a href="#3-1-1-页面（page）" class="headerlink" title="3.1.1 页面（page）"></a>3.1.1 页面（page）</h4><p>页面数据主要记录一个页面的用户访问情况，包括访问时间、停留时间、页面路径等信息。</p>
<p>（1）日志范例</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;actions&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">…</span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;common&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">…</span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;displays&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    …</span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;page&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;during_time&quot;</span><span class="punctuation">:</span> <span class="number">11622</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;57&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;last_page_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_list&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;page_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_detail&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645529967261</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>（2）所有页面类型如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">home(&quot;首页&quot;)</span><br><span class="line">course_list(&quot;列表页&quot;)</span><br><span class="line">course_detail(&quot;商品详情&quot;)</span><br><span class="line">chapter_video(&quot;章节视频&quot;)</span><br><span class="line">cart(&quot;购物车&quot;)</span><br><span class="line">order(&quot;下单结算&quot;)</span><br><span class="line">payment(&quot;支付页面&quot;)</span><br><span class="line">exam(&quot;考试&quot;)</span><br><span class="line">mine(&quot;我的&quot;)</span><br></pre></td></tr></table></figure>

<p>（3）所有页面对象类型如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">course_id(&quot;课程Id&quot;)</span><br><span class="line">keyword(&quot;搜索关键词&quot;)</span><br><span class="line">video_id(&quot;视频id&quot;)</span><br><span class="line">chapter_id(&quot;章节id&quot;)</span><br><span class="line">coupon_id(&quot;购物券id&quot;)</span><br><span class="line">order_id(&quot;订单id&quot;)</span><br><span class="line">paper_id(&quot;考卷id&quot;)</span><br><span class="line">exam_id(&quot;考试id&quot;)</span><br></pre></td></tr></table></figure>

<h4 id="3-1-2-事件（actions）"><a href="#3-1-2-事件（actions）" class="headerlink" title="3.1.2 事件（actions）"></a>3.1.2 事件（actions）</h4><p>事件数据主要记录应用内一个具体操作行为，包括操作类型、操作对象、操作对象描述等信息。</p>
<p>（1）日志范例</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;actions&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;action_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;favor_add&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;57&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645529967261</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;action_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cart_add&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;57&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645529967261</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;common&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">…</span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;displays&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">…</span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;page&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">…</span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645529967261</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>（2）所有动作类型如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">favor_add（新增收藏）</span><br><span class="line">review_add（新增课程评价）</span><br><span class="line">comment_add（新增章节评价）</span><br><span class="line">cart_add（加购物车）</span><br></pre></td></tr></table></figure>

<p>（3）所有动作目标类型如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">course_id(&quot;课程Id&quot;)</span><br><span class="line">keyword(&quot;搜索关键词&quot;)</span><br><span class="line">video_id(&quot;视频id&quot;)</span><br><span class="line">chapter_id(&quot;章节id&quot;)</span><br><span class="line">coupon_id(&quot;购物券id&quot;)</span><br><span class="line">order_id(&quot;订单id&quot;)</span><br><span class="line">paper_id(&quot;考卷id&quot;)</span><br><span class="line">exam_id(&quot;考试id&quot;)</span><br></pre></td></tr></table></figure>

<h4 id="3-1-3-曝光（displays）"><a href="#3-1-3-曝光（displays）" class="headerlink" title="3.1.3 曝光（displays）"></a>3.1.3 曝光（displays）</h4><p>曝光数据主要记录页面所展示的内容，包括曝光对象，曝光类型等信息。</p>
<p>（1）日志范例</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;actions&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    …</span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;common&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    …</span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;displays&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;query&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;6&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">4</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;query&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;8&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">5</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;query&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">4</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;query&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;promotion&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">4</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;promotion&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">4</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;query&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;9&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">7</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;page&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    …</span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645529967261</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>（2）所有曝光类型如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">promotion(&quot;商品推广&quot;)</span><br><span class="line">recommend(&quot;算法推荐商品&quot;)</span><br><span class="line">query(&quot;查询结果商品&quot;)</span><br><span class="line">activity(&quot;促销活动&quot;)</span><br></pre></td></tr></table></figure>

<p>（3）所有曝光对象类型如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">course_id(&quot;课程Id&quot;)</span><br><span class="line">keyword(&quot;搜索关键词&quot;)</span><br><span class="line">video_id(&quot;视频id&quot;)</span><br><span class="line">chapter_id(&quot;章节id&quot;)</span><br><span class="line">coupon_id(&quot;购物券id&quot;)</span><br><span class="line">order_id(&quot;订单id&quot;)</span><br><span class="line">paper_id(&quot;考卷id&quot;)</span><br><span class="line">exam_id(&quot;考试id&quot;)</span><br></pre></td></tr></table></figure>

<h4 id="3-1-4-启动（start）"><a href="#3-1-4-启动（start）" class="headerlink" title="3.1.4 启动（start）"></a>3.1.4 启动（start）</h4><p>启动数据记录应用的启动信息。</p>
<p>（1）日志范例</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;common&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    …</span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;start&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;entry&quot;</span><span class="punctuation">:</span> <span class="string">&quot;notice&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;first_open&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loading_time&quot;</span><span class="punctuation">:</span> <span class="number">17970</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;open_ad_id&quot;</span><span class="punctuation">:</span> <span class="number">20</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;open_ad_ms&quot;</span><span class="punctuation">:</span> <span class="number">2876</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;open_ad_skip_ms&quot;</span><span class="punctuation">:</span> <span class="number">0</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645532980257</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>（2）所有启动入口类型如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">icon(&quot;图标&quot;),</span><br><span class="line">notice(&quot;通知&quot;),</span><br><span class="line">install(&quot;安装后启动&quot;);</span><br></pre></td></tr></table></figure>

<h4 id="3-1-5-播放（appVideo）"><a href="#3-1-5-播放（appVideo）" class="headerlink" title="3.1.5 播放（appVideo）"></a>3.1.5 播放（appVideo）</h4><p>播放日志记录播放信息</p>
<p>（1）日志范例</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;appVideo&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;play_sec&quot;</span><span class="punctuation">:</span> <span class="number">19</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;video_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;3904&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;common&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">…</span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645526307119</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h4 id="3-1-6-错误（err）"><a href="#3-1-6-错误（err）" class="headerlink" title="3.1.6 错误（err）"></a>3.1.6 错误（err）</h4><p>错误数据记录应用使用过程中的错误信息，包括错误编号及错误信息。</p>
<p>上述五种日志都有可能包含错误信息，此处仅对 appVideo 的错误日志进行展示。</p>
<p>（1）日志范例</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;appVideo&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">  …</span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;common&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">  …</span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;err&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;error_code&quot;</span><span class="punctuation">:</span><span class="number">3485</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;msg&quot;</span><span class="punctuation">:</span><span class="string">&quot; Exception in thread \\  java.net.SocketTimeoutException\\n \\tat com.atguigu.edu2021.mock.log.AppError.main(AppError.java:xxxxxx)&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span><span class="number">1645538276217</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="3-2-数据埋点"><a href="#3-2-数据埋点" class="headerlink" title="3.2 数据埋点"></a>3.2 数据埋点</h3><h4 id="3-2-1-主流埋点方式（了解）"><a href="#3-2-1-主流埋点方式（了解）" class="headerlink" title="3.2.1 主流埋点方式（了解）"></a>3.2.1 主流埋点方式（了解）</h4><p>目前主流的埋点方式，有代码埋点（前端&#x2F;后端）、可视化埋点、全埋点三种。</p>
<p><strong>代码埋点</strong>是通过调用埋点SDK函数，在需要埋点的业务逻辑功能位置调用接口，上报埋点数据。例如，我们对页面中的某个按钮埋点后，当这个按钮被点击时，可以在这个按钮对应的 OnClick 函数里面调用SDK提供的数据发送接口，来发送数据。</p>
<p><strong>可视化埋点</strong>只需要研发人员集成采集 SDK，不需要写埋点代码，业务人员就可以通过访问分析平台的“圈选”功能，来“圈”出需要对用户行为进行捕捉的控件，并对该事件进行命名。圈选完毕后，这些配置会同步到各个用户的终端上，由采集 SDK 按照圈选的配置自动进行用户行为数据的采集和发送。</p>
<p><strong>全埋点</strong>是通过在产品中嵌入SDK，前端自动采集页面上的全部用户行为事件，上报埋点数据，相当于做了一个统一的埋点。然后再通过界面配置哪些数据需要在系统里面进行分析。</p>
<h4 id="3-2-2-埋点数据上报时机"><a href="#3-2-2-埋点数据上报时机" class="headerlink" title="3.2.2 埋点数据上报时机"></a>3.2.2 埋点数据上报时机</h4><p>埋点数据上报时机包括两种方式。</p>
<p>方式一，在离开该页面时，上传在这个页面产生的所有数据（页面、事件、曝光、错误等）。优点，批处理，减少了服务器接收数据压力。缺点，不是特别及时。</p>
<p>方式二，每个事件、动作、错误等，产生后，立即发送。优点，响应及时。缺点，对服务器接收数据压力比较大。</p>
<p>本次项目采用方式一埋点。</p>
<h4 id="3-2-3-埋点数据日志结构"><a href="#3-2-3-埋点数据日志结构" class="headerlink" title="3.2.3 埋点数据日志结构"></a>3.2.3 埋点数据日志结构</h4><p>我们的日志结构大致可分为三类，一是普通页面埋点日志，二是启动日志，三是播放日志。</p>
<p>普通页面日志结构如下，每条日志包含了，当前页面的页面信息，所有事件（动作）、所有曝光信息以及错误信息。除此之外，还包含了一系列公共信息，包括设备信息，地理位置，应用信息等，即下边的common字段。</p>
<p>（1）普通页面埋点日志格式</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;actions&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>           --用户在该页面所做的若干个动作记录</span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;action_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;favor_add&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;57&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645529967261</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;action_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cart_add&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;57&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645529967261</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;common&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>              --行为所处的各种环境信息</span><br><span class="line">    <span class="attr">&quot;ar&quot;</span><span class="punctuation">:</span> <span class="string">&quot;16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;ba&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iPhone&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;ch&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Appstore&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;is_new&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1&quot;</span><span class="punctuation">,</span> --是否首日使用，首次使用的当日，该字段值为<span class="number">1</span>，过了<span class="number">24</span><span class="punctuation">:</span><span class="number">00</span>，该字段置为<span class="number">0</span>。</span><br><span class="line">    <span class="attr">&quot;md&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iPhone 8&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;mid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mid_161&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;os&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iOS 13.3.1&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;sc&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;sid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;9acef85b-067d-49f9-9520-a0dda943304e&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;uid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;272&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;vc&quot;</span><span class="punctuation">:</span> <span class="string">&quot;v2.1.134&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;displays&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>           --若干个该页面的曝光信息</span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;query&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;6&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">4</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;query&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;8&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">5</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;query&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">4</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;query&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;promotion&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">4</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;promotion&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">4</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;query&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;9&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="number">7</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;page&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>                   --页面浏览记录</span><br><span class="line">    <span class="attr">&quot;during_time&quot;</span><span class="punctuation">:</span> <span class="number">11622</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;item&quot;</span><span class="punctuation">:</span> <span class="string">&quot;57&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;last_page_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_list&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;page_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;course_detail&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;err&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span>                     --在该页面发生的报错记录</span><br><span class="line">    <span class="attr">&quot;error_code&quot;</span><span class="punctuation">:</span><span class="number">1359</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;msg&quot;</span><span class="punctuation">:</span><span class="string">&quot; Exception in thread \\  java.net.SocketTimeoutException\\n \\tat com.atguigu.edu2021.mock.log.AppError.main(AppError.java:xxxxxx)&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645529967261</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>（2）启动日志格式</p>
<p>启动日志结构相对简单，主要包含公共信息，启动信息和错误信息。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;common&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>                   --启动时所处的环境信息</span><br><span class="line">    <span class="attr">&quot;ar&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;ba&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Redmi&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;ch&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandoujia&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;is_new&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;md&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Redmi k30&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;mid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mid_356&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;os&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Android 11.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;sc&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;sid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;76909678-abaf-41c4-916d-a0a72f546bc1&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;uid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;161&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;vc&quot;</span><span class="punctuation">:</span> <span class="string">&quot;v2.1.134&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;start&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>                      --启动记录</span><br><span class="line">    <span class="attr">&quot;entry&quot;</span><span class="punctuation">:</span> <span class="string">&quot;notice&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;first_open&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loading_time&quot;</span><span class="punctuation">:</span> <span class="number">17970</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;open_ad_id&quot;</span><span class="punctuation">:</span> <span class="number">20</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;open_ad_ms&quot;</span><span class="punctuation">:</span> <span class="number">2876</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;open_ad_skip_ms&quot;</span><span class="punctuation">:</span> <span class="number">0</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;err&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span>                       --本次启动时的报错记录</span><br><span class="line">    <span class="attr">&quot;error_code&quot;</span><span class="punctuation">:</span><span class="number">2959</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;msg&quot;</span><span class="punctuation">:</span><span class="string">&quot; Exception in thread \\  java.net.SocketTimeoutException\\n \\tat com.atguigu.edu2021.mock.log.AppError.main(AppError.java:xxxxxx)&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645532980257</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>（3）播放日志</p>
<p>播放日志结构相对简单，主要包含公共信息，播放信息和错误信息。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;appVideo&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>                 --视频信息</span><br><span class="line">    <span class="attr">&quot;play_sec&quot;</span><span class="punctuation">:</span> <span class="number">19</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;video_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;3904&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;common&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>                  --播放时所处的环境信息</span><br><span class="line">    <span class="attr">&quot;ar&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;ba&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Sumsung&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;ch&quot;</span><span class="punctuation">:</span> <span class="string">&quot;oppo&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;is_new&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;md&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Sumsung Galaxy S20&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;mid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mid_253&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;os&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Android 11.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;sc&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;sid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;47157c4a-4790-4b9a-a859-f0d36cd62a10&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;uid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;329&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;vc&quot;</span><span class="punctuation">:</span> <span class="string">&quot;v2.1.134&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;err&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span>                     --播放时报错记录</span><br><span class="line">    <span class="attr">&quot;error_code&quot;</span><span class="punctuation">:</span><span class="number">3485</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;msg&quot;</span><span class="punctuation">:</span><span class="string">&quot; Exception in thread \\  java.net.SocketTimeoutException\\n \\tat com.atguigu.edu2021.mock.log.AppError.main(AppError.java:xxxxxx)&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1645526307119</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="3-3-服务器和JDK准备"><a href="#3-3-服务器和JDK准备" class="headerlink" title="3.3 服务器和JDK准备"></a>3.3 服务器和JDK准备</h3><h4 id="3-3-1-服务器准备"><a href="#3-3-1-服务器准备" class="headerlink" title="3.3.1 服务器准备"></a>3.3.1 服务器准备</h4><p>看《Hadoop框架学习笔记》，不再赘述</p>
<h4 id="3-3-2-阿里云服务器准备（略）"><a href="#3-3-2-阿里云服务器准备（略）" class="headerlink" title="3.3.2 阿里云服务器准备（略）"></a>3.3.2 阿里云服务器准备（略）</h4><h4 id="3-3-3-编写集群分发脚本xsync（略）"><a href="#3-3-3-编写集群分发脚本xsync（略）" class="headerlink" title="3.3.3 编写集群分发脚本xsync（略）"></a>3.3.3 编写集群分发脚本xsync（略）</h4><h4 id="3-3-4-SSH无密登录配置（略）"><a href="#3-3-4-SSH无密登录配置（略）" class="headerlink" title="3.3.4 SSH无密登录配置（略）"></a>3.3.4 SSH无密登录配置（略）</h4><h4 id="3-3-5-JDK准备"><a href="#3-3-5-JDK准备" class="headerlink" title="3.3.5 JDK准备"></a>3.3.5 JDK准备</h4><h4 id="3-3-6-环境变量配置说明"><a href="#3-3-6-环境变量配置说明" class="headerlink" title="3.3.6 环境变量配置说明"></a>3.3.6 环境变量配置说明</h4><h3 id="3-4-模拟数据"><a href="#3-4-模拟数据" class="headerlink" title="3.4 模拟数据"></a>3.4 模拟数据</h3><h4 id="3-4-1-使用说明"><a href="#3-4-1-使用说明" class="headerlink" title="3.4.1 使用说明"></a>3.4.1 使用说明</h4><p>（1）将application.yml、edu2021-mock-2022-06-18.jar、path.json、edu0222.sql、logback.xml上传到hadoop102的&#x2F;opt&#x2F;module&#x2F;data_mocker目录下</p>
<p>①创建applog路径</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# mkdir /opt/module/data_mocker</span><br></pre></td></tr></table></figure>

<p>②上传上述文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 data_mocker]# ll</span><br><span class="line">总用量 26492</span><br><span class="line">-rw-r--r-- 1 root root     2209 6月   6 10:54 application.yml</span><br><span class="line">-rw-r--r-- 1 root root 27112074 6月   6 10:54 edu2021-mock-2022-06-18.jar</span><br><span class="line">-rw-r--r-- 1 root root     1137 6月   6 10:54 logback.xml</span><br><span class="line">-rw-r--r-- 1 root root      643 6月   6 10:54 path.json</span><br></pre></td></tr></table></figure>

<p>③application.yml文件</p>
<p>可以根据需求生成对应日期的用户行为日志。</p>
<p>修改配置文件，通过修改配置文件中的mock.date参数，可以得到不同日期的日志数据，其余参数可以根据注释，按照个人需求进行修改。</p>
<p>④path.json文件，可以灵活配置访问路径</p>
<p>⑤logback.xml文件，可以配置日志生成路径</p>
<p>（2）安装Mysql</p>
<p>见《Hive框架学习笔记》，mysql服务已经启动：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 data_mocker]# systemctl status mysqld</span><br><span class="line">● mysqld.service - MySQL Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 三 2023-11-08 14:29:51 CST; 52min ago</span><br><span class="line">     Docs: man:mysqld(8)</span><br><span class="line">           http://dev.mysql.com/doc/refman/en/using-systemd.html</span><br><span class="line">  Process: 1505 ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid $MYSQLD_OPTS (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 1080 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 1507 (mysqld)</span><br><span class="line">    Tasks: 27</span><br><span class="line">   CGroup: /system.slice/mysqld.service</span><br><span class="line">           └─1507 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 data_mocker]# mysql -uroot -p&#x27;wyhdhr19980418&#x27;</span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 2</span><br><span class="line">Server version: 5.7.28 MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">mysql&gt; </span></span><br></pre></td></tr></table></figure>

<p>执行以下命令，创建业务数据库edu，以及所有相关的表格</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database edu charset utf8 <span class="keyword">default</span> <span class="keyword">collate</span> utf8_general_ci;</span><br><span class="line">mysql<span class="operator">&gt;</span> use edu;</span><br><span class="line">mysql<span class="operator">&gt;</span> source <span class="operator">/</span>opt<span class="operator">/</span>software<span class="operator">/</span>edu0222.sql;</span><br></pre></td></tr></table></figure>

<h4 id="3-4-2-数据生成测试"><a href="#3-4-2-数据生成测试" class="headerlink" title="3.4.2 数据生成测试"></a>3.4.2 数据生成测试</h4><p>（1）修改application.yml配置文件中的参数，生成2022-2-21日的用户行为日志和业务数据。每次生成第一天的数据，都要将mock.clear.busi和mock.clear.user参数修改为1，表示将数据库中的实时数据和业务数据全部重置。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mock.date: &quot;2022-02-21&quot;</span><br><span class="line">mock.clear.busi: 1</span><br><span class="line">mock.clear.user: 1</span><br></pre></td></tr></table></figure>

<p>（2）模拟生成2022-2-21的数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 data_mocker]# java -jar edu2021-mock-2022-06-18.jar</span><br></pre></td></tr></table></figure>

<p>使用datagrip可以看到生成的业务数据，可以看到用户数据已经被重置了，新生成了50条创建日期为2022-02-21的用户数据</p>
<p><img src="Snipaste_2023-11-08_19-29-30.png" alt="Snipaste_2023-11-08_19-29-30"></p>
<p>（3）在配置文件application.yml中修改如下配置，第二次生成的数据不清空原有数据</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mock.date: &quot;2022-02-22&quot;</span><br><span class="line">mock.clear.busi: 0</span><br><span class="line">mock.clear.user: 0</span><br></pre></td></tr></table></figure>

<p>（4）再次执行命令，生成2022-02-22的数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 data_mocker]# java -jar edu2021-mock-2022-06-18.jar</span><br></pre></td></tr></table></figure>

<p>使用datagrip可以看到生成的业务数据，可以看到用户数据从原来的50条增加到了现在的100条，说明成功生成了新数据</p>
<p><img src="Snipaste_2023-11-08_19-35-28.png" alt="Snipaste_2023-11-08_19-35-28"></p>
<h4 id="3-4-3-数据生成脚本"><a href="#3-4-3-数据生成脚本" class="headerlink" title="3.4.3 数据生成脚本"></a>3.4.3 数据生成脚本</h4><p>脚本思路：</p>
<ul>
<li>当生成第一天（2022-02-21）的数据时，通过sed命令，修改application.yml文件中mock.clear.busi和mock.clear.user参数为1</li>
<li>当生成其他日期的数据时，通过sed命令，修改application.yml文件中mock.clear.busi和mock.clear.user参数为0</li>
</ul>
<p>（1）在~&#x2F;bin目录下创建脚本mock.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim mock.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">DATA_HOME=/opt/module/data_mocker</span><br><span class="line"></span><br><span class="line">function mock_data() &#123;</span><br><span class="line">  if [ $1 ]</span><br><span class="line">  then</span><br><span class="line">    sed -i &quot;/mock.date/s/.*/mock.date: \&quot;$1\&quot;/&quot; $DATA_HOME/application.yml</span><br><span class="line">    echo &quot;正在生成 $1 当日的数据&quot;</span><br><span class="line">  fi</span><br><span class="line">  cd $DATA_HOME</span><br><span class="line">      nohup java -jar &quot;edu2021-mock-2022-06-18.jar&quot; &gt;/dev/null 2&gt;&amp;1  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;init&quot;)</span><br><span class="line">  [ $2 ] &amp;&amp; do_date=$2 || do_date=&#x27;2022-02-21&#x27;</span><br><span class="line">  sed -i &quot;/mock.clear.busi/s/.*/mock.clear.busi: 1/&quot; $DATA_HOME/application.yml</span><br><span class="line">  sed -i &quot;/mock.clear.user/s/.*/mock.clear.user: 1/&quot; $DATA_HOME/application.yml</span><br><span class="line">  mock_data $(date -d &quot;$do_date -5 days&quot; +%F)</span><br><span class="line">  sed -i &quot;/mock.clear.busi/s/.*/mock.clear.busi: 0/&quot; $DATA_HOME/application.yml</span><br><span class="line">  sed -i &quot;/mock.clear.user/s/.*/mock.clear.user: 0/&quot; $DATA_HOME/application.yml</span><br><span class="line">  for ((i=4;i&gt;=0;i--));</span><br><span class="line">  do</span><br><span class="line">    mock_data $(date -d &quot;$do_date -$i days&quot; +%F)</span><br><span class="line">  done</span><br><span class="line">  ;;</span><br><span class="line">[0-9][0-9][0-9][0-9]-[0-1][0-9]-[0-3][0-9])</span><br><span class="line"></span><br><span class="line">    # sed -i &quot;/mock_date/s/.*/mock_date=$1/&quot; $MAXWELL_HOME/config.properties</span><br><span class="line">    # mxw.sh restart</span><br><span class="line">    sleep 1  </span><br><span class="line">    mock_data $1</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）增加脚本执行权限：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 mock.sh </span><br></pre></td></tr></table></figure>

<p>（3）脚本使用说明：传入参数init，则生成2022-02-21以及前5天的数据，传入具体日期，如2022-02-22，则只生成该日期当天的数据</p>
<p>测试：传入init</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# mock.sh init</span><br><span class="line">正在生成 2022-02-16 当日的数据</span><br><span class="line">正在生成 2022-02-17 当日的数据</span><br><span class="line">正在生成 2022-02-18 当日的数据</span><br><span class="line">正在生成 2022-02-19 当日的数据</span><br><span class="line">正在生成 2022-02-20 当日的数据</span><br><span class="line">正在生成 2022-02-21 当日的数据</span><br></pre></td></tr></table></figure>

<p>2022-02-16~2022-02-21共6天的数据，每天50条，共300条：</p>
<p><img src="Snipaste_2023-11-08_19-55-37.png" alt="Snipaste_2023-11-08_19-55-37"></p>
<p>测试：传入参数2022-02-22</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# mock.sh 2022-02-22</span><br><span class="line">正在生成 2022-02-22 当日的数据</span><br></pre></td></tr></table></figure>

<p>新增2022-02-22数据，共350条：</p>
<p><img src="Snipaste_2023-11-08_19-58-05.png" alt="Snipaste_2023-11-08_19-58-05"></p>
<h2 id="第四章-数据采集模块"><a href="#第四章-数据采集模块" class="headerlink" title="第四章 数据采集模块"></a>第四章 数据采集模块</h2><h3 id="4-1-集群所有进程查看脚本（略）"><a href="#4-1-集群所有进程查看脚本（略）" class="headerlink" title="4.1 集群所有进程查看脚本（略）"></a>4.1 集群所有进程查看脚本（略）</h3><p>xcall.sh</p>
<h3 id="4-2-Hadoop安装（略）"><a href="#4-2-Hadoop安装（略）" class="headerlink" title="4.2 Hadoop安装（略）"></a>4.2 Hadoop安装（略）</h3><h3 id="4-3-ZooKeeper安装（略）"><a href="#4-3-ZooKeeper安装（略）" class="headerlink" title="4.3 ZooKeeper安装（略）"></a>4.3 ZooKeeper安装（略）</h3><h3 id="4-4-Kafka安装（略）"><a href="#4-4-Kafka安装（略）" class="headerlink" title="4.4 Kafka安装（略）"></a>4.4 Kafka安装（略）</h3><h3 id="4-5-采集日志Flume"><a href="#4-5-采集日志Flume" class="headerlink" title="4.5 采集日志Flume"></a>4.5 采集日志Flume</h3><h4 id="4-5-1-Flume安装（略）"><a href="#4-5-1-Flume安装（略）" class="headerlink" title="4.5.1 Flume安装（略）"></a>4.5.1 Flume安装（略）</h4><h4 id="4-5-2-Flume组件选型"><a href="#4-5-2-Flume组件选型" class="headerlink" title="4.5.2 Flume组件选型"></a>4.5.2 Flume组件选型</h4><p>（1）Source</p>
<p>本项目主要从一个实时写入数据的文件夹中读取数据，Source可以选择Taildir Source、Exec Source、Spooling Directory Source</p>
<p>TailDir Source：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。</p>
<p>Exec Source可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。</p>
<p>Spooling Directory Source监控目录，支持断点续传。</p>
<p>（2）Channel</p>
<p>采用Kafka Channel，省去了Sink，提高了效率。KafkaChannel数据存储在Kafka里面，所以数据是存储在磁盘中。</p>
<p>注意在Flume1.7以前，Kafka Channel很少有人使用，因为发现parseAsFlumeEvent这个配置起不了作用。也就是无论parseAsFlumeEvent配置为true还是false，都会转为Flume Event。这样的话，造成的结果是，会始终都把Flume的headers中的信息混合着内容一起写入Kafka的消息中，这显然不是我所需要的，我只是需要把内容写入即可。</p>
<img src="Snipaste_2023-11-08_20-45-36.png" alt="Snipaste_2023-11-08_20-45-36" style="zoom:43%;">

<h4 id="4-5-3-Flume配置"><a href="#4-5-3-Flume配置" class="headerlink" title="4.5.3 Flume配置"></a>4.5.3 Flume配置</h4><p>（1）Flume配置分析</p>
<p>如上图</p>
<p>（2）具体配置</p>
<p>在&#x2F;opt&#x2F;module&#x2F;flume&#x2F;job目录下创建file-flume-kafka.conf文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# vim file-flume-kafka.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">为各组件命名</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">描述<span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /opt/module/data_mocker/log/app.*</span><br><span class="line">a1.sources.r1.positionFile = /opt/module/data_mocker/log_position.json</span><br><span class="line">a1.sources.r1.interceptors =  i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptors.ETLInterceptor$Builder</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">描述channel</span></span><br><span class="line">a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line">a1.channels.c1.kafka.topic = topic_log</span><br><span class="line">a1.channels.c1.parseAsFlumeEvent = false</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">绑定<span class="built_in">source</span>和channel以及sink和channel的关系</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure>

<h4 id="4-5-4-Flume拦截器"><a href="#4-5-4-Flume拦截器" class="headerlink" title="4.5.4 Flume拦截器"></a>4.5.4 Flume拦截器</h4><p>本层Flume需要自定义拦截器，通过自定义拦截器过滤掉JSON结构不完整的日志，做到对日志数据的初步清洗</p>
<p>（1）创建Maven工程edu-flume-interceptor</p>
<p>（2）创建包名：com.atguigu.flume.interceptor</p>
<p>（3）在pom.xml文件中添加如下依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.62<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（4）在com.atguigu.flume.interceptors包下创建JSONUtils类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">JSONUtils</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">isJSONValidate</span><span class="params">(String log)</span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            JSON.parse(log);</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;<span class="keyword">catch</span> (JSONException e)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（5）在com.atguigu.flume.interceptors包下创建ETLInterceptor类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ETLInterceptor</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initialize</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 过滤掉脏数据（不完整的JSON）</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> event</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Event <span class="title function_">intercept</span><span class="params">(Event event)</span> &#123;</span><br><span class="line">        <span class="comment">//1 获取body当中的数据</span></span><br><span class="line">        <span class="type">byte</span>[] body = event.getBody();</span><br><span class="line">        <span class="type">String</span> <span class="variable">log</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(body, StandardCharsets.UTF_8);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2 判断数据是否是完整的json</span></span><br><span class="line">        <span class="keyword">if</span> (JSONUtils.isJSONValidate(log)) &#123;</span><br><span class="line">            <span class="keyword">return</span> event;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;Event&gt; <span class="title function_">intercept</span><span class="params">(List&lt;Event&gt; list)</span> &#123;</span><br><span class="line">        Iterator&lt;Event&gt; iterator = list.iterator();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (iterator.hasNext())&#123;</span><br><span class="line">            <span class="type">Event</span> <span class="variable">next</span> <span class="operator">=</span> iterator.next();</span><br><span class="line">            <span class="keyword">if</span>(intercept(next)==<span class="literal">null</span>)&#123;</span><br><span class="line">                iterator.remove();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Builder</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span>.Builder&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Interceptor <span class="title function_">build</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ETLInterceptor</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（6）打包</p>
<img src="Snipaste_2023-11-08_21-24-10.png" alt="Snipaste_2023-11-08_21-24-10" style="zoom: 50%;">

<p>（7）将jar包上传至hadoop102的&#x2F;opt&#x2F;module&#x2F;lib目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 lib]# ls | grep inter</span><br><span class="line">edu-flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<p>（8）启动kafka集群，在Kafka集群中创建对应的topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# kafka-topics.sh --bootstrap-server hadoop102:9092 --create --replication-factor 1 --partitions 1 --topic topic_log</span><br><span class="line">Created topic topic_log.</span><br></pre></td></tr></table></figure>

<p>（9）启动Flume日志采集程序，采集目标文件中生成的日志文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# bin/flume-ng agent --name a1 --conf-file job/file-flume-kafka.conf --conf conf/ -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>（10）启动Kafka的控制台消费者，等待Flume将数据发送至Kafka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic topic_log</span><br></pre></td></tr></table></figure>

<p>（11）启动日志生成程序，模拟日志生成</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# mock.sh 2022-02-22</span><br><span class="line">正在生成 2022-02-22 当日的数据</span><br></pre></td></tr></table></figure>

<p>同时可以看到kafka控制台消费者不停地消费日志数据，表示采集日志的Flume配置成功！！！</p>
<p>kafka消费端：</p>
<img src="Snipaste_2023-11-08_21-45-42.png" alt="Snipaste_2023-11-08_21-45-42" style="zoom: 33%;">

<h4 id="4-5-5-采集日志Flume启动、停止脚本"><a href="#4-5-5-采集日志Flume启动、停止脚本" class="headerlink" title="4.5.5 采集日志Flume启动、停止脚本"></a>4.5.5 采集日志Flume启动、停止脚本</h4><p>（1）在~&#x2F;bin目录下创建脚本f1.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim f1.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">                echo &quot; --------启动 hadoop102 采集flume-------&quot;</span><br><span class="line">                ssh hadoop102 &quot;nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf/ -f /opt/module/flume/job/file-flume-kafka.conf &gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">&#125;;;	</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">                echo &quot; --------停止 hadoop102 采集flume-------&quot;</span><br><span class="line">                ssh hadoop102 &quot;ps -ef | grep file-flume-kafka | grep -v grep |awk  &#x27;&#123;print \$2&#125;&#x27; | xargs -n1 kill -9 &quot;</span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）增加脚本权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 f1.sh</span><br></pre></td></tr></table></figure>

<p>（3）启动脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# f1.sh start</span><br><span class="line"> --------启动 hadoop102 采集flume-------</span><br><span class="line">[root@hadoop102 bin]# jps</span><br><span class="line">9362 Application    #在这里</span><br><span class="line">6408 JobHistoryServer</span><br><span class="line">6536 QuorumPeerMain</span><br><span class="line">5705 NameNode</span><br><span class="line">9482 Jps</span><br><span class="line">5885 DataNode</span><br><span class="line">6221 NodeManager</span><br><span class="line">7007 Kafka</span><br></pre></td></tr></table></figure>

<p>（4）停止脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# f1.sh stop</span><br><span class="line"> --------停止 hadoop102 采集flume-------</span><br><span class="line">[root@hadoop102 bin]# jps</span><br><span class="line">9556 Jps</span><br><span class="line">6408 JobHistoryServer</span><br><span class="line">6536 QuorumPeerMain</span><br><span class="line">5705 NameNode</span><br><span class="line">5885 DataNode</span><br><span class="line">6221 NodeManager</span><br><span class="line">7007 Kafka</span><br></pre></td></tr></table></figure>

<h3 id="4-6-消费Kafka数据的Flume"><a href="#4-6-消费Kafka数据的Flume" class="headerlink" title="4.6 消费Kafka数据的Flume"></a>4.6 消费Kafka数据的Flume</h3><p><img src="Snipaste_2023-11-09_14-07-58.png" alt="Snipaste_2023-11-09_14-07-58"></p>
<img src="Snipaste_2023-11-09_14-08-37.png" alt="Snipaste_2023-11-09_14-08-37" style="zoom:43%;">

<h4 id="4-6-1-Flume组件选型"><a href="#4-6-1-Flume组件选型" class="headerlink" title="4.6.1 Flume组件选型"></a>4.6.1 Flume组件选型</h4><p>（1）FileChannel和MemoryChannel</p>
<p>金融类公司、对钱要求非常准确的公司通常会选择FileChannel。</p>
<p>传输的是普通日志信息（京东内部一天丢100万-200万条，这是非常正常的），通常选择MemoryChannel。</p>
<p>通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。checkpointDir和backupCheckpointDir也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据。</p>
<p>（2）Sink</p>
<p>选HDFS Sink；在元数据层面，每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所属组，权限，创建时间等，这些信息都保存在Namenode内存中。所以小文件过多，会占用Namenode服务器大量内存，影响Namenode性能和使用寿命。计算层面，默认情况下MR会对每个小文件启用一个Map任务计算，非常影响计算性能。同时也影响磁盘寻址时间。 </p>
<p>基于以上考虑，在对HDFS Sink进行配置时，可以通过调整Flume官方提供的三个参数避免写入HDFS大量小文件，基于以上hdfs.rollInterval&#x3D;3600，hdfs.rollSize&#x3D;134217728，hdfs.rollCount &#x3D;0几个参数综合作用，效果如下：</p>
<ul>
<li>文件在达到128M时会滚动生成新文件</li>
<li>文件创建超3600秒时会滚动生成新文件</li>
<li>不通过Event个数来决定何时滚动生成新文件</li>
</ul>
<img src="Snipaste_2023-11-09_14-19-06.png" alt="Snipaste_2023-11-09_14-19-06" style="zoom:43%;">

<h4 id="4-6-2-Flume配置"><a href="#4-6-2-Flume配置" class="headerlink" title="4.6.2 Flume配置"></a>4.6.2 Flume配置</h4><p>（1）在hadoop104的&#x2F;opt&#x2F;module&#x2F;flume&#x2F;job目录下创建kafka-flume-hdfs.conf文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 job]# vim kafka-flume-hdfs.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 组件</span></span></span><br><span class="line">a1.sources=r1</span><br><span class="line">a1.channels=c1</span><br><span class="line">a1.sinks=k1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># source1</span></span></span><br><span class="line">a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">a1.sources.r1.batchSize = 5000</span><br><span class="line">a1.sources.r1.batchDurationMillis = 2000</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line">a1.sources.r1.kafka.topics=topic_log</span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">拦截器全类名应该根据实际情况修改</span></span><br><span class="line">a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptors.TimestampInterceptor$Builder</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># channel1</span></span></span><br><span class="line">a1.channels.c1.type = file</span><br><span class="line">a1.channels.c1.checkpointDir = /opt/data/flume/checkpoint/behavior1</span><br><span class="line">a1.channels.c1.dataDirs = /opt/data/flume/data/behavior1/</span><br><span class="line">a1.channels.c1.maxFileSize = 2146435071</span><br><span class="line">a1.channels.c1.capacity = 1000000</span><br><span class="line">a1.channels.c1.keep-alive = 6</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># sink1</span></span></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /origin_data/edu/log/edu_log/%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = log</span><br><span class="line">a1.sinks.k1.hdfs.round = false</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 10</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 控制输出文件是原生文件。</span></span></span><br><span class="line">a1.sinks.k1.hdfs.fileType = CompressedStream</span><br><span class="line">a1.sinks.k1.hdfs.codeC = gzip</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 拼装</span></span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel= c1</span><br></pre></td></tr></table></figure>

<p>（2）在hadoop104节点服务器的&#x2F;opt路径下创建data目录，用于存放FileChannel相关文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 opt]# mkdir data</span><br><span class="line">[root@hadoop104 opt]# chown atguigu:atguigu data</span><br></pre></td></tr></table></figure>

<h4 id="4-6-3-时间戳拦截器（数据漂移）"><a href="#4-6-3-时间戳拦截器（数据漂移）" class="headerlink" title="4.6.3 时间戳拦截器（数据漂移）"></a>4.6.3 时间戳拦截器（数据漂移）</h4><p>由于flume默认会用linux系统时间，作为输出到HDFS路径的时间。如果数据是23:59分产生的。Flume消费kafka里面的数据时，有可能已经是第二天了，那么这部分数据会被发往第二天的HDFS路径。我们希望的是根据日志里面的实际时间，发往HDFS的路径，所以下面拦截器作用是获取日志中的实际时间。</p>
<p>思路：拦截JSON日志，通过fastjson框架解析JSON，获取实际时间ts。将获取的ts时间写入拦截器header中，header的key必须是timestamp，因为Flume框架会根据这个key值识别时间，并将数据写入HDFS对应时间的路径下。</p>
<p>（1）在com.atguigu.flume.interceptors包下创建TimestampInterceptor类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TimestampInterceptor</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;Event&gt; events = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initialize</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Event <span class="title function_">intercept</span><span class="params">(Event event)</span> &#123;</span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line">        <span class="type">String</span> <span class="variable">log</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(event.getBody(), StandardCharsets.UTF_8);</span><br><span class="line"></span><br><span class="line">        <span class="type">JSONObject</span> <span class="variable">jsonObject</span> <span class="operator">=</span> JSONObject.parseObject(log);</span><br><span class="line"></span><br><span class="line">        <span class="type">String</span> <span class="variable">ts</span> <span class="operator">=</span> jsonObject.getString(<span class="string">&quot;ts&quot;</span>);</span><br><span class="line">        headers.put(<span class="string">&quot;timestamp&quot;</span>, ts);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;Event&gt; <span class="title function_">intercept</span><span class="params">(List&lt;Event&gt; list)</span> &#123;</span><br><span class="line">        events.clear();</span><br><span class="line">        <span class="keyword">for</span> (Event event : list) &#123;</span><br><span class="line">            events.add(intercept(event));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> events;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Builder</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span>.Builder &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Interceptor <span class="title function_">build</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">TimestampInterceptor</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）重新打包</p>
<p>（3）将打好的包（带依赖）放入hadoop104的&#x2F;opt&#x2F;module&#x2F;flume&#x2F;lib文件夹下面</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 lib]# ls | grep interce</span><br><span class="line">edu-flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<h4 id="4-6-4-日志消费层Flume启动、停止脚本"><a href="#4-6-4-日志消费层Flume启动、停止脚本" class="headerlink" title="4.6.4 日志消费层Flume启动、停止脚本"></a>4.6.4 日志消费层Flume启动、停止脚本</h4><p>（1）在~&#x2F;bin目录下创建脚本f2.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim f2.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">        echo &quot; --------启动 hadoop104 消费flume-------&quot;</span><br><span class="line">        ssh hadoop104 &quot;nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf/ -f /opt/module/flume/job/kafka-flume-hdfs.conf &gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">        echo &quot; --------停止 hadoop104 消费flume-------&quot;</span><br><span class="line">        ssh hadoop104 &quot;ps -ef | grep kafka-flume-hdfs | grep -v grep |awk &#x27;&#123;print \$2&#125;&#x27; | xargs -n1 kill -9&quot;</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）增加脚本权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 f2.sh </span><br></pre></td></tr></table></figure>

<h4 id="4-6-5-数据通道测试"><a href="#4-6-5-数据通道测试" class="headerlink" title="4.6.5 数据通道测试"></a>4.6.5 数据通道测试</h4><p>分别生成2022-02-22和2022-02-23日期的数据，对用户行为日志数据的采集过程进行测试</p>
<p>（1）启动hadoop，zookeeper，kafka集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">6224 Kafka</span><br><span class="line">5702 JobHistoryServer</span><br><span class="line">5000 NameNode</span><br><span class="line">5515 NodeManager</span><br><span class="line">5180 DataNode</span><br><span class="line">5838 QuorumPeerMain</span><br><span class="line">6350 Jps</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">4576 Kafka</span><br><span class="line">3380 DataNode</span><br><span class="line">4694 Jps</span><br><span class="line">3754 NodeManager</span><br><span class="line">4186 QuorumPeerMain</span><br><span class="line">3599 ResourceManager</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">4277 Jps</span><br><span class="line">3590 NodeManager</span><br><span class="line">3498 SecondaryNameNode</span><br><span class="line">4171 Kafka</span><br><span class="line">3788 QuorumPeerMain</span><br><span class="line">3373 DataNode</span><br></pre></td></tr></table></figure>

<p>（2）执行Flume启动脚本，启动Flume的日志采集程序</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# f1.sh start</span><br><span class="line">[root@hadoop102 bin]# f2.sh start</span><br><span class="line">[root@hadoop102 bin]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">6224 Kafka</span><br><span class="line">6451 Application       # f1.sh的后台进程</span><br><span class="line">5702 JobHistoryServer</span><br><span class="line">5000 NameNode</span><br><span class="line">5515 NodeManager</span><br><span class="line">6587 Jps</span><br><span class="line">5180 DataNode</span><br><span class="line">5838 QuorumPeerMain</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">4576 Kafka</span><br><span class="line">4784 Jps</span><br><span class="line">3380 DataNode</span><br><span class="line">3754 NodeManager</span><br><span class="line">4186 QuorumPeerMain</span><br><span class="line">3599 ResourceManager</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">4338 Application       # f2.sh的后台进程</span><br><span class="line">3590 NodeManager</span><br><span class="line">3498 SecondaryNameNode</span><br><span class="line">4171 Kafka</span><br><span class="line">3788 QuorumPeerMain</span><br><span class="line">3373 DataNode</span><br><span class="line">4509 Jps</span><br></pre></td></tr></table></figure>

<p>（3）执行脚本，生成2022-02-22和2022-02-23的日志数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# mock.sh 2022-02-22</span><br><span class="line">正在生成 2022-02-22 当日的数据</span><br><span class="line">[root@hadoop102 bin]# mock.sh 2022-02-23</span><br><span class="line">正在生成 2022-02-23 当日的数据</span><br></pre></td></tr></table></figure>

<p>可以观察到HDFS上有数据生成：</p>
<p><img src="Snipaste_2023-11-09_22-34-12.png" alt="Snipaste_2023-11-09_22-34-12"></p>
<h4 id="注意！注意！注意！"><a href="#注意！注意！注意！" class="headerlink" title="注意！注意！注意！"></a><strong>注意！注意！注意！</strong></h4><p>一定要保证位于hadoop102中的cd &#x2F;opt&#x2F;module&#x2F;flume&#x2F;job&#x2F;文件下的文件file-flume-kafka.conf是独占该文件夹的，因为如果有其余的配置文件中也使用了a1就会出问题，当然可以换成a2,a3等。</p>
<p>同理也要保证hadoop104中的cd &#x2F;opt&#x2F;module&#x2F;flume&#x2F;job&#x2F;文件下的文件kafka-flume-hdfs.conf是独占该文件夹的。</p>
<h3 id="4-7-采集通道启动、停止脚本"><a href="#4-7-采集通道启动、停止脚本" class="headerlink" title="4.7 采集通道启动、停止脚本"></a>4.7 采集通道启动、停止脚本</h3><p>在没有创建该脚本前，我们需要</p>
<p>（1）启动hadoop集群</p>
<p>（2）启动Zookeeper集群</p>
<p>（3）启动Kafka集群</p>
<p>（4）启动f1.sh</p>
<p>（5）启动f2.sh</p>
<p>（6）执行mock.sh 生成某日的日志数据</p>
<p>（7）查看确实在HDFS上产生了日志数据</p>
<p>（8）停止f2.sh</p>
<p>（9）停止f1.sh</p>
<p>（10）停止Kafka集群</p>
<p>（11）停止hadoop集群</p>
<p>（12）停止停止ZooKeeper集群</p>
<p>现在我们将以上步骤写成一个统一的脚本，供后续使用（注意脚本一定要写对，之前就是因为f1.sh和f2.sh写错了导致数据传输失败）。</p>
<p>（1）在~&#x2F;bin目录下创建脚本cluster.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim cluster.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">        echo ================== 启动 集群 ==================</span><br><span class="line"></span><br><span class="line">        #启动 Zookeeper集群</span><br><span class="line">        zk.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Hadoop集群</span><br><span class="line">        myhadoop.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Kafka采集集群</span><br><span class="line">        kf.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Flume采集集群</span><br><span class="line">        f1.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Flume消费集群</span><br><span class="line">        f2.sh start</span><br><span class="line"></span><br><span class="line">        &#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">        echo ================== 停止 集群 ==================</span><br><span class="line"></span><br><span class="line">        #停止 Flume消费集群</span><br><span class="line">        f2.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Flume采集集群</span><br><span class="line">        f1.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Kafka采集集群</span><br><span class="line">        kf.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Hadoop集群</span><br><span class="line">        myhadoop.sh stop</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">		#</span><span class="language-bash">循环直至 Kafka 集群进程全部停止</span></span><br><span class="line">		kafka_count=$(xcall.sh jps | grep Kafka | wc -l)</span><br><span class="line">		while [ $kafka_count -gt 0 ]</span><br><span class="line">		do</span><br><span class="line">			sleep 1</span><br><span class="line">			kafka_count=$(xcall.sh jps | grep Kafka | wc -l)</span><br><span class="line">            echo &quot;当前未停止的 Kafka 进程数为 $kafka_count&quot;</span><br><span class="line">		done</span><br><span class="line"></span><br><span class="line">        #停止 Zookeeper集群</span><br><span class="line">        zk.sh stop</span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）增加脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 cluster.sh</span><br></pre></td></tr></table></figure>

<h1 id="第二部分-采集平台-业务数据采集平台"><a href="#第二部分-采集平台-业务数据采集平台" class="headerlink" title="第二部分 采集平台-业务数据采集平台"></a>第二部分 采集平台-业务数据采集平台</h1><h2 id="第一章-在线教育业务简介"><a href="#第一章-在线教育业务简介" class="headerlink" title="第一章 在线教育业务简介"></a>第一章 在线教育业务简介</h2><h3 id="1-1-在线教育业务流程"><a href="#1-1-在线教育业务流程" class="headerlink" title="1.1 在线教育业务流程"></a>1.1 在线教育业务流程</h3><p>在线教育的主要业务流程包括用户前台浏览课程时的课程详情的管理，用户课程加入购物车进行支付时用户个人中心&amp;支付服务的管理，用户支付完成后订单后台服务的管理，这些流程涉及到了十几个甚至几十个业务数据表，甚至更多。</p>
<img src="Snipaste_2023-11-10_14-13-41.png" alt="Snipaste_2023-11-10_14-13-41" style="zoom:50%;">

<h3 id="1-2-在线教育表结构"><a href="#1-2-在线教育表结构" class="headerlink" title="1.2 在线教育表结构"></a>1.2 在线教育表结构</h3><p><strong>在线教育业务表结构对于数据仓库的搭建来说非常重要，在进行数据导入之前，首先要做的就是熟悉业务表的结构</strong></p>
<ul>
<li>第一步观察<strong>所有表格的类型</strong>，了解表格<strong>大体分为哪几类</strong></li>
<li>第二步认真分析了解每张表的<strong>每行数据</strong>代表的是什么意义</li>
<li>第三步要详细查看每张表的<strong>每个字段的含义</strong>以及业务逻辑，通过了解每一个字段的含义，也可以知道每张表都与哪些表产生了<strong>关联</strong></li>
</ul>
<p>以下为本在线教育系统涉及到的业务数据表结构关系。这25张表以订单表、用户表、 课程信息表、测验表和用户章节进度表为中心，延伸出了支付表、订单明细表、章节表、课程评价表、科目表、试卷表、知识点表等，用户表提供用户的详细信息，支付表提供该订单的支付详情，订单详情表提供订单的课程等情况，课程表给订单明细表提供课程的详细信息。本次讲解以此25个表为例，实际项目中，业务数据库中表格远远不止这些。</p>
<img src="Snipaste_2023-11-10_18-01-27.png" alt="Snipaste_2023-11-10_18-01-27" style="zoom:50%;">

<img src="Snipaste_2023-11-10_18-01-48.png" alt="Snipaste_2023-11-10_18-01-48" style="zoom:50%;">

<p>详细的表结构见书中</p>
<h2 id="第二章-业务数据采集模块"><a href="#第二章-业务数据采集模块" class="headerlink" title="第二章 业务数据采集模块"></a>第二章 业务数据采集模块</h2><h3 id="2-1-业务数据梳理"><a href="#2-1-业务数据梳理" class="headerlink" title="2.1 业务数据梳理"></a>2.1 业务数据梳理</h3><p>业务数据通过运行mock.sh命令可以在MySQL中生成指定日期的业务数据。</p>
<p>使用EZDML数据库设计工具来梳理复杂的业务表关系。</p>
<p>（1）点击菜单“模型”-&gt;新建模型，命名为edu</p>
<p>（2）选中“模型”-&gt;点击菜单“模型”-&gt;导入数据库，如下填写：</p>
<img src="Snipaste_2023-11-10_20-59-15.png" alt="Snipaste_2023-11-10_20-59-15" style="zoom:50%;">

<p>（3）选择要到入的表，注意其中一个不勾选</p>
<img src="Snipaste_2023-11-10_21-01-17.png" alt="Snipaste_2023-11-10_21-01-17" style="zoom:50%;">

<p>（4）成功！</p>
<img src="Snipaste_2023-11-10_21-02-00.png" alt="Snipaste_2023-11-10_21-02-00" style="zoom:50%;">

<p>（5）建立表关系</p>
<p>示例：点击选中主表，再点击“连接”按钮，再点从表，配置如下：</p>
<img src="Snipaste_2023-11-10_21-06-15.png" alt="Snipaste_2023-11-10_21-06-15" style="zoom:50%;">

<p>（6）按照上述步骤，得到整个模型：</p>
<p><img src="Snipaste_2023-11-10_21-11-36.png" alt="Snipaste_2023-11-10_21-11-36"></p>
<h3 id="2-2-数据同步策略"><a href="#2-2-数据同步策略" class="headerlink" title="2.2 数据同步策略"></a>2.2 数据同步策略</h3><p>数据同步是指将数据从关系型数据库同步到大数据的存储系统。业务数据是数据仓库的重要数据来源，我们需要每日定时从业务数据库中抽取数据，传输到数据仓库中，之后再对数据进行分析统计。为保证统计结果的正确性，需要保证数据仓库中的数据与业务数据库是同步的，离线数仓的计算周期通常为天，所以数据同步周期也通常为天，即每天同步一次即可。数据的同步策略有<strong>全量同步</strong>和<strong>增量同步</strong>。</p>
<p>全量同步，就是每天都将业务数据库中的全部数据同步一份到数据仓库，这是保证两侧数据同步的最简单的方式。适用于表数据量不大，且每天既会有新数据插入，又会有旧数据修改的场景。</p>
<p>增量同步，就是每天只将业务数据中的<strong>新增及变化数据</strong>同步到数据仓库。采用每日增量同步的表，通常需要在首日先进行一次全量同步。适用于表数据两较大，且每天只会有新数据插入的场景。</p>
<img src="Snipaste_2023-11-10_21-27-50.png" alt="Snipaste_2023-11-10_21-27-50" style="zoom:50%;">

<p>若业务表数据量比较大，且每天数据变化的比例比较低，这时应采用增量同步，否则可采用全量同步。增量同步：事实表；全量同步：维度表</p>
<img src="Snipaste_2023-11-10_21-29-09.png" alt="Snipaste_2023-11-10_21-29-09" style="zoom:50%;">

<p>由于后续数仓建模需要，cart_info需进行全量同步和增量同步，此处暂不解释，后续章节会作出解释。</p>
<h3 id="2-3-数据同步工具"><a href="#2-3-数据同步工具" class="headerlink" title="2.3 数据同步工具"></a>2.3 数据同步工具</h3><p>数据同步工具种类繁多，大致可分为两类，一类是以DataX、Sqoop为代表的基于Select查询的离线、批量同步工具，另一类是以Maxwell、Canal为代表的基于数据库数据变更日志（例如MySQL的binlog，其会实时记录所有的insert、update以及delete操作）的实时流式同步工具。</p>
<p>全量同步通常使用DataX、Sqoop等基于查询的离线同步工具。而增量同步既可以使用DataX、Sqoop等工具，也可使用Maxwell、Canal等工具，下面对增量同步不同方案进行简要对比。</p>
<img src="Snipaste_2023-11-12_19-14-33.png" alt="Snipaste_2023-11-12_19-14-33" style="zoom:50%;">

<p>本项目中，全量同步采用DataX，增量同步采用Maxwell。</p>
<h3 id="2-4-数据同步工具部署"><a href="#2-4-数据同步工具部署" class="headerlink" title="2.4 数据同步工具部署"></a>2.4 数据同步工具部署</h3><h4 id="2-4-1-DataX"><a href="#2-4-1-DataX" class="headerlink" title="2.4.1 DataX"></a>2.4.1 DataX</h4><p>DataX 是阿里巴巴开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。</p>
<h5 id="2-4-1-1-DataX设计理念"><a href="#2-4-1-1-DataX设计理念" class="headerlink" title="2.4.1.1 DataX设计理念"></a>2.4.1.1 DataX设计理念</h5><p>为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。</p>
<img src="Snipaste_2023-11-12_19-24-25.png" alt="Snipaste_2023-11-12_19-24-25" style="zoom:43%;">

<h5 id="2-4-1-2-DataX框架设计"><a href="#2-4-1-2-DataX框架设计" class="headerlink" title="2.4.1.2 DataX框架设计"></a>2.4.1.2 DataX框架设计</h5><p>DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader&#x2F;Writer插件，纳入到整个同步框架中。</p>
<img src="Snipaste_2023-11-12_19-26-50.png" alt="Snipaste_2023-11-12_19-26-50" style="zoom:50%;">

<h5 id="2-4-1-3-DataX运行流程"><a href="#2-4-1-3-DataX运行流程" class="headerlink" title="2.4.1.3 DataX运行流程"></a>2.4.1.3 DataX运行流程</h5><p>下面用一个DataX作业生命周期的时序图说明DataX的运行流程、核心概念以及每个概念之间的关系。</p>
<img src="Snipaste_2023-11-12_19-38-57.png" alt="Snipaste_2023-11-12_19-38-57" style="zoom:50%;">

<p>举例来说，用户提交了一个DataX作业，并且配置了总的并发度为20，目的是对一个有100张分表的mysql数据源进行同步。DataX的调度决策思路是：</p>
<p>1）DataX Job根据分库分表切分策略，将同步工作分成100个Task。</p>
<p>2）根据配置的总的并发度20，以及每个Task Group的并发度5，DataX计算共需要分配4个TaskGroup。</p>
<p>3）4个TaskGroup平分100个Task，每一个TaskGroup负责运行25个Task。</p>
<h5 id="2-4-1-4-DataX部署"><a href="#2-4-1-4-DataX部署" class="headerlink" title="2.4.1.4 DataX部署"></a>2.4.1.4 DataX部署</h5><p>（1）将安装包DataX上传到hadoop102的&#x2F;opt&#x2F;software</p>
<p>（2）解压到&#x2F;opt&#x2F;module</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf datax.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>（3）自检，执行如下命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datax]# python /opt/module/datax/bin/datax.py /opt/module/datax/job/job.json</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">出现以下信息证明安装成功</span></span><br><span class="line"></span><br><span class="line">2023-11-12 19:52:18.978 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2023-11-12 19:52:08</span><br><span class="line">任务结束时刻                    : 2023-11-12 19:52:18</span><br><span class="line">任务总计耗时                    :                 10s</span><br><span class="line">任务平均流量                    :          253.91KB/s</span><br><span class="line">记录写入速度                    :          10000rec/s</span><br><span class="line">读出记录总数                    :              100000</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>

<h5 id="2-4-1-5-DataX使用概述"><a href="#2-4-1-5-DataX使用概述" class="headerlink" title="2.4.1.5 DataX使用概述"></a>2.4.1.5 DataX使用概述</h5><p>（1）DataX任务提交命令</p>
<p>DataX的使用十分简单，用户只需根据自己同步数据的数据源和目的地选择相应的Reader和Writer，并将Reader和Writer的信息配置在一个json文件中，然后执行如下命令提交数据同步任务即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datax]# python /opt/module/datax/bin/datax.py /opt/module/datax/job/job.json</span><br></pre></td></tr></table></figure>

<p>（2）DataX配置文件格式</p>
<p>可以使用如下命名查看DataX配置文件模板。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datax]# python bin/datax.py -r mysqlreader -w hdfswriter</span><br></pre></td></tr></table></figure>

<p>配置文件模板如下，json最外层是一个job，job包含setting和content两部分，其中setting用于对整个job进行配置，content用户配置数据源和目的地。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">DataX (DATAX-OPENSOURCE<span class="number">-3.0</span>)<span class="punctuation">,</span> From Alibaba !</span><br><span class="line">Copyright (C) <span class="number">2010</span><span class="number">-2017</span><span class="punctuation">,</span> Alibaba Group. All Rights Reserved.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Please refer to the mysqlreader document<span class="punctuation">:</span></span><br><span class="line">     https<span class="punctuation">:</span><span class="comment">//github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.md </span></span><br><span class="line"></span><br><span class="line">Please refer to the hdfswriter document<span class="punctuation">:</span></span><br><span class="line">     https<span class="punctuation">:</span><span class="comment">//github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md </span></span><br><span class="line"> </span><br><span class="line">Please save the following configuration as a json file and  use</span><br><span class="line">     python <span class="punctuation">&#123;</span>DATAX_HOME<span class="punctuation">&#125;</span>/bin/datax.py <span class="punctuation">&#123;</span>JSON_FILE_NAME<span class="punctuation">&#125;</span>.json </span><br><span class="line">to run the job.</span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>    <span class="comment">//数据源和目的地相关配置</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;reader&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>    <span class="comment">//Reader相关配置</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mysqlreader&quot;</span><span class="punctuation">,</span>    <span class="comment">//Reader名称，不可随意命名</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>    <span class="comment">//Reader配置参数</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;connection&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;jdbcUrl&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line">                                <span class="attr">&quot;table&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;password&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;where&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span> </span><br><span class="line">                <span class="attr">&quot;writer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>       <span class="comment">//Writer相关配置</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfswriter&quot;</span><span class="punctuation">,</span>      <span class="comment">//Writer名称，不可随意命名</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>          <span class="comment">//Writer配置参数</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;compress&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;defaultFS&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;fieldDelimiter&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;fileName&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;fileType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;writeMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line">        <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>       <span class="comment">//job配置参数，包括限速配置等</span></span><br><span class="line">            <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h5 id="2-4-1-6-同步MySQL数据到HDFS案例"><a href="#2-4-1-6-同步MySQL数据到HDFS案例" class="headerlink" title="2.4.1.6 同步MySQL数据到HDFS案例"></a>2.4.1.6 同步MySQL数据到HDFS案例</h5><p>案例要求：同步edu数据库中base_province表数据到HDFS的&#x2F;base_province目录</p>
<p>需求分析：要实现该功能，需选用<strong>MySQLReader</strong>和<strong>HDFSWriter</strong>，MySQLReader具有两种模式分别是TableMode和QuerySQLMode，前者使用table，column，where等属性声明需要同步的数据；后者使用一条SQL查询语句声明需要同步的数据。</p>
<p>下面分别使用两种模式进行演示。</p>
<h6 id="（1）MySQLReader之TableMode"><a href="#（1）MySQLReader之TableMode" class="headerlink" title="（1）MySQLReader之TableMode"></a>（1）MySQLReader之TableMode</h6><p>①Reader参数说明</p>
<img src="Snipaste_2023-11-12_20-56-23.png" alt="Snipaste_2023-11-12_20-56-23" style="zoom:50%;">

<p>②Writer参数说明</p>
<img src="Snipaste_2023-11-12_20-57-27.png" alt="Snipaste_2023-11-12_20-57-27" style="zoom:50%;">

<p>③Setting参数说明</p>
<img src="Snipaste_2023-11-12_21-00-20.png" alt="Snipaste_2023-11-12_21-00-20" style="zoom:50%;">

<p>④创建配置文件base_province_tm.json</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;reader&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mysqlreader&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="string">&quot;id&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;name&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;region_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;area_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;iso_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;iso_3166_2&quot;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;where&quot;</span><span class="punctuation">:</span> <span class="string">&quot;id&gt;=3&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;connection&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;jdbcUrl&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                    <span class="string">&quot;jdbc:mysql://hadoop102:3306/edu&quot;</span></span><br><span class="line">                                <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;table&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                    <span class="string">&quot;base_province&quot;</span></span><br><span class="line">                                <span class="punctuation">]</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;password&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wyhdhr19980418&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;splitPk&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span>  <span class="comment">//和channel参数配合使用，如果splitPk设置为&quot;id&quot;，channel设置为n（n&gt;1），则会按照id将表分成5*n+1个分片，推荐splitPk用户使用主键，因为主键通常情况下比较均匀，因此切分出来的分片也不容易出现数据热点，目前splitPk仅支持整形数据切分</span></span><br><span class="line">                        <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;root&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;writer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfswriter&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;id&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bigint&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;name&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;region_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;area_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iso_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iso_3166_2&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;compress&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gzip&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;defaultFS&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfs://hadoop102:8020&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fieldDelimiter&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\t&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fileName&quot;</span><span class="punctuation">:</span> <span class="string">&quot;base_province&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fileType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/base_province&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;writeMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;append&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>⑤在HDFS创建&#x2F;base_province目录</p>
<p>使用DataX向HDFS同步数据时，需确保目标路径<strong>已存在</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# hadoop fs -mkdir /base_province</span><br></pre></td></tr></table></figure>

<p>⑥执行DataX命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datax]# python bin/datax.py job/base_province_tm.json</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2023-11-12 21:16:02.540 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2023-11-12 21:15:50</span><br><span class="line">任务结束时刻                    : 2023-11-12 21:16:02</span><br><span class="line">任务总计耗时                    :                 12s</span><br><span class="line">任务平均流量                    :               66B/s</span><br><span class="line">记录写入速度                    :              3rec/s</span><br><span class="line">读出记录总数                    :                  32</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>

<p>⑦查看HDFS文件</p>
<img src="Snipaste_2023-11-12_21-30-51.png" alt="Snipaste_2023-11-12_21-30-51" style="zoom:50%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datax]# hadoop fs -cat /base_province/base_province__4c688c48_a411_4b0a_9080_6af730614b9d.gz | zcat</span><br><span class="line">2023-11-12 21:18:55,561 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">3	山西	1	140000	CN-14	CN-SX</span><br><span class="line">4	内蒙古	1	150000	CN-15	CN-NM</span><br><span class="line">5	河北	1	130000	CN-13	CN-HE</span><br><span class="line">6	上海	2	310000	CN-31	CN-SH</span><br><span class="line">7	江苏	2	320000	CN-32	CN-JS</span><br><span class="line">8	浙江	2	330000	CN-33	CN-ZJ</span><br><span class="line">9	安徽	2	340000	CN-34	CN-AH</span><br><span class="line">10	福建	2	350000	CN-35	CN-FJ</span><br><span class="line">11	江西	2	360000	CN-36	CN-JX</span><br><span class="line">12	山东	2	370000	CN-37	CN-SD</span><br><span class="line">13	重庆	6	500000	CN-50	CN-CQ</span><br><span class="line">14	台湾	2	710000	CN-71	CN-TW</span><br><span class="line">15	黑龙江	3	230000	CN-23	CN-HL</span><br><span class="line">16	吉林	3	220000	CN-22	CN-JL</span><br><span class="line">17	辽宁	3	210000	CN-21	CN-LN</span><br><span class="line">18	陕西	7	610000	CN-61	CN-SN</span><br><span class="line">19	甘肃	7	620000	CN-62	CN-GS</span><br><span class="line">20	青海	7	630000	CN-63	CN-QH</span><br><span class="line">21	宁夏	7	640000	CN-64	CN-NX</span><br><span class="line">22	新疆	7	650000	CN-65	CN-XJ</span><br><span class="line">23	河南	4	410000	CN-41	CN-HA</span><br><span class="line">24	湖北	4	420000	CN-42	CN-HB</span><br><span class="line">25	湖南	4	430000	CN-43	CN-HN</span><br><span class="line">26	广东	5	440000	CN-44	CN-GD</span><br><span class="line">27	广西	5	450000	CN-45	CN-GX</span><br><span class="line">28	海南	5	460000	CN-46	CN-HI</span><br><span class="line">29	香港	5	810000	CN-91	CN-HK</span><br><span class="line">30	澳门	5	820000	CN-92	CN-MO</span><br><span class="line">31	四川	6	510000	CN-51	CN-SC</span><br><span class="line">32	贵州	6	520000	CN-52	CN-GZ</span><br><span class="line">33	云南	6	530000	CN-53	CN-YN</span><br><span class="line">34	西藏	6	540000	CN-54	CN-XZ</span><br></pre></td></tr></table></figure>

<h6 id="（2）MySQLReader之QuerySQLMode"><a href="#（2）MySQLReader之QuerySQLMode" class="headerlink" title="（2）MySQLReader之QuerySQLMode"></a>（2）MySQLReader之QuerySQLMode</h6><p>①新建配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datax]# vim /opt/module/datax/job/base_province_qm.json</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;reader&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mysqlreader&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;connection&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;jdbcUrl&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                    <span class="string">&quot;jdbc:mysql://hadoop102:3306/edu&quot;</span></span><br><span class="line">                                <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;querySql&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                    <span class="string">&quot;select id,name,region_id,area_code,iso_code,iso_3166_2 from base_province where id&gt;=3&quot;</span></span><br><span class="line">                                <span class="punctuation">]</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;password&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wyhdhr19980418&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;root&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;writer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfswriter&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;id&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bigint&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;name&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;region_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;area_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iso_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iso_3166_2&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;compress&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gzip&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;defaultFS&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfs://hadoop102:8020&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fieldDelimiter&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\t&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fileName&quot;</span><span class="punctuation">:</span> <span class="string">&quot;base_province&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fileType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/base_province&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;writeMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;append&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>②执行DataX命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datax]# python bin/datax.py job/base_province_qm.json</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2023-11-12 21:28:36.044 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2023-11-12 21:28:24</span><br><span class="line">任务结束时刻                    : 2023-11-12 21:28:36</span><br><span class="line">任务总计耗时                    :                 11s</span><br><span class="line">任务平均流量                    :               66B/s</span><br><span class="line">记录写入速度                    :              3rec/s</span><br><span class="line">读出记录总数                    :                  32</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>

<p>③查看结果</p>
<img src="Snipaste_2023-11-12_21-31-22.png" alt="Snipaste_2023-11-12_21-31-22" style="zoom:50%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datax]# hadoop fs -cat /base_province/base_province__6807575d_d1d2_4521_81b8_8f13e4b387ca.gz | zcat</span><br><span class="line">2023-11-12 21:30:13,189 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">3	山西	1	140000	CN-14	CN-SX</span><br><span class="line">4	内蒙古	1	150000	CN-15	CN-NM</span><br><span class="line">5	河北	1	130000	CN-13	CN-HE</span><br><span class="line">6	上海	2	310000	CN-31	CN-SH</span><br><span class="line">7	江苏	2	320000	CN-32	CN-JS</span><br><span class="line">8	浙江	2	330000	CN-33	CN-ZJ</span><br><span class="line">9	安徽	2	340000	CN-34	CN-AH</span><br><span class="line">10	福建	2	350000	CN-35	CN-FJ</span><br><span class="line">11	江西	2	360000	CN-36	CN-JX</span><br><span class="line">12	山东	2	370000	CN-37	CN-SD</span><br><span class="line">13	重庆	6	500000	CN-50	CN-CQ</span><br><span class="line">14	台湾	2	710000	CN-71	CN-TW</span><br><span class="line">15	黑龙江	3	230000	CN-23	CN-HL</span><br><span class="line">16	吉林	3	220000	CN-22	CN-JL</span><br><span class="line">17	辽宁	3	210000	CN-21	CN-LN</span><br><span class="line">18	陕西	7	610000	CN-61	CN-SN</span><br><span class="line">19	甘肃	7	620000	CN-62	CN-GS</span><br><span class="line">20	青海	7	630000	CN-63	CN-QH</span><br><span class="line">21	宁夏	7	640000	CN-64	CN-NX</span><br><span class="line">22	新疆	7	650000	CN-65	CN-XJ</span><br><span class="line">23	河南	4	410000	CN-41	CN-HA</span><br><span class="line">24	湖北	4	420000	CN-42	CN-HB</span><br><span class="line">25	湖南	4	430000	CN-43	CN-HN</span><br><span class="line">26	广东	5	440000	CN-44	CN-GD</span><br><span class="line">27	广西	5	450000	CN-45	CN-GX</span><br><span class="line">28	海南	5	460000	CN-46	CN-HI</span><br><span class="line">29	香港	5	810000	CN-91	CN-HK</span><br><span class="line">30	澳门	5	820000	CN-92	CN-MO</span><br><span class="line">31	四川	6	510000	CN-51	CN-SC</span><br><span class="line">32	贵州	6	520000	CN-52	CN-GZ</span><br><span class="line">33	云南	6	530000	CN-53	CN-YN</span><br><span class="line">34	西藏	6	540000	CN-54	CN-XZ</span><br></pre></td></tr></table></figure>

<h6 id="（3）DataX传参"><a href="#（3）DataX传参" class="headerlink" title="（3）DataX传参"></a>（3）DataX传参</h6><p>通常情况下，离线数据同步任务需要每日定时重复执行，故HDFS上的目标路径通常会包含一层日期，以对每日同步的数据加以区分，也就是说每日同步数据的目标路径不是固定不变的，因此DataX配置文件中HDFS Writer的path参数的值应该是动态的。为实现这一效果，就需要使用DataX传参的功能。</p>
<p>DataX传参的用法如下，在JSON配置文件中使用${param}引用参数，在提交任务时使用-p”-Dparam&#x3D;value”传入参数值，具体示例如下。</p>
<p>①新增配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# vim /opt/module/datax/job/base_province_qm_param.json</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;reader&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mysqlreader&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;connection&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;jdbcUrl&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                    <span class="string">&quot;jdbc:mysql://hadoop102:3306/edu&quot;</span></span><br><span class="line">                                <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;querySql&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                    <span class="string">&quot;select id,name,region_id,area_code,iso_code,iso_3166_2 from base_province where id&gt;=3&quot;</span></span><br><span class="line">                                <span class="punctuation">]</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;password&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wyhdhr19980418&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;root&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;writer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfswriter&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;id&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bigint&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;name&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;region_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;area_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iso_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iso_3166_2&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;compress&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gzip&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;defaultFS&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfs://hadoop102:8020&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fieldDelimiter&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\t&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fileName&quot;</span><span class="punctuation">:</span> <span class="string">&quot;base_province&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fileType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/base_province/$&#123;dt&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;writeMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;append&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>②创建目标路径</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# hadoop fs -mkdir /base_province/2022-02-21</span><br></pre></td></tr></table></figure>

<p>③执行如下命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datax]# python bin/datax.py -p&quot;-Ddt=2022-02-21&quot; job/base_province_qm_param.json </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2023-11-13 13:05:45.849 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2023-11-13 13:05:33</span><br><span class="line">任务结束时刻                    : 2023-11-13 13:05:45</span><br><span class="line">任务总计耗时                    :                 12s</span><br><span class="line">任务平均流量                    :               66B/s</span><br><span class="line">记录写入速度                    :              3rec/s</span><br><span class="line">读出记录总数                    :                  32</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>

<p>④查看结果</p>
<img src="Snipaste_2023-11-13_13-06-45.png" alt="Snipaste_2023-11-13_13-06-45" style="zoom:50%;">

<h5 id="2-4-1-7-同步HDFS数据到MySQL案例"><a href="#2-4-1-7-同步HDFS数据到MySQL案例" class="headerlink" title="2.4.1.7 同步HDFS数据到MySQL案例"></a>2.4.1.7 同步HDFS数据到MySQL案例</h5><p>案例要求：同步HDFS上的&#x2F;base_province目录下的数据到MySQL edu 数据库下的test_province表。</p>
<p>需求分析：要实现该功能，需选用HDFSReader和MySQLWriter。</p>
<p>①Reader参数说明</p>
<img src="Snipaste_2023-11-13_13-12-21.png" alt="Snipaste_2023-11-13_13-12-21" style="zoom:50%;">

<p>②Writer参数说明</p>
<img src="Snipaste_2023-11-13_13-14-49.png" alt="Snipaste_2023-11-13_13-14-49" style="zoom:50%;">

<p>③创建配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datax]# vim /opt/module/datax/job/test_province.json</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;reader&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfsreader&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;defaultFS&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfs://hadoop102:8020&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/base_province&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="string">&quot;*&quot;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fileType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;compress&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gzip&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;encoding&quot;</span><span class="punctuation">:</span> <span class="string">&quot;UTF-8&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;nullFormat&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\\N&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fieldDelimiter&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\t&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;writer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mysqlwriter&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;root&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;password&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wyhdhr19980418&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;connection&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;table&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                    <span class="string">&quot;test_province&quot;</span></span><br><span class="line">                                <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;jdbcUrl&quot;</span><span class="punctuation">:</span> <span class="string">&quot;jdbc:mysql://hadoop102:3306/edu?useUnicode=true&amp;characterEncoding=utf-8&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="string">&quot;id&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;name&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;region_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;area_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;iso_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;iso_3166_2&quot;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;writeMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;replace&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>④在MySQL中的edu数据库创建test_province表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> IF <span class="keyword">EXISTS</span> `test_province`;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `test_province`  (</span><br><span class="line">  `id` <span class="type">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `name` <span class="type">varchar</span>(<span class="number">20</span>) <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="keyword">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `region_id` <span class="type">varchar</span>(<span class="number">20</span>) <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="keyword">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `area_code` <span class="type">varchar</span>(<span class="number">20</span>) <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="keyword">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `iso_code` <span class="type">varchar</span>(<span class="number">20</span>) <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="keyword">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `iso_3166_2` <span class="type">varchar</span>(<span class="number">20</span>) <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="keyword">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (`id`)</span><br><span class="line">) ENGINE <span class="operator">=</span> InnoDB <span class="type">CHARACTER</span> <span class="keyword">SET</span> <span class="operator">=</span> utf8 <span class="keyword">COLLATE</span> <span class="operator">=</span> utf8_general_ci ROW_FORMAT <span class="operator">=</span> <span class="keyword">Dynamic</span>;</span><br></pre></td></tr></table></figure>

<p>⑤执行如下命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datax]# python bin/datax.py job/test_province.json</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2023-11-13 13:25:57.981 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2023-11-13 13:25:46</span><br><span class="line">任务结束时刻                    : 2023-11-13 13:25:57</span><br><span class="line">任务总计耗时                    :                 11s</span><br><span class="line">任务平均流量                    :              200B/s</span><br><span class="line">记录写入速度                    :              9rec/s</span><br><span class="line">读出记录总数                    :                  96</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>

<p>⑥查看MySQL目标表数据</p>
<p>可以看到只有32条数据，因为我们使用了replace，相同的id被替换</p>
<img src="Snipaste_2023-11-13_13-27-32.png" alt="Snipaste_2023-11-13_13-27-32" style="zoom:50%;">

<h5 id="2-4-1-8-DataX优化"><a href="#2-4-1-8-DataX优化" class="headerlink" title="2.4.1.8 DataX优化"></a>2.4.1.8 DataX优化</h5><h6 id="（1）速度控制"><a href="#（1）速度控制" class="headerlink" title="（1）速度控制"></a>（1）速度控制</h6><p>DataX3.0提供了包括通道(并发)、记录流、字节流三种流控模式，可以随意控制你的作业速度，让你的作业在数据库可以承受的范围内达到最佳的同步速度。</p>
<p>关键优化参数如下：</p>
<img src="Snipaste_2023-11-13_14-10-59.png" alt="Snipaste_2023-11-13_14-10-59" style="zoom:33%;">

<p><strong>注意事项：</strong></p>
<p>1.若配置了总record限速，则必须配置单个channel的record限速</p>
<p>2.若配置了总byte限速，则必须配置单个channe的byte限速</p>
<p>3.若配置了总record限速和总byte限速，channel并发数参数就会失效。因为配置了总record限速和总byte限速之后，实际channel并发数是通过计算得到的：</p>
<p><strong>计算公式为:</strong></p>
<p>min(总byte限速&#x2F;单个channle的byte限速，总record限速&#x2F;单个channel的record限速)</p>
<p>配置示例：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;core&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;transport&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;byte&quot;</span><span class="punctuation">:</span> <span class="number">1048576</span> <span class="comment">//单个channel byte限速1M/s</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;byte&quot;</span> <span class="punctuation">:</span> <span class="number">5242880</span> <span class="comment">//总byte限速5M/s</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h6 id="（2）内存调整"><a href="#（2）内存调整" class="headerlink" title="（2）内存调整"></a>（2）内存调整</h6><p>当提升DataX Job内Channel并发数时，内存的占用会显著增加，因为DataX作为数据交换通道，在内存中会缓存较多的数据。例如Channel中会有一个Buffer，作为临时的数据交换的缓冲区，而在部分Reader和Writer的中，也会存在一些Buffer，为了防止OOM等错误，需调大JVM的堆内存。</p>
<p>建议将内存设置为4G或者8G，这个也可以根据实际情况来调整。</p>
<p>调整JVM xms xmx参数的两种方式：一种是直接更改datax.py脚本；另一种是在启动的时候，加上对应的参数，如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python datax/bin/datax.py --jvm=&quot;-Xms8G -Xmx8G&quot; /path/to/your/job.json</span><br></pre></td></tr></table></figure>

<h4 id="2-4-2-Maxwell"><a href="#2-4-2-Maxwell" class="headerlink" title="2.4.2 Maxwell"></a>2.4.2 Maxwell</h4><p>Maxwell 是由美国Zendesk公司开源，用Java编写的MySQL变更数据抓取软件。它会实时监控Mysql数据库的数据变更操作（包括insert、update、delete），并将变更数据以 JSON 格式发送给 Kafka、Kinesi等流数据处理平台。</p>
<h5 id="2-4-2-1-Maxwell输出数据格式"><a href="#2-4-2-1-Maxwell输出数据格式" class="headerlink" title="2.4.2.1 Maxwell输出数据格式"></a>2.4.2.1 Maxwell输出数据格式</h5><img src="Snipaste_2023-11-13_14-27-43.png" alt="Snipaste_2023-11-13_14-27-43" style="zoom:50%;">

<img src="Snipaste_2023-11-13_14-32-38.png" alt="Snipaste_2023-11-13_14-32-38" style="zoom:50%;">

<h5 id="2-4-2-2-Maxwell原理"><a href="#2-4-2-2-Maxwell原理" class="headerlink" title="2.4.2.2 Maxwell原理"></a>2.4.2.2 Maxwell原理</h5><p>Maxwell的工作原理是实时读取MySQL数据库的二进制日志（Binlog），从中获取变更数据，再将变更数据以JSON格式发送至Kafka等流处理平台。二进制日志（Binlog）是MySQL服务端非常重要的一种日志，它会保存MySQL数据库的所有数据变更记录。Binlog的主要作用包括主从复制和数据恢复。Maxwell的工作原理和主从复制密切相关。</p>
<p><strong>MySQL主从复制</strong></p>
<p>MySQL的主从复制，就是用来建立一个和主数据库完全一样的数据库环境，这个数据库称为从数据库。</p>
<p><strong>1）主从复制的应用场景如下：</strong></p>
<p>（1）做数据库的热备：主数据库服务器故障后，可切换到从数据库继续工作。</p>
<p>（2）读写分离：主数据库只负责业务数据的写入操作，而多个从数据库只负责业务数据的查询工作，在读多写少场景下，可以提高数据库工作效率。</p>
<p><strong>2）主从复制的工作原理如下：</strong></p>
<p>（1）Master主库将数据变更记录，写到二进制日志(binary log)中</p>
<p>（2）Slave从库向mysql master发送dump协议，将master主库的binary log events拷贝到它的中继日志(relay log)</p>
<p>（3）Slave从库读取并回放中继日志中的事件，将改变的数据同步到自己的数据库。</p>
<img src="Snipaste_2023-11-13_14-49-46.png" alt="Snipaste_2023-11-13_14-49-46" style="zoom:50%;">

<p>先写数据库，再写日志。</p>
<p>Maxwell原理就是将自己伪装成slave，并遵循MySQL主从复制的协议，从master同步数据。</p>
<h5 id="2-4-2-3-Maxwell部署"><a href="#2-4-2-3-Maxwell部署" class="headerlink" title="2.4.2.3 Maxwell部署"></a>2.4.2.3 Maxwell部署</h5><p>（1）将安装包上传到&#x2F;opt&#x2F;software目录（用教学版）</p>
<p>（2）将安装包解压至&#x2F;opt&#x2F;module</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf maxwell-1.29.2.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>（3）修改名称</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# mv maxwell-1.29.2/ maxwell</span><br></pre></td></tr></table></figure>

<p>（4）启用MySQL Binlog</p>
<p>①修改MySQL配置文件&#x2F;etc&#x2F;my.cnf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# vim /etc/my.cnf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">数据库<span class="built_in">id</span></span></span><br><span class="line">server-id = 1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动binlog，该参数的值会作为binlog的文件名</span></span><br><span class="line">log-bin=mysql-bin</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">binlog类型，maxwell要求为row类型</span></span><br><span class="line">binlog_format=row</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启用binlog的数据库，需根据实际情况作出修改</span></span><br><span class="line">binlog-do-db=edu</span><br></pre></td></tr></table></figure>

<p>注：MySQL Binlog模式</p>
<p>Statement：基于语句，Binlog会记录所有写操作的SQL语句，包括insert、update、delete等。</p>
<p>优点： 节省空间</p>
<p>缺点： 有可能造成数据不一致，例如insert语句中包含now()函数，写入binlog和读取binlog时函数的所得值不同。</p>
<p>Row：基于行，Binlog会记录每次写操作后被操作行记录的变化。</p>
<p>优点：保持数据的绝对一致性。</p>
<p>缺点：占用较大空间。</p>
<p>mixed：混合模式，默认是Statement，如果SQL语句可能导致数据不一致，就自动切换到Row。</p>
<p><strong>Maxwell要求Binlog采用Row模式。</strong></p>
<p>②重启MySQL服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# systemctl restart mysqld</span><br></pre></td></tr></table></figure>

<p>③验证Binlog开启</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> master status;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+----------+--------------+------------------+-------------------+</span></span><br><span class="line"><span class="operator">|</span> File             <span class="operator">|</span> Position <span class="operator">|</span> Binlog_Do_DB <span class="operator">|</span> Binlog_Ignore_DB <span class="operator">|</span> Executed_Gtid_Set <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+----------+--------------+------------------+-------------------+</span></span><br><span class="line"><span class="operator">|</span> mysql<span class="operator">-</span>bin<span class="number">.000001</span> <span class="operator">|</span>      <span class="number">154</span> <span class="operator">|</span> edu          <span class="operator">|</span>                  <span class="operator">|</span>                   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+----------+--------------+------------------+-------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>

<p>（5）创建Maxwell所需数据库和用户</p>
<p>Maxwell需要在MySQL中存储其运行过程中的所需的一些数据，包括binlog同步的断点位置（Maxwell支持断点续传）等等，故需要在MySQL为Maxwell创建数据库及用户。</p>
<p>①创建数据库</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msyql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> DATABASE maxwell;</span><br></pre></td></tr></table></figure>

<p>②调整MySQL数据库密码级别</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> validate_password_policy<span class="operator">=</span><span class="number">0</span>;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> validate_password_length<span class="operator">=</span><span class="number">4</span>;</span><br></pre></td></tr></table></figure>

<p>③创建Maxwell用户并赋予其必要权限</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">&#x27;maxwell&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">&#x27;maxwell&#x27;</span>;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">GRANT</span> <span class="keyword">ALL</span> <span class="keyword">ON</span> maxwell.<span class="operator">*</span> <span class="keyword">TO</span> <span class="string">&#x27;maxwell&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">GRANT</span> <span class="keyword">SELECT</span>, REPLICATION CLIENT, REPLICATION SLAVE <span class="keyword">ON</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">TO</span> <span class="string">&#x27;maxwell&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>（6）配置Maxwell</p>
<p>①修改Maxwell配置文件名称</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# cp config.properties.example config.properties</span><br></pre></td></tr></table></figure>

<p>②修改Maxwell配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# vim config.properties</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">Maxwell数据发送目的地，可选配置有stdout|file|kafka|kinesis|pubsub|sqs|rabbitmq|redis</span></span><br><span class="line">producer=kafka</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">目标Kafka集群地址</span></span><br><span class="line">kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">目标Kafka topic，可静态配置，例如:maxwell，也可动态配置，例如：%&#123;database&#125;_%&#123;table&#125;</span></span><br><span class="line">kafka_topic=maxwell</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">MySQL相关配置</span></span><br><span class="line">host=hadoop102</span><br><span class="line">user=maxwell</span><br><span class="line">password=maxwell</span><br><span class="line">jdbc_options=useSSL=false&amp;serverTimezone=Asia/Shanghai</span><br></pre></td></tr></table></figure>

<h5 id="2-4-2-4-Maxwell使用"><a href="#2-4-2-4-Maxwell使用" class="headerlink" title="2.4.2.4 Maxwell使用"></a>2.4.2.4 Maxwell使用</h5><h6 id="（1）启动ZooKeeper和Kafka集群"><a href="#（1）启动ZooKeeper和Kafka集群" class="headerlink" title="（1）启动ZooKeeper和Kafka集群"></a>（1）启动ZooKeeper和Kafka集群</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">2864 NameNode</span><br><span class="line">6192 Jps</span><br><span class="line">3044 DataNode</span><br><span class="line">3572 JobHistoryServer</span><br><span class="line">6105 Kafka</span><br><span class="line">3387 NodeManager</span><br><span class="line">5677 QuorumPeerMain</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">5458 Kafka</span><br><span class="line">5028 QuorumPeerMain</span><br><span class="line">5541 Jps</span><br><span class="line">3017 NodeManager</span><br><span class="line">2637 DataNode</span><br><span class="line">2862 ResourceManager</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">2642 DataNode</span><br><span class="line">2866 NodeManager</span><br><span class="line">5380 Kafka</span><br><span class="line">4966 QuorumPeerMain</span><br><span class="line">5498 Jps</span><br><span class="line">2767 SecondaryNameNode</span><br></pre></td></tr></table></figure>

<h6 id="（2）Maxwell启停脚本"><a href="#（2）Maxwell启停脚本" class="headerlink" title="（2）Maxwell启停脚本"></a>（2）Maxwell启停脚本</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim mxw.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">MAXWELL_HOME=/opt/module/maxwell</span><br><span class="line"></span><br><span class="line">status_maxwell()&#123;</span><br><span class="line">    result=`ps -ef | grep com.zendesk.maxwell.Maxwell | grep -v grep | wc -l`</span><br><span class="line">    return $result</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">start_maxwell()&#123;</span><br><span class="line">    status_maxwell</span><br><span class="line">    if [[ $? -lt 1 ]]; then</span><br><span class="line">        echo &quot;启动Maxwell&quot;</span><br><span class="line">        $MAXWELL_HOME/bin/maxwell --config $MAXWELL_HOME/config.properties --daemon</span><br><span class="line">    else</span><br><span class="line">        echo &quot;Maxwell正在运行&quot;</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">stop_maxwell()&#123;</span><br><span class="line">    status_maxwell</span><br><span class="line">    if [[ $? -gt 0 ]]; then</span><br><span class="line">        echo &quot;停止Maxwell&quot;</span><br><span class="line">        ps -ef | grep com.zendesk.maxwell.Maxwell | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9</span><br><span class="line">    else</span><br><span class="line">        echo &quot;Maxwell未在运行&quot;</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    start )</span><br><span class="line">        start_maxwell</span><br><span class="line">    ;;</span><br><span class="line">    stop )</span><br><span class="line">        stop_maxwell</span><br><span class="line">    ;;</span><br><span class="line">    restart )</span><br><span class="line">       stop_maxwell</span><br><span class="line">       start_maxwell</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 mxw.sh </span><br></pre></td></tr></table></figure>

<h6 id="（3）增量数据同步"><a href="#（3）增量数据同步" class="headerlink" title="（3）增量数据同步"></a>（3）增量数据同步</h6><p>①启动kafka消费者</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic maxwell</span><br></pre></td></tr></table></figure>

<p>②启动Maxwell</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# mxw.sh start</span><br></pre></td></tr></table></figure>

<p>③模拟生成数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# mock.sh 2022-02-22</span><br></pre></td></tr></table></figure>

<p>④观察Kafka消费者</p>
<img src="Snipaste_2023-11-13_15-49-05.png" alt="Snipaste_2023-11-13_15-49-05" style="zoom: 33%;">

<h6 id="（4）历史数据全量同步"><a href="#（4）历史数据全量同步" class="headerlink" title="（4）历史数据全量同步"></a>（4）历史数据全量同步</h6><p>上一节，我们已经实现了使用Maxwell实时增量同步MySQL变更数据的功能。但有时只有增量数据是不够的，我们可能需要使用到MySQL数据库中从历史至今的一个完整的数据集。<strong>这就需要我们在进行增量同步之前，先进行一次历史数据的全量同步。这样就能保证得到一个完整的数据集。</strong></p>
<p>Maxwell提供了bootstrap功能来进行历史数据的全量同步，命令如下（执行该命令之前要求Maxwell是启动的状态）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# /opt/module/maxwell/bin/maxwell-bootstrap --database edu --table base_province --config /opt/module/maxwell/config.properties</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br></pre></td></tr></table></figure>

<p>观察Kafka消费端：采用bootstrap方式同步的输出数据格式如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;database&quot;</span><span class="punctuation">:</span><span class="string">&quot;edu&quot;</span><span class="punctuation">,</span><span class="attr">&quot;table&quot;</span><span class="punctuation">:</span><span class="string">&quot;base_province&quot;</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;bootstrap-start&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span><span class="number">1699862655</span><span class="punctuation">,</span><span class="attr">&quot;data&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">	<span class="attr">&quot;database&quot;</span><span class="punctuation">:</span> <span class="string">&quot;edu&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;table&quot;</span><span class="punctuation">:</span> <span class="string">&quot;base_province&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bootstrap-insert&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span> <span class="number">1699862655</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;data&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">		<span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;北京&quot;</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;region_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1&quot;</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;area_code&quot;</span><span class="punctuation">:</span> <span class="string">&quot;110000&quot;</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;iso_code&quot;</span><span class="punctuation">:</span> <span class="string">&quot;CN-11&quot;</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;iso_3166_2&quot;</span><span class="punctuation">:</span> <span class="string">&quot;CN-BJ&quot;</span></span><br><span class="line">	<span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;database&quot;</span><span class="punctuation">:</span><span class="string">&quot;edu&quot;</span><span class="punctuation">,</span><span class="attr">&quot;table&quot;</span><span class="punctuation">:</span><span class="string">&quot;base_province&quot;</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;bootstrap-insert&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span><span class="number">1699862655</span><span class="punctuation">,</span><span class="attr">&quot;data&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;天津&quot;</span><span class="punctuation">,</span><span class="attr">&quot;region_id&quot;</span><span class="punctuation">:</span><span class="string">&quot;1&quot;</span><span class="punctuation">,</span><span class="attr">&quot;area_code&quot;</span><span class="punctuation">:</span><span class="string">&quot;120000&quot;</span><span class="punctuation">,</span><span class="attr">&quot;iso_code&quot;</span><span class="punctuation">:</span><span class="string">&quot;CN-12&quot;</span><span class="punctuation">,</span><span class="attr">&quot;iso_3166_2&quot;</span><span class="punctuation">:</span><span class="string">&quot;CN-TJ&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;database&quot;</span><span class="punctuation">:</span><span class="string">&quot;edu&quot;</span><span class="punctuation">,</span><span class="attr">&quot;table&quot;</span><span class="punctuation">:</span><span class="string">&quot;base_province&quot;</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;bootstrap-insert&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span><span class="number">1699862655</span><span class="punctuation">,</span><span class="attr">&quot;data&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;山西&quot;</span><span class="punctuation">,</span><span class="attr">&quot;region_id&quot;</span><span class="punctuation">:</span><span class="string">&quot;1&quot;</span><span class="punctuation">,</span><span class="attr">&quot;area_code&quot;</span><span class="punctuation">:</span><span class="string">&quot;140000&quot;</span><span class="punctuation">,</span><span class="attr">&quot;iso_code&quot;</span><span class="punctuation">:</span><span class="string">&quot;CN-14&quot;</span><span class="punctuation">,</span><span class="attr">&quot;iso_3166_2&quot;</span><span class="punctuation">:</span><span class="string">&quot;CN-SX&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">......</span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span><span class="number">33</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;云南&quot;</span><span class="punctuation">,</span><span class="attr">&quot;region_id&quot;</span><span class="punctuation">:</span><span class="string">&quot;6&quot;</span><span class="punctuation">,</span><span class="attr">&quot;area_code&quot;</span><span class="punctuation">:</span><span class="string">&quot;530000&quot;</span><span class="punctuation">,</span><span class="attr">&quot;iso_code&quot;</span><span class="punctuation">:</span><span class="string">&quot;CN-53&quot;</span><span class="punctuation">,</span><span class="attr">&quot;iso_3166_2&quot;</span><span class="punctuation">:</span><span class="string">&quot;CN-YN&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;database&quot;</span><span class="punctuation">:</span><span class="string">&quot;edu&quot;</span><span class="punctuation">,</span><span class="attr">&quot;table&quot;</span><span class="punctuation">:</span><span class="string">&quot;base_province&quot;</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;bootstrap-insert&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span><span class="number">1699862655</span><span class="punctuation">,</span><span class="attr">&quot;data&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span><span class="number">34</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;西藏&quot;</span><span class="punctuation">,</span><span class="attr">&quot;region_id&quot;</span><span class="punctuation">:</span><span class="string">&quot;6&quot;</span><span class="punctuation">,</span><span class="attr">&quot;area_code&quot;</span><span class="punctuation">:</span><span class="string">&quot;540000&quot;</span><span class="punctuation">,</span><span class="attr">&quot;iso_code&quot;</span><span class="punctuation">:</span><span class="string">&quot;CN-54&quot;</span><span class="punctuation">,</span><span class="attr">&quot;iso_3166_2&quot;</span><span class="punctuation">:</span><span class="string">&quot;CN-XZ&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;database&quot;</span><span class="punctuation">:</span><span class="string">&quot;edu&quot;</span><span class="punctuation">,</span><span class="attr">&quot;table&quot;</span><span class="punctuation">:</span><span class="string">&quot;base_province&quot;</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;bootstrap-complete&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span><span class="number">1699862655</span><span class="punctuation">,</span><span class="attr">&quot;data&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>第一条type为bootstrap-start和最后一条type为bootstrap-complete的数据，是bootstrap开始和结束的标志，不包含数据，中间的type为bootstrap-insert的数据才包含数据。</p>
<p>一次bootstrap输出的所有记录的ts都相同，为bootstrap开始的时间。</p>
<h3 id="2-5-全量表数据同步"><a href="#2-5-全量表数据同步" class="headerlink" title="2.5 全量表数据同步"></a>2.5 全量表数据同步</h3><h4 id="2-5-1-数据通道"><a href="#2-5-1-数据通道" class="headerlink" title="2.5.1 数据通道"></a>2.5.1 数据通道</h4><p>全量表数据由DataX从MySQL业务数据库直接同步到HDFS，具体数据流向如下图所示。</p>
<img src="Snipaste_2023-11-13_16-15-57.png" alt="Snipaste_2023-11-13_16-15-57" style="zoom:50%;">

<h4 id="2-5-2-DataX配置文件（配置文件生成脚本）"><a href="#2-5-2-DataX配置文件（配置文件生成脚本）" class="headerlink" title="2.5.2 DataX配置文件（配置文件生成脚本）"></a>2.5.2 DataX配置文件（配置文件生成脚本）</h4><p>方便起见，此处提供了DataX配置文件批量生成脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim ~/bin/gen_import_config.py</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> getopt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> MySQLdb</span><br><span class="line"></span><br><span class="line"><span class="comment">#MySQL相关配置，需根据实际情况作出修改</span></span><br><span class="line">mysql_host = <span class="string">&quot;hadoop102&quot;</span></span><br><span class="line">mysql_port = <span class="string">&quot;3306&quot;</span></span><br><span class="line">mysql_user = <span class="string">&quot;root&quot;</span></span><br><span class="line">mysql_passwd = <span class="string">&quot;wyhdhr19980418&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#HDFS NameNode相关配置，需根据实际情况作出修改</span></span><br><span class="line">hdfs_nn_host = <span class="string">&quot;hadoop102&quot;</span></span><br><span class="line">hdfs_nn_port = <span class="string">&quot;8020&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成配置文件的目标路径，可根据实际情况作出修改</span></span><br><span class="line">output_path = <span class="string">&quot;/opt/module/datax/job/import&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_connection</span>():</span><br><span class="line">    <span class="keyword">return</span> MySQLdb.connect(host=mysql_host, port=<span class="built_in">int</span>(mysql_port), user=mysql_user, passwd=mysql_passwd)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_mysql_meta</span>(<span class="params">database, table</span>):</span><br><span class="line">    connection = get_connection()</span><br><span class="line">    cursor = connection.cursor()</span><br><span class="line">    sql = <span class="string">&quot;SELECT COLUMN_NAME,DATA_TYPE from information_schema.COLUMNS WHERE TABLE_SCHEMA=%s AND TABLE_NAME=%s ORDER BY ORDINAL_POSITION&quot;</span></span><br><span class="line">    cursor.execute(sql, [database, table])</span><br><span class="line">    fetchall = cursor.fetchall()</span><br><span class="line">    cursor.close()</span><br><span class="line">    connection.close()</span><br><span class="line">    <span class="keyword">return</span> fetchall</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_mysql_columns</span>(<span class="params">database, table</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], get_mysql_meta(database, table))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_hive_columns</span>(<span class="params">database, table</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">type_mapping</span>(<span class="params">mysql_type</span>):</span><br><span class="line">        mappings = &#123;</span><br><span class="line">            <span class="string">&quot;bigint&quot;</span>: <span class="string">&quot;bigint&quot;</span>,</span><br><span class="line">            <span class="string">&quot;int&quot;</span>: <span class="string">&quot;bigint&quot;</span>,</span><br><span class="line">            <span class="string">&quot;smallint&quot;</span>: <span class="string">&quot;bigint&quot;</span>,</span><br><span class="line">            <span class="string">&quot;tinyint&quot;</span>: <span class="string">&quot;bigint&quot;</span>,</span><br><span class="line">            <span class="string">&quot;decimal&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">            <span class="string">&quot;double&quot;</span>: <span class="string">&quot;double&quot;</span>,</span><br><span class="line">            <span class="string">&quot;float&quot;</span>: <span class="string">&quot;float&quot;</span>,</span><br><span class="line">            <span class="string">&quot;binary&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">            <span class="string">&quot;char&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">            <span class="string">&quot;varchar&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">            <span class="string">&quot;datetime&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">            <span class="string">&quot;time&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">            <span class="string">&quot;timestamp&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">            <span class="string">&quot;date&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">            <span class="string">&quot;text&quot;</span>: <span class="string">&quot;string&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> mappings[mysql_type]</span><br><span class="line"></span><br><span class="line">    meta = get_mysql_meta(database, table)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">map</span>(<span class="keyword">lambda</span> x: &#123;<span class="string">&quot;name&quot;</span>: x[<span class="number">0</span>], <span class="string">&quot;type&quot;</span>: type_mapping(x[<span class="number">1</span>].lower())&#125;, meta)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_json</span>(<span class="params">source_database, source_table</span>):</span><br><span class="line">    job = &#123;</span><br><span class="line">        <span class="string">&quot;job&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;setting&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;speed&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;channel&quot;</span>: <span class="number">3</span></span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;errorLimit&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;record&quot;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&quot;percentage&quot;</span>: <span class="number">0.02</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: [&#123;</span><br><span class="line">                <span class="string">&quot;reader&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;mysqlreader&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;parameter&quot;</span>: &#123;</span><br><span class="line">                        <span class="string">&quot;username&quot;</span>: mysql_user,</span><br><span class="line">                        <span class="string">&quot;password&quot;</span>: mysql_passwd,</span><br><span class="line">                        <span class="string">&quot;column&quot;</span>: get_mysql_columns(source_database, source_table),</span><br><span class="line">                        <span class="string">&quot;splitPk&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;connection&quot;</span>: [&#123;</span><br><span class="line">                            <span class="string">&quot;table&quot;</span>: [source_table],</span><br><span class="line">                            <span class="string">&quot;jdbcUrl&quot;</span>: [<span class="string">&quot;jdbc:mysql://&quot;</span> + mysql_host + <span class="string">&quot;:&quot;</span> + mysql_port + <span class="string">&quot;/&quot;</span> + source_database]</span><br><span class="line">                        &#125;]</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;writer&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;hdfswriter&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;parameter&quot;</span>: &#123;</span><br><span class="line">                        <span class="string">&quot;defaultFS&quot;</span>: <span class="string">&quot;hdfs://&quot;</span> + hdfs_nn_host + <span class="string">&quot;:&quot;</span> + hdfs_nn_port,</span><br><span class="line">                        <span class="string">&quot;fileType&quot;</span>: <span class="string">&quot;text&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;path&quot;</span>: <span class="string">&quot;$&#123;targetdir&#125;&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;fileName&quot;</span>: source_table,</span><br><span class="line">                        <span class="string">&quot;column&quot;</span>: get_hive_columns(source_database, source_table),</span><br><span class="line">                        <span class="string">&quot;writeMode&quot;</span>: <span class="string">&quot;append&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;fieldDelimiter&quot;</span>: <span class="string">&quot;\t&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;compress&quot;</span>: <span class="string">&quot;gzip&quot;</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_path):</span><br><span class="line">        os.makedirs(output_path)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(output_path, <span class="string">&quot;.&quot;</span>.join([source_database, source_table, <span class="string">&quot;json&quot;</span>])), <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(job, f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    source_database = <span class="string">&quot;&quot;</span></span><br><span class="line">    source_table = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    options, arguments = getopt.getopt(args, <span class="string">&#x27;-d:-t:&#x27;</span>, [<span class="string">&#x27;sourcedb=&#x27;</span>, <span class="string">&#x27;sourcetbl=&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> opt_name, opt_value <span class="keyword">in</span> options:</span><br><span class="line">        <span class="keyword">if</span> opt_name <span class="keyword">in</span> (<span class="string">&#x27;-d&#x27;</span>, <span class="string">&#x27;--sourcedb&#x27;</span>):</span><br><span class="line">            source_database = opt_value</span><br><span class="line">        <span class="keyword">if</span> opt_name <span class="keyword">in</span> (<span class="string">&#x27;-t&#x27;</span>, <span class="string">&#x27;--sourcetbl&#x27;</span>):</span><br><span class="line">            source_table = opt_value</span><br><span class="line"></span><br><span class="line">    generate_json(source_database, source_table)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main(sys.argv[<span class="number">1</span>:])</span><br></pre></td></tr></table></figure>

<p>①安装Python Mysql驱动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# yum install MySQL-python</span><br></pre></td></tr></table></figure>

<p>②脚本使用说明</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python gen_import_config.py -d database -t table</span><br></pre></td></tr></table></figure>

<p>通过-d传入数据库名，-t传入表名，执行上述命令即可生成该表的DataX同步配置文件</p>
<p>③在~&#x2F;bin目录下创建gen_import_config.sh脚本，用于调用配置文件生成脚本，生成批量的配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim gen_import_config.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">python /bin/gen_import_config.py -d edu -t base_category_info</span><br><span class="line">python /bin/gen_import_config.py -d edu -t base_source</span><br><span class="line">python /bin/gen_import_config.py -d edu -t base_province</span><br><span class="line">python /bin/gen_import_config.py -d edu -t base_subject_info</span><br><span class="line">python /bin/gen_import_config.py -d edu -t cart_info</span><br><span class="line">python /bin/gen_import_config.py -d edu -t chapter_info</span><br><span class="line">python /bin/gen_import_config.py -d edu -t course_info</span><br><span class="line">python /bin/gen_import_config.py -d edu -t knowledge_point</span><br><span class="line">python /bin/gen_import_config.py -d edu -t test_paper</span><br><span class="line">python /bin/gen_import_config.py -d edu -t test_paper_question</span><br><span class="line">python /bin/gen_import_config.py -d edu -t test_point_question</span><br><span class="line">python /bin/gen_import_config.py -d edu -t test_question_info</span><br><span class="line">python /bin/gen_import_config.py -d edu -t user_chapter_process</span><br><span class="line">python /bin/gen_import_config.py -d edu -t test_question_option</span><br><span class="line">python /bin/gen_import_config.py -d edu -t video_info</span><br></pre></td></tr></table></figure>

<p>④为gen_import_config.sh脚本增加权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 gen_import_config.sh</span><br></pre></td></tr></table></figure>

<p>⑤执行gen_import_config.sh脚本，生成配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# gen_import_config.sh </span><br></pre></td></tr></table></figure>

<p>⑥观察生成的配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# cd /opt/module/datax/job/import/</span><br><span class="line">[root@hadoop102 import]# ll</span><br><span class="line">总用量 60</span><br><span class="line">-rw-r--r-- 1 root root  849 11月 13 21:49 edu.base_category_info.json</span><br><span class="line">-rw-r--r-- 1 root root  871 11月 13 21:49 edu.base_province.json</span><br><span class="line">-rw-r--r-- 1 root root  721 11月 13 21:49 edu.base_source.json</span><br><span class="line">-rw-r--r-- 1 root root  903 11月 13 21:49 edu.base_subject_info.json</span><br><span class="line">-rw-r--r-- 1 root root 1137 11月 13 21:49 edu.cart_info.json</span><br><span class="line">-rw-r--r-- 1 root root 1051 11月 13 21:49 edu.chapter_info.json</span><br><span class="line">-rw-r--r-- 1 root root 1435 11月 13 21:49 edu.course_info.json</span><br><span class="line">-rw-r--r-- 1 root root 1063 11月 13 21:49 edu.knowledge_point.json</span><br><span class="line">-rw-r--r-- 1 root root  943 11月 13 21:49 edu.test_paper.json</span><br><span class="line">-rw-r--r-- 1 root root  947 11月 13 21:49 edu.test_paper_question.json</span><br><span class="line">-rw-r--r-- 1 root root  901 11月 13 21:49 edu.test_point_question.json</span><br><span class="line">-rw-r--r-- 1 root root 1079 11月 13 21:49 edu.test_question_info.json</span><br><span class="line">-rw-r--r-- 1 root root  961 11月 13 21:49 edu.test_question_option.json</span><br><span class="line">-rw-r--r-- 1 root root 1011 11月 13 21:49 edu.user_chapter_process.json</span><br><span class="line">-rw-r--r-- 1 root root 1345 11月 13 21:49 edu.video_info.json</span><br></pre></td></tr></table></figure>

<p>⑦测试生成的DataX配置文件</p>
<p>以base_province为例，测试用脚本生成的配置文件是否可用。</p>
<p>创建目标路径</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 import]# hadoop fs -mkdir -p /origin_data/edu/db/base_province_full/2022-02-21</span><br></pre></td></tr></table></figure>

<p>执行DataX同步命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 import]# python /opt/module/datax/bin/datax.py -p&quot;-Dtargetdir=/origin_data/edu/db/base_province_full/2022-02-21&quot; /opt/module/datax/job/import/edu.base_province.json</span><br><span class="line"></span><br><span class="line">2023-11-13 21:55:51.868 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2023-11-13 21:55:39</span><br><span class="line">任务结束时刻                    : 2023-11-13 21:55:51</span><br><span class="line">任务总计耗时                    :                 11s</span><br><span class="line">任务平均流量                    :               70B/s</span><br><span class="line">记录写入速度                    :              3rec/s</span><br><span class="line">读出记录总数                    :                  34</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>

<p>查看HDFS目标路径出现数据</p>
<img src="Snipaste_2023-11-13_21-57-15.png" alt="Snipaste_2023-11-13_21-57-15" style="zoom:50%;">

<p><strong>至此，我们编写gen_import_config.py为了生成某个datax配置文件，编写gen_import_config.sh为了批量生成15个datax配置文件，下面我们再编写mysql_tp_hdfs.sh为了批量运行这15个配置文件从而将数据从mysql数据库同步到HDFS。</strong></p>
<h4 id="2-5-3-全量表数据同步脚本"><a href="#2-5-3-全量表数据同步脚本" class="headerlink" title="2.5.3 全量表数据同步脚本"></a>2.5.3 全量表数据同步脚本</h4><p>为方便使用以及后续的任务调度，此处编写一个全量表数据同步脚本。</p>
<p>（1）创建mysql_to_hdfs_full.sh脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim mysql_to_hdfs_full.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">DATAX_HOME=/opt/module/datax</span><br><span class="line">DATAX_DATA=/opt/module/datax/job</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">清理脏数据</span></span><br><span class="line">handle_targetdir() &#123;</span><br><span class="line">  hadoop fs -rm -r $1 &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">  hadoop fs -mkdir -p $1</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">数据同步</span></span><br><span class="line">import_data() &#123;</span><br><span class="line">  local datax_config=$1</span><br><span class="line">  local target_dir=$2</span><br><span class="line"></span><br><span class="line">  handle_targetdir &quot;$target_dir&quot;</span><br><span class="line">  echo &quot;正在处理$1&quot;</span><br><span class="line">  python $DATAX_HOME/bin/datax.py -p&quot;-Dtargetdir=$target_dir&quot; $datax_config &gt;/tmp/datax_run.log 2&gt;&amp;1</span><br><span class="line">  if [ $? -ne 0 ]</span><br><span class="line">  then</span><br><span class="line">    echo &quot;处理失败, 日志如下:&quot;</span><br><span class="line">    cat /tmp/datax_run.log </span><br><span class="line">  fi</span><br><span class="line">  rm /tmp/datax_run.log </span><br><span class="line">&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">接收表名变量</span></span><br><span class="line">tab=$1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果传入日期则do_date等于传入的日期，否则等于前一天日期</span></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    do_date=$(date -d &quot;-1 day&quot; +%F)</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case $&#123;tab&#125; in</span><br><span class="line">base_category_info | base_province | base_source | base_subject_info | cart_info | chapter_info | course_info | knowledge_point | test_paper | test_paper_question | test_point_question | test_question_info | test_question_option | user_chapter_process | video_info)</span><br><span class="line">  import_data $DATAX_DATA/import/edu.$&#123;tab&#125;.json /origin_data/edu/db/$&#123;tab&#125;_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">&quot;all&quot;)</span><br><span class="line">  for tmp in base_category_info base_province base_source base_subject_info cart_info chapter_info course_info knowledge_point test_paper test_paper_question test_point_question test_question_info test_question_option user_chapter_process video_info</span><br><span class="line">  do</span><br><span class="line">    import_data $DATAX_DATA/import/edu.$&#123;tmp&#125;.json /origin_data/edu/db/$&#123;tmp&#125;_full/$do_date</span><br><span class="line">  done</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）为脚本增加执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 mysql_to_hdfs_full.sh</span><br></pre></td></tr></table></figure>

<p>（3）测试同步脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# mysql_to_hdfs_full.sh all 2022-02-21</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.base_category_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.base_province.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.base_source.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.base_subject_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.cart_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.chapter_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.course_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.knowledge_point.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.test_paper.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.test_paper_question.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.test_point_question.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.test_question_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.test_question_option.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.user_chapter_process.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.video_info.json</span><br></pre></td></tr></table></figure>

<p>（4）查看HDFS目标路径是否出现全量数据，全量共15张表</p>
<img src="Snipaste_2023-11-14_15-22-27.png" alt="Snipaste_2023-11-14_15-22-27" style="zoom:50%;">

<p>全量表同步逻辑比较简单，只需每天执行全量表数据同步脚本mysql_to_hdfs_full.sh即可。</p>
<h3 id="2-6-增量表数据同步"><a href="#2-6-增量表数据同步" class="headerlink" title="2.6 增量表数据同步"></a>2.6 增量表数据同步</h3><h4 id="2-6-1-数据通道"><a href="#2-6-1-数据通道" class="headerlink" title="2.6.1 数据通道"></a>2.6.1 数据通道</h4><p>首先通过Maxwell将需要执行增量策略的表格变动数据发送至Kafka的对应topic中，然后使用Flume将kafka中的数据采集落盘至HDFS中。</p>
<img src="Snipaste_2023-11-14_15-44-48.png" alt="Snipaste_2023-11-14_15-44-48" style="zoom:50%;">

<h4 id="2-6-2-Maxwell配置"><a href="#2-6-2-Maxwell配置" class="headerlink" title="2.6.2 Maxwell配置"></a>2.6.2 Maxwell配置</h4><p>（1）修改Maxwell配置文件config.properties</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# vim config.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">log_level=info</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">Maxwell数据发送目的地，可选配置有stdout|file|kafka|kinesis|pubsub|sqs|rabbitmq|redis</span></span><br><span class="line">producer=kafka</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">目标Kafka集群地址</span></span><br><span class="line">kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">目标Kafka topic，可静态配置，例如:maxwell，也可动态配置，例如：%&#123;database&#125;_%&#123;table&#125;</span></span><br><span class="line">kafka_topic=topic_db</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">MySQL相关配置</span></span><br><span class="line">host=hadoop102</span><br><span class="line">user=maxwell</span><br><span class="line">password=maxwell</span><br><span class="line">jdbc_options=useSSL=false&amp;serverTimezone=Asia/Shanghai</span><br></pre></td></tr></table></figure>

<p>（2）重启Maxwell</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# mxw.sh restart</span><br></pre></td></tr></table></figure>

<p>（3）采集通道测试</p>
<p>①启动zookeeper和kafka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# zk.sh start</span><br><span class="line">[root@hadoop102 maxwell]# kf.sh start</span><br></pre></td></tr></table></figure>

<p>②启动一个kafka控制台消费者，消费topic_db主题的数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic topic_db</span><br></pre></td></tr></table></figure>

<p>③生成模拟数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# mock.sh 2022-02-22</span><br></pre></td></tr></table></figure>

<p>④观察kafka消费者是否能消费到数据</p>
<img src="Snipaste_2023-11-14_16-01-28.png" alt="Snipaste_2023-11-14_16-01-28" style="zoom: 33%;">

<h4 id="2-6-3-Flume配置"><a href="#2-6-3-Flume配置" class="headerlink" title="2.6.3 Flume配置"></a>2.6.3 Flume配置</h4><p>Flume需要将Kafka中的数据传输至HDFS，所以需要选用Kafka Source和HDFS Sink，Channel选用File Channel。需要注意的是，Maxwell将监控到的业务数据库中的全部变动数据均发往了同一个Kafka主题，所以不同表格的变动数据是混合在一起的。因此我们需要自定义一个拦截器，在拦截器中识别数据中的表格信息，获取tableName，添加至header中。HDFS Sink在将数据落盘至HDFS时，通过识别header中的tableName，可以将不同表格的数据写入不同的路径下。</p>
<img src="Snipaste_2023-11-14_16-11-55.png" alt="Snipaste_2023-11-14_16-11-55" style="zoom:50%;">

<p>具体示例如下：一条变动数据被Maxwell采集发送至Kafka的topic_db主题中，其中包含时间戳ts。Flume的Kafka Source在采集到这条数据之后。通过如下图的关键配置，将tableName-&gt;order_info和ts-&gt;1645425636两个关键键值对写入header。HDFS将这条数据落盘时，即可根据header中封装的tableName和ts信息，写入对应的文件夹中。通过以上操作，使得数据可以存放于对应表名命名的文件夹下对应时间命名的文件中。</p>
<img src="Snipaste_2023-11-14_16-12-13.png" alt="Snipaste_2023-11-14_16-12-13" style="zoom:50%;">

<p>（1）创建Flume配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 job]# vim kafka_to_hdfs_db.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">a1.sources.r1.batchSize = 5000</span><br><span class="line">a1.sources.r1.batchDurationMillis = 2000</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092</span><br><span class="line">a1.sources.r1.kafka.topics = topic_db</span><br><span class="line">a1.sources.r1.kafka.consumer.group.id = flume</span><br><span class="line">a1.sources.r1.setTopicHeader = true</span><br><span class="line">a1.sources.r1.topicHeader = topic</span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptors.TimestampAndTableNameInterceptor$Builder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.channels.c1.type = file</span><br><span class="line">a1.channels.c1.checkpointDir = /opt/data/flume/checkpoint/behavior2</span><br><span class="line">a1.channels.c1.dataDirs = /opt/data/flume/data/behavior2</span><br><span class="line">a1.channels.c1.maxFileSize = 2146435071</span><br><span class="line">a1.channels.c1.capacity = 1000000</span><br><span class="line">a1.channels.c1.keep-alive = 6</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># sink1</span></span></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /origin_data/edu/db/%&#123;tableName&#125;_inc/%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = db</span><br><span class="line">a1.sinks.k1.hdfs.round = false</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 10</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.fileType = CompressedStream</span><br><span class="line">a1.sinks.k1.hdfs.codeC = gzip</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 拼装</span></span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel= c1</span><br></pre></td></tr></table></figure>

<p>（2）编写Flume拦截器</p>
<p>此拦截器用于提取数据中包含的表名信息和时间戳信息，将秒级时间戳转换至毫秒级时间戳，并将表名和时间戳添加至header中。</p>
<p>创建TimestampAndTableNameInterceptor类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TimestampAndTableNameInterceptor</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initialize</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Event <span class="title function_">intercept</span><span class="params">(Event event)</span> &#123;</span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line">        <span class="type">String</span> <span class="variable">log</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(event.getBody(), StandardCharsets.UTF_8);</span><br><span class="line"></span><br><span class="line">        <span class="type">JSONObject</span> <span class="variable">jsonObject</span> <span class="operator">=</span> JSONObject.parseObject(log);</span><br><span class="line"></span><br><span class="line">        <span class="type">Long</span> <span class="variable">ts</span> <span class="operator">=</span> jsonObject.getLong(<span class="string">&quot;ts&quot;</span>);</span><br><span class="line">        <span class="comment">//Maxwell输出的数据中的ts字段时间戳单位为秒，Flume HDFSSink要求单位为毫秒</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">timeMills</span> <span class="operator">=</span> String.valueOf(ts * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">String</span> <span class="variable">tableName</span> <span class="operator">=</span> jsonObject.getString(<span class="string">&quot;table&quot;</span>);</span><br><span class="line"></span><br><span class="line">        headers.put(<span class="string">&quot;timestamp&quot;</span>, timeMills);</span><br><span class="line">        headers.put(<span class="string">&quot;tableName&quot;</span>, tableName);</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;Event&gt; <span class="title function_">intercept</span><span class="params">(List&lt;Event&gt; events)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (Event event : events) &#123;</span><br><span class="line">            intercept(event);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> events;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Builder</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span>.Builder &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Interceptor <span class="title function_">build</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">TimestampAndTableNameInterceptor</span> ();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）打包</p>
<p>（4）删除原来的jar包，将打包好的jar包放入hadoop104的&#x2F;opt&#x2F;module&#x2F;flume&#x2F;lib文件夹下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 lib]# ls | grep inter</span><br><span class="line">edu-flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<p>（5）采集通道测试</p>
<p>①确保启动Zookeeper、Kafka、Maxwell</p>
<p>②启动hadoop104节点的Flume Agent</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 flume]# bin/flume-ng agent -n a1 -c conf/ -f job/kafka_to_hdfs_db.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>③模拟业务数据生成</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# mock.sh 2022-02-22</span><br></pre></td></tr></table></figure>

<p>④在HDFS端出现了新采集的，以inc结尾的增量数据</p>
<img src="Snipaste_2023-11-14_19-21-11.png" alt="Snipaste_2023-11-14_19-21-11" style="zoom: 50%;">

<p>（6）数据目标路径的日期说明</p>
<p>仔细观察，会发现目标路径中的日期，并非模拟数据的业务日期，而是当前日期。这是由于Maxwell输出的JSON字符串中的ts字段的值，是数据的变动日期。而真实场景下，数据的业务日期与变动日期应当是一致的。</p>
<img src="Snipaste_2023-11-14_19-27-19.png" alt="Snipaste_2023-11-14_19-27-19" style="zoom:50%;">

<p>此处为了模拟真实环境，对Maxwell源码进行了改动，增加了一个参数mock_date，该参数的作用就是指定Maxwell输出JSON字符串的ts时间戳的日期。</p>
<p>接下来进行测试：</p>
<p>①修改Maxwell配置文件config.properties，增加mock_date参数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">教学环境添加的配置项，使 Maxwell 输出数据中时间戳对应日期与业务数据的日期相同</span></span><br><span class="line">mock_date=2022-02-21</span><br></pre></td></tr></table></figure>

<p>②重启Maxwell</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# mxw.sh restart</span><br></pre></td></tr></table></figure>

<p>③重新生成模拟数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# cd /opt/module/data_mocker/</span><br><span class="line">[root@hadoop102 data_mocker]# java -jar edu2021-mock-2022-06-18.jar </span><br></pre></td></tr></table></figure>

<p>④可以看到，日期与模拟数据日期一致：</p>
<img src="Snipaste_2023-11-14_19-37-38.png" alt="Snipaste_2023-11-14_19-37-38" style="zoom:50%;">

<p>（7）上述操作在每次需要模拟数据时，都需要修改Maxwell的配置文件并且重启Maxwell，过于繁琐。鉴于Maxwell中配置文件修改的mock.date参数，需要与模拟数据时配置文件中的mock.date参数相同，所以我们修改mock.sh脚本，在其中添加代码，同时修改两项配置文件，并重启Maxwell。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">DATA_HOME=/opt/module/data_mocker</span><br><span class="line">MAXWELL_HOME=/opt/module/maxwell</span><br><span class="line"></span><br><span class="line">function mock_data() &#123;</span><br><span class="line">  if [ $1 ]</span><br><span class="line">  then</span><br><span class="line">    sed -i &quot;/mock.date/s/.*/mock.date: \&quot;$1\&quot;/&quot; $DATA_HOME/application.yml</span><br><span class="line">    echo &quot;正在生成 $1 当日的数据&quot;</span><br><span class="line">  fi</span><br><span class="line">  cd $DATA_HOME</span><br><span class="line">      nohup java -jar &quot;edu2021-mock-2022-06-18.jar&quot; &gt;/dev/null 2&gt;&amp;1  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;init&quot;)</span><br><span class="line">  [ $2 ] &amp;&amp; do_date=$2 || do_date=&#x27;2022-02-21&#x27;</span><br><span class="line">  sed -i &quot;/mock.clear.busi/s/.*/mock.clear.busi: 1/&quot; $DATA_HOME/application.yml</span><br><span class="line">  sed -i &quot;/mock.clear.user/s/.*/mock.clear.user: 1/&quot; $DATA_HOME/application.yml</span><br><span class="line">  mock_data $(date -d &quot;$do_date -5 days&quot; +%F)</span><br><span class="line">  sed -i &quot;/mock.clear.busi/s/.*/mock.clear.busi: 0/&quot; $DATA_HOME/application.yml</span><br><span class="line">  sed -i &quot;/mock.clear.user/s/.*/mock.clear.user: 0/&quot; $DATA_HOME/application.yml</span><br><span class="line">  for ((i=4;i&gt;=0;i--));</span><br><span class="line">  do</span><br><span class="line">    mock_data $(date -d &quot;$do_date -$i days&quot; +%F)</span><br><span class="line">  done</span><br><span class="line">  ;;</span><br><span class="line">[0-9][0-9][0-9][0-9]-[0-1][0-9]-[0-3][0-9])</span><br><span class="line"></span><br><span class="line">    sed -i &quot;/mock_date/s/.*/mock_date=$1/&quot; $MAXWELL_HOME/config.properties</span><br><span class="line">    mxw.sh restart</span><br><span class="line">    sleep 1  </span><br><span class="line">    mock_data $1</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>修改脚本后，重新执行脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# mock.sh 2022-02-22</span><br><span class="line">停止Maxwell</span><br><span class="line">启动Maxwell</span><br><span class="line">Redirecting STDOUT to /opt/module/maxwell/bin/../logs/MaxwellDaemon.out</span><br><span class="line">Using kafka version: 1.0.0</span><br><span class="line">正在生成 2022-02-22 当日的数据</span><br></pre></td></tr></table></figure>

<p>再次观察HDFS目标路径下的数据，路径的时间已经被修正为数据中的时间</p>
<img src="Snipaste_2023-11-14_19-59-39.png" alt="Snipaste_2023-11-14_19-59-39" style="zoom:50%;">

<p>（8）编写Flume启停脚本</p>
<p>在hadoop102节点服务器的~&#x2F;bin目录下创建f3.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim f3.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; --------启动 hadoop104 业务数据flume-------&quot;</span><br><span class="line">        ssh hadoop104 &quot;nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf -f /opt/module/flume/job/kafka_to_hdfs_db.conf &gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line"></span><br><span class="line">        echo &quot; --------停止 hadoop104 业务数据flume-------&quot;</span><br><span class="line">        ssh hadoop104 &quot;ps -ef | grep kafka_to_hdfs_db | grep -v grep |awk &#x27;&#123;print \$2&#125;&#x27; | xargs -n1 kill&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>增加脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 f3.sh</span><br></pre></td></tr></table></figure>

<h4 id="2-6-4-增量表首日全量同步"><a href="#2-6-4-增量表首日全量同步" class="headerlink" title="2.6.4 增量表首日全量同步"></a>2.6.4 增量表首日全量同步</h4><p>通常情况下，增量表需要在首日进行一次全量同步，后续每日再进行增量同步，首日全量同步可以使用Maxwell的bootstrap功能，方便起见，下面编写一个<strong>增量表首日全量同步脚本</strong>。</p>
<p>（1）在~&#x2F;bin目录创建mysql_to_kafka_inc_init.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim mysql_to_kafka_inc_init.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">该脚本的作用是初始化所有的增量表，只需执行一次</span></span><br><span class="line"></span><br><span class="line">MAXWELL_HOME=/opt/module/maxwell</span><br><span class="line"></span><br><span class="line">import_data() &#123;</span><br><span class="line">    $MAXWELL_HOME/bin/maxwell-bootstrap --database edu --table $1 --config $MAXWELL_HOME/config.properties</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">cart_info | comment_info | favor_info | order_detail | order_info | payment_info | review_info | test_exam | test_exam_question | user_info | vip_change_detail)</span><br><span class="line">  import_data $1</span><br><span class="line">  ;;</span><br><span class="line">&quot;all&quot;)</span><br><span class="line">  for tmp in cart_info comment_info favor_info order_detail order_info payment_info review_info test_exam test_exam_question user_info vip_change_detail</span><br><span class="line">  do</span><br><span class="line">    import_data $tmp</span><br><span class="line">  done</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）增加执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 mysql_to_kafka_inc_init.sh</span><br></pre></td></tr></table></figure>

<p>（3）清理历史数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# hadoop fs -ls /origin_data/edu/db | grep _inc | awk &#x27;&#123;print $8&#125;&#x27; | xargs hadoop fs -rm -r -f</span><br></pre></td></tr></table></figure>

<p>（4）执行同步脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# mysql_to_kafka_inc_init.sh all</span><br></pre></td></tr></table></figure>

<p>（5）检查同步结果</p>
<img src="Snipaste_2023-11-14_20-45-35.png" alt="Snipaste_2023-11-14_20-45-35" style="zoom:50%;">

<h4 id="2-6-5-总结"><a href="#2-6-5-总结" class="headerlink" title="2.6.5 总结"></a>2.6.5 总结</h4><p>增量表同步，需要在首日进行一次全量同步，后续每日才是增量同步。首日进行全量同步时，需先启动数据通道，包括Maxwell、Kafka、Flume，然后执行增量表首日同步脚本mysql_to_kafka_inc_init.sh进行同步。后续每日只需保证采集通道正常运行即可，Maxwell便会实时将变动数据发往Kafka。</p>
<h3 id="2-7-数据采集流程总结"><a href="#2-7-数据采集流程总结" class="headerlink" title="2.7 数据采集流程总结"></a>2.7 数据采集流程总结</h3><h4 id="2-7-1-清除所有数据"><a href="#2-7-1-清除所有数据" class="headerlink" title="2.7.1 清除所有数据"></a>2.7.1 清除所有数据</h4><p>（1）清空hadoop102节点服务器上模拟生成的用户行为日志数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# rm -rf /opt/module/data_mocker/log</span><br></pre></td></tr></table></figure>

<p>（2）清空HDFS上所有已经采集成功的数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# hadoop fs -rm -r -f /origin_data</span><br><span class="line">Deleted /origin_data</span><br></pre></td></tr></table></figure>

<h4 id="2-7-2-启动系统"><a href="#2-7-2-启动系统" class="headerlink" title="2.7.2 启动系统"></a>2.7.2 启动系统</h4><p>（1）执行集群启动脚本，开启hadoop、zookeeper、kafka以及用户行为数据采集系统</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# cluster.sh start</span><br><span class="line">[root@hadoop102 bin]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">14770 JobHistoryServer</span><br><span class="line">15155 Kafka</span><br><span class="line">14070 NameNode</span><br><span class="line">14583 NodeManager</span><br><span class="line">14250 DataNode</span><br><span class="line">15226 Application</span><br><span class="line">15437 Jps</span><br><span class="line">13838 QuorumPeerMain</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">7840 ResourceManager</span><br><span class="line">7623 DataNode</span><br><span class="line">8700 Kafka</span><br><span class="line">8828 Jps</span><br><span class="line">7518 QuorumPeerMain</span><br><span class="line">8175 NodeManager</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">9280 Jps</span><br><span class="line">8337 DataNode</span><br><span class="line">9075 Application</span><br><span class="line">8551 NodeManager</span><br><span class="line">8460 SecondaryNameNode</span><br><span class="line">8238 QuorumPeerMain</span><br><span class="line">9022 Kafka</span><br></pre></td></tr></table></figure>

<p>（2）启动Maxwell</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# mxw.sh start</span><br></pre></td></tr></table></figure>

<p>（3）启动业务数据采集Flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# f3.sh start</span><br></pre></td></tr></table></figure>

<h4 id="2-7-3-首日数据生成及采集"><a href="#2-7-3-首日数据生成及采集" class="headerlink" title="2.7.3 首日数据生成及采集"></a>2.7.3 首日数据生成及采集</h4><p>（1）进行首日数据模拟</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# mock.sh init</span><br><span class="line">正在生成 2022-02-16 当日的数据</span><br><span class="line">正在生成 2022-02-17 当日的数据</span><br><span class="line">正在生成 2022-02-18 当日的数据</span><br><span class="line">正在生成 2022-02-19 当日的数据</span><br><span class="line">正在生成 2022-02-20 当日的数据</span><br><span class="line">正在生成 2022-02-21 当日的数据</span><br></pre></td></tr></table></figure>

<p>（2）将全量数据同步采集至HDFS中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# mysql_to_hdfs_full.sh all 2022-02-21</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.base_category_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.base_province.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.base_source.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.base_subject_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.cart_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.chapter_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.course_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.knowledge_point.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.test_paper.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.test_paper_question.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.test_point_question.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.test_question_info.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.test_question_option.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.user_chapter_process.json</span><br><span class="line">正在处理/opt/module/datax/job/import/edu.video_info.json</span><br></pre></td></tr></table></figure>

<p>（3）使用Maxwell将执行增量同步策略的业务数据表格进行初始化同步（bootstrap）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# mysql_to_kafka_inc_init.sh all</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br><span class="line">connecting to jdbc:mysql://hadoop102:3306/maxwell?allowPublicKeyRetrieval=true&amp;connectTimeout=5000&amp;serverTimezone=Asia%2FShanghai&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=false</span><br></pre></td></tr></table></figure>

<p>（4）观察HDFS上的数据采集情况，在HDFS上已经采集到了所有业务数据，包括增量数据和全量数据</p>
<p>业务数据：</p>
<img src="Snipaste_2023-11-14_21-06-49.png" alt="Snipaste_2023-11-14_21-06-49" style="zoom:50%;">

<p>用户行为数据：</p>
<img src="Snipaste_2023-11-14_21-08-24.png" alt="Snipaste_2023-11-14_21-08-24" style="zoom:50%;">

<h4 id="2-7-4-每日数据生成及采集"><a href="#2-7-4-每日数据生成及采集" class="headerlink" title="2.7.4 每日数据生成及采集"></a>2.7.4 每日数据生成及采集</h4><p>（1）每日数据模拟</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# mock.sh 2022-02-22</span><br><span class="line">停止Maxwell</span><br><span class="line">启动Maxwell</span><br><span class="line">Redirecting STDOUT to /opt/module/maxwell/bin/../logs/MaxwellDaemon.out</span><br><span class="line">Using kafka version: 1.0.0</span><br><span class="line">正在生成 2022-02-22 当日的数据</span><br></pre></td></tr></table></figure>

<p>（2）业务数据全量同步</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# mysql_to_hdfs_full.sh all 2022-02-22</span><br></pre></td></tr></table></figure>

<p>（3）观察HDFS上的数据采集情况</p>
<p>用户行为数据：</p>
<img src="Snipaste_2023-11-14_21-16-24.png" alt="Snipaste_2023-11-14_21-16-24" style="zoom:50%;">

<p>业务数据：</p>
<img src="Snipaste_2023-11-14_21-17-34.png" alt="Snipaste_2023-11-14_21-17-34" style="zoom:50%;">

<img src="Snipaste_2023-11-14_21-18-09.png" alt="Snipaste_2023-11-14_21-18-09" style="zoom:50%;">

<p>在数据仓库系统的运行过程中，要保证hadoop、zookeeper、kafka、Flume采集程序、Maxwell等持续运行，此后模拟生成每日数据，用户行为日志数据和业务数据中的增量数据会通过kafka和Flume自动采集至HDFS中。而业务数据中的全量数据则依靠每日执行业务数据全量同步脚本进行定时采集。</p>
<p>关闭顺序：f3.sh stop –&gt;  mxw.sh stop –&gt;  cluster.sh stop</p>
<h1 id="第三部分-数据仓库系统"><a href="#第三部分-数据仓库系统" class="headerlink" title="第三部分 数据仓库系统"></a>第三部分 数据仓库系统</h1><h2 id="第一章-数据仓库概述"><a href="#第一章-数据仓库概述" class="headerlink" title="第一章 数据仓库概述"></a>第一章 数据仓库概述</h2><h3 id="1-1-数据仓库概述"><a href="#1-1-数据仓库概述" class="headerlink" title="1.1 数据仓库概述"></a>1.1 数据仓库概述</h3><h4 id="1-1-1-数据仓库的定义"><a href="#1-1-1-数据仓库的定义" class="headerlink" title="1.1.1 数据仓库的定义"></a>1.1.1 数据仓库的定义</h4><p>数据仓库（Data Warehouse）是一个为数据分析而设计的企业级数据管理系统。数据仓库可集中、整合多个信息源的大量数据，借助数据仓库的分析能力，企业可从数据中获得宝贵的信息进而改进决策。同时，随着时间的推移，数据仓库中积累的大量历史数据对于数据科学家和业务分析师也是十分宝贵的。————<strong>构建面向数据分析的集成化数据环境，为企业提供分析性报告和决策支持</strong></p>
<p>数据仓库本身不产生数据，自身不消耗任何数据，数据来源于外部，并且开放给外部使用，顾名思义叫“仓库”。</p>
<h4 id="1-1-2-数据仓库基本特征"><a href="#1-1-2-数据仓库基本特征" class="headerlink" title="1.1.2 数据仓库基本特征"></a>1.1.2 数据仓库基本特征</h4><ul>
<li>面向主题：对企业某一宏观领域所涉及的分析对象</li>
<li>集成的：对分散、独立、异构的数据进行ETL得到数据仓库中的工具</li>
<li>非易失的（不可更新性）：数仓的用户对数据操作大多是数据查询或数据挖掘，入仓后会长时间保留，不会频繁更新和删除数据。</li>
<li>时变的：数仓中的数据往往带有时间属性，根据业务的变化进行追加更新</li>
</ul>
<h4 id="1-1-3-数据库与数据仓库的区别"><a href="#1-1-3-数据库与数据仓库的区别" class="headerlink" title="1.1.3 数据库与数据仓库的区别"></a>1.1.3 数据库与数据仓库的区别</h4><p>数据库与数据仓库的区别实际讲的是 <strong>OLTP</strong> 与 <strong>OLAP</strong> 的区别。</p>
<table>
<thead>
<tr>
<th></th>
<th>数据库</th>
<th>数据仓库</th>
</tr>
</thead>
<tbody><tr>
<td>处理方式</td>
<td>联机事务处理OLTP</td>
<td>联机分析处理OLAP</td>
</tr>
<tr>
<td>读特性</td>
<td>每次查询返回少量记录</td>
<td>对大量数据进行汇总查询</td>
</tr>
<tr>
<td>写特性</td>
<td>随机、低延时写入</td>
<td>批量导入</td>
</tr>
<tr>
<td>数据存储</td>
<td>业务数据</td>
<td>历史数据</td>
</tr>
<tr>
<td>设计理念</td>
<td>面向事务设计，为了捕获数据，避免冗余</td>
<td>面向主题设计，为了分析数据，引入冗余</td>
</tr>
<tr>
<td>数据量</td>
<td>GB</td>
<td>TB、PB</td>
</tr>
</tbody></table>
<h4 id="1-1-4-数据仓库元数据的管理"><a href="#1-1-4-数据仓库元数据的管理" class="headerlink" title="1.1.4 数据仓库元数据的管理"></a>1.1.4 数据仓库元数据的管理</h4><p><strong>元数据（Meta Date），主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态</strong>。一般会通过元数据资料库（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。</p>
<h3 id="1-2-数据仓库核心架构"><a href="#1-2-数据仓库核心架构" class="headerlink" title="1.2 数据仓库核心架构"></a>1.2 数据仓库核心架构</h3><img src="Snipaste_2023-11-15_13-12-16.png" alt="Snipaste_2023-11-15_13-12-16" style="zoom:50%;">

<p>ODS层保留原始数据，将分析后的数据落入其他层，使得数据仓库是分层设计的</p>
<img src="Snipaste_2023-11-30_19-22-14.png" alt="Snipaste_2023-11-30_19-22-14" style="zoom:50%;">

<p>按照数据流入流出的过程，数据仓库架构可分为：<strong>源数据</strong>（日志数据、文档数据、爬虫数据）、<strong>数据仓库</strong>、<strong>数据应用</strong>（报表、可视化展示等）</p>
<p>数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（<strong>抽取Extra, 转化Transfer, 装载Load</strong>）的过程</p>
<h2 id="第二章-数据仓库建模概述（部分参考阿里巴巴大数据之路）"><a href="#第二章-数据仓库建模概述（部分参考阿里巴巴大数据之路）" class="headerlink" title="第二章 数据仓库建模概述（部分参考阿里巴巴大数据之路）"></a>第二章 数据仓库建模概述（部分参考阿里巴巴大数据之路）</h2><h3 id="2-1-数据仓库建模的意义"><a href="#2-1-数据仓库建模的意义" class="headerlink" title="2.1 数据仓库建模的意义"></a>2.1 数据仓库建模的意义</h3><p>数据模型就是数据组织和存储方法，它强调从业务、数据存取和使用角度合理存储数据。只有将数据有序的组织和存储起来之后，数据才能得到高性能、低成本、高效率、高质量的使用。</p>
<p>高性能：良好的数据模型能够帮助我们快速查询所需要的数据。</p>
<p>低成本：良好的数据模型能减少重复计算，实现计算结果的复用，降低计算成本。</p>
<p>高效率：良好的数据模型能极大的改善用户使用数据的体验，提高使用数据的效率。</p>
<p>高质量：良好的数据模型能改善数据统计口径的混乱，减少计算错误的可能性。</p>
<blockquote>
<p>建设数据仓库的目标：将数据有序、有结构地分类组织和存储</p>
</blockquote>
<h3 id="2-2-数据仓库建模方法论"><a href="#2-2-数据仓库建模方法论" class="headerlink" title="2.2 数据仓库建模方法论"></a>2.2 数据仓库建模方法论</h3><p>主流的数据仓库设计模型有两种，Bill Inmon支持的关系模型和Ralph Kimball支持的维度模型。</p>
<h4 id="2-2-1-ER模型"><a href="#2-2-1-ER模型" class="headerlink" title="2.2.1 ER模型"></a>2.2.1 ER模型</h4><p>数据仓库之父Bill Inmon提出的建模方法是从全企业的高度，用实体关系（Entity Relationship，ER）模型来描述企业业务，并用规范化的方式表示出来，<strong>在范式理论上符合3NF</strong>。                    </p>
<p>（1）实体关系模型</p>
<p>实体关系模型将复杂的数据抽象为两个概念——实体和关系。实体表示一个对象，例如学生、班级，关系是指两个实体之间的关系，例如学生和班级之间的从属关系。</p>
<p>（2）数据库规范化</p>
<p>数据库规范化是使用一系列范式设计数据库（通常是关系型数据库）的过程，其目的是减少数据冗余，增强数据的一致性。</p>
<p>这一系列范式就是指在设计关系型数据库时，需要遵从的不同的规范。关系型数据库的范式一共有<strong>六种</strong>，分别是<strong>第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）</strong>。遵循的范式级别越高，数据冗余性就越低。</p>
<p>（3）函数依赖</p>
<p><img src="Snipaste_2023-11-15_13-40-46.png" alt="Snipaste_2023-11-15_13-40-46"></p>
<p>（4）第一范式</p>
<img src="Snipaste_2023-11-15_13-42-21.png" alt="Snipaste_2023-11-15_13-42-21" style="zoom:50%;">

<p>（5）第二范式</p>
<img src="Snipaste_2023-11-15_14-12-40.png" alt="Snipaste_2023-11-15_14-12-40" style="zoom:50%;">

<p>（6）第三范式</p>
<img src="Snipaste_2023-11-15_14-21-22.png" alt="Snipaste_2023-11-15_14-21-22" style="zoom:50%;">

<p>下图为一个采用Bill Inmon倡导的建模方法构建的模型，从图中可以看出，较为松散、零碎，物理表数量多。关系模型主要应用于OLTP系统中。</p>
<img src="Snipaste_2023-11-15_14-27-00.png" alt="Snipaste_2023-11-15_14-27-00" style="zoom:50%;">

<p>这种建模方法的出发点是整合数据，其目的是将整个企业的数据进行组合和合并，并进行规范处理，减少数据冗余性，保证数据的一致性。这种模型并不适合直接用于分析统计。</p>
<h4 id="2-2-2-维度模型"><a href="#2-2-2-维度模型" class="headerlink" title="2.2.2 维度模型"></a>2.2.2 维度模型</h4><p>数据处理大致可以分成两大类：联机事务处理（On-Line Transaction Processing，OLTP）、联机分析处理（On-Line Analytical Processing，OLAP）。OLTP是传统的关系型数据库的主要应用，而OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且可以提供直观、易懂的查询结果。</p>
<table>
<thead>
<tr>
<th>对比属性</th>
<th>OLTP</th>
<th>OLAP</th>
</tr>
</thead>
<tbody><tr>
<td>读特性</td>
<td>每次查询只返回少量记录</td>
<td>对大量记录进行汇总</td>
</tr>
<tr>
<td>写特性</td>
<td>随机、低延时写入用户的数据</td>
<td>批量导入</td>
</tr>
<tr>
<td>使用场景</td>
<td>用户，javaEE项目</td>
<td>内部分析师，为决策提供支持</td>
</tr>
<tr>
<td>数据表征</td>
<td>最新数据状态</td>
<td>随时间变化的历史状态</td>
</tr>
<tr>
<td>数据规模</td>
<td>GB</td>
<td>TB到PB</td>
</tr>
</tbody></table>
<p>维度建模是一种将大量数据结构化的逻辑设计手段，包含维度和度量指标。它不是为了消除冗余数据，而是面向分析设计，<strong>最终目的是提高查询性能，最终结果会增加数据冗余，并且违法三范式。</strong></p>
<p>数据仓库领域的令一位大师——Ralph Kimball倡导的建模方法为维度建模。维度模型将复杂的业务通过<strong>事实</strong>和<strong>维度</strong>两个概念进行呈现。<strong>事实</strong>通常对应<strong>业务过程</strong>，而<strong>维度</strong>通常对应<strong>业务过程发生时所处的环境</strong>。</p>
<p>注：业务过程可以概括为一个个不可拆分的行为事件，例如在线教育交易中的下单，付款，加购等，都是业务过程。</p>
<blockquote>
<p>下图为一个典型的维度模型，其中位于中心的SalesOrder为事实表，其中保存的是下单这个业务过程的所有记录。位于周围每张表都是维度表，包括Date（日期），Customer（顾客），Product（产品），Location（地区）等，这些维度表就组成了每个订单发生时所处的环境，即何人、何时、在何地下单了何种产品。从图中可以看出，模型相对清晰、简洁。</p>
</blockquote>
<img src="图片1.png" alt="图片1" style="zoom:50%;">

<p>维度表模型主要用于OLAP系统中，通常以某一张<strong>事实表</strong>为中心进行表的组织，主要面向查询，特征是可能存在数据的冗余，但是用户能方便地得到数据。</p>
<p>关系模型虽然数据冗余程度低，但是在大规模数据中进行跨表分析统计查询时，会造成<strong>多表关联</strong>，这会大大降低执行效率，所以通常我们采用<strong>维度模型</strong>建模，把各种相关表整理成<strong>事实表</strong>和<strong>维度表</strong>两种。所有的维度表围绕事实表进行解释。</p>
<h2 id="第三章-维度建模理论之事实表"><a href="#第三章-维度建模理论之事实表" class="headerlink" title="第三章 维度建模理论之事实表"></a>第三章 维度建模理论之事实表</h2><h3 id="3-1-事实表概述"><a href="#3-1-事实表概述" class="headerlink" title="3.1 事实表概述"></a>3.1 事实表概述</h3><p>事实表作为数据仓库维度建模的核心，紧紧围绕着业务过程来设计。其包含与该业务过程有关的维度引用（维度表外键）以及该业务过程的度量（通常是可累加的数字类型字段）。</p>
<h4 id="3-1-1-事实表特点"><a href="#3-1-1-事实表特点" class="headerlink" title="3.1.1 事实表特点"></a>3.1.1 事实表特点</h4><p>（1）通常数据量比较大</p>
<p>（2）内容比较窄，细长，列较少（主要是一些外键ID和度量值字段），行较多</p>
<p>（3）经常发生变化，每天行都会增加新数据</p>
<h4 id="3-1-2-事实表分类"><a href="#3-1-2-事实表分类" class="headerlink" title="3.1.2 事实表分类"></a>3.1.2 事实表分类</h4><p>事实表有三种类型：分别是事务事实表、周期快照事实表和累积快照事实表，每种事实表都具有不同的特点和适用场景，下面逐个介绍。</p>
<h3 id="3-2-事务事实表"><a href="#3-2-事务事实表" class="headerlink" title="3.2 事务事实表"></a>3.2 事务事实表</h3><h4 id="3-2-1-概述"><a href="#3-2-1-概述" class="headerlink" title="3.2.1 概述"></a>3.2.1 概述</h4><p>事务事实表用来记录各业务过程，它保存的是各业务过程的原子操作事件，即最细粒度的操作事件。粒度是指事实表中一行数据所表达的业务细节程度。</p>
<p>事务型事实表可用于分析与各业务过程相关的各项统计指标，由于其保存了最细粒度的记录，可以提供最大限度的灵活性，可以支持无法预期的各种细节层次的统计需求。</p>
<h4 id="3-2-2-设计流程"><a href="#3-2-2-设计流程" class="headerlink" title="3.2.2 设计流程"></a>3.2.2 设计流程</h4><p>设计事务事实表时一般可遵循以下四个步骤：</p>
<p><strong>选择业务过程→声明粒度→确认维度→确认事实</strong></p>
<p><em><strong>1）选择业务过程</strong></em></p>
<p>在业务系统中，挑选我们感兴趣的业务过程，业务过程可以概括为一个个不可拆分的行为事件，例如在线教育交易中的下单，付款，加购等，都是业务过程。通常情况下，一个业务过程对应一张事务型事实表。</p>
<p><em><strong>2）声明粒度</strong></em></p>
<p>业务过程确定后，需要为每个业务过程声明粒度。即精确定义每张事务型事实表的每行数据表示什么，应该尽可能选择最细粒度，以此来应对各种细节程度的需求。</p>
<p><em><strong>典型的粒度声明如下：</strong></em></p>
<p>订单事实表中一行数据表示的是一个订单中的一门课程。</p>
<p><em><strong>3）确定维度</strong></em></p>
<p>确定维度具体是指，确定与每张事务型事实表相关的维度有哪些。</p>
<p>确定维度时应尽量多的选择与业务过程相关的环境信息。因为维度的丰富程度就决定了维度模型能够支持的指标丰富程度。</p>
<p><em><strong>4）确定事实</strong></em></p>
<p>此处的“事实”一词，指的是每个业务过程的度量值（通常是可累加的数字类型的值，例如：次数、个数、件数、金额等）。</p>
<p>经过上述四个步骤，事务型事实表就基本设计完成了。</p>
<p>第一步选择业务过程可以确定<strong>有哪些事务型事实表</strong>，</p>
<p>第二步可以确定每张事务型事实<strong>表的每行数据是什么</strong>，</p>
<p>第三步可以确定每张事务型事实<strong>表的维度外键</strong>，</p>
<p>第四步可以确定每张事务型事实<strong>表的度量值字段</strong>。</p>
<h4 id="3-2-3-不足"><a href="#3-2-3-不足" class="headerlink" title="3.2.3 不足"></a>3.2.3 不足</h4><p>事务型事实表可以保存所有业务过程的最细粒度的操作事件，故理论上其可以支撑与各业务过程相关的各种统计粒度的需求。但对于某些特定类型的需求，其逻辑可能会比较复杂，或者效率会比较低下。例如：</p>
<p><em><strong>1）存量型指标</strong></em></p>
<p>例如购物车存量，账户余额等。此处以在线教育中的加购业务为例，加购业务包含的业务过程主要包括加购物车和减购物车，两个业务过程各自对应一张事务型事实表，一张存储所有加购物车的原子操作事件，另一张存储所有减购物车的原子操作事件。</p>
<p>假定现有一个需求，要求统计截至当日的各用户各科目的购物车存量。由于加购物车和减购物车操作均会影响到购物车存量，故需要对两张事务型事实表进行聚合，且需要区分两者对购物车存量的影响（加或减），另外需要对两张表的全表数据聚合才能得到统计结果。</p>
<p>可以看到，不论是从逻辑上还是效率上考虑，这都不是一个好的方案。</p>
<p><em><strong>2）多事务关联统计</strong></em></p>
<p>例如，现需要统计最近30天，用户下单到支付的时间间隔的平均值。统计思路应该是找到下单事务事实表和支付事务事实表，过滤出最近30天的记录，然后按照订单id对两张事实表进行关联，之后用支付时间减去下单时间，然后再求平均值。</p>
<p>逻辑上虽然并不复杂，但是其效率较低，因为下单事务事实表和支付事务事实表均为大表，大表join大表的操作应尽量避免。</p>
<p>可以看到，在上述两种场景下事务型事实表的表现并不理想。下面要介绍的另外两种类型的事实表就是为了弥补事务型事实表的不足的。</p>
<h3 id="3-3-周期型快照事实表"><a href="#3-3-周期型快照事实表" class="headerlink" title="3.3 周期型快照事实表"></a>3.3 周期型快照事实表</h3><h4 id="3-3-1-概述"><a href="#3-3-1-概述" class="headerlink" title="3.3.1 概述"></a>3.3.1 概述</h4><p>周期快照事实表以具有规律性的、可预见的<strong>时间间隔</strong>来记录事实，主要用于分析一些<strong>存量型</strong>（例如购物车存量，账户余额）或者<strong>状态型</strong>（空气温度，行驶速度）指标。</p>
<p>对于购物车存量、账户余额这些存量型指标，业务系统中通常就会计算并保存最新结果，所以定期同步一份全量数据到数据仓库，构建周期型快照事实表，就能轻松应对此类统计需求，而无需再对事务型事实表中大量的历史记录进行聚合了。</p>
<p>对于空气温度、行驶速度这些状态型指标，由于它们的值往往是连续的，我们无法捕获其变动的原子事务操作，所以无法使用事务型事实表统计此类需求。而只能定期对其进行采样，构建周期型快照事实表。</p>
<h4 id="3-3-2-设计流程"><a href="#3-3-2-设计流程" class="headerlink" title="3.3.2 设计流程"></a>3.3.2 设计流程</h4><p><em><strong>1）确定粒度</strong></em></p>
<p>周期型快照事实表的粒度可由采样周期和维度描述，<strong>故确定采样周期和维度后即可确定粒度</strong>。采样周期通常选择每日。</p>
<p>维度可根据统计指标决定，例如指标为统计每个用户每个科目的购物车存量，则可确定维度为用户和科目。</p>
<p>确定完采样周期和维度后，即可确定该表粒度为每日-用户-科目。</p>
<p><em><strong>2）确认事实</strong></em></p>
<p>事实也可根据统计指标决定，例如指标为统计每个用户每个科目的购物车存量，则事实为购物车存量。</p>
<h4 id="3-3-3-事实类型"><a href="#3-3-3-事实类型" class="headerlink" title="3.3.3 事实类型"></a>3.3.3 事实类型</h4><p>此处的事实类型是指<strong>度量值的类型</strong>，而非事实表的类型。事实（度量值）共分为三类，分别是<strong>可加事实</strong>，<strong>半可加事实</strong>和<strong>不可加事实。</strong></p>
<p><em><strong>1）可加事实</strong></em></p>
<p>可加事实是指可以按照与事实表相关的所有维度进行累加，例如事务型事实表中的事实。</p>
<p><em><strong>2）半可加事实</strong></em></p>
<p>半可加事实是指<strong>只能按照与事实表相关的一部分维度进行累加</strong>，例如周期型快照事实表中的事实。以上述各仓库中各用户购物车存量每天快照事实表为例，这张表中的购物车存量事实可以按照用户或者科目维度进行累加，但是不能按照时间维度进行累加，因为将每天的购物车存量累加起来是没有任何意义的。</p>
<p><em><strong>3）不可加事实</strong></em></p>
<p>不可加事实是指完全不具备可加性，例如比率型事实。不可加事实通常需要转化为可加事实，例如比率可转化为分子和分母。</p>
<h3 id="3-4-累计型快照事实表"><a href="#3-4-累计型快照事实表" class="headerlink" title="3.4 累计型快照事实表"></a>3.4 累计型快照事实表</h3><h4 id="3-4-1-概述"><a href="#3-4-1-概述" class="headerlink" title="3.4.1 概述"></a>3.4.1 概述</h4><p>累积型快照事实表是基于一个业务流程中的多个关键业务过程联合处理而构建的事实表，如交易流程中的试听、下单、支付等业务过程。</p>
<p>累积型快照事实表通常具有多个日期字段，每个日期对应业务流程中的一个关键业务过程（里程碑）。</p>
<img src="Snipaste_2023-11-15_16-11-21.png" alt="Snipaste_2023-11-15_16-11-21" style="zoom:50%;">

<p>累积型快照事实表主要用于分析业务过程（里程碑）之间的时间间隔等需求。例如前文提到的用户下单到支付的平均时间间隔，使用累积型快照事实表进行统计，就能避免两个事务事实表的关联操作，从而变得十分简单高效。</p>
<h4 id="3-4-2-设计流程"><a href="#3-4-2-设计流程" class="headerlink" title="3.4.2 设计流程"></a>3.4.2 设计流程</h4><p>累积型快照事实表的设计流程同事务型事实表类似，也可采用以下四个步骤，下面重点描述与事务型事实表的不同之处。</p>
<p><strong>选择业务过程→声明粒度→确认维度→确认事实。</strong></p>
<p><em><strong>1）选择业务过程</strong></em></p>
<p>选择一个业务流程中需要关联分析的多个关键业务过程，多个业务过程对应一张累积型快照事实表。</p>
<p><em><strong>2）声明粒度</strong></em></p>
<p>精确定义每行数据表示的是什么，尽量选择最小粒度。</p>
<p><em><strong>3）确认维度</strong></em></p>
<p>选择与各业务过程相关的维度，需要注意的是，每各业务过程均需要一个日期维度。</p>
<p><em><strong>4）确认事实</strong></em></p>
<p>选择各业务过程的度量值。</p>
<h2 id="第四章-维度建模理论之维度表"><a href="#第四章-维度建模理论之维度表" class="headerlink" title="第四章 维度建模理论之维度表"></a>第四章 维度建模理论之维度表</h2><h3 id="4-1-维度表概述"><a href="#4-1-维度表概述" class="headerlink" title="4.1 维度表概述"></a>4.1 维度表概述</h3><p>维度表是维度建模的基础和灵魂。前文提到，事实表紧紧围绕业务过程进行设计，而维度表则围绕业务过程所处的环境进行设计。维度表主要包含一个主键和各种维度字段，维度字段称为维度属性。</p>
<p>维度表通常具有以下三个特点：</p>
<ul>
<li>维度表的范围很宽（列多行少），通常具有很多属性</li>
<li>与事实表相比，行数相对较少，通常小于10万条</li>
<li>内容相对固定，不会轻易发生修改</li>
</ul>
<h3 id="4-2-维度表设计步骤"><a href="#4-2-维度表设计步骤" class="headerlink" title="4.2 维度表设计步骤"></a>4.2 维度表设计步骤</h3><p><em><strong>1）确定维度（表）</strong></em></p>
<p>在设计事实表时，已经确定了与每个事实表相关的维度，理论上每个相关维度均需对应一张维度表。需要注意到，可能存在多个事实表与同一个维度都相关的情况，这种情况<strong>需保证维度的唯一性</strong>，即只创建一张维度表。另外，如果某些维度表的维度属性很少，例如只有一个国家名称，则可不创建该维度表，而把该表的维度属性直接增加到与之相关的事实表中，这个操作称为<strong>维度退化</strong>。</p>
<p><em><strong>2）确定主维表和相关维表</strong></em></p>
<p>此处的主维表和相关维表均指<strong>业务系统</strong>中与某维度相关的表。例如业务系统中与课程相关的表有course_info，chapter_info，base_subject_info，base_category_info，video_info等，其中course_info就称为课程维度的主维表，其余表称为课程维度的相关维表。维度表的粒度通常与主维表相同。</p>
<p><em><strong>3）确定维度属性</strong></em></p>
<p>确定维度属性即确定维度表字段。维度属性主要来自于业务系统中与该维度对应的主维表和相关维表。维度属性可直接从主维表或相关维表中选择，也可通过进一步加工得到。</p>
<p>确定维度属性时，需要遵循以下要求：</p>
<p>（1）尽可能生成丰富的维度属性</p>
<p>维度属性是后续做分析统计时的查询约束条件、分组字段的基本来源，是数据易用性的关键。维度属性的丰富程度直接影响到数据模型能够支持的指标的丰富程度。</p>
<p>（2）尽量不使用编码，而使用明确的文字说明，一般可以编码和文字共存。</p>
<p>（3）尽量沉淀出通用的维度属性</p>
<p>有些维度属性的获取需要进行比较复杂的逻辑处理，例如需要通过多个字段拼接得到。为避免后续每次使用时的重复处理，可将这些维度属性沉淀到维度表中。</p>
<h3 id="4-3-维度设计要点"><a href="#4-3-维度设计要点" class="headerlink" title="4.3 维度设计要点"></a>4.3 维度设计要点</h3><h4 id="4-3-1-规范化与反规范化"><a href="#4-3-1-规范化与反规范化" class="headerlink" title="4.3.1 规范化与反规范化"></a>4.3.1 规范化与反规范化</h4><p><strong>规范化</strong>是指使用一系列范式设计数据库的过程，其目的是减少数据冗余，增强数据的一致性。通常情况下，规范化之后，一张表的字段会拆分到多张表。</p>
<p><strong>反规范化</strong>是指将多张表的数据冗余到一张表，其目的是减少join操作，提高查询性能。</p>
<p>在设计维度表时，如果对其进行规范化，得到的维度模型称为雪花模型，如果对其进行反规范化，得到的模型称为星型模型。</p>
<p><img src="Snipaste_2023-11-15_20-46-10.png" alt="Snipaste_2023-11-15_20-46-10"></p>
<p>数据仓库系统的主要目的是用于数据分析和统计，所以是否方便用户进行统计分析决定了模型的优劣。采用雪花模型，用户在统计分析的过程中需要大量的关联操作，使用复杂度高，同时查询性能很差，而采用星型模型，则方便、易用且性能好。所以出于易用性和性能的考虑，维度表一般是很不规范化的。</p>
<h4 id="4-3-2-维度变化"><a href="#4-3-2-维度变化" class="headerlink" title="4.3.2 维度变化"></a>4.3.2 维度变化</h4><p>维度属性通常不是静态的，而是会随时间变化的，数据仓库的一个重要特点就是反映历史的变化，所以如何保存维度的历史状态是维度设计的重要工作之一。保存维度数据的历史状态，通常有以下两种做法，分别是全量快照表和拉链表。</p>
<p><em><strong>1）全量快照表</strong></em></p>
<p>离线数据仓库的计算周期通常为每天一次，所以可以每天保存一份全量的维度数据。这种方式的优点和缺点都很明显。</p>
<p>优点是简单而有效，开发和维护成本低，且方便理解和使用。</p>
<p>缺点是浪费存储空间，尤其是当数据的变化比例比较低时。</p>
<p><em><strong>2）拉链表</strong></em></p>
<p>拉链表的意义就在于能够更加高效的保存维度信息的历史状态。</p>
<img src="Snipaste_2023-11-15_21-24-17.png" alt="Snipaste_2023-11-15_21-24-17" style="zoom:50%;">

<img src="Snipaste_2023-11-15_21-27-30.png" alt="Snipaste_2023-11-15_21-27-30" style="zoom:50%;">

<img src="Snipaste_2023-11-15_21-28-17.png" alt="Snipaste_2023-11-15_21-28-17" style="zoom:50%;">

<h4 id="4-3-3-多值纬度"><a href="#4-3-3-多值纬度" class="headerlink" title="4.3.3 多值纬度"></a>4.3.3 多值纬度</h4><p>如果事实表中一条记录在某个维度表中有多条记录与之对应，称为多值维度。例如，下单事实表中的一条记录为一个订单，一个订单可能包含多个课程，所以课程维度表中就可能有多条数据与之对应。</p>
<p>针对这种情况，通常采用以下两种方案解决。</p>
<p><strong>第一种：降低事实表的粒度，例如将订单事实表的粒度由一个订单降低为一个订单中的一门课程。</strong></p>
<img src="Snipaste_2023-11-15_21-47-20.png" alt="Snipaste_2023-11-15_21-47-20" style="zoom:33%;">

<p>第二种：在事实表中采用多字段保存多个维度值，每个字段保存一个维度id。这种方案只适用于多值维度个数固定的情况。</p>
<p>建议尽量采用第一种方案解决多值维度问题。</p>
<h4 id="4-3-4-多值属性"><a href="#4-3-4-多值属性" class="headerlink" title="4.3.4 多值属性"></a>4.3.4 多值属性</h4><p>维表中的某个属性同时有多个值，称之为“多值属性”，例如课程维度的课程类别，每个课程均有多个属性值。</p>
<p>针对这种情况，通常有可以采用以下两种方案。</p>
<p><strong>第一种：将多值属性放到一个字段，该字段内容为key1:value1，key2:value2或者 value1，value2的形式。</strong></p>
<img src="Snipaste_2023-11-15_21-57-07.png" alt="Snipaste_2023-11-15_21-57-07" style="zoom:33%;">

<p>第二种：将多值属性放到多个字段，每个字段对应一个属性。这种方案只适用于多值属性个数固定的情况。</p>
<h3 id="4-4-星形模型、雪花模型与星座模型"><a href="#4-4-星形模型、雪花模型与星座模型" class="headerlink" title="4.4 星形模型、雪花模型与星座模型"></a>4.4 星形模型、雪花模型与星座模型</h3><p>在维度建模的基础上，数据由分为三种：<strong>星形模型</strong>、<strong>雪花模型</strong>、<strong>星座模型</strong>，其中最常用的是星形模型</p>
<h4 id="4-4-1-星形模型"><a href="#4-4-1-星形模型" class="headerlink" title="4.4.1 星形模型"></a>4.4.1 星形模型</h4><img src="Snipaste_2023-11-16_13-16-42.png" alt="Snipaste_2023-11-16_13-16-42" style="zoom:50%;">

<p>星形模型有<strong>一张</strong>事实表，以及<strong>0个或多个维度表</strong>，事实表与维度表通过主键外键相关联，维度表之间没有关联。星形模型最简单也最常见，相对于其他模型更适合于大数据处理，而其他模型也可以通过一定的转换，变为星形模型。星形模型存在一定的数据冗余。</p>
<h4 id="4-4-2-雪花模型"><a href="#4-4-2-雪花模型" class="headerlink" title="4.4.2 雪花模型"></a>4.4.2 雪花模型</h4><img src="Snipaste_2023-11-16_13-27-12.png" alt="Snipaste_2023-11-16_13-27-12" style="zoom: 50%;">

<p>雪花模型就是有一张或多张维度表没有直接连接到事实表上，而是通过其他维度表连接到事实表上。雪花模型是对星形模型的扩展，它是对星形模型的维度表进一步层次化，原有的各维度表可能被扩展为小的事实表，形成一些局部的“层次”区域，这些被分解的表都连接到主维度表而不是事实表上。雪花模型的优点是通过最大限度地减少数据存储量，以及联合较小的维度表来改善查询性能。雪花模型去除了数据冗余，比较靠近第三范式，但是无法完全遵守，因为遵守第三范式成本太高。</p>
<h4 id="4-4-3-星座模型"><a href="#4-4-3-星座模型" class="headerlink" title="4.4.3 星座模型"></a>4.4.3 星座模型</h4><img src="Snipaste_2023-11-16_13-45-43.png" alt="Snipaste_2023-11-16_13-45-43" style="zoom: 33%;">

<p>星座模型与前两种模型的区别是事实表的数量，星座模型是基于<strong>多张</strong>事实表的，且事实表之间<strong>共享</strong>一些维度表。星座模型与前两种模型并不冲突。星座模型基本上是很多数据仓库的常态，因为很多数据仓库都有多张事实表。</p>
<p>三种模型对比来看，星形模型一般情况下效率比雪花模型高，设计和实现也比较简单。</p>
<p>数仓大多时候比较适合使用<strong>星形模型</strong>构建底层Hive数据库，星形模型对于OLAP系统是非常友好的，而雪花模型更常用于关系型数据库中。目前在企业实际开发过程中，不会只选择一种，而是根据情况灵活组合，甚至并存（一层维度和多层维度都保存）。但是从整体看，企业更倾向于维度更少的星形模型。尤其是Hadoop体系，减少join就是减少中间数据的传输和计算，性能差距较大。</p>
<h2 id="第五章-数据仓库设计"><a href="#第五章-数据仓库设计" class="headerlink" title="第五章 数据仓库设计"></a>第五章 数据仓库设计</h2><h3 id="5-1-名词解释（面试）"><a href="#5-1-名词解释（面试）" class="headerlink" title="5.1 名词解释（面试）"></a>5.1 名词解释（面试）</h3><ul>
<li>宽表：字段比较多的表（列多），通常是指业务主题相关的指标与维度、属性关联在一起的表。</li>
<li>粒度：是指数仓的数据单位中保存数据的细化或综合程度的级别。细化程度越高，粒度级就越小；相反，细化程度越低，粒度级就越大。粒度就是维度的组合。</li>
<li>维度退化：将一些常用的维度属性直接写到事实表中的维度操作称为维度退化</li>
<li>维度层次：维度中一些描述属性以及层次方式或一对多的方式相互关联，可以被理解为包含连续主从关系的属性层次。层次的最底层代表维度中描述最低级别的详细信息，最高层代表最高级别的概要信息。维度常常有多个这样的嵌入式层次结构</li>
<li>下钻：数据明细从粗粒度到细粒度的过程，会细化某些维度。下钻是商业用户分析数据的最基本方法。下钻仅需要在查询上增加一个维度属性，附加在SQL的GROUP BY语句中。属性可以来自任何与查询使用的事实表关联的维度。下钻不需要存在层次的定义或是下钻路径。</li>
<li>上卷：数据的汇总聚合，从细粒度到粗粒度的过程，会无视某些维度</li>
<li>规范化：按照三范式设计，使用事实表和维度表的方式管理数据称为规范化，规范化常用于OLTP系统的设计，雪花模型是典型应用</li>
<li>反规范化：将维度的属性合并到单个维度中的操作称为反规范化。常用于OLAP系统的设计</li>
<li>业务过程：组织完成的操作型活动，如获得订单、付款、退款等，多数事实表关注某一业务过程的结果。每个业务过程对应企业数据仓库总线矩阵的一行。</li>
<li>原子指标：基于某一业务过程的度量值，是业务定义中不可再拆解的指标，原子指标的核心功能就是对指标的聚合逻辑进行了定义，原子指标包括：业务过程、度量值、聚合逻辑</li>
<li>派生指标：基于原子指标、时间周期和维度，圈定业务统计范围并分析获取业务统计指标的数值。</li>
<li>衍生指标：是在一个或多个派生指标的基础上，通过各种逻辑运算复合而成的，如比率、比例等类型的指标。衍生指标也会对应实际的统计需求</li>
<li>数据域：数据域是联系较为紧密的数据主题的集合，通常是根据业务类型、数据来源、数据用途等多个维度，对企业的业务数据进行区域划分。将同类型数据存放在一起，便于快速查找需要的内容。不同使用目的的数据，分类标准不同。例如在线教育行业通常可以划分为交易域、流量域、用户域、学习域等等。</li>
<li>业务总线矩阵：企业数据仓库业务总线矩阵是用于设计并与企业数据仓库总线架构交互的基本工具。矩阵的行表示业务过程，列表示维度。矩阵中的点表示维度与给定的业务过程是否存在关联关系。</li>
</ul>
<h3 id="5-2-数据仓库分层规划"><a href="#5-2-数据仓库分层规划" class="headerlink" title="5.2 数据仓库分层规划"></a>5.2 数据仓库分层规划</h3><p>数仓分层的好处：</p>
<p>原始数据层（ODS）—公共维度层（DIM）—明细数据层（DWD）—汇总数据层（DWS）—数据应用层（ADS）</p>
<ul>
<li>把复杂的问题简单化，减少重复开发</li>
<li>隔离原始数据；如果不分层，原始数据业务规则发生变化将会影响后续整个数据清洗工作，牵一发而动全身</li>
<li>用空间换时间，用大量预处理来提升效率，但数仓中也会存在大量冗余数据</li>
<li>方便数据血缘追踪（相当于数据家谱，方便寻亲寻祖）</li>
</ul>
<img src="Snipaste_2023-11-16_14-36-54.png" alt="Snipaste_2023-11-16_14-36-54" style="zoom:50%;">

<h3 id="5-3-数据仓库构建流程"><a href="#5-3-数据仓库构建流程" class="headerlink" title="5.3 数据仓库构建流程"></a>5.3 数据仓库构建流程</h3><img src="Snipaste_2023-11-16_14-37-42.png" alt="Snipaste_2023-11-16_14-37-42" style="zoom:50%;">

<h4 id="5-3-1-数据调研"><a href="#5-3-1-数据调研" class="headerlink" title="5.3.1 数据调研"></a>5.3.1 数据调研</h4><p>数据调研重点要做两项工作，分别是业务调研和需求分析。这两项工作做的是否充分，直接影响着数据仓库的质量。</p>
<p><strong>1）业务调研</strong></p>
<p>业务调研的主要目标是<strong>熟悉业务流程</strong>、<strong>熟悉业务数据</strong>。</p>
<p><em><strong>熟悉业务流程</strong></em>要求做到，明确每个业务的具体流程，需要将该业务所包含的每个<em><strong>业务过程</strong></em>一一列举出来。</p>
<p><em><strong>熟悉业务数据</strong></em>要求做到，将数据（包括埋点日志和业务数据表）与业务过程对应起来，明确每个业务过程会对哪些表的数据产生影响，以及产生什么影响。产生的影响，需要具体到，是新增一条数据，还是修改一条数据，并且需要明确新增的内容或者是修改的逻辑。</p>
<p>下面以在线教育中的交易业务为例进行演示，交易业务涉及到的业务过程有用户试听、用户下单、用户支付，具体流程如下图。</p>
<img src="Snipaste_2023-11-16_15-23-37.png" alt="Snipaste_2023-11-16_15-23-37" style="zoom:50%;">

<p><strong>2）需求分析</strong></p>
<p>典型的需求指标如，最近一天各省份 Java 学科订单总额。</p>
<p>分析需求时，需要明确需求所需的<strong>业务过程</strong>及<strong>维度</strong>，例如该需求所需的业务过程就是用户下单，所需的维度有日期，省份，科目。</p>
<p><strong>3）总结</strong></p>
<p>做完业务分析和需求分析之后，要保证每个需求都能找到与之对应的业务过程及维度。若现有数据无法满足需求，则需要和业务方进行沟通，例如某个页面需要新增某个行为的埋点。</p>
<h4 id="5-3-2-明确数据域"><a href="#5-3-2-明确数据域" class="headerlink" title="5.3.2 明确数据域"></a>5.3.2 明确数据域</h4><p>数据仓库模型设计除横向的分层外，通常也需要根据业务情况进行纵向划分数据域。</p>
<p>划分数据域的意义是<strong>便于数据的管理和应用</strong>。</p>
<p>通常可以根据业务过程或者部门进行划分，本项目根据业务过程进行划分，需要注意的是一个业务过程只能属于一个数据域。</p>
<p>下面是本数仓项目所需的所有业务过程及数据域划分详情。</p>
<img src="666666_2023-11-16_15-29-11.png" alt="Snipaste_2023-11-16_15-29-11" style="zoom:50%;">

<h4 id="5-3-3-构建业务总线矩阵"><a href="#5-3-3-构建业务总线矩阵" class="headerlink" title="5.3.3 构建业务总线矩阵"></a>5.3.3 构建业务总线矩阵</h4><p>业务总线矩阵中包含维度模型所需的所有事实（业务过程）以及维度，以及各业务过程与各维度的关系。矩阵的行是一个个业务过程，矩阵的列是一个个的维度，行列的交点表示业务过程与维度的关系。</p>
<img src="772023-11-16_15-34-37.png" alt="Snipaste_2023-11-16_15-34-37" style="zoom:50%;">

<p>一个业务过程对应维度模型中一张事务型事实表，一个维度则对应维度模型中的一张维度表。所以构建业务总线矩阵的过程就是设计维度模型的过程。但是需要注意的是，总线矩阵中通常只包含事务型事实表，另外两种类型的事实表需单独设计。</p>
<p>按照事务型事实表的设计流程，选择业务过程——》声明粒度——》确认维度——》确认事实，得到的最终的业务总线矩阵见以下表格。</p>
<p><img src="Snipaste_2023-11-16_15-37-59.png" alt="Snipaste_2023-11-16_15-37-59"></p>
<p>后续的DWD层（构建事实表）以及DIM层（构建维度表）的搭建需参考业务总线矩阵。</p>
<h4 id="5-3-4-明细统计指标"><a href="#5-3-4-明细统计指标" class="headerlink" title="5.3.4 明细统计指标"></a>5.3.4 明细统计指标</h4><p>明确统计指标具体的工作是，深入分析需求，构建指标体系。构建指标体系的主要意义就是指标定义标准化。所有指标的定义，都必须遵循同一套标准，这样能有效的避免指标定义存在歧义，指标定义重复等问题。</p>
<blockquote>
<p>以下可以理解为量化交易中的原始因子、派生因子、衍生因子。</p>
</blockquote>
<p>（1）原子指标</p>
<p>原子指标基于某一<em><strong>业务过程</strong></em>的<em><strong>度量值</strong></em>，是业务定义中不可再拆解的指标，原子指标的核心功能就是对指标的<em><strong>聚合逻辑</strong></em>进行了定义。我们可以得出结论，原子指标包含三要素，分别是业务过程、度量值和聚合逻辑。</p>
<p>例如<em><strong>订单总额</strong></em>就是一个典型的原子指标，其中的业务过程为用户下单、度量值为订单金额，聚合逻辑为sum()求和。需要注意的是原子指标只是用来辅助定义指标一个概念，通常不会对应有实际统计需求与之对应。</p>
<p>（2）派生指标</p>
<p>派生指标基于原子指标，其与原子指标的关系如下图所示。</p>
<img src="Snipaste_2023-11-16_15-43-58.png" alt="Snipaste_2023-11-16_15-43-58" style="zoom:50%;">

<p>与原子指标不同，派生指标通常会对应实际的统计需求。请从图中的例子中，体会指标定义标准化的含义。</p>
<p>（3）衍生指标</p>
<p>衍生指标是在一个或多个派生指标的基础上，通过各种逻辑运算复合而成的。例如比率、比例等类型的指标。衍生指标也会对应实际的统计需求。</p>
<img src="Snipaste_2023-11-16_15-44-28.png" alt="Snipaste_2023-11-16_15-44-28" style="zoom:50%;">

<p>通过上述两个具体的案例可以看出，绝大多数的统计需求，都可以使用<strong>原子指标、派生指标以及衍生指标</strong>这套标准去定义。同时能够发现这些统计需求都直接的或间接的对应一个或者是多个派生指标。</p>
<p>当统计需求足够多时，必然会出现部分统计需求对应的派生指标相同的情况。这种情况下，我们就可以考虑将这些公共的派生指标保存下来，这样做的主要目的就是减少重复计算，提高数据的复用性。</p>
<p>这些公共的派生指标统一保存在数据仓库的DWS层（汇总数据层）。因此DWS层设计，就可以参考我们根据现有的统计需求整理出的派生指标。</p>
<h4 id="5-3-5-维度模型设计"><a href="#5-3-5-维度模型设计" class="headerlink" title="5.3.5 维度模型设计"></a>5.3.5 维度模型设计</h4><p>维度模型的设计参照上述得到的业务总线矩阵即可。事实表存储在DWD层，维度表存储在DIM层。</p>
<h4 id="5-3-6-汇总模型设计"><a href="#5-3-6-汇总模型设计" class="headerlink" title="5.3.6 汇总模型设计"></a>5.3.6 汇总模型设计</h4><p>汇总模型的设计参考上述整理出的指标体系（主要是派生指标）即可。<strong>汇总表与派生指标的对应关系是，一张汇总表通常包含业务过程相同、统计周期相同、统计粒度相同的多个派生指标。</strong></p>
<p>汇总表与事实表的对应关系：<strong>一张事实表可能会产生多张汇总表，但是一张汇总表只能来源于一张事实表</strong></p>
<h2 id="第六章-数据仓库环境准备"><a href="#第六章-数据仓库环境准备" class="headerlink" title="第六章 数据仓库环境准备"></a>第六章 数据仓库环境准备</h2><h3 id="6-1-数据仓库运行环境"><a href="#6-1-数据仓库运行环境" class="headerlink" title="6.1 数据仓库运行环境"></a>6.1 数据仓库运行环境</h3><h4 id="6-1-1-Hive环境搭建"><a href="#6-1-1-Hive环境搭建" class="headerlink" title="6.1.1 Hive环境搭建"></a>6.1.1 Hive环境搭建</h4><p>Hive引擎简介：</p>
<p>Hive引擎包括：默认MR、tez、spark</p>
<p>Hive on Spark：Hive既作为存储元数据又负责SQL的解析优化，语法是HQL语法，执行引擎变成了Spark，Spark负责采用RDD执行。</p>
<p>Spark on Hive : Hive只作为存储元数据，Spark负责SQL解析优化，语法是Spark SQL语法，Spark负责采用RDD执行。</p>
<table>
<thead>
<tr>
<th></th>
<th>存储元数据</th>
<th>SQL解析优化</th>
<th>SQL语法</th>
<th>计算引擎</th>
</tr>
</thead>
<tbody><tr>
<td>Hive on Spark</td>
<td>Hive</td>
<td>Hive</td>
<td>HQL</td>
<td>Spark</td>
</tr>
<tr>
<td>Spark on Hive</td>
<td>Hive</td>
<td>Spark</td>
<td>Spark SQL</td>
<td>Spark</td>
</tr>
</tbody></table>
<p>（1）将编译过的Hive安装包上传到&#x2F;opt&#x2F;software目录，解压到&#x2F;opt&#x2F;module目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>（2）修改名字为hive-3.1.2</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# mv apache-hive-3.1.2-bin/ hive-3.1.2</span><br></pre></td></tr></table></figure>

<p>（3）添加环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HIVE_HOME</span></span><br><span class="line">export HIVE_HOME=/opt/module/hive-3.1.2</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure>

<p>source一下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# source /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>（4）解决日志Jar包冲突，进入</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# cd /opt/module/hive-3.1.2/lib</span><br><span class="line">[root@hadoop102 lib]# mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak</span><br></pre></td></tr></table></figure>

<p>（5）拷贝驱动到&#x2F;opt&#x2F;module&#x2F;hive-3.1.2&#x2F;lib目录下，用于稍后启动Hive时连接Mysql</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 lib]# cp /opt/software/mysql-connector-java-5.1.37.jar /opt/module/hive-3.1.2/lib/</span><br></pre></td></tr></table></figure>

<p>（6）配置Metastore到MySQL</p>
<p>在&#x2F;opt&#x2F;module&#x2F;hive-3.1.2&#x2F;conf目录下创建一个hive-site.xml文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 conf]# vim hive-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>wyhdhr19980418<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive-3.1.2/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 配置命令行通过客户端直连 hive 时展示查询表头 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置命令行通过客户端直连 hive 时可以展示当前数据库 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hive 的bug，如果没有配置 HA 则 hiveserver2 启动时会找 Tez，做无用功，启动很慢，且出现四个 session_id 才可以通过 jdbc 的方式连接 hive，此处配置 HA 之后启动耗时缩短且只要出现两个 session_id 即可连接 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.active.passive.ha.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 在 DataGrip 中可以正确加载序列化和反序列化器 SerDe 为</span></span><br><span class="line"><span class="comment">          &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27; </span></span><br><span class="line"><span class="comment">          的表的元数据信息，点击表名左侧的三角可以查看表的字段 --&gt;</span>    </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>metastore.storage.schema.reader.impl<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 关闭 MapJoin 优化，hive 的 bug，MapJoin 有时会导致 SQL 执行失败，这里不建议关闭，因为 MapJoin 是一种优化手段，永久关闭影响性能</span></span><br><span class="line"><span class="comment">应在执行 SQL 报错时通过 set hive.auto.convert.join=false 临时关闭 MapJoin 功能 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">    &lt;property&gt;</span></span><br><span class="line"><span class="comment">        &lt;name&gt;hive.auto.convert.join&lt;/name&gt;</span></span><br><span class="line"><span class="comment">        &lt;value&gt;false&lt;/value&gt;</span></span><br><span class="line"><span class="comment">&lt;/property&gt;</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（7）Hive on Spark配置</p>
<p>解压spark3.0.0安装包到&#x2F;opt&#x2F;module，配置SPARK_HOME环境变量并source</p>
<p>（8）在Hive中创建spark配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# vim /opt/module/hive-3.1.2/conf/spark-defaults.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.master                               yarn</span><br><span class="line">spark.eventLog.enabled                   true</span><br><span class="line">spark.eventLog.dir                        hdfs://hadoop102:8020/spark-history</span><br><span class="line">spark.executor.memory                    1g</span><br><span class="line">spark.driver.memory					   1g</span><br></pre></td></tr></table></figure>

<p>（9）在HDFS创建如下路径，用于存储历史日志</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# hadoop fs -mkdir /spark-history</span><br></pre></td></tr></table></figure>

<p>（10）上传并解压spark3.0.0纯净版于&#x2F;opt&#x2F;software，上传Spark纯净版jar包到HDFS</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# hadoop fs -mkdir /spark-jars</span><br><span class="line">[root@hadoop102 software]# hadoop fs -put spark-3.0.0-bin-without-hadoop/jars/* /spark-jars</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-11-16_20-26-47.png" alt="Snipaste_2023-11-16_20-26-47"></p>
<p>（11）修改hive-site.xml文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# vim /opt/module/hive-3.1.2/conf/hive-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>spark.yarn.jars<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:8020/spark-jars/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">&lt;!--Hive执行引擎--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--Hive执行引擎--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.spark.client.connect.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000ms<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（12）Hive on Spark测试（略）</p>
<p>按照书中来说，本项目采用Hive on Spark，但是Hive3.1.2和Spark3.0.0不兼容，除非删除之前安装的Hive，没必要，学习过程中，我们用本地模式速度一样快。</p>
<h4 id="6-1-2-Yarn-环境配置"><a href="#6-1-2-Yarn-环境配置" class="headerlink" title="6.1.2 Yarn 环境配置"></a>6.1.2 Yarn 环境配置</h4><p>（1）在hadoop102的&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;etc&#x2F;hadoop&#x2F;capacity-scheduler.xml文件中<strong>修改</strong>如下参数值</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# cd /opt/module/hadoop-3.1.3/etc/hadoop/</span><br><span class="line">[root@hadoop102 hadoop]# vim capacity-scheduler.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.maximum-am-resource-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）分发capacity-scheduler.xml配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# xsync capacity-scheduler.xml</span><br></pre></td></tr></table></figure>

<p>（3）关闭正在运行的任务，重新启动yarn集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 hadoop]# myhadoop.sh stop</span><br><span class="line">[root@hadoop102 hadoop]# myhadoop.sh start</span><br></pre></td></tr></table></figure>

<h3 id="6-2-数据仓库开发环境"><a href="#6-2-数据仓库开发环境" class="headerlink" title="6.2 数据仓库开发环境"></a>6.2 数据仓库开发环境</h3><p>略</p>
<h3 id="6-3-模拟数据准备"><a href="#6-3-模拟数据准备" class="headerlink" title="6.3 模拟数据准备"></a>6.3 模拟数据准备</h3><p>见2.7数据采集流程总结，有数据产生就行，不用纠结是不是和视频或者书中一模一样，没有意义，主要是学习理论和建模方法，数据分析方法等等。</p>
<p>用户行为数据（只保留2022-02-21即可）</p>
<img src="Snipaste_2023-11-16_21-29-16.png" alt="Snipaste_2023-11-16_21-29-16" style="zoom:50%;">

<p>业务数据</p>
<img src="Snipaste_2023-11-16_21-40-52.png" alt="Snipaste_2023-11-16_21-40-52" style="zoom:50%;">

<p>全量业务数据：</p>
<img src="Snipaste_2023-11-16_21-41-30.png" alt="Snipaste_2023-11-16_21-41-30" style="zoom:50%;">

<p>增量业务数据：</p>
<img src="Snipaste_2023-11-16_21-42-19.png" alt="Snipaste_2023-11-16_21-42-19" style="zoom:50%;">

<h2 id="第七章-数仓开发之ODS层"><a href="#第七章-数仓开发之ODS层" class="headerlink" title="第七章 数仓开发之ODS层"></a>第七章 数仓开发之ODS层</h2><p>ODS层为原始数据层，设计原则如下：</p>
<ul>
<li>要求保持数据<strong>原貌</strong>，不进行任何修改，表结构设计依托业务系统同步过来的数据结构，起到<strong>备份数据</strong>的作用</li>
<li>数据适当采用<strong>压缩格式</strong>，以减少磁盘存储空间。要保存全部历史数据，故其压缩格式应选择压缩比较高的，此处选择gzip（也是默认压缩格式）</li>
<li>创建分区表，可以避免后续对表进行查询时全表扫描</li>
<li>创建外部表，在企业开发中，除了自己用的临时表、创建内部表，绝大多数场景都是创建外部表</li>
<li>ODS层表名的命名规范为：ods_表名_单分区增量全量标识（inc&#x2F;full）</li>
</ul>
<h3 id="7-1-处理json文件的方法"><a href="#7-1-处理json文件的方法" class="headerlink" title="7.1 处理json文件的方法"></a>7.1 处理json文件的方法</h3><h4 id="7-1-1-使用get-json-object处理json格式数据"><a href="#7-1-1-使用get-json-object处理json格式数据" class="headerlink" title="7.1.1 使用get_json_object处理json格式数据"></a>7.1.1 使用get_json_object处理json格式数据</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 maxwell]# hadoop fs -text /origin_data/edu/log/edu_log/2022-02-21/log.1700141703015.gz</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">。。。</span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;common&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="attr">&quot;ar&quot;</span><span class="punctuation">:</span><span class="string">&quot;31&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ba&quot;</span><span class="punctuation">:</span><span class="string">&quot;Huawei&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ch&quot;</span><span class="punctuation">:</span><span class="string">&quot;xiaomi&quot;</span><span class="punctuation">,</span><span class="attr">&quot;is_new&quot;</span><span class="punctuation">:</span><span class="string">&quot;0&quot;</span><span class="punctuation">,</span><span class="attr">&quot;md&quot;</span><span class="punctuation">:</span><span class="string">&quot;Huawei P30&quot;</span><span class="punctuation">,</span><span class="attr">&quot;mid&quot;</span><span class="punctuation">:</span><span class="string">&quot;mid_206&quot;</span><span class="punctuation">,</span><span class="attr">&quot;os&quot;</span><span class="punctuation">:</span><span class="string">&quot;Android 11.0&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sc&quot;</span><span class="punctuation">:</span><span class="string">&quot;1&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sid&quot;</span><span class="punctuation">:</span><span class="string">&quot;d51883ff-6b15-48ad-8957-db8d8c02ce52&quot;</span><span class="punctuation">,</span><span class="attr">&quot;uid&quot;</span><span class="punctuation">:</span><span class="string">&quot;166&quot;</span><span class="punctuation">,</span><span class="attr">&quot;vc&quot;</span><span class="punctuation">:</span><span class="string">&quot;v2.0.1&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="attr">&quot;displays&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;8&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;5&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">4</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;recommend&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;9&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;recommend&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;5&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">4</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">5</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;5&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">5</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">4</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;7&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">6</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;recommend&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;2&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">7</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">5</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;9&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">8</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">4</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;4&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">9</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span><span class="attr">&quot;page&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="attr">&quot;during_time&quot;</span><span class="punctuation">:</span><span class="number">18534</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;68169&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;order_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;last_page_id&quot;</span><span class="punctuation">:</span><span class="string">&quot;cart&quot;</span><span class="punctuation">,</span><span class="attr">&quot;page_id&quot;</span><span class="punctuation">:</span><span class="string">&quot;order&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span><span class="number">1645444134638</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;common&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="attr">&quot;ar&quot;</span><span class="punctuation">:</span><span class="string">&quot;31&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ba&quot;</span><span class="punctuation">:</span><span class="string">&quot;Huawei&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ch&quot;</span><span class="punctuation">:</span><span class="string">&quot;xiaomi&quot;</span><span class="punctuation">,</span><span class="attr">&quot;is_new&quot;</span><span class="punctuation">:</span><span class="string">&quot;0&quot;</span><span class="punctuation">,</span><span class="attr">&quot;md&quot;</span><span class="punctuation">:</span><span class="string">&quot;Huawei P30&quot;</span><span class="punctuation">,</span><span class="attr">&quot;mid&quot;</span><span class="punctuation">:</span><span class="string">&quot;mid_206&quot;</span><span class="punctuation">,</span><span class="attr">&quot;os&quot;</span><span class="punctuation">:</span><span class="string">&quot;Android 11.0&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sc&quot;</span><span class="punctuation">:</span><span class="string">&quot;1&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sid&quot;</span><span class="punctuation">:</span><span class="string">&quot;d51883ff-6b15-48ad-8957-db8d8c02ce52&quot;</span><span class="punctuation">,</span><span class="attr">&quot;uid&quot;</span><span class="punctuation">:</span><span class="string">&quot;166&quot;</span><span class="punctuation">,</span><span class="attr">&quot;vc&quot;</span><span class="punctuation">:</span><span class="string">&quot;v2.0.1&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="attr">&quot;displays&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;promotion&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;1&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;promotion&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;3&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;promotion&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;6&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">4</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;9&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">4</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;3&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">5</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;5&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">6</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;10&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">7</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">4</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span><span class="attr">&quot;page&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="attr">&quot;during_time&quot;</span><span class="punctuation">:</span><span class="number">6662</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;68169&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;order_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;last_page_id&quot;</span><span class="punctuation">:</span><span class="string">&quot;order&quot;</span><span class="punctuation">,</span><span class="attr">&quot;page_id&quot;</span><span class="punctuation">:</span><span class="string">&quot;payment&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span><span class="number">1645444141300</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>上传要测试的json文件到HDFS上</p>
<img src="Snipaste_2023-11-17_15-47-16.png" alt="Snipaste_2023-11-17_15-47-16" style="zoom:50%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create external table json_text(</span><br><span class="line">    json_text string</span><br><span class="line">)location &quot;/jsontext&quot;;</span><br><span class="line"></span><br><span class="line">select * from json_text;</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;common&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="attr">&quot;ar&quot;</span><span class="punctuation">:</span><span class="string">&quot;31&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ba&quot;</span><span class="punctuation">:</span><span class="string">&quot;Huawei&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ch&quot;</span><span class="punctuation">:</span><span class="string">&quot;xiaomi&quot;</span><span class="punctuation">,</span><span class="attr">&quot;is_new&quot;</span><span class="punctuation">:</span><span class="string">&quot;0&quot;</span><span class="punctuation">,</span><span class="attr">&quot;md&quot;</span><span class="punctuation">:</span><span class="string">&quot;Huawei P30&quot;</span><span class="punctuation">,</span><span class="attr">&quot;mid&quot;</span><span class="punctuation">:</span><span class="string">&quot;mid_206&quot;</span><span class="punctuation">,</span><span class="attr">&quot;os&quot;</span><span class="punctuation">:</span><span class="string">&quot;Android 11.0&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sc&quot;</span><span class="punctuation">:</span><span class="string">&quot;1&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sid&quot;</span><span class="punctuation">:</span><span class="string">&quot;d51883ff-6b15-48ad-8957-db8d8c02ce52&quot;</span><span class="punctuation">,</span><span class="attr">&quot;uid&quot;</span><span class="punctuation">:</span><span class="string">&quot;166&quot;</span><span class="punctuation">,</span><span class="attr">&quot;vc&quot;</span><span class="punctuation">:</span><span class="string">&quot;v2.0.1&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="attr">&quot;displays&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;promotion&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;1&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;promotion&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;3&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;promotion&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;6&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">4</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;9&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">4</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;3&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">5</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;5&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">6</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;display_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;query&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;10&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;course_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;order&quot;</span><span class="punctuation">:</span><span class="number">7</span><span class="punctuation">,</span><span class="attr">&quot;pos_id&quot;</span><span class="punctuation">:</span><span class="number">4</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span><span class="attr">&quot;page&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="attr">&quot;during_time&quot;</span><span class="punctuation">:</span><span class="number">6662</span><span class="punctuation">,</span><span class="attr">&quot;item&quot;</span><span class="punctuation">:</span><span class="string">&quot;68169&quot;</span><span class="punctuation">,</span><span class="attr">&quot;item_type&quot;</span><span class="punctuation">:</span><span class="string">&quot;order_id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;last_page_id&quot;</span><span class="punctuation">:</span><span class="string">&quot;order&quot;</span><span class="punctuation">,</span><span class="attr">&quot;page_id&quot;</span><span class="punctuation">:</span><span class="string">&quot;payment&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="attr">&quot;ts&quot;</span><span class="punctuation">:</span><span class="number">1645444141300</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select get_json_object(json_text, &quot;$.common&quot;)</span><br><span class="line">from json_text;</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;ar&quot;</span><span class="punctuation">:</span><span class="string">&quot;31&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ba&quot;</span><span class="punctuation">:</span><span class="string">&quot;Huawei&quot;</span><span class="punctuation">,</span><span class="attr">&quot;ch&quot;</span><span class="punctuation">:</span><span class="string">&quot;xiaomi&quot;</span><span class="punctuation">,</span><span class="attr">&quot;is_new&quot;</span><span class="punctuation">:</span><span class="string">&quot;0&quot;</span><span class="punctuation">,</span><span class="attr">&quot;md&quot;</span><span class="punctuation">:</span><span class="string">&quot;Huawei P30&quot;</span><span class="punctuation">,</span><span class="attr">&quot;mid&quot;</span><span class="punctuation">:</span><span class="string">&quot;mid_206&quot;</span><span class="punctuation">,</span><span class="attr">&quot;os&quot;</span><span class="punctuation">:</span><span class="string">&quot;Android 11.0&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sc&quot;</span><span class="punctuation">:</span><span class="string">&quot;1&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sid&quot;</span><span class="punctuation">:</span><span class="string">&quot;d51883ff-6b15-48ad-8957-db8d8c02ce52&quot;</span><span class="punctuation">,</span><span class="attr">&quot;uid&quot;</span><span class="punctuation">:</span><span class="string">&quot;166&quot;</span><span class="punctuation">,</span><span class="attr">&quot;vc&quot;</span><span class="punctuation">:</span><span class="string">&quot;v2.0.1&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select get_json_object(json_text, &quot;$.common.sid&quot;)</span><br><span class="line">from json_text;</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d51883ff<span class="number">-6</span>b15<span class="number">-48</span>ad<span class="number">-8957</span>-db8d8c02ce52</span><br></pre></td></tr></table></figure>

<h4 id="7-1-2-使用serde处理json数据"><a href="#7-1-2-使用serde处理json数据" class="headerlink" title="7.1.2 使用serde处理json数据"></a>7.1.2 使用serde处理json数据</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create external table json_text1(</span><br><span class="line">    common struct&lt;ar: string, ba: string, ch: string, is_new: string, md: string, mid: string,</span><br><span class="line">    os: string, sc: string, sid: string, uid: string, vc: string&gt;,</span><br><span class="line">    displays array&lt;struct&lt;display_type: string, item: string, item_type: string, `order`: string, pos_id: string&gt;&gt;,</span><br><span class="line">    page struct&lt;during_time: string, item: string, item_type: string, last_page_id: string, page_id: string&gt;,</span><br><span class="line">    ts bigint</span><br><span class="line">)</span><br><span class="line">row format serde &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">stored as textfile</span><br><span class="line">location &quot;/jsontext&quot;;</span><br><span class="line"></span><br><span class="line">select * from json_text1;</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-11-17_16-37-38.png" alt="Snipaste_2023-11-17_16-37-38"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select common.ar,</span><br><span class="line">       displays.item,</span><br><span class="line">       page.during_time,</span><br><span class="line">       ts</span><br><span class="line">from json_text1;</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-11-17_16-47-38.png" alt="Snipaste_2023-11-17_16-47-38"></p>
<h3 id="7-2-日志表"><a href="#7-2-日志表" class="headerlink" title="7.2 日志表"></a>7.2 日志表</h3><p>（1）建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ods_log_inc;</span><br><span class="line">create external table ods_log_inc</span><br><span class="line">(</span><br><span class="line">    `common`   STRUCT&lt;ar :STRING,sid :STRING,ba :STRING,ch :STRING,is_new :STRING,md :STRING,mid :STRING,os :STRING,uid :STRING,vc :STRING,sc :STRING&gt; COMMENT &#x27;公共信息&#x27;,</span><br><span class="line">    `page`     STRUCT&lt;during_time :STRING,item :STRING,item_type :STRING,last_page_id :STRING,page_id :STRING,source_type :STRING&gt; COMMENT &#x27;页面信息&#x27;,</span><br><span class="line">    `actions`  ARRAY&lt;STRUCT&lt;action_id:STRING,item:STRING,item_type:STRING,ts:BIGINT&gt;&gt; COMMENT &#x27;动作信息&#x27;,</span><br><span class="line">    `displays` ARRAY&lt;STRUCT&lt;display_type :STRING,item :STRING,item_type :STRING,`order` :STRING,pos_id :STRING&gt;&gt; COMMENT &#x27;曝光信息&#x27;,</span><br><span class="line">    `start`    STRUCT&lt;entry :STRING,loading_time :BIGINT,open_ad_id :BIGINT,open_ad_ms :BIGINT,open_ad_skip_ms :BIGINT,first_open :STRING&gt; COMMENT &#x27;启动信息&#x27;,</span><br><span class="line">    `err`     STRUCT&lt;error_code:BIGINT,msg:STRING&gt; COMMENT &#x27;错误信息&#x27;,</span><br><span class="line">    `appVideo`     STRUCT&lt;video_id:STRING, position_sec:BIGINT, play_sec: BIGINT&gt; COMMENT &#x27;视频播放信息&#x27;,</span><br><span class="line">    `ts`      BIGINT COMMENT &#x27;时间戳&#x27;</span><br><span class="line">) COMMENT &#x27;日志增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_log_inc/&#x27;;--如果装载数据后数据落盘至HDFS的目录</span><br></pre></td></tr></table></figure>

<p>（2）数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--数据装载</span><br><span class="line">load data inpath &#x27;/origin_data/edu/log/edu_log/2022-02-21&#x27;</span><br><span class="line">    into table edu.ods_log_inc partition(dt=&#x27;2022-02-21&#x27;);</span><br></pre></td></tr></table></figure>

<p>此时位于HDFS上&#x2F;origin_data&#x2F;edu&#x2F;log&#x2F;edu_log&#x2F;2022-02-21的数据被剪切到了HDFS的&#x2F;warehouse&#x2F;edu&#x2F;ods&#x2F;ods_log_inc&#x2F;路径上。</p>
<img src="Snipaste_2023-11-17_17-46-30.png" alt="Snipaste_2023-11-17_17-46-30" style="zoom:50%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select *</span><br><span class="line">from ods_log_inc</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-11-17_17-50-34.png" alt="Snipaste_2023-11-17_17-50-34"></p>
<p>对于用户行为日志，有时需要分类查询，比如page不为空或者其余字段不为空显示，此时需要设置Hive cbo参数为false</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable = false;</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from ods_log_inc</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">and page is not null ;</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-11-17_20-58-02.png" alt="Snipaste_2023-11-17_20-58-02"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">--查看启动日志</span><br><span class="line">select *</span><br><span class="line">from ods_log_inc</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">and `start` is not null ;</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-11-17_20-59-34.png" alt="Snipaste_2023-11-17_20-59-34"></p>
<p>（3）编写每日数据装载脚本</p>
<p>在hadoop102节点服务器的~&#x2F;bin目录下创建脚本hdfs_to_ods_log.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim hdfs_to_ods_log.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义变量方便修改</span></span><br><span class="line">APP=&quot;edu&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span></span><br><span class="line">if [ -n &quot;$1&quot; ] ;then</span><br><span class="line">   do_date=$1</span><br><span class="line">else</span><br><span class="line">   do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo ================== 日志日期为 $do_date ==================</span><br><span class="line">sql=&quot;</span><br><span class="line">load data inpath &#x27;/origin_data/edu/log/edu_log/$do_date&#x27; into table $&#123;APP&#125;.ods_log_inc partition(dt=&#x27;$do_date&#x27;);</span><br><span class="line">&quot;</span><br><span class="line">hive -e &quot;$sql&quot;</span><br></pre></td></tr></table></figure>

<p>增加执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 hdfs_to_ods_log.sh</span><br></pre></td></tr></table></figure>

<h3 id="7-3-业务表"><a href="#7-3-业务表" class="headerlink" title="7.3 业务表"></a>7.3 业务表</h3><p>业务表分为两种：全量表（full）和增量表（inc）。其中全量表使用的是DataX进行同步的，DataX同步的数据字段间通过“\t”进行分割，所以在创建这一类表格的ODS层表结构时，直接对应业务数据表的原结构创建字段，然后用“\t”进行分隔即可。</p>
<p>增量表使用的是Maxwell，Maxwell通过监控MySQL的binlog变化来获取到变动数据，最终落盘至HDFS的变动数据是JSON数据结构的，所以此处还是使用JsonSerDe对Json格式的变动数据进行处理。Maxwell采集到的数据结构，真正的数据包含在以“data”为键的对象中。</p>
<h4 id="1-分类信息全量表"><a href="#1-分类信息全量表" class="headerlink" title="1. 分类信息全量表"></a>1. 分类信息全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_base_category_info_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_base_category_info_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `category_name` STRING COMMENT &#x27;分类名称&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;更新时间&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;</span><br><span class="line">) COMMENT &#x27;分类信息全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_base_category_info_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="2-来源信息全量表"><a href="#2-来源信息全量表" class="headerlink" title="2. 来源信息全量表"></a>2. 来源信息全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_base_source_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_base_source_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `source_site` STRING COMMENT &#x27;来源&#x27;,</span><br><span class="line">    `source_url` STRING COMMENT &#x27;来源网址&#x27;</span><br><span class="line">) COMMENT &#x27;来源信息全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_base_source_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="3-省份全量表"><a href="#3-省份全量表" class="headerlink" title="3. 省份全量表"></a>3. 省份全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_base_province_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_base_province_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `name` STRING COMMENT &#x27;省名称&#x27;,</span><br><span class="line">    `region_id` STRING COMMENT &#x27;地区id&#x27;,</span><br><span class="line">    `area_code` STRING COMMENT &#x27;行政区位码&#x27;,</span><br><span class="line">    `iso_code` STRING COMMENT &#x27;国际编码&#x27;,</span><br><span class="line">    `iso_3166_2` STRING COMMENT &#x27;ISO3166编码&#x27;</span><br><span class="line">) COMMENT &#x27;省份全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_base_province_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="4-科目信息全量表"><a href="#4-科目信息全量表" class="headerlink" title="4. 科目信息全量表"></a>4. 科目信息全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_base_subject_info_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_base_subject_info_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `subject_name` STRING COMMENT &#x27;科目名称&#x27;,</span><br><span class="line">    `category_id` STRING COMMENT &#x27;分类id&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;更新时间&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;</span><br><span class="line">) COMMENT &#x27;科目信息全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_base_subject_info_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="5-购物车全量表"><a href="#5-购物车全量表" class="headerlink" title="5. 购物车全量表"></a>5. 购物车全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_cart_info_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_cart_info_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `user_id` STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `course_id` STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `course_name` STRING COMMENT &#x27;课程名称 (冗余)&#x27;,</span><br><span class="line">    `cart_price` DEC(16, 2) COMMENT &#x27;放入购物车时价格&#x27;,</span><br><span class="line">    `img_url` STRING COMMENT &#x27;图片文件&#x27;,</span><br><span class="line">    `session_id` STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;修改时间&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否已删&#x27;,</span><br><span class="line">    `sold` STRING COMMENT &#x27;是否已售&#x27;</span><br><span class="line">) COMMENT &#x27;购物车全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_cart_info_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="6-章节信息全量表"><a href="#6-章节信息全量表" class="headerlink" title="6. 章节信息全量表"></a>6. 章节信息全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_chapter_info_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_chapter_info_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `chapter_name` STRING COMMENT &#x27;章节名称&#x27;,</span><br><span class="line">    `course_id` STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `video_id` STRING COMMENT &#x27;视频id&#x27;,</span><br><span class="line">    `publisher_id` STRING COMMENT &#x27;发布者id&#x27;,</span><br><span class="line">    `is_free` STRING COMMENT &#x27;是否免费&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;更新时间&#x27;</span><br><span class="line">) COMMENT &#x27;章节信息全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_chapter_info_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="7-课程信息全量表"><a href="#7-课程信息全量表" class="headerlink" title="7. 课程信息全量表"></a>7. 课程信息全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_course_info_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_course_info_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `course_name` STRING COMMENT &#x27;课程名称&#x27;,</span><br><span class="line">    `course_slogan` STRING COMMENT &#x27;课程标语&#x27;,</span><br><span class="line">    `course_cover_url` STRING COMMENT &#x27;课程封面&#x27;,</span><br><span class="line">    `subject_id` STRING COMMENT &#x27;学科id&#x27;,</span><br><span class="line">    `teacher` STRING COMMENT &#x27;讲师名称&#x27;,</span><br><span class="line">    `publisher_id` STRING COMMENT &#x27;发布者id&#x27;,</span><br><span class="line">    `chapter_num` BIGINT COMMENT &#x27;章节数&#x27;,</span><br><span class="line">    `origin_price` DECIMAL(16, 2) COMMENT &#x27;价格&#x27;,</span><br><span class="line">    `reduce_amount` DECIMAL(16, 2) COMMENT &#x27;优惠金额&#x27;,</span><br><span class="line">    `actual_price` DECIMAL(16, 2) COMMENT &#x27;实际价格&#x27;,</span><br><span class="line">    `course_introduce` STRING COMMENT &#x27;课程介绍&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;更新时间&#x27;</span><br><span class="line">) COMMENT &#x27;课程信息全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_course_info_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="8-知识点信息全量表"><a href="#8-知识点信息全量表" class="headerlink" title="8. 知识点信息全量表"></a>8. 知识点信息全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_knowledge_point_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_knowledge_point_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `point_txt` STRING COMMENT &#x27;知识点内容  &#x27;,</span><br><span class="line">    `point_level` STRING COMMENT &#x27;知识点级别&#x27;,</span><br><span class="line">    `course_id` STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `chapter_id` STRING COMMENT &#x27;章节id&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;修改时间&#x27;,</span><br><span class="line">    `publisher_id` STRING COMMENT &#x27;发布者id&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;</span><br><span class="line">) COMMENT &#x27;知识点信息全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_knowledge_point_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="9-试卷全量表"><a href="#9-试卷全量表" class="headerlink" title="9. 试卷全量表"></a>9. 试卷全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_test_paper_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_test_paper_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `paper_title` STRING COMMENT &#x27;试卷名称&#x27;,</span><br><span class="line">    `course_id` STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;更新时间&#x27;,</span><br><span class="line">    `publisher_id` STRING COMMENT &#x27;发布者id&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;</span><br><span class="line">) COMMENT &#x27;试卷全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_test_paper_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="10-试卷题目全量表"><a href="#10-试卷题目全量表" class="headerlink" title="10. 试卷题目全量表"></a>10. 试卷题目全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_test_paper_question_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_test_paper_question_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `paper_id` STRING COMMENT &#x27;试卷id&#x27;,</span><br><span class="line">    `question_id` STRING COMMENT &#x27;题目id&#x27;,</span><br><span class="line">    `score` DECIMAL(16, 2) COMMENT &#x27;得分&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;,</span><br><span class="line">    `publisher_id` STRING COMMENT &#x27;发布者id&#x27;</span><br><span class="line">) COMMENT &#x27;试卷题目全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_test_paper_question_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="11-知识点题目全量表"><a href="#11-知识点题目全量表" class="headerlink" title="11. 知识点题目全量表"></a>11. 知识点题目全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_test_point_question_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_test_point_question_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `point_id` STRING COMMENT &#x27;知识点id&#x27;,</span><br><span class="line">    `question_id` STRING COMMENT &#x27;题目id&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `publisher_id` STRING COMMENT &#x27;发布者id&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;</span><br><span class="line">) COMMENT &#x27;知识点题目全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_test_point_question_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="12-题目信息全量表"><a href="#12-题目信息全量表" class="headerlink" title="12. 题目信息全量表"></a>12. 题目信息全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_test_question_info_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_test_question_info_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `question_txt` STRING COMMENT &#x27;题目内容&#x27;,</span><br><span class="line">    `chapter_id` STRING COMMENT &#x27;章节id&#x27;,</span><br><span class="line">    `course_id` STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `question_type` STRING COMMENT &#x27;题目类型&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;更新时间&#x27;,</span><br><span class="line">    `publisher_id` STRING COMMENT &#x27;发布者id&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;</span><br><span class="line">) COMMENT &#x27;题目信息全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_test_question_info_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="13-用户章节进度全量表"><a href="#13-用户章节进度全量表" class="headerlink" title="13. 用户章节进度全量表"></a>13. 用户章节进度全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_user_chapter_process_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_user_chapter_process_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `course_id` STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `chapter_id` STRING COMMENT &#x27;章节id&#x27;,</span><br><span class="line">    `user_id` STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `position_sec` BIGINT COMMENT &#x27;时长位置&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;更新时间&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;</span><br><span class="line">) COMMENT &#x27;用户章节进度全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_user_chapter_process_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="14-题目选项全量表"><a href="#14-题目选项全量表" class="headerlink" title="14. 题目选项全量表"></a>14. 题目选项全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_test_question_option_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_test_question_option_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `option_txt` STRING COMMENT &#x27;选项内容&#x27;,</span><br><span class="line">    `question_id` STRING COMMENT &#x27;题目id&#x27;,</span><br><span class="line">    `is_correct` STRING COMMENT &#x27;是否正确&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;更新时间&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;</span><br><span class="line">) COMMENT &#x27;题目选项全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_test_question_option_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="15-视频信息全量表"><a href="#15-视频信息全量表" class="headerlink" title="15. 视频信息全量表"></a>15. 视频信息全量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_video_info_full`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_video_info_full`</span><br><span class="line">(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `video_name` STRING COMMENT &#x27;视频名称&#x27;,</span><br><span class="line">    `during_sec` BIGINT COMMENT &#x27;时长&#x27;,</span><br><span class="line">    `video_status` STRING COMMENT &#x27;状态 未上传，上传中，上传完&#x27;,</span><br><span class="line">    `video_size` BIGINT COMMENT &#x27;大小&#x27;,</span><br><span class="line">    `video_url` STRING COMMENT &#x27;视频存储路径&#x27;,</span><br><span class="line">    `video_source_id` STRING COMMENT &#x27;云端资源编号&#x27;,</span><br><span class="line">    `version_id` STRING COMMENT &#x27;版本号&#x27;,</span><br><span class="line">    `chapter_id` STRING COMMENT &#x27;章节id&#x27;,</span><br><span class="line">    `course_id` STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `publisher_id` STRING COMMENT &#x27;发布者id&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;更新时间&#x27;,</span><br><span class="line">    `deleted` STRING COMMENT &#x27;是否删除&#x27;</span><br><span class="line">) COMMENT &#x27;视频信息全量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">        NULL DEFINED AS &#x27;&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_video_info_full/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="16-购物车增量表"><a href="#16-购物车增量表" class="headerlink" title="16. 购物车增量表"></a>16. 购物车增量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ods_cart_info_inc;</span><br><span class="line">CREATE EXTERNAL TABLE ods_cart_info_inc</span><br><span class="line">(</span><br><span class="line">    `type` STRING COMMENT &#x27;变动类型&#x27;,</span><br><span class="line">    `ts` STRING COMMENT &#x27;变动时间&#x27;,</span><br><span class="line">    `data` STRUCT&lt;id : STRING, user_id : STRING, course_id : STRING, course_name : DEC(16, 2), cart_price : DECIMAL(16, 2), img_url : STRING, session_id : STRING, create_time : STRING, update_time : STRING, deleted : STRING, sold : STRING&gt; COMMENT &#x27;数据&#x27;,</span><br><span class="line">    `old` MAP&lt;STRING,STRING&gt; COMMENT &#x27;旧值&#x27;</span><br><span class="line">) COMMENT &#x27;购物车增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_cart_info_inc/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="17-章节评价增量表"><a href="#17-章节评价增量表" class="headerlink" title="17. 章节评价增量表"></a>17. 章节评价增量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ods_comment_info_inc;</span><br><span class="line">CREATE EXTERNAL TABLE ods_comment_info_inc</span><br><span class="line">(</span><br><span class="line">    `type` STRING COMMENT &#x27;变动类型&#x27;,</span><br><span class="line">    `ts` STRING COMMENT &#x27;变动时间&#x27;,</span><br><span class="line">    `data` STRUCT&lt;id : STRING, user_id : STRING, chapter_id : STRING, course_id : STRING, comment_txt : STRING, create_time :STRING, deleted : STRING&gt; COMMENT &#x27;数据&#x27;,</span><br><span class="line">    `old` MAP&lt;STRING,STRING&gt; COMMENT &#x27;旧值&#x27;</span><br><span class="line">) COMMENT &#x27;章节评价增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_comment_info_inc/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="18-收藏增量表"><a href="#18-收藏增量表" class="headerlink" title="18. 收藏增量表"></a>18. 收藏增量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ods_favor_info_inc;</span><br><span class="line">CREATE EXTERNAL TABLE ods_favor_info_inc</span><br><span class="line">(</span><br><span class="line">    `type` STRING COMMENT &#x27;变动类型&#x27;,</span><br><span class="line">    `ts` STRING COMMENT &#x27;变动时间&#x27;,</span><br><span class="line">    `data` STRUCT&lt;id : STRING, course_id : STRING, user_id : STRING, create_time : STRING, update_time : STRING, deleted : STRING&gt; COMMENT &#x27;数据&#x27;,</span><br><span class="line">    `old`  MAP&lt;STRING,STRING&gt; COMMENT &#x27;旧值&#x27;</span><br><span class="line">) COMMENT &#x27;收藏增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_favor_info_inc/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="19-订单明细增量表"><a href="#19-订单明细增量表" class="headerlink" title="19. 订单明细增量表"></a>19. 订单明细增量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ods_order_detail_inc;</span><br><span class="line">CREATE EXTERNAL TABLE ods_order_detail_inc</span><br><span class="line">(</span><br><span class="line">    `type` STRING COMMENT &#x27;变动类型&#x27;,</span><br><span class="line">    `ts` STRING COMMENT &#x27;变动时间&#x27;,</span><br><span class="line">    `data` STRUCT&lt;id : STRING, course_id : STRING, course_name : STRING, order_id : STRING, user_id : STRING, origin_amount : DECIMAL(16, 2), coupon_reduce : DECIMAL(16, 2), final_amount : DECIMAL(16, 2), create_time : STRING, update_time : STRING&gt; COMMENT &#x27;数据&#x27;,</span><br><span class="line">    `old`  MAP&lt;STRING,STRING&gt; COMMENT &#x27;旧值&#x27;</span><br><span class="line">) COMMENT &#x27;订单明细增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_order_detail_inc/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="20-订单增量表"><a href="#20-订单增量表" class="headerlink" title="20. 订单增量表"></a>20. 订单增量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ods_order_info_inc;</span><br><span class="line">CREATE EXTERNAL TABLE ods_order_info_inc</span><br><span class="line">(</span><br><span class="line">    `type` STRING COMMENT &#x27;变动类型&#x27;,</span><br><span class="line">    `ts` STRING COMMENT &#x27;变动时间&#x27;,</span><br><span class="line">    `data` STRUCT&lt;id : String, user_id : String, origin_amount : DECIMAL(16, 2), coupon_reduce : DECIMAL(16, 2), final_amount : DECIMAL(16, 2), order_status : String, out_trade_no : String, trade_body : String, session_id : String, province_id  : String, create_time : String, expire_time : String, update_time : String&gt; COMMENT &#x27;数据&#x27;,</span><br><span class="line">    `old`  MAP&lt;STRING,STRING&gt; COMMENT &#x27;旧值&#x27;</span><br><span class="line">) COMMENT &#x27;订单增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_order_info_inc/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="21-支付增量表"><a href="#21-支付增量表" class="headerlink" title="21. 支付增量表"></a>21. 支付增量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ods_payment_info_inc;</span><br><span class="line">CREATE EXTERNAL TABLE ods_payment_info_inc</span><br><span class="line">(</span><br><span class="line">    `type` STRING COMMENT &#x27;变动类型&#x27;,</span><br><span class="line">    `ts` STRING COMMENT &#x27;变动时间&#x27;,</span><br><span class="line">    `data` STRUCT&lt;id : STRING, out_trade_no : STRING, order_id : STRING, alipay_trade_no : STRING, total_amount : DECIMAL(16, 2), trade_body : STRING, payment_type : STRING, payment_status : STRING, create_time : STRING, update_time : STRING, callback_content : STRING, callback_time : STRING&gt; COMMENT &#x27;数据&#x27;,</span><br><span class="line">    `old`  MAP&lt;STRING,STRING&gt; COMMENT &#x27;旧值&#x27;</span><br><span class="line">) COMMENT &#x27;支付增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_payment_info_inc/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="22-课程评价增量表"><a href="#22-课程评价增量表" class="headerlink" title="22. 课程评价增量表"></a>22. 课程评价增量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ods_review_info_inc;</span><br><span class="line">CREATE EXTERNAL TABLE ods_review_info_inc</span><br><span class="line">(</span><br><span class="line">    `type` STRING COMMENT &#x27;变动类型&#x27;,</span><br><span class="line">    `ts` STRING COMMENT &#x27;变动时间&#x27;,</span><br><span class="line">    `data` STRUCT&lt;id : STRING, user_id : STRING, course_id : STRING, review_txt : STRING, review_stars : STRING, create_time : STRING, deleted : STRING&gt; COMMENT &#x27;数据&#x27;,</span><br><span class="line">    `old`  MAP&lt;STRING,STRING&gt; COMMENT &#x27;旧值&#x27;</span><br><span class="line">) COMMENT &#x27;课程评价增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_review_info_inc/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="23-考试增量表"><a href="#23-考试增量表" class="headerlink" title="23. 考试增量表"></a>23. 考试增量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ods_test_exam_inc;</span><br><span class="line">CREATE EXTERNAL TABLE ods_test_exam_inc</span><br><span class="line">(</span><br><span class="line">    `type` STRING COMMENT &#x27;变动类型&#x27;,</span><br><span class="line">    `ts` STRING COMMENT &#x27;变动时间&#x27;,</span><br><span class="line">    `data` STRUCT&lt;id : STRING, paper_id : STRING, user_id : STRING, score : DECIMAL(16, 2), duration_sec : BIGINT, create_time : STRING, submit_time : STRING, update_time : STRING, deleted : STRING&gt; COMMENT &#x27;数据&#x27;,</span><br><span class="line">    `old`  MAP&lt;STRING,STRING&gt; COMMENT &#x27;旧值&#x27;</span><br><span class="line">) COMMENT &#x27;考试增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_test_exam_inc/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="24-用户增量表"><a href="#24-用户增量表" class="headerlink" title="24. 用户增量表"></a>24. 用户增量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ods_user_info_inc;</span><br><span class="line">CREATE EXTERNAL TABLE ods_user_info_inc</span><br><span class="line">(</span><br><span class="line">    `type` STRING COMMENT &#x27;变动类型&#x27;,</span><br><span class="line">    `ts` STRING COMMENT &#x27;变动时间&#x27;,</span><br><span class="line">    `data` STRUCT&lt;id : STRING, login_name : STRING, nick_name : STRING, passwd : STRING, real_name : STRING, phone_num : STRING, email : STRING, head_img : STRING, user_level : STRING, birthday : STRING, gender : STRING, create_time : STRING, operate_time : STRING, status : STRING&gt; COMMENT &#x27;数据&#x27;,</span><br><span class="line">    `old`  MAP&lt;STRING,STRING&gt; COMMENT &#x27;旧值&#x27;</span><br><span class="line">) COMMENT &#x27;用户增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_user_info_inc/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="25-VIP等级变动明细增量表"><a href="#25-VIP等级变动明细增量表" class="headerlink" title="25. VIP等级变动明细增量表"></a>25. VIP等级变动明细增量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ods_vip_change_detail_inc;</span><br><span class="line">CREATE EXTERNAL TABLE ods_vip_change_detail_inc</span><br><span class="line">(</span><br><span class="line">    `type` STRING COMMENT &#x27;变动类型&#x27;,</span><br><span class="line">    `ts` STRING COMMENT &#x27;变动时间&#x27;,</span><br><span class="line">    `data` STRUCT&lt;id : STRING, user_id : STRING, from_vip : STRING, to_vip : STRING, create_time : STRING&gt; COMMENT &#x27;数据&#x27;,</span><br><span class="line">    `old`  MAP&lt;STRING,STRING&gt; COMMENT &#x27;旧值&#x27;</span><br><span class="line">) COMMENT &#x27;VIP等级变动明细增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_vip_change_detail_inc/&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="26-考试题目增量表"><a href="#26-考试题目增量表" class="headerlink" title="26. 考试题目增量表"></a>26. 考试题目增量表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS `ods_test_exam_question_inc`;</span><br><span class="line">CREATE EXTERNAL TABLE `ods_test_exam_question_inc`</span><br><span class="line">(</span><br><span class="line">    `type` STRING COMMENT &#x27;变动类型&#x27;,</span><br><span class="line">    `ts` STRING COMMENT &#x27;变动时间&#x27;,</span><br><span class="line">    `data` STRUCT&lt; id : STRING, exam_id : STRING, paper_id : STRING, question_id : STRING,user_id : STRING, answer : STRING, is_correct : STRING,score : decimal(16, 2), create_time : STRING,update_time : STRING,deleted : STRING&gt; COMMENT &#x27;数据&#x27;,</span><br><span class="line">    `old`  MAP&lt;STRING,STRING&gt; COMMENT &#x27;旧值&#x27;</span><br><span class="line">) COMMENT &#x27;考试题目增量表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    ROW FORMAT SERDE &#x27;org.apache.hadoop.hive.serde2.JsonSerDe&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ods/ods_test_exam_question_inc/&#x27;;</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-11-17_22-05-32.png" alt="Snipaste_2023-11-17_22-05-32" style="zoom: 50%;">

<p>共27张表格，一张用户行为日志表，26张业务数据表</p>
<p>数据装载脚本</p>
<p>（1）在hadoop102的~&#x2F;bin目录下创建hdfs_to_ods_db.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim hdfs_to_ods_db.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">APP=&#x27;edu&#x27;</span><br><span class="line"></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">   do_date=$2</span><br><span class="line">else </span><br><span class="line">   do_date=`date -d &#x27;-1 day&#x27; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">load_data()&#123;</span><br><span class="line">    sql=&quot;&quot;</span><br><span class="line">    for i in $*; do</span><br><span class="line">        #判断路径是否存在</span><br><span class="line">        hadoop fs -test -e /origin_data/edu/db/$&#123;i:4&#125;/$do_date</span><br><span class="line">        #路径存在方可装载数据</span><br><span class="line">        if [[ $? = 0 ]]; then</span><br><span class="line">            sql=$sql&quot;load data inpath &#x27;/origin_data/edu/db/$&#123;i:4&#125;/$do_date&#x27; OVERWRITE into table $&#123;APP&#125;.$&#123;i&#125; partition(dt=&#x27;$do_date&#x27;);&quot;</span><br><span class="line">        fi</span><br><span class="line">    done</span><br><span class="line">    hive -e &quot;$sql&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    ods_base_category_info_full | ods_base_province_full | ods_base_source_full | ods_base_subject_info_full | ods_cart_info_full | ods_cart_info_inc | ods_chapter_info_full | ods_comment_info_inc | ods_course_info_full | ods_favor_info_inc | ods_knowledge_point_full | ods_order_detail_inc | ods_order_info_inc | ods_payment_info_inc | ods_review_info_inc | ods_test_exam_inc | ods_test_exam_question_inc | ods_test_paper_full | ods_test_paper_question_full | ods_test_point_question_full | ods_test_question_info_full | ods_test_question_option_full | ods_user_chapter_process_full | ods_user_info_inc | ods_video_info_full | ods_vip_change_detail_inc)</span><br><span class="line">        load_data $1</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot;)</span><br><span class="line">        load_data ods_base_category_info_full ods_base_province_full ods_base_source_full ods_base_subject_info_full ods_cart_info_full ods_cart_info_inc ods_chapter_info_full ods_comment_info_inc ods_course_info_full ods_favor_info_inc ods_knowledge_point_full ods_order_detail_inc ods_order_info_inc ods_payment_info_inc ods_review_info_inc ods_test_exam_inc ods_test_exam_question_inc ods_test_paper_full ods_test_paper_question_full ods_test_point_question_full ods_test_question_info_full ods_test_question_option_full ods_user_chapter_process_full ods_user_info_inc ods_video_info_full ods_vip_change_detail_inc</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）增加执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 hdfs_to_ods_db.sh</span><br></pre></td></tr></table></figure>

<p>（3）执行脚本，第一个参数传入all，第二个参数传入2022-02-21，导入2022-02-21的数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# hdfs_to_ods_db.sh all 2022-02-21</span><br></pre></td></tr></table></figure>

<p>至此，一张用户行为日志表格数据和26张业务表格数据全部落盘至HDFS的&#x2F;warehouse&#x2F;edu&#x2F;ods目录下</p>
<img src="Snipaste_2023-11-21_09-11-23.png" alt="Snipaste_2023-11-21_09-11-23" style="zoom:50%;">

<h2 id="第八章-数据仓库开发之DIM层"><a href="#第八章-数据仓库开发之DIM层" class="headerlink" title="第八章 数据仓库开发之DIM层"></a>第八章 数据仓库开发之DIM层</h2><p>DIM层存储维度表，在业务总线矩阵中，共出现了10种维度，其中的设备维度已经冗杂在了对应的事实表中，无须设计，所以最终形成9个维度表</p>
<p><img src="Snipaste_2023-11-16_15-37-59.png" alt="Snipaste_2023-11-16_15-37-59"></p>
<p>DIM层设计要点：</p>
<p>（1）DIM层的设计依据是维度建模理论，该层存储维度模型的维度表。</p>
<p>（2）DIM层的数据存储格式为orc列式存储+snappy压缩。</p>
<p>（3）DIM层表名的命名规范为dim_表名_全量表或者拉链表标识（full&#x2F;zip）</p>
<h3 id="1-章节维度表（全量）"><a href="#1-章节维度表（全量）" class="headerlink" title="1. 章节维度表（全量）"></a>1. 章节维度表（全量）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">-- 1）建表语句</span><br><span class="line">DROP TABLE IF EXISTS dim_chapter_full;</span><br><span class="line">CREATE EXTERNAL TABLE dim_chapter_full</span><br><span class="line">(</span><br><span class="line">    `id`           STRING COMMENT &#x27;章节ID&#x27;,</span><br><span class="line">    `chapter_name` STRING COMMENT &#x27;章节名称&#x27;,</span><br><span class="line">    `course_id`    STRING COMMENT &#x27;课程ID&#x27;,</span><br><span class="line">    `video_id`     STRING COMMENT &#x27;视频ID&#x27;,</span><br><span class="line">    `publisher_id` STRING COMMENT &#x27;发布者ID&#x27;,</span><br><span class="line">    `is_free`      STRING COMMENT &#x27;是否免费&#x27;,</span><br><span class="line">    `create_time`  STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time`  STRING COMMENT &#x27;更新时间&#x27;</span><br><span class="line">) COMMENT &#x27;章节维度表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dim/dim_chapter_full/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br><span class="line">-- 2）数据装载</span><br><span class="line">insert overwrite table edu.dim_chapter_full</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       course_id,</span><br><span class="line">       video_id,</span><br><span class="line">       publisher_id,</span><br><span class="line">       is_free,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time</span><br><span class="line">from edu.ods_chapter_info_full</span><br><span class="line">where deleted = &#x27;0&#x27;</span><br><span class="line">  and dt = &#x27;2022-02-21&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="2-课程维度表（全量）"><a href="#2-课程维度表（全量）" class="headerlink" title="2. 课程维度表（全量）"></a>2. 课程维度表（全量）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">-- 1）建表语句</span><br><span class="line">DROP TABLE IF EXISTS dim_course_full;</span><br><span class="line">CREATE EXTERNAL TABLE dim_course_full</span><br><span class="line">(</span><br><span class="line">    `id`               STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `course_name`      STRING COMMENT &#x27;课程名称&#x27;,</span><br><span class="line">    `subject_id`       STRING COMMENT &#x27;学科id&#x27;,</span><br><span class="line">    `subject_name`     STRING COMMENT &#x27;学科名称&#x27;,</span><br><span class="line">    `category_id`      STRING COMMENT &#x27;分类id&#x27;,</span><br><span class="line">    `category_name`    STRING COMMENT &#x27;分类名称&#x27;,</span><br><span class="line">    `teacher`          STRING COMMENT &#x27;讲师名称&#x27;,</span><br><span class="line">    `publisher_id`     STRING COMMENT &#x27;发布者id&#x27;,</span><br><span class="line">    `chapter_num`      BIGINT COMMENT &#x27;章节数&#x27;,</span><br><span class="line">    `origin_price`     decimal(16, 2) COMMENT &#x27;价格&#x27;,</span><br><span class="line">    `reduce_amount`    decimal(16, 2) COMMENT &#x27;优惠金额&#x27;,</span><br><span class="line">    `actual_price`     decimal(16, 2) COMMENT &#x27;实际价格&#x27;,</span><br><span class="line">    `create_time`      STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time`      STRING COMMENT &#x27;更新时间&#x27;,</span><br><span class="line">    `chapters`       ARRAY&lt;STRUCT&lt;chapter_id : STRING,chapter_name : STRING, video_id : STRING,is_free  : STRING&gt;&gt; COMMENT &#x27;章节&#x27;</span><br><span class="line">) COMMENT &#x27;课程维度表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dim/dim_course_full/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br><span class="line">-- 2）数据装载</span><br><span class="line">with a as</span><br><span class="line">         (</span><br><span class="line">             select id, category_name</span><br><span class="line">                 from edu.ods_base_category_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;2022-02-21&#x27;</span><br><span class="line">         ),</span><br><span class="line">     b as</span><br><span class="line">         (</span><br><span class="line">             select id, subject_name, category_id</span><br><span class="line">                 from edu.ods_base_subject_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;2022-02-21&#x27;</span><br><span class="line">         ),</span><br><span class="line">     c as</span><br><span class="line">         (</span><br><span class="line">             select id,</span><br><span class="line">                    course_name,</span><br><span class="line">                    subject_id,</span><br><span class="line">                    teacher,</span><br><span class="line">                    publisher_id,</span><br><span class="line">                    chapter_num,</span><br><span class="line">                    origin_price,</span><br><span class="line">                    reduce_amount,</span><br><span class="line">                    actual_price,</span><br><span class="line">                    create_time,</span><br><span class="line">                    update_time</span><br><span class="line">                 from edu.ods_course_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;2022-02-21&#x27;</span><br><span class="line">         ),</span><br><span class="line">     d as</span><br><span class="line">         (</span><br><span class="line">             select course_id,</span><br><span class="line">                    collect_set(named_struct(&#x27;chapter_id&#x27;, id, &#x27;chapter_name&#x27;, chapter_name, &#x27;video_id&#x27;, video_id, &#x27;is_free&#x27;, is_free)) chapters</span><br><span class="line">                 from edu.ods_chapter_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;2022-02-21&#x27;</span><br><span class="line">             group by course_id --多值属性问题，聚合成课程id，将其余结构封装起来</span><br><span class="line"></span><br><span class="line">--              126,&quot;[&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24592&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;Android与H5互调_内容介绍&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3697&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24593&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;Android与H5互调_案例主页面&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3698&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24594&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;Android与H5互调_WebView简介&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3699&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24595&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;Android与H5互调_Java调用JavaScript&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3700&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24596&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;Android与H5互调_JavaScript调Java&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3701&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24597&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;Android与H5互调_H5调用Android播放视频&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3702&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24598&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;Android与H5互调_H5调用Android拨打电话&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3703&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24599&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;Android与H5互调_运行商城案例&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3704&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24600&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;1_尚硅谷_Android与H5互调_内容介绍&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3705&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24601&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;2_尚硅谷_Android与H5互调_案例主页面&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3706&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24602&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;3_尚硅谷_Android与H5互调_WebView简介&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3707&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24603&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;4_尚硅谷_Android与H5互调_Java调用JavaScript&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3708&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24604&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;5_尚硅谷_Android与H5互调_JavaScript调Java&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3709&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24605&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;6_尚硅谷_Android与H5互调_H5调用Android播放视频&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3710&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24606&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;7_尚硅谷_Android与H5互调_H5调用Android拨打电话&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3711&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;,&#123;&quot;&quot;chapter_id&quot;&quot;:&quot;&quot;24607&quot;&quot;,&quot;&quot;chapter_name&quot;&quot;:&quot;&quot;8_尚硅谷_Android与H5互调_运行商城案例&quot;&quot;,&quot;&quot;video_id&quot;&quot;:&quot;&quot;3712&quot;&quot;,&quot;&quot;is_free&quot;&quot;:&quot;&quot;1&quot;&quot;&#125;]&quot;</span><br><span class="line">         )</span><br><span class="line">insert overwrite table edu.dim_course_full</span><br><span class="line">partition(dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select c.id,</span><br><span class="line">       course_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       teacher,</span><br><span class="line">       publisher_id,</span><br><span class="line">       chapter_num,</span><br><span class="line">       origin_price,</span><br><span class="line">       reduce_amount,</span><br><span class="line">       actual_price,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time,</span><br><span class="line">       chapters</span><br><span class="line">from c</span><br><span class="line">         left join b</span><br><span class="line">                   on c.subject_id = b.id</span><br><span class="line">         left join a</span><br><span class="line">                   on b.category_id = a.id</span><br><span class="line">         left join d</span><br><span class="line">                   on c.id = d.course_id;</span><br></pre></td></tr></table></figure>

<h3 id="3-视频维度表（全量）"><a href="#3-视频维度表（全量）" class="headerlink" title="3. 视频维度表（全量）"></a>3. 视频维度表（全量）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">-- 1）建表语句</span><br><span class="line">DROP TABLE IF EXISTS dim_video_full;</span><br><span class="line">CREATE EXTERNAL TABLE dim_video_full</span><br><span class="line">(</span><br><span class="line">    `id`              STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `video_name`      STRING COMMENT &#x27;视频名称&#x27;,</span><br><span class="line">    `during_sec`      BIGINT COMMENT &#x27;时长&#x27;,</span><br><span class="line">    `video_status`    STRING COMMENT &#x27;状态 未上传，上传中，上传完&#x27;,</span><br><span class="line">    `video_size`      BIGINT COMMENT &#x27;大小&#x27;,</span><br><span class="line">    `version_id`      STRING COMMENT &#x27;版本号&#x27;,</span><br><span class="line">    `chapter_id`      STRING COMMENT &#x27;章节id&#x27;,</span><br><span class="line">    `chapter_name`    STRING COMMENT &#x27;章节名称&#x27;,</span><br><span class="line">    `is_free`         STRING COMMENT &#x27;是否免费&#x27;,</span><br><span class="line">    `course_id`       STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `publisher_id`    STRING COMMENT &#x27;发布者id&#x27;,</span><br><span class="line">    `create_time`     STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time`     STRING COMMENT &#x27;更新时间&#x27;</span><br><span class="line">) COMMENT &#x27;视频维度表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dim/dim_video_zip/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br><span class="line">-- 2）数据装载</span><br><span class="line">insert overwrite table edu.dim_video_full partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select vt.id,</span><br><span class="line">       video_name,</span><br><span class="line">       during_sec,</span><br><span class="line">       video_status,</span><br><span class="line">       video_size,</span><br><span class="line">       version_id,</span><br><span class="line">       chapter_id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       is_free,</span><br><span class="line">       course_id,</span><br><span class="line">       publisher_id,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time</span><br><span class="line">from (</span><br><span class="line">         select id,</span><br><span class="line">                video_name,</span><br><span class="line">                during_sec,</span><br><span class="line">                video_status,</span><br><span class="line">                video_size,</span><br><span class="line">                version_id,</span><br><span class="line">                chapter_id,</span><br><span class="line">                course_id,</span><br><span class="line">                publisher_id,</span><br><span class="line">                create_time,</span><br><span class="line">                update_time</span><br><span class="line">         from edu.ods_video_info_full</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27; and deleted = &#x27;0&#x27;</span><br><span class="line">     ) vt</span><br><span class="line">         join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                chapter_name,</span><br><span class="line">                is_free</span><br><span class="line">         from edu.ods_chapter_info_full</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">     ) cht</span><br><span class="line">     on vt.chapter_id = cht.id;</span><br></pre></td></tr></table></figure>

<h3 id="4-试卷维度表（全量）"><a href="#4-试卷维度表（全量）" class="headerlink" title="4. 试卷维度表（全量）"></a>4. 试卷维度表（全量）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">-- 1）建表语句</span><br><span class="line">DROP TABLE IF EXISTS dim_paper_full;</span><br><span class="line">CREATE EXTERNAL TABLE dim_paper_full</span><br><span class="line">(</span><br><span class="line">    `id`           STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `paper_title`  STRING COMMENT &#x27;试卷名称&#x27;,</span><br><span class="line">    `course_id`    STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `create_time`  STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time`  STRING COMMENT &#x27;更新时间&#x27;,</span><br><span class="line">    `publisher_id` STRING COMMENT &#x27;发布者id&#x27;,</span><br><span class="line">    `questions`   ARRAY&lt;STRUCT&lt;question_id: STRING, score: DECIMAL(16, 2)&gt;&gt; COMMENT &#x27;题目&#x27;</span><br><span class="line">) COMMENT &#x27;试卷维度表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dim/dim_paper_full/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br><span class="line">-- 2）数据装载</span><br><span class="line">insert overwrite table edu.dim_paper_full partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select t1.id,</span><br><span class="line">       paper_title,</span><br><span class="line">       course_id,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time,</span><br><span class="line">       publisher_id,</span><br><span class="line">       questions</span><br><span class="line">from edu.ods_test_paper_full t1</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select paper_id,</span><br><span class="line">                 collect_set(named_struct(&#x27;question_id&#x27;, question_id, &#x27;score&#x27;, score)) questions</span><br><span class="line">         from edu.ods_test_paper_question_full</span><br><span class="line">         where deleted = &#x27;0&#x27; and dt = &#x27;2022-02-21&#x27;</span><br><span class="line">         group by paper_id</span><br><span class="line">     ) t2</span><br><span class="line">     on t1.id = t2.paper_id</span><br><span class="line">where t1.deleted = &#x27;0&#x27; and t1.dt = &#x27;2022-02-21&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="5-来源维度表（全量）"><a href="#5-来源维度表（全量）" class="headerlink" title="5. 来源维度表（全量）"></a>5. 来源维度表（全量）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">-- 1）建表语句</span><br><span class="line">DROP TABLE IF EXISTS dim_source_full;</span><br><span class="line">CREATE EXTERNAL TABLE dim_source_full</span><br><span class="line">(</span><br><span class="line">    `id`            STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `source_site` STRING COMMENT &#x27;来源&#x27;</span><br><span class="line">) COMMENT &#x27;来源维度表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dim/dim_source_full/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br><span class="line">-- 2）数据装载</span><br><span class="line">insert overwrite table edu.dim_source_full partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       source_site</span><br><span class="line">from edu.ods_base_source_full obsf</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="6-题目维度表（全量）"><a href="#6-题目维度表（全量）" class="headerlink" title="6. 题目维度表（全量）"></a>6. 题目维度表（全量）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">-- 1）建表语句</span><br><span class="line">DROP TABLE IF EXISTS dim_question_full;</span><br><span class="line">CREATE EXTERNAL TABLE dim_question_full</span><br><span class="line">(</span><br><span class="line">    `id`            STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `chapter_id`    STRING COMMENT &#x27;章节id&#x27;,</span><br><span class="line">    `course_id`     STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `question_type` BIGINT COMMENT &#x27;题目类型&#x27;,</span><br><span class="line">    `create_time`   STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `update_time`   STRING COMMENT &#x27;更新时间&#x27;,</span><br><span class="line">    `publisher_id`  STRING COMMENT &#x27;发布者id&#x27;</span><br><span class="line">) COMMENT &#x27;题目维度表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dim/dim_question_full/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br><span class="line">-- 2）数据装载</span><br><span class="line">insert overwrite table edu.dim_question_full</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       chapter_id,</span><br><span class="line">       course_id,</span><br><span class="line">       question_type,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time,</span><br><span class="line">       publisher_id</span><br><span class="line">from edu.ods_test_question_info_full</span><br><span class="line">where deleted = &#x27;0&#x27;</span><br><span class="line">  and dt = &#x27;2022-02-21&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="7-地区维度表（全量）"><a href="#7-地区维度表（全量）" class="headerlink" title="7. 地区维度表（全量）"></a>7. 地区维度表（全量）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">-- 1）建表语句</span><br><span class="line">DROP TABLE IF EXISTS dim_province_full;</span><br><span class="line">CREATE EXTERNAL TABLE dim_province_full</span><br><span class="line">(</span><br><span class="line">    `id`         STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `name`       STRING COMMENT &#x27;省名称&#x27;,</span><br><span class="line">    `region_id`  STRING COMMENT &#x27;地区id&#x27;,</span><br><span class="line">    `area_code`  STRING COMMENT &#x27;行政区位码&#x27;,</span><br><span class="line">    `iso_code`   STRING COMMENT &#x27;国际编码&#x27;,</span><br><span class="line">    `iso_3166_2` STRING COMMENT &#x27;ISO3166编码&#x27;</span><br><span class="line">) COMMENT &#x27;地区维度表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dim/dim_province_full/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br><span class="line">-- 2）数据装载</span><br><span class="line">insert overwrite table edu.dim_province_full partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       name,</span><br><span class="line">       region_id,</span><br><span class="line">       area_code,</span><br><span class="line">       iso_code,</span><br><span class="line">       iso_3166_2</span><br><span class="line">from edu.ods_base_province_full</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="8-时间维度表（特殊）"><a href="#8-时间维度表（特殊）" class="headerlink" title="8. 时间维度表（特殊）"></a>8. 时间维度表（特殊）</h3><p>时间维度表的数据装载相对特殊。通常情况下，该维度表的数据并不是来自业务系统，而是开发人员手动写入，并且由于时间维度表数据的可预见性，无须每日导入，一般可一次性导入一年的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">-- 1）建表语句</span><br><span class="line">DROP TABLE IF EXISTS dim_date;</span><br><span class="line">CREATE EXTERNAL TABLE dim_date</span><br><span class="line">(</span><br><span class="line">    `date_id`    STRING COMMENT &#x27;日期id&#x27;,</span><br><span class="line">    `week_id`    STRING COMMENT &#x27;周id,一年中的第几周&#x27;,</span><br><span class="line">    `week_day`   STRING COMMENT &#x27;周几&#x27;,</span><br><span class="line">    `day`        STRING COMMENT &#x27;每月的第几天&#x27;,</span><br><span class="line">    `month`      STRING COMMENT &#x27;一年中的第几月&#x27;,</span><br><span class="line">    `quarter`    STRING COMMENT &#x27;一年中的第几季度&#x27;,</span><br><span class="line">    `year`       STRING COMMENT &#x27;年份&#x27;,</span><br><span class="line">    `is_workday` STRING COMMENT &#x27;是否是工作日&#x27;,</span><br><span class="line">    `holiday_id` STRING COMMENT &#x27;节假日&#x27;</span><br><span class="line">) COMMENT &#x27;时间维度表&#x27;</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dim/dim_date/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br><span class="line">-- 2）创建临时表</span><br><span class="line">DROP TABLE IF EXISTS tmp_dim_date_info;</span><br><span class="line">CREATE EXTERNAL TABLE tmp_dim_date_info (</span><br><span class="line">    `date_id` STRING COMMENT &#x27;日&#x27;,</span><br><span class="line">    `week_id` STRING COMMENT &#x27;周id&#x27;,</span><br><span class="line">    `week_day` STRING COMMENT &#x27;周几&#x27;,</span><br><span class="line">    `day` STRING COMMENT &#x27;每月的第几天&#x27;,</span><br><span class="line">    `month` STRING COMMENT &#x27;第几月&#x27;,</span><br><span class="line">    `quarter` STRING COMMENT &#x27;第几季度&#x27;,</span><br><span class="line">    `year` STRING COMMENT &#x27;年&#x27;,</span><br><span class="line">    `is_workday` STRING COMMENT &#x27;是否是工作日&#x27;,</span><br><span class="line">    `holiday_id` STRING COMMENT &#x27;节假日&#x27;</span><br><span class="line">) COMMENT &#x27;时间维度表&#x27;</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">LOCATION &#x27;/warehouse/edu/tmp/tmp_dim_date_info/&#x27;;</span><br></pre></td></tr></table></figure>

<p>将数据文件date.info上传到HDFS上临时表指定的路径&#x2F;warehouse&#x2F;edu&#x2F;tmp&#x2F;tmp_dim_date_info</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--执行以下语句将其导入时间维度表。</span><br><span class="line">insert overwrite table dim_date select * from tmp_dim_date_info;</span><br></pre></td></tr></table></figure>

<h3 id="9-用户维度表（拉链表）"><a href="#9-用户维度表（拉链表）" class="headerlink" title="9. 用户维度表（拉链表）"></a>9. 用户维度表（拉链表）</h3><p>用户维度表中需要存储所有用户的相关信息。用户信息通常数据量比较庞大，若每日同步全量用户信息表的话，将会占用大量的存储空间。为此，我们对用户维度表采用拉链表策略。拉链表的意义在于可以更加高效地保存维度信息的历史状态。</p>
<p><strong>分区规划</strong></p>
<p>每日分区中存放的是当日过期的用户数据，在9999-12-31分区中存放的是全量最新的用户数据</p>
<img src="Snipaste_2023-11-21_11-22-32.png" alt="Snipaste_2023-11-21_11-22-32" style="zoom:50%;">

<p><strong>数据装载过程</strong></p>
<img src="Snipaste_2023-11-21_11-23-26.png" alt="Snipaste_2023-11-21_11-23-26" style="zoom: 50%;">

<p><strong>数据流向</strong></p>
<p>用户维度表的数据装载，需要将当日的新增及变化数据与分区为9999-12-31的全量最新数据进行合并，将过期数据放入当日过期数据，最新数据放在9999-12-31分区中，用户维度表的数据装载思路：</p>
<img src="Snipaste_2023-11-21_11-25-52.png" alt="Snipaste_2023-11-21_11-25-52" style="zoom:50%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">-- 1）建表语句</span><br><span class="line">DROP TABLE IF EXISTS dim_user_zip;</span><br><span class="line">CREATE EXTERNAL TABLE dim_user_zip</span><br><span class="line">(</span><br><span class="line">    `id`           STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `login_name`   STRING COMMENT &#x27;用户名称&#x27;,</span><br><span class="line">    `nick_name`    STRING COMMENT &#x27;用户昵称&#x27;,</span><br><span class="line">    `real_name`    STRING COMMENT &#x27;用户姓名&#x27;,</span><br><span class="line">    `phone_num`    STRING COMMENT &#x27;手机号&#x27;,</span><br><span class="line">    `email`        STRING COMMENT &#x27;邮箱&#x27;,</span><br><span class="line">    `user_level`   STRING COMMENT &#x27;用户级别&#x27;,</span><br><span class="line">    `birthday`     STRING COMMENT &#x27;用户生日&#x27;,</span><br><span class="line">    `gender`       STRING COMMENT &#x27;性别 M男,F女&#x27;,</span><br><span class="line">    `create_time`  STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `operate_time` STRING COMMENT &#x27;修改时间&#x27;,</span><br><span class="line">    `status`       STRING COMMENT &#x27;状态&#x27;,</span><br><span class="line">    `start_date`   STRING COMMENT &#x27;开始日期&#x27;,</span><br><span class="line">    `end_date`     STRING COMMENT &#x27;结束日期&#x27;</span><br><span class="line">) COMMENT &#x27;用户表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dim/dim_user_zip/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br><span class="line"></span><br><span class="line">--首日数据装载</span><br><span class="line">--将截至到初始化当日的全部历史用户导入，一次性导入到拉链表中，目前ods_user_info_inc表的第一个分区，即2022-02-21</span><br><span class="line">--分区中就是全部的历史用户，故将该分区的数据进行一定脱敏处理后导入拉链表的9999-12-31分区即可</span><br><span class="line">insert overwrite table edu.dim_user_zip</span><br><span class="line">    partition (dt = &#x27;9999-12-31&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.login_name,</span><br><span class="line">       data.nick_name,</span><br><span class="line">       md5(data.real_name),</span><br><span class="line">       md5(if(data.phone_num regexp &#x27;^(13[0-9]|14[01456879]|15[0-35-9]|16[2567]|17[0-8]|18[0-9]|19[0-35-9])\\d&#123;8&#125;$&#x27;,data.phone_num,null)),</span><br><span class="line">       md5(if(data.email regexp &#x27;^[a-zA-Z0-9_-]+@[a-zA-Z0-9_-]+(\\.[a-zA-Z0-9_-]+)+$&#x27;,data.email,null)),</span><br><span class="line">       data.user_level,</span><br><span class="line">       data.birthday,</span><br><span class="line">       data.gender,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.operate_time,</span><br><span class="line">       data.status,</span><br><span class="line">       &#x27;2022-02-21&#x27; start_date,</span><br><span class="line">       &#x27;9999-12-31&#x27; end_date</span><br><span class="line">from edu.ods_user_info_inc</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">  and type = &#x27;bootstrap-insert&#x27;;</span><br></pre></td></tr></table></figure>

<p>每日装载：</p>
<p>先将截至前一日的全量最新数据与当日变动数据进行union操作。</p>
<p>然后使用开窗函数row_number()，对上述数据中每个用户的新老状态进行标识，row_number()函数需按照user_id分区，start_data降序排序。得到的结果中，序号为1的状态为最新状态，序号为2的状态为过期状态。根据序号对数据进行修改，将序号为2的状态的结束日期修改为前一日的日期，序号为1的数据不做修改，最后使用动态分区，将数据分别写入9999-12-31分区和前一日分区。</p>
<img src="Snipaste_2023-11-21_11-43-56.png" alt="Snipaste_2023-11-21_11-43-56" style="zoom:50%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table edu.dim_user_zip </span><br><span class="line">partition(dt)</span><br><span class="line">select</span><br><span class="line">    id,</span><br><span class="line">    login_name,</span><br><span class="line">    nick_name,</span><br><span class="line">    real_name,</span><br><span class="line">    phone_num,</span><br><span class="line">    email,</span><br><span class="line">    user_level,</span><br><span class="line">    birthday,</span><br><span class="line">    gender,</span><br><span class="line">    create_time,</span><br><span class="line">    operate_time,</span><br><span class="line">    status,</span><br><span class="line">    start_date,</span><br><span class="line">    if(rn=1,&#x27;9999-12-31&#x27;,date_sub(&#x27;2022-02-22&#x27;,1)) end_date,</span><br><span class="line">    if(rn=1,&#x27;9999-12-31&#x27;,date_sub(&#x27;2022-02-22&#x27;,1)) dt</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        login_name,</span><br><span class="line">        nick_name,</span><br><span class="line">        real_name,</span><br><span class="line">        phone_num,</span><br><span class="line">        email,</span><br><span class="line">        user_level,</span><br><span class="line">        birthday,</span><br><span class="line">        gender,</span><br><span class="line">        create_time,</span><br><span class="line">        operate_time,</span><br><span class="line">        status,</span><br><span class="line">        start_date,</span><br><span class="line">        end_date,</span><br><span class="line">        row_number() over (partition by id order by start_date desc) rn</span><br><span class="line">    from</span><br><span class="line">    (</span><br><span class="line">        select</span><br><span class="line">            id,</span><br><span class="line">            login_name,</span><br><span class="line">            nick_name,</span><br><span class="line">            real_name,</span><br><span class="line">            phone_num,</span><br><span class="line">            email,</span><br><span class="line">            user_level,</span><br><span class="line">            birthday,</span><br><span class="line">            gender,</span><br><span class="line">            create_time,</span><br><span class="line">            operate_time,</span><br><span class="line">            status,</span><br><span class="line">            start_date,</span><br><span class="line">            end_date</span><br><span class="line">        from edu.dim_user_zip</span><br><span class="line">        where dt=&#x27;9999-12-31&#x27;</span><br><span class="line">        union</span><br><span class="line">        select</span><br><span class="line">            id,</span><br><span class="line">            login_name,</span><br><span class="line">            nick_name,</span><br><span class="line">            real_name,</span><br><span class="line">            phone_num,</span><br><span class="line">            email,</span><br><span class="line">            user_level,</span><br><span class="line">            birthday,</span><br><span class="line">            gender,</span><br><span class="line">            create_time,</span><br><span class="line">            operate_time,</span><br><span class="line">            status,</span><br><span class="line">            &#x27;2020-02-22&#x27; start_date,</span><br><span class="line">            &#x27;9999-12-31&#x27; end_date</span><br><span class="line">        from</span><br><span class="line">        (</span><br><span class="line">            select</span><br><span class="line">                data.id,</span><br><span class="line">                data.login_name,</span><br><span class="line">                data.nick_name,</span><br><span class="line">                md5(data.real_name) real_name,</span><br><span class="line">                md5(if(data.phone_num regexp &#x27;^(13[0-9]|14[01456879]|15[0-35-9]|16[2567]|17[0-8]|18[0-9]|19[0-35-9])\\d&#123;8&#125;$&#x27;,data.phone_num,null)) phone_num,</span><br><span class="line">                md5(if(data.email regexp &#x27;^[a-zA-Z0-9_-]+@[a-zA-Z0-9_-]+(\\.[a-zA-Z0-9_-]+)+$&#x27;,data.email,null)) email,</span><br><span class="line">                data.user_level,</span><br><span class="line">                data.birthday,</span><br><span class="line">                data.gender,</span><br><span class="line">                data.create_time,</span><br><span class="line">                data.operate_time,</span><br><span class="line">                data.status,</span><br><span class="line">                row_number() over (partition by data.id order by ts desc) rn</span><br><span class="line">            from edu.ods_user_info_inc</span><br><span class="line">            where dt=&#x27;2022-02-22&#x27;</span><br><span class="line">        )t1</span><br><span class="line">        where rn=1</span><br><span class="line">    )t2</span><br><span class="line">)t3;</span><br></pre></td></tr></table></figure>

<h3 id="8-10-数据装载脚本"><a href="#8-10-数据装载脚本" class="headerlink" title="8.10 数据装载脚本"></a>8.10 数据装载脚本</h3><p>在DIM层的搭建中，用户维度表使用了拉链表的形式，首日数据装载与每日数据装载方法存在不同，所以DIM层的数据装载脚本也将分为首日脚本与每日脚本。</p>
<p>脚本设计思路与ODS层相似，首先，获取到执行日期变量，然后，进行每个维度表装载数据的SQL拼接工作，将日期变量拼接进执行SQL中，最后通过判断输入的表名决定执行哪张表的数据装载工作。</p>
<h4 id="8-10-1-首日装载脚本"><a href="#8-10-1-首日装载脚本" class="headerlink" title="8.10.1 首日装载脚本"></a>8.10.1 首日装载脚本</h4><p>（1）在hadoop102的~&#x2F;bin目录下创建ods_to_dim_init.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim ods_to_dim_init.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    echo &quot;请传入日期参数&quot;</span><br><span class="line">    exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">APP=edu</span><br><span class="line"></span><br><span class="line">dim_chapter_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_chapter_full</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       course_id,</span><br><span class="line">       video_id,</span><br><span class="line">       publisher_id,</span><br><span class="line">       is_free,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time</span><br><span class="line">from $&#123;APP&#125;.ods_chapter_info_full</span><br><span class="line">where deleted = &#x27;0&#x27;</span><br><span class="line">  and dt = &#x27;$do_date&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dim_course_full=&quot;</span><br><span class="line">with a as</span><br><span class="line">         (</span><br><span class="line">             select id, category_name</span><br><span class="line">                 from $&#123;APP&#125;.ods_base_category_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;$do_date&#x27;</span><br><span class="line">         ),</span><br><span class="line">     b as</span><br><span class="line">         (</span><br><span class="line">             select id, subject_name, category_id</span><br><span class="line">                 from $&#123;APP&#125;.ods_base_subject_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;$do_date&#x27;</span><br><span class="line">         ),</span><br><span class="line">     c as</span><br><span class="line">         (</span><br><span class="line">             select id,</span><br><span class="line">                    course_name,</span><br><span class="line">                    subject_id,</span><br><span class="line">                    teacher,</span><br><span class="line">                    publisher_id,</span><br><span class="line">                    chapter_num,</span><br><span class="line">                    origin_price,</span><br><span class="line">                    reduce_amount,</span><br><span class="line">                    actual_price,</span><br><span class="line">                    create_time,</span><br><span class="line">                    update_time</span><br><span class="line">                 from $&#123;APP&#125;.ods_course_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;$do_date&#x27;</span><br><span class="line">         ),</span><br><span class="line">     d as</span><br><span class="line">         (</span><br><span class="line">             select course_id,</span><br><span class="line">                    collect_set(named_struct(&#x27;chapter_id&#x27;, id, &#x27;chapter_name&#x27;, chapter_name, &#x27;video_id&#x27;, video_id, &#x27;is_free&#x27;, is_free)) chapters</span><br><span class="line">                 from $&#123;APP&#125;.ods_chapter_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;$do_date&#x27;</span><br><span class="line">             group by course_id</span><br><span class="line">         )</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_course_full</span><br><span class="line">partition(dt = &#x27;$do_date&#x27;)</span><br><span class="line">select c.id,</span><br><span class="line">       course_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       teacher,</span><br><span class="line">       publisher_id,</span><br><span class="line">       chapter_num,</span><br><span class="line">       origin_price,</span><br><span class="line">       reduce_amount,</span><br><span class="line">       actual_price,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time,</span><br><span class="line">       chapters</span><br><span class="line">from c</span><br><span class="line">         left join b</span><br><span class="line">                   on c.subject_id = b.id</span><br><span class="line">         left join a</span><br><span class="line">                   on b.category_id = a.id</span><br><span class="line">         left join d</span><br><span class="line">                   on c.id = d.course_id;&quot;</span><br><span class="line"></span><br><span class="line">dim_video_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_video_full partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select vt.id,</span><br><span class="line">       video_name,</span><br><span class="line">       during_sec,</span><br><span class="line">       video_status,</span><br><span class="line">       video_size,</span><br><span class="line">       version_id,</span><br><span class="line">       chapter_id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       is_free,</span><br><span class="line">       course_id,</span><br><span class="line">       publisher_id,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time</span><br><span class="line">from (</span><br><span class="line">         select id,</span><br><span class="line">                video_name,</span><br><span class="line">                during_sec,</span><br><span class="line">                video_status,</span><br><span class="line">                video_size,</span><br><span class="line">                version_id,</span><br><span class="line">                chapter_id,</span><br><span class="line">                course_id,</span><br><span class="line">                publisher_id,</span><br><span class="line">                create_time,</span><br><span class="line">                update_time</span><br><span class="line">         from $&#123;APP&#125;.ods_video_info_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27; and deleted = &#x27;0&#x27;</span><br><span class="line">     ) vt</span><br><span class="line">         join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                chapter_name,</span><br><span class="line">                is_free</span><br><span class="line">         from $&#123;APP&#125;.ods_chapter_info_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) cht</span><br><span class="line">     on vt.chapter_id = cht.id;&quot;</span><br><span class="line"></span><br><span class="line">dim_paper_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_paper_full partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select t1.id,</span><br><span class="line">       paper_title,</span><br><span class="line">       course_id,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time,</span><br><span class="line">       publisher_id,</span><br><span class="line">       questions</span><br><span class="line">from $&#123;APP&#125;.ods_test_paper_full t1</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select paper_id,</span><br><span class="line">                 collect_set(named_struct(&#x27;question_id&#x27;, question_id, &#x27;score&#x27;, score)) questions</span><br><span class="line">         from $&#123;APP&#125;.ods_test_paper_question_full</span><br><span class="line">         where deleted = &#x27;0&#x27; and dt = &#x27;$do_date&#x27;</span><br><span class="line">         group by paper_id</span><br><span class="line">     ) t2</span><br><span class="line">     on t1.id = t2.paper_id</span><br><span class="line">where t1.deleted = &#x27;0&#x27; and t1.dt = &#x27;$do_date&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dim_source_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_source_full partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       source_site</span><br><span class="line">from $&#123;APP&#125;.ods_base_source_full obsf</span><br><span class="line">where dt = &#x27;$do_date&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dim_question_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_question_full</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       chapter_id,</span><br><span class="line">       course_id,</span><br><span class="line">       question_type,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time,</span><br><span class="line">       publisher_id</span><br><span class="line">from $&#123;APP&#125;.ods_test_question_info_full</span><br><span class="line">where deleted = &#x27;0&#x27;</span><br><span class="line">  and dt = &#x27;$do_date&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dim_province_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_province_full partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       name,</span><br><span class="line">       region_id,</span><br><span class="line">       area_code,</span><br><span class="line">       iso_code,</span><br><span class="line">       iso_3166_2</span><br><span class="line">from $&#123;APP&#125;.ods_base_province_full</span><br><span class="line">where dt = &#x27;$do_date&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dim_user_zip=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_user_zip</span><br><span class="line">    partition (dt = &#x27;9999-12-31&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.login_name,</span><br><span class="line">       data.nick_name,</span><br><span class="line">       md5(data.real_name),</span><br><span class="line">       md5(if(data.phone_num regexp &#x27;^(13[0-9]|14[01456879]|15[0-35-9]|16[2567]|17[0-8]|18[0-9]|19[0-35-9])\\d&#123;8&#125;$&#x27;,data.phone_num,null)),</span><br><span class="line">       md5(if(data.email regexp &#x27;^[a-zA-Z0-9_-]+@[a-zA-Z0-9_-]+(\\.[a-zA-Z0-9_-]+)+$&#x27;,data.email,null)),</span><br><span class="line">       data.user_level,</span><br><span class="line">       data.birthday,</span><br><span class="line">       data.gender,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.operate_time,</span><br><span class="line">       data.status,</span><br><span class="line">       &#x27;$do_date&#x27; start_date,</span><br><span class="line">       &#x27;9999-12-31&#x27; end_date</span><br><span class="line">from $&#123;APP&#125;.ods_user_info_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and type = &#x27;bootstrap-insert&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    dim_chapter_full|dim_course_full|dim_video_full|dim_paper_full|dim_source_full|dim_question_full|dim_province_full|dim_user_zip)</span><br><span class="line">        eval &quot;hive -e \&quot;\$$1\&quot;&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot; )</span><br><span class="line">        hive -e &quot;$&#123;dim_chapter_full&#125;$&#123;dim_course_full&#125;$&#123;dim_video_full&#125;$&#123;dim_paper_full&#125;$&#123;dim_source_full&#125;$&#123;dim_question_full&#125;$&#123;dim_province_full&#125;$&#123;dim_user_zip&#125;&quot;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）增加脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 ods_to_dim_init.sh</span><br></pre></td></tr></table></figure>

<p>（3）执行首日脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# ods_to_dim_init.sh all 2022-02-21</span><br></pre></td></tr></table></figure>

<h4 id="8-10-2-每日装载脚本"><a href="#8-10-2-每日装载脚本" class="headerlink" title="8.10.2 每日装载脚本"></a>8.10.2 每日装载脚本</h4><p>（1）在hadoop~&#x2F;bin目录下创建ods_to_dim.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim ods_to_dim.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">APP=edu</span><br><span class="line"></span><br><span class="line">dim_chapter_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_chapter_full</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       course_id,</span><br><span class="line">       video_id,</span><br><span class="line">       publisher_id,</span><br><span class="line">       is_free,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time</span><br><span class="line">from $&#123;APP&#125;.ods_chapter_info_full</span><br><span class="line">where deleted = &#x27;0&#x27;</span><br><span class="line">  and dt = &#x27;$do_date&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dim_course_full=&quot;</span><br><span class="line">with a as</span><br><span class="line">         (</span><br><span class="line">             select id, category_name</span><br><span class="line">                 from $&#123;APP&#125;.ods_base_category_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;$do_date&#x27;</span><br><span class="line">         ),</span><br><span class="line">     b as</span><br><span class="line">         (</span><br><span class="line">             select id, subject_name, category_id</span><br><span class="line">                 from $&#123;APP&#125;.ods_base_subject_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;$do_date&#x27;</span><br><span class="line">         ),</span><br><span class="line">     c as</span><br><span class="line">         (</span><br><span class="line">             select id,</span><br><span class="line">                    course_name,</span><br><span class="line">                    subject_id,</span><br><span class="line">                    teacher,</span><br><span class="line">                    publisher_id,</span><br><span class="line">                    chapter_num,</span><br><span class="line">                    origin_price,</span><br><span class="line">                    reduce_amount,</span><br><span class="line">                    actual_price,</span><br><span class="line">                    create_time,</span><br><span class="line">                    update_time</span><br><span class="line">                 from $&#123;APP&#125;.ods_course_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;$do_date&#x27;</span><br><span class="line">         ),</span><br><span class="line">     d as</span><br><span class="line">         (</span><br><span class="line">             select course_id,</span><br><span class="line">                    collect_set(named_struct(&#x27;chapter_id&#x27;, id, &#x27;chapter_name&#x27;, chapter_name, &#x27;video_id&#x27;, video_id, &#x27;is_free&#x27;, is_free)) chapters</span><br><span class="line">                 from $&#123;APP&#125;.ods_chapter_info_full</span><br><span class="line">             where deleted = &#x27;0&#x27;</span><br><span class="line">               and dt = &#x27;$do_date&#x27;</span><br><span class="line">             group by course_id</span><br><span class="line">         )</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_course_full</span><br><span class="line">partition(dt = &#x27;$do_date&#x27;)</span><br><span class="line">select c.id,</span><br><span class="line">       course_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       teacher,</span><br><span class="line">       publisher_id,</span><br><span class="line">       chapter_num,</span><br><span class="line">       origin_price,</span><br><span class="line">       reduce_amount,</span><br><span class="line">       actual_price,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time,</span><br><span class="line">       chapters</span><br><span class="line">from c</span><br><span class="line">         left join b</span><br><span class="line">                   on c.subject_id = b.id</span><br><span class="line">         left join a</span><br><span class="line">                   on b.category_id = a.id</span><br><span class="line">         left join d</span><br><span class="line">                   on c.id = d.course_id;&quot;</span><br><span class="line"></span><br><span class="line">dim_video_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_video_full partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select vt.id,</span><br><span class="line">       video_name,</span><br><span class="line">       during_sec,</span><br><span class="line">       video_status,</span><br><span class="line">       video_size,</span><br><span class="line">       version_id,</span><br><span class="line">       chapter_id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       is_free,</span><br><span class="line">       course_id,</span><br><span class="line">       publisher_id,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time</span><br><span class="line">from (</span><br><span class="line">         select id,</span><br><span class="line">                video_name,</span><br><span class="line">                during_sec,</span><br><span class="line">                video_status,</span><br><span class="line">                video_size,</span><br><span class="line">                version_id,</span><br><span class="line">                chapter_id,</span><br><span class="line">                course_id,</span><br><span class="line">                publisher_id,</span><br><span class="line">                create_time,</span><br><span class="line">                update_time</span><br><span class="line">         from $&#123;APP&#125;.ods_video_info_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27; and deleted = &#x27;0&#x27;</span><br><span class="line">     ) vt</span><br><span class="line">         join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                chapter_name,</span><br><span class="line">                is_free</span><br><span class="line">         from $&#123;APP&#125;.ods_chapter_info_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) cht</span><br><span class="line">     on vt.chapter_id = cht.id;&quot;</span><br><span class="line"></span><br><span class="line">dim_paper_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_paper_full partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select t1.id,</span><br><span class="line">       paper_title,</span><br><span class="line">       course_id,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time,</span><br><span class="line">       publisher_id,</span><br><span class="line">       questions</span><br><span class="line">from $&#123;APP&#125;.ods_test_paper_full t1</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select paper_id,</span><br><span class="line">                 collect_set(named_struct(&#x27;question_id&#x27;, question_id, &#x27;score&#x27;, score)) questions</span><br><span class="line">         from $&#123;APP&#125;.ods_test_paper_question_full</span><br><span class="line">         where deleted = &#x27;0&#x27; and dt = &#x27;$do_date&#x27;</span><br><span class="line">         group by paper_id</span><br><span class="line">     ) t2</span><br><span class="line">     on t1.id = t2.paper_id</span><br><span class="line">where t1.deleted = &#x27;0&#x27; and t1.dt = &#x27;$do_date&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dim_source_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_source_full partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       source_site</span><br><span class="line">from $&#123;APP&#125;.ods_base_source_full obsf</span><br><span class="line">where dt = &#x27;$do_date&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dim_question_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_question_full</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       chapter_id,</span><br><span class="line">       course_id,</span><br><span class="line">       question_type,</span><br><span class="line">       create_time,</span><br><span class="line">       update_time,</span><br><span class="line">       publisher_id</span><br><span class="line">from $&#123;APP&#125;.ods_test_question_info_full</span><br><span class="line">where deleted = &#x27;0&#x27;</span><br><span class="line">  and dt = &#x27;$do_date&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dim_province_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_province_full partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       name,</span><br><span class="line">       region_id,</span><br><span class="line">       area_code,</span><br><span class="line">       iso_code,</span><br><span class="line">       iso_3166_2</span><br><span class="line">from $&#123;APP&#125;.ods_base_province_full</span><br><span class="line">where dt = &#x27;$do_date&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dim_user_zip=&quot;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dim_user_zip </span><br><span class="line">partition(dt)</span><br><span class="line">select</span><br><span class="line">    id,</span><br><span class="line">    login_name,</span><br><span class="line">    nick_name,</span><br><span class="line">    real_name,</span><br><span class="line">    phone_num,</span><br><span class="line">    email,</span><br><span class="line">    user_level,</span><br><span class="line">    birthday,</span><br><span class="line">    gender,</span><br><span class="line">    create_time,</span><br><span class="line">    operate_time,</span><br><span class="line">    status,</span><br><span class="line">    start_date,</span><br><span class="line">    if(rn=1,&#x27;9999-12-31&#x27;,date_sub(&#x27;$do_date&#x27;,1)) end_date,</span><br><span class="line">    if(rn=1,&#x27;9999-12-31&#x27;,date_sub(&#x27;$do_date&#x27;,1)) dt</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">        id,</span><br><span class="line">        login_name,</span><br><span class="line">        nick_name,</span><br><span class="line">        real_name,</span><br><span class="line">        phone_num,</span><br><span class="line">        email,</span><br><span class="line">        user_level,</span><br><span class="line">        birthday,</span><br><span class="line">        gender,</span><br><span class="line">        create_time,</span><br><span class="line">        operate_time,</span><br><span class="line">        status,</span><br><span class="line">        start_date,</span><br><span class="line">        end_date,</span><br><span class="line">        row_number() over (partition by id order by start_date desc) rn</span><br><span class="line">    from</span><br><span class="line">    (</span><br><span class="line">        select</span><br><span class="line">            id,</span><br><span class="line">            login_name,</span><br><span class="line">            nick_name,</span><br><span class="line">            real_name,</span><br><span class="line">            phone_num,</span><br><span class="line">            email,</span><br><span class="line">            user_level,</span><br><span class="line">            birthday,</span><br><span class="line">            gender,</span><br><span class="line">            create_time,</span><br><span class="line">            operate_time,</span><br><span class="line">            status,</span><br><span class="line">            start_date,</span><br><span class="line">            end_date</span><br><span class="line">        from $&#123;APP&#125;.dim_user_zip</span><br><span class="line">        where dt=&#x27;9999-12-31&#x27;</span><br><span class="line">        union</span><br><span class="line">        select</span><br><span class="line">            id,</span><br><span class="line">            login_name,</span><br><span class="line">            nick_name,</span><br><span class="line">            real_name,</span><br><span class="line">            phone_num,</span><br><span class="line">            email,</span><br><span class="line">            user_level,</span><br><span class="line">            birthday,</span><br><span class="line">            gender,</span><br><span class="line">            create_time,</span><br><span class="line">            operate_time,</span><br><span class="line">            status,</span><br><span class="line">            &#x27;2020-02-22&#x27; start_date,</span><br><span class="line">            &#x27;9999-12-31&#x27; end_date</span><br><span class="line">        from</span><br><span class="line">        (</span><br><span class="line">            select</span><br><span class="line">                data.id,</span><br><span class="line">                data.login_name,</span><br><span class="line">                data.nick_name,</span><br><span class="line">                md5(data.real_name) real_name,</span><br><span class="line">                md5(if(data.phone_num regexp &#x27;^(13[0-9]|14[01456879]|15[0-35-9]|16[2567]|17[0-8]|18[0-9]|19[0-35-9])\\d&#123;8&#125;$&#x27;,data.phone_num,null)) phone_num,</span><br><span class="line">                md5(if(data.email regexp &#x27;^[a-zA-Z0-9_-]+@[a-zA-Z0-9_-]+(\\.[a-zA-Z0-9_-]+)+$&#x27;,data.email,null)) email,</span><br><span class="line">                data.user_level,</span><br><span class="line">                data.birthday,</span><br><span class="line">                data.gender,</span><br><span class="line">                data.create_time,</span><br><span class="line">                data.operate_time,</span><br><span class="line">                data.status,</span><br><span class="line">                row_number() over (partition by data.id order by ts desc) rn</span><br><span class="line">            from $&#123;APP&#125;.ods_user_info_inc</span><br><span class="line">            where dt=&#x27;$do_date&#x27;</span><br><span class="line">        )t1</span><br><span class="line">        where rn=1</span><br><span class="line">    )t2</span><br><span class="line">)t3;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    dim_chapter_full|dim_course_full|dim_video_full|dim_paper_full|dim_source_full|dim_question_full|dim_province_full|dim_user_zip)</span><br><span class="line">        eval &quot;hive -e \&quot;\$$1\&quot;&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot; )</span><br><span class="line">        hive -e &quot;$&#123;dim_chapter_full&#125;$&#123;dim_course_full&#125;$&#123;dim_video_full&#125;$&#123;dim_paper_full&#125;$&#123;dim_source_full&#125;$&#123;dim_question_full&#125;$&#123;dim_province_full&#125;$&#123;dim_user_zip&#125;&quot;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）增加脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 ods_to_dim.sh</span><br></pre></td></tr></table></figure>

<p>此时数仓还没有采集2022-02-22的数据，后续在调度器中执行</p>
<p>（3）用法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# ods_to_dim.sh all 2022-02-22</span><br></pre></td></tr></table></figure>

<h2 id="第九章-数仓开发之DWD层"><a href="#第九章-数仓开发之DWD层" class="headerlink" title="第九章 数仓开发之DWD层"></a>第九章 数仓开发之DWD层</h2><p>数据仓库的DWD层，Data Warehouse Detail，细节数据层。这一层主要是<strong>原始数据与数据仓库的主要隔离层</strong>，需要对原始数据进行初步的清洗和规范化操作。例如对用户行为数据进行规范化解析，使其能真正融入数据仓库体系，对业务数据进行系统化建模设计，使其更加规范化。</p>
<p>DWD层设计要点：</p>
<p>（1）DWD层的设计依据是维度建模理论，该层存储维度模型的事实表。</p>
<p>（2）DWD层的数据存储格式为orc列式存储+snappy压缩。</p>
<p>（3）DWD层表名的命名规范为dwd_数据域_表名_单分区增量全量标识（inc&#x2F;full）</p>
<p>我们将围绕6个数据域14个业务过程构建事务事实表</p>
<img src="Snipaste_2023-11-16_15-29-11.png" alt="Snipaste_2023-11-16_15-29-11" style="zoom:50%;">

<h3 id="1-交易域加购事务事实表"><a href="#1-交易域加购事务事实表" class="headerlink" title="1. 交易域加购事务事实表"></a>1. 交易域加购事务事实表</h3><p>建表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_trade_cart_add_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_trade_cart_add_inc</span><br><span class="line">(</span><br><span class="line">    `id`          STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `user_id`     STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `course_id`   STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `date_id`     STRING COMMENT &#x27;时间id&#x27;,</span><br><span class="line">    `session_id`  STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;加购时间&#x27;,</span><br><span class="line">    `cart_price`  DECIMAL(16, 2) COMMENT &#x27;加购时价格&#x27;</span><br><span class="line">) COMMENT &#x27;交易域加购事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_trade_cart_add_inc/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载：</p>
<p>筛选出ods_cart_info_inc表中type类型为bootstrap-insert，并且分区为首日日期的数据。增量数据表的首日数据中包含所有历史数据，所以使用动态分区功能，根据插入数据的最后一个字段值（create time）进行分区，将数据插入到对应的日期分区中去。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_trade_cart_add_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       data.session_id,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.cart_price,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) dt</span><br><span class="line">from edu.ods_cart_info_inc</span><br><span class="line">where dt = &#x27;2022-02-21&#x27; and type = &#x27;bootstrap-insert&#x27;;</span><br></pre></td></tr></table></figure>

<p>每日数据装载（暂不执行，因为没有数据）：</p>
<img src="Snipaste_2023-11-21_15-32-02.png" alt="Snipaste_2023-11-21_15-32-02" style="zoom:50%;">

<img src="Snipaste_2023-11-21_15-32-20.png" alt="Snipaste_2023-11-21_15-32-20" style="zoom:50%;">

<p>每日数据装载要对每日新增的购物车变动数据进行过滤，选取type为insert类型的变动操作。当日变动数据在分析处理后直接放入当日分区。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_trade_cart_add_inc partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       data.session_id,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.cart_price</span><br><span class="line">from edu.ods_cart_info_inc</span><br><span class="line">where dt = &#x27;2022-02-22&#x27; and type = &#x27;insert&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="2-交易域加购周期快照事实表"><a href="#2-交易域加购周期快照事实表" class="headerlink" title="2. 交易域加购周期快照事实表"></a>2. 交易域加购周期快照事实表</h3><p>用户每一次将课程添加进购物车的行为，将会在购物车表中insert一条数据，当用户将一个课程从购物车中删除或者结算购物车中的一个课程时，并不会删除（delete）这条数据，而是将deleted字段或者sold字段值更改为1，所以用户购物车中真正的存量课程，应该是deleted字段和sold字段值为0的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">--建表语句</span><br><span class="line">DROP TABLE IF EXISTS dwd_trade_cart_full;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_trade_cart_full</span><br><span class="line">(</span><br><span class="line">    `id`          STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `user_id`     STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `course_id`   STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `date_id`     STRING COMMENT &#x27;时间id&#x27;,</span><br><span class="line">    `session_id`  STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `course_name` STRING COMMENT &#x27;课程名称&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;加购时间&#x27;,</span><br><span class="line">    `cart_price`  DECIMAL(16, 2) COMMENT &#x27;加购时价格&#x27;</span><br><span class="line">) COMMENT &#x27;交易域加购周期快照事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_trade_cart_full/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br><span class="line">--数据加载</span><br><span class="line">insert overwrite table edu.dwd_trade_cart_full  partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       date_format(create_time, &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       session_id,</span><br><span class="line">       course_name,</span><br><span class="line">       create_time,</span><br><span class="line">       cart_price</span><br><span class="line">from edu.ods_cart_info_full</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">  and deleted = &#x27;0&#x27; and sold = &#x27;0&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="3-交易域试听下单累计快照事实表"><a href="#3-交易域试听下单累计快照事实表" class="headerlink" title="3. 交易域试听下单累计快照事实表"></a>3. 交易域试听下单累计快照事实表</h3><p>用户会有对某课程的试听权限，在试听之后决定是否下单。将用户从试听到下单的过程联合起来，构成一个试听下单事实。在这个过程中，用户有三个关键时间点：开始试听时间、课程下单时间和试听结束时间，体现了用户从试听到下单的过程进展。</p>
<img src="Snipaste_2023-11-21_16-34-37.png" alt="Snipaste_2023-11-21_16-34-37" style="zoom:50%;">

<p>建表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_trade_course_order_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_trade_course_order_inc</span><br><span class="line">(</span><br><span class="line">    `id`                   STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `user_id`              STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `course_id`            STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `course_name`          STRING COMMENT &#x27;课程名称&#x27;,</span><br><span class="line">    `category_id`          STRING COMMENT &#x27;分类id&#x27;,</span><br><span class="line">    `category_name`        STRING COMMENT &#x27;分类名称&#x27;,</span><br><span class="line">    `subject_id`           STRING COMMENT &#x27;科目id&#x27;,</span><br><span class="line">    `subject_name`         STRING COMMENT &#x27;科目名称&#x27;,</span><br><span class="line">    `order_id`             STRING COMMENT &#x27;订单id&#x27;,</span><br><span class="line">    `province_id`          STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `play_time`            STRING COMMENT &#x27;首次播放时间&#x27;,</span><br><span class="line">    `play_date`            STRING COMMENT &#x27;首次播放日期&#x27;,</span><br><span class="line">    `order_time`           STRING COMMENT &#x27;首次下单时间&#x27;,</span><br><span class="line">    `order_date`           STRING COMMENT &#x27;首次下单日期&#x27;,</span><br><span class="line">    `end_date`             STRING COMMENT &#x27;结束日期,试听后七天内未下单即为结束,试听日期+7为结束日期&#x27;,</span><br><span class="line">    `session_id`           STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `original_amount`      DECIMAL(16, 2) COMMENT &#x27;原始金额分摊&#x27;,</span><br><span class="line">    `coupon_reduce_amount` DECIMAL(16, 2) COMMENT &#x27;优惠金额分摊&#x27;,</span><br><span class="line">    `final_amount`         DECIMAL(16, 2) COMMENT &#x27;最终价格分摊&#x27;</span><br><span class="line">) COMMENT &#x27;交易域试听下单累积快照事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_trade_course_order_inc/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载：</p>
<img src="Snipaste_2023-11-21_16-40-57.png" alt="Snipaste_2023-11-21_16-40-57" style="zoom:50%;">

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">with play as</span><br><span class="line">         (</span><br><span class="line">             select min(id)                                     id,</span><br><span class="line">                    user_id,</span><br><span class="line">                    course_id,</span><br><span class="line">                    min(create_time)                            play_time,</span><br><span class="line">                    date_format(min(create_time), &#x27;yyyy-MM-dd&#x27;) play_date</span><br><span class="line">             from edu.ods_user_chapter_process_full</span><br><span class="line">             where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">             group by user_id, course_id</span><br><span class="line">         ),</span><br><span class="line">     oi as</span><br><span class="line">         (</span><br><span class="line">             select data.id,</span><br><span class="line">                    data.province_id,</span><br><span class="line">                    data.session_id</span><br><span class="line">             from edu.ods_order_info_inc</span><br><span class="line">             where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">               and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">         ),</span><br><span class="line">     od as</span><br><span class="line">         (</span><br><span class="line">             select data.id,</span><br><span class="line">                    data.course_id,</span><br><span class="line">                    data.order_id,</span><br><span class="line">                    data.user_id,</span><br><span class="line">                    data.origin_amount,</span><br><span class="line">                    data.coupon_reduce,</span><br><span class="line">                    data.final_amount,</span><br><span class="line">                    data.create_time order_time,</span><br><span class="line">                    date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) order_date</span><br><span class="line">             from edu.ods_order_detail_inc</span><br><span class="line">             where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">               and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">         ),</span><br><span class="line">     dim_course as (</span><br><span class="line">         select id,</span><br><span class="line">                course_name,</span><br><span class="line">                category_id,</span><br><span class="line">                category_name,</span><br><span class="line">                subject_id,</span><br><span class="line">                subject_name</span><br><span class="line">         from edu.dim_course_full</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">     )</span><br><span class="line">insert overwrite table edu.dwd_trade_course_order_inc partition (dt)</span><br><span class="line">select final.id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       order_id,</span><br><span class="line">       province_id,</span><br><span class="line">       play_time,</span><br><span class="line">       play_date,</span><br><span class="line">       order_time,</span><br><span class="line">       order_date,</span><br><span class="line">       end_date,</span><br><span class="line">       session_id,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount,</span><br><span class="line">       case</span><br><span class="line">		  when end_date is not null then end_date</span><br><span class="line">		  when order_date is not null then order_date</span><br><span class="line">           else &#x27;9999-12-31&#x27; end dt</span><br><span class="line">from (select play.id,</span><br><span class="line">             play.user_id,</span><br><span class="line">             play.course_id,</span><br><span class="line">             od.order_id,</span><br><span class="line">             oi.province_id,</span><br><span class="line">             play.play_time,</span><br><span class="line">             play.play_date,</span><br><span class="line">             od.order_time,</span><br><span class="line">             od.order_date,</span><br><span class="line">             if(od.order_date is null and</span><br><span class="line">                date_add(play.play_date, 7) &lt;= &#x27;2022-02-21&#x27;,</span><br><span class="line">                date_add(play.play_date, 7), null) end_date,</span><br><span class="line">             oi.session_id,</span><br><span class="line">             od.origin_amount,</span><br><span class="line">             od.coupon_reduce,</span><br><span class="line">             od.final_amount</span><br><span class="line">      from play</span><br><span class="line">               left join od on play.user_id = od.user_id and play.course_id = od.course_id</span><br><span class="line">               left join oi on od.order_id = oi.id</span><br><span class="line">where od.order_time is null</span><br><span class="line">or od.order_time &gt; play.play_time</span><br><span class="line">) final</span><br><span class="line">         left join dim_course on course_id = dim_course.id;</span><br></pre></td></tr></table></figure>

<p>每日数据装载（不运行）：</p>
<p><img src="Snipaste_2023-11-21_16-46-19.png" alt="Snipaste_2023-11-21_16-46-19"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">with play as</span><br><span class="line">         (select id,</span><br><span class="line">                 user_id,</span><br><span class="line">                 course_id,</span><br><span class="line">                 play_time,</span><br><span class="line">                 play_date</span><br><span class="line">          from edu.dwd_trade_course_order_inc</span><br><span class="line">          where dt = &#x27;9999-12-31&#x27;</span><br><span class="line">          union</span><br><span class="line">          select min(id)                                     id,</span><br><span class="line">                 user_id,</span><br><span class="line">                 course_id,</span><br><span class="line">                 min(create_time),</span><br><span class="line">                 date_format(min(create_time), &#x27;yyyy-MM-dd&#x27;) play_date</span><br><span class="line">          from edu.ods_user_chapter_process_full</span><br><span class="line">          where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">          group by user_id, course_id</span><br><span class="line">          having date_format(min(create_time), &#x27;yyyy-MM-dd&#x27;) = &#x27;2022-02-22&#x27;),</span><br><span class="line">     oi as</span><br><span class="line">         (</span><br><span class="line">             select data.id,</span><br><span class="line">                    data.province_id,</span><br><span class="line">                    data.session_id,</span><br><span class="line">                    data.create_time order_time</span><br><span class="line">             from edu.ods_order_info_inc</span><br><span class="line">             where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">               and type = &#x27;insert&#x27;</span><br><span class="line">         ),</span><br><span class="line">     od as</span><br><span class="line">         (</span><br><span class="line">             select data.id,</span><br><span class="line">                    data.course_id,</span><br><span class="line">                    data.order_id,</span><br><span class="line">                    data.user_id,</span><br><span class="line">                    data.origin_amount,</span><br><span class="line">                    data.coupon_reduce,</span><br><span class="line">                    data.final_amount,</span><br><span class="line">                    date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) order_date</span><br><span class="line">             from edu.ods_order_detail_inc</span><br><span class="line">             where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">               and type = &#x27;insert&#x27;</span><br><span class="line">         ),</span><br><span class="line">     dim_course as</span><br><span class="line">         (</span><br><span class="line">             select id,</span><br><span class="line">                    course_name,</span><br><span class="line">                    category_id,</span><br><span class="line">                    category_name,</span><br><span class="line">                    subject_id,</span><br><span class="line">                    subject_name</span><br><span class="line">             from edu.dim_course_full</span><br><span class="line">             where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">         )</span><br><span class="line">insert overwrite table edu.dwd_trade_course_order_inc</span><br><span class="line">partition (dt)</span><br><span class="line">select final.id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       order_id,</span><br><span class="line">       province_id,</span><br><span class="line">       play_time,</span><br><span class="line">       play_date,</span><br><span class="line">       order_time,</span><br><span class="line">       order_date,</span><br><span class="line">       end_date,</span><br><span class="line">       session_id,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount,</span><br><span class="line">       case</span><br><span class="line">           when end_date is not null then end_date</span><br><span class="line">           when order_date is not null then order_date</span><br><span class="line">           else &#x27;9999-12-31&#x27; end dt</span><br><span class="line">from (select play.id,</span><br><span class="line">             play.user_id,</span><br><span class="line">             play.course_id,</span><br><span class="line">             od.order_id,</span><br><span class="line">             oi.province_id,</span><br><span class="line">             play.play_time,</span><br><span class="line">             play.play_date,</span><br><span class="line">             oi.order_time,</span><br><span class="line">             od.order_date,</span><br><span class="line">             if(order_date is null and date_add(play_date, 7) = &#x27;2022-02-22&#x27;, &#x27;2022-02-22&#x27;, null) end_date,</span><br><span class="line">             oi.session_id,</span><br><span class="line">             od.origin_amount,</span><br><span class="line">             od.coupon_reduce,</span><br><span class="line">             od.final_amount</span><br><span class="line">      from play</span><br><span class="line">               left join od on play.user_id = od.user_id and play.course_id = od.course_id</span><br><span class="line">               left join oi on od.order_id = oi.id</span><br><span class="line">where order_time is null </span><br><span class="line">or order_time &gt; play_time </span><br><span class="line">) final</span><br><span class="line">         left join dim_course on course_id = dim_course.id;</span><br></pre></td></tr></table></figure>

<h3 id="4-交易域下单事务事实表"><a href="#4-交易域下单事务事实表" class="headerlink" title="4. 交易域下单事务事实表"></a>4. 交易域下单事务事实表</h3><p><img src="Snipaste_2023-11-16_15-37-59.png" alt="Snipaste_2023-11-16_15-37-59"></p>
<p>主要字段来源自表ods_order_detail_inc，通过关联表ods_order_info_inc获取用户维度、地区维度，通过关联ods_log_inc来获取来源维度。（可以理解为，事实表的建立实际上就是看业务总线矩阵，看与哪些维度关联，找出关联的维度所在的ods表或dim表，将这些ods表和dim表（一张或多张）关联起来）</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_trade_order_detail_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_trade_order_detail_inc</span><br><span class="line">(</span><br><span class="line">    `id`                   STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `order_id`             STRING COMMENT &#x27;订单id&#x27;,</span><br><span class="line">    `user_id`              STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `course_id`            STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `course_name`          STRING COMMENT &#x27;课程名称&#x27;,</span><br><span class="line">    `category_id`          STRING COMMENT &#x27;分类id&#x27;,</span><br><span class="line">    `category_name`        STRING COMMENT &#x27;分类名称&#x27;,</span><br><span class="line">    `subject_id`           STRING COMMENT &#x27;科目id&#x27;,</span><br><span class="line">    `subject_name`         STRING COMMENT &#x27;科目名称&#x27;,</span><br><span class="line">    `province_id`          STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `date_id`              STRING COMMENT &#x27;下单日期id&#x27;,</span><br><span class="line">    `session_id`           STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `source_id`            STRING COMMENT &#x27;来源id&#x27;,</span><br><span class="line">    `create_time`          STRING COMMENT &#x27;下单时间&#x27;,</span><br><span class="line">    `original_amount`      DECIMAL(16, 2) COMMENT &#x27;原始金额分摊&#x27;,</span><br><span class="line">    `coupon_reduce_amount` DECIMAL(16, 2) COMMENT &#x27;优惠金额分摊&#x27;,</span><br><span class="line">    `final_amount`         DECIMAL(16, 2) COMMENT &#x27;最终价格分摊&#x27;,</span><br><span class="line">    `out_trade_no`         STRING COMMENT &#x27;订单交易编号&#x27;,</span><br><span class="line">    `trade_body`           STRING COMMENT &#x27;订单描述&#x27;</span><br><span class="line">) COMMENT &#x27;交易域下单事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_trade_order_detail_inc/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载：</p>
<p>筛选每个表中type为bootstrap-insert的数据，将表ods_order_detail_inc，ods_order_info_inc，ods_log_inc进行关联操作，获取对应的维度id字段，使用动态分区功能，根据插入数据的最后一个字段值（create_time）进行分区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table edu.dwd_trade_order_detail_inc</span><br><span class="line">    partition (dt)</span><br><span class="line">select odt.id,</span><br><span class="line">       order_id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       province_id,</span><br><span class="line">       date_id,</span><br><span class="line">       session_id,</span><br><span class="line">       source_id,</span><br><span class="line">       create_time,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount,</span><br><span class="line">       out_trade_no,</span><br><span class="line">       trade_body,</span><br><span class="line">       date_id</span><br><span class="line">from (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.user_id,</span><br><span class="line">                data.course_id,</span><br><span class="line">                date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">                data.create_time,</span><br><span class="line">                data.origin_amount,</span><br><span class="line">                data.coupon_reduce,</span><br><span class="line">                data.final_amount</span><br><span class="line">         from edu.ods_order_detail_inc</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">           and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">     ) odt</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.province_id,</span><br><span class="line">                data.out_trade_no,</span><br><span class="line">                data.session_id,</span><br><span class="line">                data.trade_body</span><br><span class="line">         from edu.ods_order_info_inc</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">           and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">     ) od</span><br><span class="line">     on odt.order_id = od.id</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select distinct common.sid,</span><br><span class="line">                         common.sc source_id</span><br><span class="line">         from edu.ods_log_inc oli</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">     ) log</span><br><span class="line">     on od.session_id = log.sid</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                course_name,</span><br><span class="line">                category_id,</span><br><span class="line">                category_name,</span><br><span class="line">                subject_id,</span><br><span class="line">                subject_name</span><br><span class="line">         from edu.dim_course_full</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">     ) dim_course</span><br><span class="line">     on course_id = dim_course.id;</span><br></pre></td></tr></table></figure>

<p>每日数据装载（不运行）：</p>
<p>筛选每个表中type为insert的字段，将表ods_order_detail_inc，ods_order_info_inc，ods_log_inc进行关联操作，获取对应的维度id字段，首日的全量下单记录经过处理后，再放入数据的create_time字段对应的日期分区中，每日的增量下单记录则放入当日分区中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_trade_order_detail_inc </span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select odt.id,</span><br><span class="line">       order_id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       province_id,</span><br><span class="line">       date_id,</span><br><span class="line">       session_id,</span><br><span class="line">       source_id,</span><br><span class="line">       create_time,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount,</span><br><span class="line">       out_trade_no,</span><br><span class="line">       trade_body</span><br><span class="line">from (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.user_id,</span><br><span class="line">                data.course_id,</span><br><span class="line">                date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">                data.create_time,</span><br><span class="line">                data.origin_amount,</span><br><span class="line">                data.coupon_reduce,</span><br><span class="line">                data.final_amount</span><br><span class="line">         from edu.ods_order_detail_inc</span><br><span class="line">         where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">           and type = &#x27;insert&#x27;</span><br><span class="line">     ) odt</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.province_id,</span><br><span class="line">                data.session_id,</span><br><span class="line">                data.out_trade_no,</span><br><span class="line">                data.trade_body</span><br><span class="line">         from edu.ods_order_info_inc</span><br><span class="line">         where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">           and type = &#x27;insert&#x27;</span><br><span class="line">     ) od</span><br><span class="line">     on odt.order_id = od.id</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select distinct common.sid,</span><br><span class="line">                         common.sc source_id</span><br><span class="line">         from edu.ods_log_inc oli</span><br><span class="line">         where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">     ) log</span><br><span class="line">     on od.session_id = log.sid</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                course_name,</span><br><span class="line">                category_id,</span><br><span class="line">                category_name,</span><br><span class="line">                subject_id,</span><br><span class="line">                subject_name</span><br><span class="line">         from edu.dim_course_full</span><br><span class="line">         where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">     ) dim_course</span><br><span class="line">     on course_id = dim_course.id;</span><br></pre></td></tr></table></figure>

<h3 id="5-交易域支付成功事务事实表"><a href="#5-交易域支付成功事务事实表" class="headerlink" title="5. 交易域支付成功事务事实表"></a>5. 交易域支付成功事务事实表</h3><p>主要字段来源于表ids_order_detail_inc和表ods_payment_info_inc，再通过表ods_order_info_inc关联获取地区维度</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_trade_pay_detail_suc_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_trade_pay_detail_suc_inc</span><br><span class="line">(</span><br><span class="line">    `id`                   STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `order_id`             STRING COMMENT &#x27;订单id&#x27;,</span><br><span class="line">    `user_id`              STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `course_id`            STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `province_id`          STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `date_id`              STRING COMMENT &#x27;支付日期id&#x27;,</span><br><span class="line">    `alipay_trade_no`      STRING COMMENT &#x27;支付宝交易编号&#x27;,</span><br><span class="line">    `trade_body`           STRING COMMENT &#x27;交易内容&#x27;,</span><br><span class="line">    `payment_type`         STRING COMMENT &#x27;支付类型名称&#x27;,</span><br><span class="line">    `payment_status`       STRING COMMENT &#x27;支付状态&#x27;,</span><br><span class="line">    `callback_time`        STRING COMMENT &#x27;支付成功时间&#x27;,</span><br><span class="line">    `callback_content`     STRING COMMENT &#x27;回调信息&#x27;,</span><br><span class="line">    `original_amount`      DECIMAL(16, 2) COMMENT &#x27;原始支付金额分摊&#x27;,</span><br><span class="line">    `coupon_reduce_amount` DECIMAL(16, 2) COMMENT &#x27;优惠支付金额分摊&#x27;,</span><br><span class="line">    `final_amount`         DECIMAL(16, 2) COMMENT &#x27;最终支付金额分摊&#x27;</span><br><span class="line">) COMMENT &#x27;交易域支付成功事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_trade_pay_detail_suc_inc/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table edu.dwd_trade_pay_detail_suc_inc</span><br><span class="line">partition(dt)</span><br><span class="line">select odt.id,</span><br><span class="line">       od.id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       province_id,</span><br><span class="line">       date_format(create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       alipay_trade_no,</span><br><span class="line">       trade_body,</span><br><span class="line">       payment_type,</span><br><span class="line">       payment_status,</span><br><span class="line">       callback_time,</span><br><span class="line">       callback_content,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount,</span><br><span class="line">       date_format(create_time, &#x27;yyyy-MM-dd&#x27;) date_id</span><br><span class="line">from (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.user_id,</span><br><span class="line">                data.course_id,</span><br><span class="line">                data.origin_amount,</span><br><span class="line">                data.coupon_reduce,</span><br><span class="line">                data.final_amount,</span><br><span class="line">                data.create_time</span><br><span class="line">         from edu.ods_order_detail_inc</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27; and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">     ) odt</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.province_id</span><br><span class="line">         from edu.ods_order_info_inc</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27; and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">     ) od</span><br><span class="line">     on odt.order_id = od.id</span><br><span class="line">         join</span><br><span class="line">     (</span><br><span class="line">         select data.alipay_trade_no,</span><br><span class="line">                data.trade_body,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.payment_type,</span><br><span class="line">                data.payment_status,</span><br><span class="line">                data.callback_time,</span><br><span class="line">                data.callback_content</span><br><span class="line">         from edu.ods_payment_info_inc</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27; and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">           and data.callback_time is not null</span><br><span class="line">     ) pi</span><br><span class="line">     on od.id = pi.order_id;</span><br></pre></td></tr></table></figure>

<p>每日数据装载（不运行）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_trade_pay_detail_suc_inc</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select</span><br><span class="line"> odt.id,</span><br><span class="line">       od.id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       province_id,</span><br><span class="line">       date_format(create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       alipay_trade_no,</span><br><span class="line">       trade_body,</span><br><span class="line">       payment_type,</span><br><span class="line">       payment_status,</span><br><span class="line">       callback_time,</span><br><span class="line">       callback_content,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount</span><br><span class="line">from (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.user_id,</span><br><span class="line">                data.course_id,</span><br><span class="line">                data.origin_amount,</span><br><span class="line">                data.coupon_reduce,</span><br><span class="line">                data.final_amount,</span><br><span class="line">                data.create_time</span><br><span class="line">         from edu.ods_order_detail_inc</span><br><span class="line">         where (dt = &#x27;2022-02-22&#x27; or dt = date_add(&#x27;2022-02-22&#x27;, -1))</span><br><span class="line">           and (type = &#x27;insert&#x27; or type = &#x27;bootstrap-insert&#x27;)</span><br><span class="line">     ) odt</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.province_id</span><br><span class="line">         from edu.ods_order_info_inc</span><br><span class="line">         where (dt = &#x27;2022-02-22&#x27; or dt = date_add(&#x27;2022-02-22&#x27;, -1))</span><br><span class="line">           and (type = &#x27;insert&#x27; or type = &#x27;bootstrap-insert&#x27;)</span><br><span class="line">     ) od</span><br><span class="line">     on odt.order_id = od.id</span><br><span class="line">         join</span><br><span class="line">     (</span><br><span class="line">         select data.alipay_trade_no,</span><br><span class="line">                data.trade_body,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.payment_type,</span><br><span class="line">                data.payment_status,</span><br><span class="line">                data.callback_time,</span><br><span class="line">                data.callback_content</span><br><span class="line">         from edu.ods_payment_info_inc</span><br><span class="line">         where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">           and type = &#x27;update&#x27;</span><br><span class="line">           and array_contains(map_keys(old), &#x27;callback_time&#x27;)</span><br><span class="line">     ) pi</span><br><span class="line">     on od.id = pi.order_id;</span><br></pre></td></tr></table></figure>

<h3 id="6-流量域页面浏览事务事实表"><a href="#6-流量域页面浏览事务事实表" class="headerlink" title="6. 流量域页面浏览事务事实表"></a>6. 流量域页面浏览事务事实表</h3><p>页面浏览事务事实表的数据来自用户行为日志，用户行为日志从数据仓库搭建开始收集，不存在首日装载与每日装载的区别，在进行数据装载时，首先需要从表ods_log_inc中过滤page字段部位空的页面浏览日志，然后从中解析出所有的common字段和page字段中的详细信息</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_traffic_page_view_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_traffic_page_view_inc</span><br><span class="line">(</span><br><span class="line">    `mid_id`         STRING COMMENT &#x27;手机唯一编号&#x27;,</span><br><span class="line">    `province_id`    STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `brand`          STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `is_new`         STRING COMMENT &#x27;是否新用户&#x27;,</span><br><span class="line">    `model`          STRING COMMENT &#x27;手机型号&#x27;,</span><br><span class="line">    `os`             STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `session_id`     STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `user_id`        STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `version_code`   STRING COMMENT &#x27;版本号&#x27;,</span><br><span class="line">    `source_id`         STRING COMMENT &#x27;数据来源&#x27;,</span><br><span class="line">    `during_time`    BIGINT COMMENT &#x27;持续时间毫秒&#x27;,</span><br><span class="line">    `page_item`      STRING COMMENT &#x27;目标id &#x27;,</span><br><span class="line">    `page_item_type` STRING COMMENT &#x27;目标类型&#x27;,</span><br><span class="line">    `page_id`        STRING COMMENT &#x27;页面id &#x27;,</span><br><span class="line">    `last_page_id`   STRING COMMENT &#x27;上页类型&#x27;,</span><br><span class="line">    `ts`             STRING COMMENT &#x27;跳入时间&#x27;</span><br><span class="line">)</span><br><span class="line">    COMMENT &#x27;流量域页面浏览事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_traffic_page_view_inc&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据装载:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table edu.dwd_traffic_page_view_inc partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar      province_id,</span><br><span class="line">       common.ba      brand,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md      model,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid     session_id,</span><br><span class="line">       common.uid     user_id,</span><br><span class="line">       common.vc      version_code,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item      page_item,</span><br><span class="line">       page.item_type page_item_type,</span><br><span class="line">       page.page_id,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       ts</span><br><span class="line">from edu.ods_log_inc</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">  and page is not null;</span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>

<h3 id="7-流量域启动事务事实表"><a href="#7-流量域启动事务事实表" class="headerlink" title="7. 流量域启动事务事实表"></a>7. 流量域启动事务事实表</h3><p>流量域启动事务事实表的数据来源自用户行为日志，装载思路，分区设计与页面浏览事实事实表相同</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_traffic_start_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_traffic_start_inc</span><br><span class="line">(</span><br><span class="line">    `mid_id`          STRING COMMENT &#x27;手机唯一编号&#x27;,</span><br><span class="line">    `province_id`     STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `brand`           STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `is_new`          STRING COMMENT &#x27;是否新用户&#x27;,</span><br><span class="line">    `model`           STRING COMMENT &#x27;手机型号&#x27;,</span><br><span class="line">    `os`              STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `session_id`      STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `user_id`         STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `version_code`    STRING COMMENT &#x27;版本号&#x27;,</span><br><span class="line">    `source_id`          STRING COMMENT &#x27;数据来源&#x27;,</span><br><span class="line">    `entry`           STRING COMMENT &#x27;icon手机图标 notice 通知&#x27;,</span><br><span class="line">    `open_ad_id`      STRING COMMENT &#x27;广告页id&#x27;,</span><br><span class="line">    `first_open`      STRING COMMENT &#x27;是否首次启动&#x27;,</span><br><span class="line">    `date_id`         STRING COMMENT &#x27;日期id&#x27;,</span><br><span class="line">    `start_time`      STRING COMMENT &#x27;启动时间&#x27;,</span><br><span class="line">    `loading_time_ms` BIGINT COMMENT &#x27;启动加载时间&#x27;,</span><br><span class="line">    `open_ad_ms`      BIGINT COMMENT &#x27;广告总共播放时间&#x27;,</span><br><span class="line">    `open_ad_skip_ms` BIGINT COMMENT &#x27;用户跳过广告时点&#x27;</span><br><span class="line">) COMMENT &#x27;流量域启动事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_traffic_start_inc&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table edu.dwd_traffic_start_inc partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar            province_id,</span><br><span class="line">       common.ba            brand,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md            model,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid           session_id,</span><br><span class="line">       common.uid           user_id,</span><br><span class="line">       common.vc            version_code,</span><br><span class="line">       common.sc,</span><br><span class="line">       `start`.entry,</span><br><span class="line">       `start`.open_ad_id,</span><br><span class="line">       `start`.first_open,</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd HH:mm:ss&#x27;),</span><br><span class="line">       `start`.loading_time loading_time_ms,</span><br><span class="line">       `start`.open_ad_ms,</span><br><span class="line">       `start`.open_ad_skip_ms</span><br><span class="line">from edu.ods_log_inc</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">  and `start` is not null;</span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>

<h3 id="8-流量域动作事务事实表"><a href="#8-流量域动作事务事实表" class="headerlink" title="8. 流量域动作事务事实表"></a>8. 流量域动作事务事实表</h3><p>流量域动作事务事实表的数据来源自用户行为日志，装载思路，分区设计与页面浏览事实事实表相同</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_traffic_action_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_traffic_action_inc</span><br><span class="line">(</span><br><span class="line">    `mid_id`           STRING COMMENT &#x27;手机唯一编号&#x27;,</span><br><span class="line">    `province_id`      STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `brand`            STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `is_new`           STRING COMMENT &#x27;是否新用户&#x27;,</span><br><span class="line">    `model`            STRING COMMENT &#x27;手机型号&#x27;,</span><br><span class="line">    `os`               STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `session_id`       STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `user_id`          STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `version_code`     STRING COMMENT &#x27;版本号&#x27;,</span><br><span class="line">    `source_id`           STRING COMMENT &#x27;数据来源&#x27;,</span><br><span class="line">    `during_time`      BIGINT COMMENT &#x27;持续时间毫秒&#x27;,</span><br><span class="line">    `page_item`        STRING COMMENT &#x27;目标id&#x27;,</span><br><span class="line">    `page_item_type`   STRING COMMENT &#x27;目标类型&#x27;,</span><br><span class="line">    `page_id`          STRING COMMENT &#x27;页面id&#x27;,</span><br><span class="line">    `last_page_id`     STRING COMMENT &#x27;上页类型&#x27;,</span><br><span class="line">    `action_id`        STRING COMMENT &#x27;动作id&#x27;,</span><br><span class="line">    `action_item`      STRING COMMENT &#x27;目标id&#x27;,</span><br><span class="line">    `action_item_type` STRING COMMENT &#x27;目标类型&#x27;,</span><br><span class="line">    `date_id`          STRING COMMENT &#x27;日期id&#x27;,</span><br><span class="line">    `action_time`      STRING COMMENT &#x27;动作发生时间&#x27;</span><br><span class="line">) COMMENT &#x27;流量域动作事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_traffic_action_inc&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table edu.dwd_traffic_action_inc partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar,</span><br><span class="line">       common.ba,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid,</span><br><span class="line">       common.uid,</span><br><span class="line">       common.vc,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item,</span><br><span class="line">       page.item_type,</span><br><span class="line">       page.page_id,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       action.action_id,</span><br><span class="line">       action.item,</span><br><span class="line">       action.item_type,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       action.ts</span><br><span class="line">from edu.ods_log_inc oli lateral view explode(actions) tmp as action</span><br><span class="line">where oli.dt = &#x27;2022-02-21&#x27;</span><br><span class="line">  and actions is not null;</span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>

<h3 id="9-流量域曝光事务事实表"><a href="#9-流量域曝光事务事实表" class="headerlink" title="9. 流量域曝光事务事实表"></a>9. 流量域曝光事务事实表</h3><p>流量域曝光事务事实表的数据来源自用户行为日志，装载思路，分区设计与页面浏览事实事实表相同</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_traffic_display_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_traffic_display_inc</span><br><span class="line">(</span><br><span class="line">    `mid_id`            STRING COMMENT &#x27;手机唯一编号&#x27;,</span><br><span class="line">    `province_id`       STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `brand`             STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `is_new`            STRING COMMENT &#x27;是否新用户&#x27;,</span><br><span class="line">    `model`             STRING COMMENT &#x27;手机型号&#x27;,</span><br><span class="line">    `os`                STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `session_id`        STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `user_id`           STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `version_code`      STRING COMMENT &#x27;版本号&#x27;,</span><br><span class="line">    `source_id`            STRING COMMENT &#x27;数据来源&#x27;,</span><br><span class="line">    `during_time`       BIGINT COMMENT &#x27;页面时间&#x27;,</span><br><span class="line">    `page_item`         STRING COMMENT &#x27;目标id &#x27;,</span><br><span class="line">    `page_item_type`    STRING COMMENT &#x27;目标类型&#x27;,</span><br><span class="line">    `page_id`           STRING COMMENT &#x27;页面id &#x27;,</span><br><span class="line">    `last_page_id`      STRING COMMENT &#x27;上页类型&#x27;,</span><br><span class="line">    `date_id`           STRING COMMENT &#x27;日期id&#x27;,</span><br><span class="line">    `display_time`      STRING COMMENT &#x27;曝光时间&#x27;,</span><br><span class="line">    `display_type`      STRING COMMENT &#x27;曝光类型&#x27;,</span><br><span class="line">    `display_item`      STRING COMMENT &#x27;曝光对象id &#x27;,</span><br><span class="line">    `display_item_type` STRING COMMENT &#x27;app版本号&#x27;,</span><br><span class="line">    `display_order`     BIGINT COMMENT &#x27;曝光顺序&#x27;,</span><br><span class="line">    `display_pos_id`    BIGINT COMMENT &#x27;曝光位置&#x27;</span><br><span class="line">) COMMENT &#x27;流量域曝光事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_traffic_display_inc&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据装载：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table edu.dwd_traffic_display_inc partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar,</span><br><span class="line">       common.ba,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid,</span><br><span class="line">       common.uid,</span><br><span class="line">       common.vc,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item,</span><br><span class="line">       page.item_type,</span><br><span class="line">       page.page_id,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       ts,</span><br><span class="line">       display.display_type,</span><br><span class="line">       display.item,</span><br><span class="line">       display.item_type,</span><br><span class="line">       display.`order`,</span><br><span class="line">       display.pos_id</span><br><span class="line">from edu.ods_log_inc oli lateral view explode(displays) tmp as display</span><br><span class="line">where oli.dt = &#x27;2022-02-21&#x27;</span><br><span class="line">  and displays is not null;</span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>

<h3 id="10-流量域错误事务事实表"><a href="#10-流量域错误事务事实表" class="headerlink" title="10. 流量域错误事务事实表"></a>10. 流量域错误事务事实表</h3><p>流量域错误事务事实表的数据来源自用户行为日志，装载思路，分区设计与页面浏览事实事实表相同</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_traffic_error_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_traffic_error_inc</span><br><span class="line">(</span><br><span class="line">    `mid_id`          STRING COMMENT &#x27;手机唯一编号&#x27;,</span><br><span class="line">    `province_id`     STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `brand`           STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `is_new`          STRING COMMENT &#x27;是否新用户&#x27;,</span><br><span class="line">    `model`           STRING COMMENT &#x27;手机型号&#x27;,</span><br><span class="line">    `os`              STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `session_id`      STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `user_id`         STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `version_code`    STRING COMMENT &#x27;版本号&#x27;,</span><br><span class="line">    `source_id`          STRING COMMENT &#x27;数据来源&#x27;,</span><br><span class="line">    `during_time`     BIGINT COMMENT &#x27;页面时间&#x27;,</span><br><span class="line">    `page_item`       STRING COMMENT &#x27;目标id &#x27;,</span><br><span class="line">    `page_item_type`  STRING COMMENT &#x27;目标类型&#x27;,</span><br><span class="line">    `last_page_id`    STRING COMMENT &#x27;上页类型&#x27;,</span><br><span class="line">    `page_id`         STRING COMMENT &#x27;页面id&#x27;,</span><br><span class="line">    `entry`           STRING COMMENT &#x27;icon手机图标  notice 通知&#x27;,</span><br><span class="line">    `loading_time`    STRING COMMENT &#x27;启动加载时间&#x27;,</span><br><span class="line">    `open_ad_id`      STRING COMMENT &#x27;广告页id&#x27;,</span><br><span class="line">    `open_ad_ms`      STRING COMMENT &#x27;广告总共播放时间&#x27;,</span><br><span class="line">    `open_ad_skip_ms` STRING COMMENT &#x27;用户跳过广告时点&#x27;,</span><br><span class="line">    `actions`         ARRAY&lt;STRUCT&lt;action_id:STRING,item:STRING,item_type:STRING,ts:BIGINT&gt;&gt; COMMENT &#x27;动作信息&#x27;,</span><br><span class="line">    `displays`        ARRAY&lt;STRUCT&lt;display_type :STRING,item :STRING,item_type :STRING,`order` :STRING,pos_id</span><br><span class="line">                                   :STRING&gt;&gt; COMMENT &#x27;曝光信息&#x27;,</span><br><span class="line">    `date_id`         STRING COMMENT &#x27;日期id&#x27;,</span><br><span class="line">    `error_time`      STRING COMMENT &#x27;错误时间&#x27;,</span><br><span class="line">    `error_code`      STRING COMMENT &#x27;错误码&#x27;,</span><br><span class="line">    `error_msg`       STRING COMMENT &#x27;错误信息&#x27;</span><br><span class="line">) COMMENT &#x27;流量域错误事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_traffic_error_inc&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table edu.dwd_traffic_error_inc partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar,</span><br><span class="line">       common.ba,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid,</span><br><span class="line">       common.uid,</span><br><span class="line">       common.vc,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item,</span><br><span class="line">       page.item_type,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       page.page_id,</span><br><span class="line">       `start`.entry,</span><br><span class="line">       `start`.loading_time,</span><br><span class="line">       `start`.open_ad_id,</span><br><span class="line">       `start`.open_ad_ms,</span><br><span class="line">       `start`.open_ad_skip_ms,</span><br><span class="line">       actions,</span><br><span class="line">       displays,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       ts,</span><br><span class="line">       err.error_code,</span><br><span class="line">       err.msg</span><br><span class="line">from edu.ods_log_inc</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">  and err is not null;</span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>

<h3 id="11-互动域收藏事务事实表"><a href="#11-互动域收藏事务事实表" class="headerlink" title="11. 互动域收藏事务事实表"></a>11. 互动域收藏事务事实表</h3><p>该表的所有相关维度和度量都在ODS层的表ods_favor_info_inc中存在，不需要额外获取。首日数据装载与每日数据装载的不同在于，从ODS层表格筛选数据时使用的条件不同，以及数据所使用的分区不同。</p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_interaction_favor_add_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_interaction_favor_add_inc</span><br><span class="line">(</span><br><span class="line">    `id`          STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `user_id`     STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `course_id`   STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `date_id`     STRING COMMENT &#x27;日期id&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;收藏时间&#x27;</span><br><span class="line">) COMMENT &#x27;互动域收藏事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_interaction_favor_add_inc/&#x27;</span><br><span class="line">    TBLPROPERTIES (&quot;orc.compress&quot; = &quot;snappy&quot;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_interaction_favor_add_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.create_time,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;)</span><br><span class="line">from edu.ods_favor_info_inc</span><br><span class="line">where dt=&#x27;2022-02-21&#x27; and type = &#x27;bootstrap-insert&#x27;;</span><br></pre></td></tr></table></figure>

<p>每日数据装载（不运行）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_interaction_favor_add_inc</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.create_time</span><br><span class="line">from edu.ods_favor_info_inc</span><br><span class="line">where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">  and type = &#x27;insert&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="12-互动域章节评价事务事实表"><a href="#12-互动域章节评价事务事实表" class="headerlink" title="12. 互动域章节评价事务事实表"></a>12. 互动域章节评价事务事实表</h3><p>该表的所有相关维度和度量都在ODS层的表ods_comment_info_inc中存在，不需要额外获取。首日数据装载与每日数据装载的不同在于，从ODS层表格筛选数据时使用的条件不同，以及数据所使用的分区不同。</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_interaction_comment_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_interaction_comment_inc</span><br><span class="line">(</span><br><span class="line">    `id`          STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `user_id`     STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `chapter_id`  STRING COMMENT &#x27;章节id&#x27;,</span><br><span class="line">    `course_id`   STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `date_id`     STRING COMMENT &#x27;日期id&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;评价时间&#x27;,</span><br><span class="line">    `comment_txt` STRING COMMENT &#x27;评价内容&#x27;</span><br><span class="line">) COMMENT &#x27;互动域章节评价事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_interaction_comment_inc/&#x27;</span><br><span class="line">    TBLPROPERTIES (&quot;orc.compress&quot; = &quot;snappy&quot;);</span><br></pre></td></tr></table></figure>

<p>首日装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_interaction_comment_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.chapter_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.comment_txt,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;)</span><br><span class="line">from edu.ods_comment_info_inc</span><br><span class="line">where dt=&#x27;2022-02-21&#x27; and type = &#x27;bootstrap-insert&#x27;;</span><br></pre></td></tr></table></figure>

<p>每日装载（不运行）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_interaction_comment_inc</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.chapter_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.comment_txt</span><br><span class="line">from edu.ods_comment_info_inc</span><br><span class="line">where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">  and type = &#x27;insert&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="13-互动域课程评价事务事实表"><a href="#13-互动域课程评价事务事实表" class="headerlink" title="13. 互动域课程评价事务事实表"></a>13. 互动域课程评价事务事实表</h3><p>该表的所有相关维度和度量都在ODS层的表ods_review_info_inc中存在，不需要额外获取。首日数据装载与每日数据装载的不同在于，从ODS层表格筛选数据时使用的条件不同，以及数据所使用的分区不同。</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_interaction_review_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_interaction_review_inc</span><br><span class="line">(</span><br><span class="line">    `id`           STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `user_id`      STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `course_id`    STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `date_id`      STRING COMMENT &#x27;日期id&#x27;,</span><br><span class="line">    `review_txt`   STRING COMMENT &#x27;评论文本&#x27;,</span><br><span class="line">    `review_stars` BIGINT COMMENT &#x27;评级&#x27;,</span><br><span class="line">    `create_time`  STRING COMMENT &#x27;评价时间&#x27;</span><br><span class="line">) COMMENT &#x27;互动域课程评价事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_interaction_review_inc/&#x27;</span><br><span class="line">    TBLPROPERTIES (&quot;orc.compress&quot; = &quot;snappy&quot;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_interaction_review_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.review_txt,</span><br><span class="line">       data.review_stars,</span><br><span class="line">       data.create_time,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;)</span><br><span class="line">from edu.ods_review_info_inc</span><br><span class="line">where dt=&#x27;2022-02-21&#x27; and type = &#x27;bootstrap-insert&#x27;;</span><br></pre></td></tr></table></figure>

<p>每日数据装载（不运行）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_interaction_review_inc</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.review_txt,</span><br><span class="line">       data.review_stars,</span><br><span class="line">       data.create_time</span><br><span class="line">from edu.ods_review_info_inc</span><br><span class="line">where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">  and type = &#x27;insert&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="14-考试域答卷事务事实表"><a href="#14-考试域答卷事务事实表" class="headerlink" title="14. 考试域答卷事务事实表"></a>14. 考试域答卷事务事实表</h3><p>该表的所有相关维度和度量都在ODS层的表ods_test_exam_inc中存在，不需要额外获取。首日数据装载与每日数据装载的不同在于，从ODS层表格筛选数据时使用的条件不同，以及数据所使用的分区不同。</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_examination_test_paper_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_examination_test_paper_inc</span><br><span class="line">(</span><br><span class="line">    `id`           STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `paper_id`     STRING COMMENT &#x27;试卷id&#x27;,</span><br><span class="line">    `user_id`      STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `date_id`      STRING COMMENT &#x27;日期id&#x27;,</span><br><span class="line">    `score`        decimal(16, 2) COMMENT &#x27;分数&#x27;,</span><br><span class="line">    `duration_sec` STRING COMMENT &#x27;所用时长&#x27;,</span><br><span class="line">    `create_time`  STRING COMMENT &#x27;创建时间&#x27;,</span><br><span class="line">    `submit_time`  STRING COMMENT &#x27;提交时间&#x27;,</span><br><span class="line">    `update_time`  STRING COMMENT &#x27;更新时间&#x27;</span><br><span class="line">) COMMENT &#x27;考试域答卷事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_examination_test_paper_inc/&#x27;</span><br><span class="line">    TBLPROPERTIES (&quot;orc.compress&quot; = &quot;snappy&quot;);</span><br></pre></td></tr></table></figure>

<p>首日数据加载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_examination_test_paper_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.paper_id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.score,</span><br><span class="line">       data.duration_sec,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.submit_time,</span><br><span class="line">       data.update_time,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;)</span><br><span class="line">from edu.ods_test_exam_inc</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">  and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">  and data.deleted = &#x27;0&#x27;;</span><br></pre></td></tr></table></figure>

<p>每日数据加载（不运行）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_examination_test_paper_inc</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.paper_id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.score,</span><br><span class="line">       data.duration_sec,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.submit_time,</span><br><span class="line">       data.update_time</span><br><span class="line">from edu.ods_test_exam_inc</span><br><span class="line">where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">  and type = &#x27;insert&#x27;</span><br><span class="line">  and data.deleted = &#x27;0&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="15-考试域答题事务事实表"><a href="#15-考试域答题事务事实表" class="headerlink" title="15. 考试域答题事务事实表"></a>15. 考试域答题事务事实表</h3><p>该表的所有相关维度和度量都在ODS层的表ods_test_exam_question_inc中存在，不需要额外获取。首日数据装载与每日数据装载的不同在于，从ODS层表格筛选数据时使用的条件不同，以及数据所使用的分区不同。</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_examination_test_question_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_examination_test_question_inc</span><br><span class="line">(</span><br><span class="line">    `id`          STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `user_id`     STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `paper_id`    STRING COMMENT &#x27;试卷id&#x27;,</span><br><span class="line">    `question_id` STRING COMMENT &#x27;题目id&#x27;,</span><br><span class="line">    `date_id`     STRING COMMENT &#x27;日期id&#x27;,</span><br><span class="line">    `answer`      STRING COMMENT &#x27;答案&#x27;,</span><br><span class="line">    `is_correct`  STRING COMMENT &#x27;是否正确&#x27;,</span><br><span class="line">    `score`       DECIMAL(16, 2) COMMENT &#x27;分数&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;开始时间&#x27;,</span><br><span class="line">    `update_time` STRING COMMENT &#x27;更新时间&#x27;</span><br><span class="line">) COMMENT &#x27;考试域答题事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_examination_test_question_inc/&#x27;</span><br><span class="line">    TBLPROPERTIES (&quot;orc.compress&quot; = &quot;snappy&quot;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_examination_test_question_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.paper_id,</span><br><span class="line">       data.question_id,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.answer,</span><br><span class="line">       data.is_correct,</span><br><span class="line">       data.score,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.update_time,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;)</span><br><span class="line">from edu.ods_test_exam_question_inc</span><br><span class="line">where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">  and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">  and data.deleted = &#x27;0&#x27;;</span><br></pre></td></tr></table></figure>

<p>每日数据装载（不运行）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dwd_examination_test_question_inc</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.paper_id,</span><br><span class="line">       data.question_id,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.answer,</span><br><span class="line">       data.is_correct,</span><br><span class="line">       data.score,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.update_time</span><br><span class="line">from edu.ods_test_exam_question_inc</span><br><span class="line">where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">  and type = &#x27;insert&#x27;</span><br><span class="line">  and data.deleted = &#x27;0&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="16-学习域播放周期快照事实表"><a href="#16-学习域播放周期快照事实表" class="headerlink" title="16. 学习域播放周期快照事实表"></a>16. 学习域播放周期快照事实表</h3><p>用户每一次播放视频的行为，都会产生一条播放日志。我们希望通过播放周期快照事实表记录用户对一个视频的播放进度。在这个表中主要记录了用户播放该视频的当前进度、历史最大播放进度、累计播放时长和完播日期等字段。</p>
<p>当用户播放一个视频的累积播放时长达到了此视频总时长的90%，或者播放此视频的最大播放进度达到了视频总时长的90%，都可以认为视频完播，前者称之为累计播放时长完播，达成的时间为首次累计时长完播日期，后者称为进度完播，达成的时间为进度首次完播时间。在播放周期快照事实表中，只有当累积时长和进度条都达到完播要求时，才算视频正式完播，并取两个要求达成时间的最大值为首次完播时间。</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_learn_play_stats_full;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_learn_play_stats_full</span><br><span class="line">(</span><br><span class="line"></span><br><span class="line">    `user_id`                     STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `video_id`                    STRING COMMENT &#x27;视频id&#x27;,</span><br><span class="line">    `video_name`                  STRING COMMENT &#x27;视频名称&#x27;,</span><br><span class="line">    `chapter_id`                  STRING COMMENT &#x27;章节id&#x27;,</span><br><span class="line">    `chapter_name`                STRING COMMENT &#x27;章节名称&#x27;,</span><br><span class="line">    `course_id`                   STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `total_play_sec`              BIGINT COMMENT &#x27;累计播放时长&#x27;,</span><br><span class="line">    `position_sec`                BIGINT COMMENT &#x27;当前播放进度&#x27;,</span><br><span class="line">    `max_position_sec`            BIGINT COMMENT &#x27;历史最大播放进度&#x27;,</span><br><span class="line">    `first_sec_complete_date`     STRING COMMENT &#x27;首次累计时长完播日期&#x27;,</span><br><span class="line">    `first_process_complete_date` STRING COMMENT &#x27;进度首次完播日期&#x27;,</span><br><span class="line">    `first_complete_date`         STRING COMMENT &#x27;首次完播日期&#x27;</span><br><span class="line">) COMMENT &#x27;学习域播放周期快照事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_learn_play_stats_full&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日装载语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable=false;</span><br><span class="line">with curpos as (</span><br><span class="line">    select user_id,</span><br><span class="line">           video_id,</span><br><span class="line">           position_sec</span><br><span class="line">    from (select common.uid            user_id,</span><br><span class="line">                 appvideo.video_id,</span><br><span class="line">                 appvideo.position_sec,</span><br><span class="line">                 row_number() over (partition by common.uid, appvideo.video_id</span><br><span class="line">                     order by ts desc) rk</span><br><span class="line">          from edu.ods_log_inc</span><br><span class="line">          where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">and appvideo is not null) origin</span><br><span class="line">    where rk = 1</span><br><span class="line">),</span><br><span class="line">     aggred as (</span><br><span class="line">         select common.uid                 user_id,</span><br><span class="line">                appvideo.video_id,</span><br><span class="line">                sum(appvideo.play_sec)     total_play_sec,</span><br><span class="line">                max(appvideo.position_sec) max_position_sec</span><br><span class="line">         from edu.ods_log_inc</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">         and appvideo is not null</span><br><span class="line">         group by common.uid, appvideo.video_id</span><br><span class="line">     ),</span><br><span class="line">     dim_video as (</span><br><span class="line">         select id,</span><br><span class="line">                video_name,</span><br><span class="line">                chapter_id,</span><br><span class="line">                chapter_name,</span><br><span class="line">                course_id,</span><br><span class="line">                during_sec</span><br><span class="line">         from edu.dim_video_full</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">     )</span><br><span class="line">insert overwrite table edu.dwd_learn_play_stats_full partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select calculated.user_id,</span><br><span class="line">       calculated.video_id,</span><br><span class="line">       video_name,</span><br><span class="line">       chapter_id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       course_id,</span><br><span class="line">       total_play_sec,</span><br><span class="line">       position_sec,</span><br><span class="line">       max_position_sec,</span><br><span class="line">       first_sec_complete_date,</span><br><span class="line">       first_process_complete_date,</span><br><span class="line">       first_complete_date</span><br><span class="line">from (select user_id,</span><br><span class="line">             video_id,</span><br><span class="line">             video_name,</span><br><span class="line">             chapter_id,</span><br><span class="line">             chapter_name,</span><br><span class="line">             course_id,</span><br><span class="line">             total_play_sec,</span><br><span class="line">             max_position_sec,</span><br><span class="line">             if(total_play_sec / during_sec &gt;= 0.9, &#x27;2022-02-21&#x27;, null)   first_sec_complete_date,</span><br><span class="line">             if(max_position_sec / during_sec &gt;= 0.9, &#x27;2022-02-21&#x27;, null) first_process_complete_date,</span><br><span class="line">             if(total_play_sec / during_sec &gt;= 0.9 and</span><br><span class="line">                max_position_sec / during_sec &gt;= 0.9, &#x27;2022-02-21&#x27;, null) first_complete_date</span><br><span class="line">      from (select user_id,</span><br><span class="line">                   video_id,</span><br><span class="line">                   video_name,</span><br><span class="line">                   chapter_id,</span><br><span class="line">                   chapter_name,</span><br><span class="line">                   course_id,</span><br><span class="line">                   total_play_sec,</span><br><span class="line">                   max_position_sec,</span><br><span class="line">                   during_sec</span><br><span class="line">            from aggred</span><br><span class="line">                     left join dim_video</span><br><span class="line">                               on aggred.video_id = dim_video.id</span><br><span class="line">           ) joined) calculated</span><br><span class="line">         left join curpos</span><br><span class="line">                   on calculated.user_id = curpos.user_id</span><br><span class="line">                       and calculated.video_id = curpos.video_id;</span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>

<p>每日装载语句（不运行）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable=false;</span><br><span class="line">with curpos as (</span><br><span class="line">    select user_id,</span><br><span class="line">           video_id,</span><br><span class="line">           position_sec</span><br><span class="line">    from (select common.uid            user_id,</span><br><span class="line">                 appvideo.video_id,</span><br><span class="line">                 appvideo.position_sec,</span><br><span class="line">                 row_number() over (partition by common.uid, appvideo.video_id</span><br><span class="line">                     order by ts desc) rk</span><br><span class="line">          from edu.ods_log_inc</span><br><span class="line">          where appvideo is not null</span><br><span class="line">            and dt = &#x27;2022-02-22&#x27;) origin</span><br><span class="line">    where rk = 1</span><br><span class="line">),</span><br><span class="line">     aggred as (</span><br><span class="line">         select common.uid                 user_id,</span><br><span class="line">                appvideo.video_id,</span><br><span class="line">                sum(appvideo.play_sec)     total_play_sec,</span><br><span class="line">                max(appvideo.position_sec) max_position_sec</span><br><span class="line">         from edu.ods_log_inc</span><br><span class="line">         where appvideo is not null</span><br><span class="line">           and dt = &#x27;2022-02-22&#x27;</span><br><span class="line">         group by common.uid, appvideo.video_id</span><br><span class="line">     ),</span><br><span class="line">     dim_video as (</span><br><span class="line">         select id,</span><br><span class="line">                video_name,</span><br><span class="line">                chapter_id,</span><br><span class="line">                chapter_name,</span><br><span class="line">                course_id,</span><br><span class="line">                during_sec</span><br><span class="line">         from edu.dim_video_full</span><br><span class="line">         where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">     )</span><br><span class="line">insert overwrite table edu.dwd_learn_play_stats_full partition(dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       video_id,</span><br><span class="line">       video_name,</span><br><span class="line">       chapter_id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       course_id,</span><br><span class="line">       total_play_sec,</span><br><span class="line">       position_sec,</span><br><span class="line">       max_position_sec,</span><br><span class="line">       nvl(first_sec_complete_date,</span><br><span class="line">           if(total_play_sec / during_sec &gt;= 0.9, &#x27;2022-02-22&#x27;, null))   first_sec_complete_date,</span><br><span class="line">       nvl(first_process_complete_date,</span><br><span class="line">           if(max_position_sec / during_sec &gt;= 0.9, &#x27;2022-02-22&#x27;, null)) first_process_complete_date,</span><br><span class="line">       nvl(first_complete_date,</span><br><span class="line">           if(total_play_sec / during_sec &gt;= 0.9 and</span><br><span class="line">              max_position_sec / during_sec &gt;= 0.9, &#x27;2022-02-22&#x27;, null)) first_complete_date</span><br><span class="line">from (</span><br><span class="line">         select nvl(new.user_id, old.user_id)                             user_id,</span><br><span class="line">                nvl(new.video_id, old.video_id)                           video_id,</span><br><span class="line">                nvl(new.video_name, old.video_name)                       video_name,</span><br><span class="line">                nvl(new.chapter_id, old.chapter_id)                       chapter_id,</span><br><span class="line">                nvl(new.chapter_name, old.chapter_name)                   chapter_name,</span><br><span class="line">                nvl(new.course_id, old.course_id)                         course_id,</span><br><span class="line">                nvl(new.total_play_sec, 0L) + nvl(old.total_play_sec, 0L) total_play_sec,</span><br><span class="line">                nvl(new.position_sec, old.position_sec)                   position_sec,</span><br><span class="line">                if(new.max_position_sec is null, old.max_position_sec,</span><br><span class="line">                   if(old.max_position_sec is null, new.max_position_sec,</span><br><span class="line">                      if(new.max_position_sec &gt; old.max_position_sec,</span><br><span class="line">                         new.max_position_sec, old.max_position_sec)))    max_position_sec,</span><br><span class="line">                old.first_sec_complete_date,</span><br><span class="line">                old.first_process_complete_date,</span><br><span class="line">                old.first_complete_date,</span><br><span class="line">                during_sec</span><br><span class="line">         from (select calculated.user_id  user_id,</span><br><span class="line">                      calculated.video_id video_id,</span><br><span class="line">                      video_name,</span><br><span class="line">                      chapter_id,</span><br><span class="line">                      chapter_name,</span><br><span class="line">                      course_id,</span><br><span class="line">                      total_play_sec,</span><br><span class="line">                      position_sec,</span><br><span class="line">                      max_position_sec,</span><br><span class="line">                      during_sec</span><br><span class="line">               from (select user_id,</span><br><span class="line">                            video_id,</span><br><span class="line">                            video_name,</span><br><span class="line">                            chapter_id,</span><br><span class="line">                            chapter_name,</span><br><span class="line">                            course_id,</span><br><span class="line">                            total_play_sec,</span><br><span class="line">                            max_position_sec,</span><br><span class="line">                            during_sec</span><br><span class="line">                     from (select user_id,</span><br><span class="line">                                  video_id,</span><br><span class="line">                                  video_name,</span><br><span class="line">                                  chapter_id,</span><br><span class="line">                                  chapter_name,</span><br><span class="line">                                  course_id,</span><br><span class="line">                                  total_play_sec,</span><br><span class="line">                                  max_position_sec,</span><br><span class="line">                                  during_sec</span><br><span class="line">                           from aggred</span><br><span class="line">                                    left join dim_video</span><br><span class="line">                                              on aggred.video_id = dim_video.id</span><br><span class="line">                          ) joined) calculated</span><br><span class="line">                        left join curpos</span><br><span class="line">                                  on calculated.user_id = curpos.user_id</span><br><span class="line">                                      and calculated.video_id = curpos.video_id) new</span><br><span class="line">                  full outer join</span><br><span class="line">              (</span><br><span class="line">                  select user_id,</span><br><span class="line">                         video_id,</span><br><span class="line">                         video_name,</span><br><span class="line">                         chapter_id,</span><br><span class="line">                         chapter_name,</span><br><span class="line">                         course_id,</span><br><span class="line">                         total_play_sec,</span><br><span class="line">                         position_sec,</span><br><span class="line">                         max_position_sec,</span><br><span class="line">                         first_sec_complete_date,</span><br><span class="line">                         first_process_complete_date,</span><br><span class="line">                         first_complete_date</span><br><span class="line">                  from edu.dwd_learn_play_stats_full</span><br><span class="line">                  where dt = date_add(&#x27;2022-02-22&#x27;, -1)</span><br><span class="line">              ) old</span><br><span class="line">              on new.user_id = old.user_id</span><br><span class="line">                  and new.video_id = old.video_id</span><br><span class="line">     ) final;</span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>

<h3 id="17-学习域播放事务事实表"><a href="#17-学习域播放事务事实表" class="headerlink" title="17. 学习域播放事务事实表"></a>17. 学习域播放事务事实表</h3><p>该表的主要字段均来自ods_log_inc。该表的装载思路、时间戳转换、字段获取、分区设计均与流量域页面浏览事务事实表相同</p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_learn_play_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_learn_play_inc</span><br><span class="line">(</span><br><span class="line">    `mid_id`       STRING COMMENT &#x27;手机唯一编号&#x27;,</span><br><span class="line">    `province_id`  STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `brand`        STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `is_new`       STRING COMMENT &#x27;是否新用户&#x27;,</span><br><span class="line">    `model`        STRING COMMENT &#x27;手机型号&#x27;,</span><br><span class="line">    `os`           STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `session_id`   STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `user_id`      STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `version_code` STRING COMMENT &#x27;版本号&#x27;,</span><br><span class="line">    `source_id`    STRING COMMENT &#x27;数据来源&#x27;,</span><br><span class="line">    `video_id`     STRING COMMENT &#x27;视频id&#x27;,</span><br><span class="line">    `video_name`   STRING COMMENT &#x27;视频名称&#x27;,</span><br><span class="line">    `chapter_id`   STRING COMMENT &#x27;章节id&#x27;,</span><br><span class="line">    `chapter_name` STRING COMMENT &#x27;章节名称&#x27;,</span><br><span class="line">    `course_id`    STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">	`course_name`    STRING COMMENT &#x27;课程名称&#x27;,</span><br><span class="line">    `play_sec`     BIGINT COMMENT &#x27;播放时长&#x27;,</span><br><span class="line">    `ts`           BIGINT COMMENT &#x27;跳入时间&#x27;</span><br><span class="line">) COMMENT &#x27;学习域播放事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_learn_play_inc&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table edu.dwd_learn_play_inc</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select mid,</span><br><span class="line">       province_id,</span><br><span class="line">       brand,</span><br><span class="line">       is_new,</span><br><span class="line">       model,</span><br><span class="line">       os,</span><br><span class="line">       session_id,</span><br><span class="line">       user_id,</span><br><span class="line">       version_code,</span><br><span class="line">       sc,</span><br><span class="line">       video_id,</span><br><span class="line">       video_name,</span><br><span class="line">       chapter_id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       play_sec,</span><br><span class="line">       ts</span><br><span class="line">from (select common.mid,</span><br><span class="line">             common.ar              province_id,</span><br><span class="line">             common.ba              brand,</span><br><span class="line">             common.is_new,</span><br><span class="line">             common.md              model,</span><br><span class="line">             common.os,</span><br><span class="line">             common.sid             session_id,</span><br><span class="line">             common.uid             user_id,</span><br><span class="line">             common.vc              version_code,</span><br><span class="line">             common.sc,</span><br><span class="line">             appvideo.video_id,</span><br><span class="line">             sum(appvideo.play_sec) play_sec,</span><br><span class="line">             max(ts)                ts</span><br><span class="line">      from edu.ods_log_inc</span><br><span class="line">      where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">        and appvideo is not null</span><br><span class="line">      group by common.mid,</span><br><span class="line">               common.ar,</span><br><span class="line">               common.ba,</span><br><span class="line">               common.is_new,</span><br><span class="line">               common.md,</span><br><span class="line">               common.os,</span><br><span class="line">               common.sid,</span><br><span class="line">               common.uid,</span><br><span class="line">               common.vc,</span><br><span class="line">               common.sc,</span><br><span class="line">               appvideo.video_id) aggred</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             video_name,</span><br><span class="line">             chapter_id,</span><br><span class="line">             chapter_name,</span><br><span class="line">             course_id</span><br><span class="line">      from edu.dim_video_full</span><br><span class="line">      where dt = &#x27;2022-02-21&#x27;) dim_video</span><br><span class="line">     on aggred.video_id = dim_video.id</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             course_name</span><br><span class="line">      from edu.dim_course_full</span><br><span class="line">      where dt = &#x27;2022-02-21&#x27;) dim_course</span><br><span class="line">     on course_id = dim_course.id;</span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>

<h3 id="18-用户域用户注册事务事实表"><a href="#18-用户域用户注册事务事实表" class="headerlink" title="18. 用户域用户注册事务事实表"></a>18. 用户域用户注册事务事实表</h3><p>将当日的创建用户与所有的启动页面日志数据进行关联，可以获取时间、用户和设备等维度信息。</p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_user_register_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_user_register_inc</span><br><span class="line">(</span><br><span class="line">    `user_id`        STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `register_time`  STRING COMMENT &#x27;注册时间&#x27;,</span><br><span class="line">    `register_date`  STRING COMMENT &#x27;注册日期&#x27;,</span><br><span class="line">    `province_id`    STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `version_code`   STRING COMMENT &#x27;应用版本&#x27;,</span><br><span class="line">    `mid_id`         STRING COMMENT &#x27;设备id&#x27;,</span><br><span class="line">    `brand`          STRING COMMENT &#x27;设备品牌&#x27;,</span><br><span class="line">    `model`          STRING COMMENT &#x27;设备型号&#x27;,</span><br><span class="line">    `operate_system` STRING COMMENT &#x27;设备操作系统&#x27;</span><br><span class="line">) COMMENT &#x27;用户域用户注册事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_user_register_inc/&#x27;</span><br><span class="line">    TBLPROPERTIES (&quot;orc.compress&quot; = &quot;snappy&quot;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table edu.dwd_user_register_inc</span><br><span class="line">select register.id,</span><br><span class="line">       register_time,</span><br><span class="line">       register_date,</span><br><span class="line">       province_id,</span><br><span class="line">       version_code,</span><br><span class="line">       mid_id,</span><br><span class="line">       brand,</span><br><span class="line">       model,</span><br><span class="line">       operate_system,</span><br><span class="line">       register_date</span><br><span class="line">from (</span><br><span class="line">select data.id,</span><br><span class="line">             data.create_time register_time,</span><br><span class="line">             date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) register_date</span><br><span class="line">      from edu.ods_user_info_inc</span><br><span class="line">      where type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">        and dt = &#x27;2022-02-21&#x27;</span><br><span class="line">) register</span><br><span class="line">         left join</span><br><span class="line">     (select common.uid      user_id,</span><br><span class="line">             min(common.ar)  province_id,</span><br><span class="line">             min(common.vc)  version_code,</span><br><span class="line">             min(common.mid) mid_id,</span><br><span class="line">             min(common.ba)  brand,</span><br><span class="line">             min(common.md)  model,</span><br><span class="line">             min(common.os)  operate_system</span><br><span class="line">      from edu.ods_log_inc</span><br><span class="line">      where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">        and `start` is not null</span><br><span class="line">      group by common.uid) log_dim</span><br><span class="line">     on register.id = log_dim.user_id;</span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>

<p>每日数据装载（不运行）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table edu.dwd_user_register_inc partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select </span><br><span class="line">	register.id,</span><br><span class="line">	register_time,</span><br><span class="line">	register_date,</span><br><span class="line">	province_id,</span><br><span class="line">	version_code,</span><br><span class="line">	mid_id,</span><br><span class="line">	brand,</span><br><span class="line">	model,</span><br><span class="line">	operate_system</span><br><span class="line">from </span><br><span class="line">	(</span><br><span class="line">	select </span><br><span class="line">		data.id,</span><br><span class="line">		data.create_time register_time,</span><br><span class="line">		date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) register_date</span><br><span class="line">	from edu.ods_user_info_inc</span><br><span class="line">	where type = &#x27;insert&#x27; and dt = &#x27;2022-02-22&#x27;</span><br><span class="line">	) register</span><br><span class="line">left join</span><br><span class="line">	(</span><br><span class="line">	select</span><br><span class="line">		common.uid      user_id,</span><br><span class="line">		min(common.ar)  province_id,</span><br><span class="line">		min(common.vc)  version_code,</span><br><span class="line">		min(common.mid) mid_id,</span><br><span class="line">		min(common.ba)  brand,</span><br><span class="line">		min(common.md)  model,</span><br><span class="line">		min(common.os)  operate_system</span><br><span class="line">	from edu.ods_log_inc</span><br><span class="line">	where dt = &#x27;2022-02-22&#x27; and `start` is not null</span><br><span class="line">	group by common.uid</span><br><span class="line">	) log_dim</span><br><span class="line">	on register.id = log_dim.user_id;</span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>

<h3 id="19-用户域用户登录事务事实表"><a href="#19-用户域用户登录事务事实表" class="headerlink" title="19. 用户域用户登录事务事实表"></a>19. 用户域用户登录事务事实表</h3><p>该表字段全部来源ods_log_inc</p>
<p>如何在众多的页面浏览行为日志中找到用户登录的日志？</p>
<p>首先通过会话id找到同一个会话下的所有页面的浏览日志，然后使用开窗函数，将同一会话下的所有页面日志按照时间排序，排名第一的页面浏览日志，即用户的登录行为。</p>
<p>建表语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dwd_user_login_inc;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_user_login_inc</span><br><span class="line">(</span><br><span class="line">    `user_id`        STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `date_id`        STRING COMMENT &#x27;日期id&#x27;,</span><br><span class="line">    `login_time`     STRING COMMENT &#x27;登录时间&#x27;,</span><br><span class="line">    `province_id`    STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `version_code`   STRING COMMENT &#x27;应用版本&#x27;,</span><br><span class="line">    `mid_id`         STRING COMMENT &#x27;设备id&#x27;,</span><br><span class="line">    `brand`          STRING COMMENT &#x27;设备品牌&#x27;,</span><br><span class="line">    `model`          STRING COMMENT &#x27;设备型号&#x27;,</span><br><span class="line">    `operate_system` STRING COMMENT &#x27;设备操作系统&#x27;</span><br><span class="line">) COMMENT &#x27;用户域用户登录事务事实表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dwd/dwd_user_login_inc/&#x27;</span><br><span class="line">    TBLPROPERTIES (&quot;orc.compress&quot; = &quot;snappy&quot;);</span><br></pre></td></tr></table></figure>

<p>数据装载：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table edu.dwd_user_login_inc</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;) login_time,</span><br><span class="line">       province_id,</span><br><span class="line">       version_code,</span><br><span class="line">       mid_id,</span><br><span class="line">       brand,</span><br><span class="line">       model,</span><br><span class="line">       operate_system</span><br><span class="line">from (select user_id,</span><br><span class="line">             province_id,</span><br><span class="line">             version_code,</span><br><span class="line">             mid_id,</span><br><span class="line">             brand,</span><br><span class="line">             model,</span><br><span class="line">             operate_system,</span><br><span class="line">             row_number() over (partition by session_id order by ts) rk,</span><br><span class="line">             ts</span><br><span class="line">      from (select common.ar  province_id,</span><br><span class="line">                   common.ba  brand,</span><br><span class="line">                   common.md  model,</span><br><span class="line">                   common.mid mid_id,</span><br><span class="line">                   common.os  operate_system,</span><br><span class="line">                   common.sid session_id,</span><br><span class="line">                   common.vc  version_code,</span><br><span class="line">                   common.uid user_id,</span><br><span class="line">                   ts</span><br><span class="line">            from edu.ods_log_inc</span><br><span class="line">            where page is not null</span><br><span class="line">   				and common.uid is not null</span><br><span class="line">              and dt = &#x27;2022-02-21&#x27;) log</span><br><span class="line">     ) rkt</span><br><span class="line">where rk = 1;</span><br><span class="line">set hive.cbo.enable=true;</span><br></pre></td></tr></table></figure>

<h3 id="9-20-数据装载脚本"><a href="#9-20-数据装载脚本" class="headerlink" title="9.20 数据装载脚本"></a>9.20 数据装载脚本</h3><h4 id="9-20-1-首日装载脚本"><a href="#9-20-1-首日装载脚本" class="headerlink" title="9.20.1 首日装载脚本"></a>9.20.1 首日装载脚本</h4><p>（1）在hadoop102的~&#x2F;bin目录下创建ods_to_dwd_init.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim ods_to_dwd_init.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    echo &quot;请传入日期参数&quot;</span><br><span class="line">    exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">APP=edu</span><br><span class="line"></span><br><span class="line">dwd_trade_cart_add_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_trade_cart_add_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       data.session_id,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.cart_price,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) dt</span><br><span class="line">from $&#123;APP&#125;.ods_cart_info_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27; and type = &#x27;bootstrap-insert&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_trade_cart_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_trade_cart_full partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       date_format(create_time, &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       session_id,</span><br><span class="line">       course_name,</span><br><span class="line">       create_time,</span><br><span class="line">       cart_price</span><br><span class="line">from $&#123;APP&#125;.ods_cart_info_full</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and deleted = &#x27;0&#x27; and sold = &#x27;0&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_trade_course_order_inc=&quot;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">with play as</span><br><span class="line">         (</span><br><span class="line">             select min(id)                                     id,</span><br><span class="line">                    user_id,</span><br><span class="line">                    course_id,</span><br><span class="line">                    min(create_time)                            play_time,</span><br><span class="line">                    date_format(min(create_time), &#x27;yyyy-MM-dd&#x27;) play_date</span><br><span class="line">             from $&#123;APP&#125;.ods_user_chapter_process_full</span><br><span class="line">             where dt = &#x27;$do_date&#x27;</span><br><span class="line">             group by user_id, course_id</span><br><span class="line">         ),</span><br><span class="line">     oi as</span><br><span class="line">         (</span><br><span class="line">             select data.id,</span><br><span class="line">                    data.province_id,</span><br><span class="line">                    data.session_id</span><br><span class="line">             from $&#123;APP&#125;.ods_order_info_inc</span><br><span class="line">             where dt = &#x27;$do_date&#x27;</span><br><span class="line">               and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">         ),</span><br><span class="line">     od as</span><br><span class="line">         (</span><br><span class="line">             select data.id,</span><br><span class="line">                    data.course_id,</span><br><span class="line">                    data.order_id,</span><br><span class="line">                    data.user_id,</span><br><span class="line">                    data.origin_amount,</span><br><span class="line">                    data.coupon_reduce,</span><br><span class="line">                    data.final_amount,</span><br><span class="line">                    data.create_time order_time,</span><br><span class="line">                    date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) order_date</span><br><span class="line">             from $&#123;APP&#125;.ods_order_detail_inc</span><br><span class="line">             where dt = &#x27;$do_date&#x27;</span><br><span class="line">               and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">         ),</span><br><span class="line">     dim_course as (</span><br><span class="line">         select id,</span><br><span class="line">                course_name,</span><br><span class="line">                category_id,</span><br><span class="line">                category_name,</span><br><span class="line">                subject_id,</span><br><span class="line">                subject_name</span><br><span class="line">         from $&#123;APP&#125;.dim_course_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     )</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_trade_course_order_inc partition (dt)</span><br><span class="line">select final.id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       order_id,</span><br><span class="line">       province_id,</span><br><span class="line">       play_time,</span><br><span class="line">       play_date,</span><br><span class="line">       order_time,</span><br><span class="line">       order_date,</span><br><span class="line">       end_date,</span><br><span class="line">       session_id,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount,</span><br><span class="line">       case</span><br><span class="line">      when end_date is not null then end_date</span><br><span class="line">      when order_date is not null then order_date</span><br><span class="line">           else &#x27;9999-12-31&#x27; end dt</span><br><span class="line">from (select play.id,</span><br><span class="line">             play.user_id,</span><br><span class="line">             play.course_id,</span><br><span class="line">             od.order_id,</span><br><span class="line">             oi.province_id,</span><br><span class="line">             play.play_time,</span><br><span class="line">             play.play_date,</span><br><span class="line">             od.order_time,</span><br><span class="line">             od.order_date,</span><br><span class="line">             if(od.order_date is null and</span><br><span class="line">                date_add(play.play_date, 7) &lt;= &#x27;$do_date&#x27;,</span><br><span class="line">                date_add(play.play_date, 7), null) end_date,</span><br><span class="line">             oi.session_id,</span><br><span class="line">             od.origin_amount,</span><br><span class="line">             od.coupon_reduce,</span><br><span class="line">             od.final_amount</span><br><span class="line">      from play</span><br><span class="line">               left join od on play.user_id = od.user_id and play.course_id = od.course_id</span><br><span class="line">               left join oi on od.order_id = oi.id</span><br><span class="line">where od.order_time is null</span><br><span class="line">or od.order_time &gt; play.play_time</span><br><span class="line">) final</span><br><span class="line">         left join dim_course on course_id = dim_course.id;&quot;</span><br><span class="line"></span><br><span class="line">dwd_trade_order_detail_inc=&quot;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_trade_order_detail_inc</span><br><span class="line">    partition (dt)</span><br><span class="line">select odt.id,</span><br><span class="line">       order_id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       province_id,</span><br><span class="line">       date_id,</span><br><span class="line">       session_id,</span><br><span class="line">       source_id,</span><br><span class="line">       create_time,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount,</span><br><span class="line">       out_trade_no,</span><br><span class="line">       trade_body,</span><br><span class="line">       date_id</span><br><span class="line">from (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.user_id,</span><br><span class="line">                data.course_id,</span><br><span class="line">                date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">                data.create_time,</span><br><span class="line">                data.origin_amount,</span><br><span class="line">                data.coupon_reduce,</span><br><span class="line">                data.final_amount</span><br><span class="line">         from $&#123;APP&#125;.ods_order_detail_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">           and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">     ) odt</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.province_id,</span><br><span class="line">                data.out_trade_no,</span><br><span class="line">                data.session_id,</span><br><span class="line">                data.trade_body</span><br><span class="line">         from $&#123;APP&#125;.ods_order_info_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">           and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">     ) od</span><br><span class="line">     on odt.order_id = od.id</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select distinct common.sid,</span><br><span class="line">                         common.sc source_id</span><br><span class="line">         from $&#123;APP&#125;.ods_log_inc oli</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) log</span><br><span class="line">     on od.session_id = log.sid</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                course_name,</span><br><span class="line">                category_id,</span><br><span class="line">                category_name,</span><br><span class="line">                subject_id,</span><br><span class="line">                subject_name</span><br><span class="line">         from $&#123;APP&#125;.dim_course_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) dim_course</span><br><span class="line">     on course_id = dim_course.id;&quot;</span><br><span class="line"></span><br><span class="line">dwd_trade_pay_detail_suc_inc=&quot;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_trade_pay_detail_suc_inc</span><br><span class="line">partition(dt)</span><br><span class="line">select odt.id,</span><br><span class="line">       od.id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       province_id,</span><br><span class="line">       date_format(create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       alipay_trade_no,</span><br><span class="line">       trade_body,</span><br><span class="line">       payment_type,</span><br><span class="line">       payment_status,</span><br><span class="line">       callback_time,</span><br><span class="line">       callback_content,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount,</span><br><span class="line">       date_format(create_time, &#x27;yyyy-MM-dd&#x27;) date_id</span><br><span class="line">from (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.user_id,</span><br><span class="line">                data.course_id,</span><br><span class="line">                data.origin_amount,</span><br><span class="line">                data.coupon_reduce,</span><br><span class="line">                data.final_amount,</span><br><span class="line">                data.create_time</span><br><span class="line">         from $&#123;APP&#125;.ods_order_detail_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27; and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">     ) odt</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.province_id</span><br><span class="line">         from $&#123;APP&#125;.ods_order_info_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27; and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">     ) od</span><br><span class="line">     on odt.order_id = od.id</span><br><span class="line">         join</span><br><span class="line">     (</span><br><span class="line">         select data.alipay_trade_no,</span><br><span class="line">                data.trade_body,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.payment_type,</span><br><span class="line">                data.payment_status,</span><br><span class="line">                data.callback_time,</span><br><span class="line">                data.callback_content</span><br><span class="line">         from $&#123;APP&#125;.ods_payment_info_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27; and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">           and data.callback_time is not null</span><br><span class="line">     ) pi</span><br><span class="line">     on od.id = pi.order_id;&quot;</span><br><span class="line"></span><br><span class="line">dwd_traffic_page_view_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_traffic_page_view_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar      province_id,</span><br><span class="line">       common.ba      brand,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md      model,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid     session_id,</span><br><span class="line">       common.uid     user_id,</span><br><span class="line">       common.vc      version_code,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item      page_item,</span><br><span class="line">       page.item_type page_item_type,</span><br><span class="line">       page.page_id,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       ts</span><br><span class="line">from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and page is not null;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_traffic_start_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_traffic_start_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar            province_id,</span><br><span class="line">       common.ba            brand,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md            model,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid           session_id,</span><br><span class="line">       common.uid           user_id,</span><br><span class="line">       common.vc            version_code,</span><br><span class="line">       common.sc,</span><br><span class="line">       \`start\`.entry,</span><br><span class="line">       \`start\`.open_ad_id,</span><br><span class="line">       \`start\`.first_open,</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd HH:mm:ss&#x27;),</span><br><span class="line">       \`start\`.loading_time loading_time_ms,</span><br><span class="line">       \`start\`.open_ad_ms,</span><br><span class="line">       \`start\`.open_ad_skip_ms</span><br><span class="line">from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and \`start\` is not null;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_traffic_action_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_traffic_action_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar,</span><br><span class="line">       common.ba,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid,</span><br><span class="line">       common.uid,</span><br><span class="line">       common.vc,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item,</span><br><span class="line">       page.item_type,</span><br><span class="line">       page.page_id,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       action.action_id,</span><br><span class="line">       action.item,</span><br><span class="line">       action.item_type,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       action.ts</span><br><span class="line">from $&#123;APP&#125;.ods_log_inc oli lateral view explode(actions) tmp as action</span><br><span class="line">where oli.dt = &#x27;$do_date&#x27;</span><br><span class="line">  and actions is not null;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_traffic_display_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_traffic_display_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar,</span><br><span class="line">       common.ba,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid,</span><br><span class="line">       common.uid,</span><br><span class="line">       common.vc,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item,</span><br><span class="line">       page.item_type,</span><br><span class="line">       page.page_id,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       ts,</span><br><span class="line">       display.display_type,</span><br><span class="line">       display.item,</span><br><span class="line">       display.item_type,</span><br><span class="line">       display.\`order\`,</span><br><span class="line">       display.pos_id</span><br><span class="line">from $&#123;APP&#125;.ods_log_inc oli lateral view explode(displays) tmp as display</span><br><span class="line">where oli.dt = &#x27;$do_date&#x27;</span><br><span class="line">  and displays is not null;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_traffic_error_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_traffic_error_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar,</span><br><span class="line">       common.ba,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid,</span><br><span class="line">       common.uid,</span><br><span class="line">       common.vc,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item,</span><br><span class="line">       page.item_type,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       page.page_id,</span><br><span class="line">       \`start\`.entry,</span><br><span class="line">       \`start\`.loading_time,</span><br><span class="line">       \`start\`.open_ad_id,</span><br><span class="line">       \`start\`.open_ad_ms,</span><br><span class="line">       \`start\`.open_ad_skip_ms,</span><br><span class="line">       actions,</span><br><span class="line">       displays,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       ts,</span><br><span class="line">       err.error_code,</span><br><span class="line">       err.msg</span><br><span class="line">from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and err is not null;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_interaction_favor_add_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_interaction_favor_add_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.create_time,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;)</span><br><span class="line">from $&#123;APP&#125;.ods_favor_info_inc</span><br><span class="line">where dt=&#x27;$do_date&#x27; and type = &#x27;bootstrap-insert&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_interaction_comment_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_interaction_comment_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.chapter_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.comment_txt,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;)</span><br><span class="line">from $&#123;APP&#125;.ods_comment_info_inc</span><br><span class="line">where dt=&#x27;$do_date&#x27; and type = &#x27;bootstrap-insert&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_interaction_review_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_interaction_review_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.review_txt,</span><br><span class="line">       data.review_stars,</span><br><span class="line">       data.create_time,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;)</span><br><span class="line">from $&#123;APP&#125;.ods_review_info_inc</span><br><span class="line">where dt=&#x27;$do_date&#x27; and type = &#x27;bootstrap-insert&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_examination_test_paper_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_examination_test_paper_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.paper_id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.score,</span><br><span class="line">       data.duration_sec,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.submit_time,</span><br><span class="line">       data.update_time,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;)</span><br><span class="line">from $&#123;APP&#125;.ods_test_exam_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">  and data.deleted = &#x27;0&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_examination_test_question_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_examination_test_question_inc</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.paper_id,</span><br><span class="line">       data.question_id,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.answer,</span><br><span class="line">       data.is_correct,</span><br><span class="line">       data.score,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.update_time,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;)</span><br><span class="line">from $&#123;APP&#125;.ods_test_exam_question_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">  and data.deleted = &#x27;0&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_learn_play_stats_full=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">with curpos as (</span><br><span class="line">    select user_id,</span><br><span class="line">           video_id,</span><br><span class="line">           position_sec</span><br><span class="line">    from (select common.uid            user_id,</span><br><span class="line">                 appvideo.video_id,</span><br><span class="line">                 appvideo.position_sec,</span><br><span class="line">                 row_number() over (partition by common.uid, appvideo.video_id</span><br><span class="line">                     order by ts desc) rk</span><br><span class="line">          from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">          where dt = &#x27;$do_date&#x27;</span><br><span class="line">and appvideo is not null) origin</span><br><span class="line">    where rk = 1</span><br><span class="line">),</span><br><span class="line">     aggred as (</span><br><span class="line">         select common.uid                 user_id,</span><br><span class="line">                appvideo.video_id,</span><br><span class="line">                sum(appvideo.play_sec)     total_play_sec,</span><br><span class="line">                max(appvideo.position_sec) max_position_sec</span><br><span class="line">         from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">         and appvideo is not null</span><br><span class="line">         group by common.uid, appvideo.video_id</span><br><span class="line">     ),</span><br><span class="line">     dim_video as (</span><br><span class="line">         select id,</span><br><span class="line">                video_name,</span><br><span class="line">                chapter_id,</span><br><span class="line">                chapter_name,</span><br><span class="line">                course_id,</span><br><span class="line">                during_sec</span><br><span class="line">         from $&#123;APP&#125;.dim_video_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     )</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_learn_play_stats_full partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select calculated.user_id,</span><br><span class="line">       calculated.video_id,</span><br><span class="line">       video_name,</span><br><span class="line">       chapter_id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       course_id,</span><br><span class="line">       total_play_sec,</span><br><span class="line">       position_sec,</span><br><span class="line">       max_position_sec,</span><br><span class="line">       first_sec_complete_date,</span><br><span class="line">       first_process_complete_date,</span><br><span class="line">       first_complete_date</span><br><span class="line">from (select user_id,</span><br><span class="line">             video_id,</span><br><span class="line">             video_name,</span><br><span class="line">             chapter_id,</span><br><span class="line">             chapter_name,</span><br><span class="line">             course_id,</span><br><span class="line">             total_play_sec,</span><br><span class="line">             max_position_sec,</span><br><span class="line">             if(total_play_sec / during_sec &gt;= 0.9, &#x27;$do_date&#x27;, null)   first_sec_complete_date,</span><br><span class="line">             if(max_position_sec / during_sec &gt;= 0.9, &#x27;$do_date&#x27;, null) first_process_complete_date,</span><br><span class="line">             if(total_play_sec / during_sec &gt;= 0.9 and</span><br><span class="line">                max_position_sec / during_sec &gt;= 0.9, &#x27;$do_date&#x27;, null) first_complete_date</span><br><span class="line">      from (select user_id,</span><br><span class="line">                   video_id,</span><br><span class="line">                   video_name,</span><br><span class="line">                   chapter_id,</span><br><span class="line">                   chapter_name,</span><br><span class="line">                   course_id,</span><br><span class="line">                   total_play_sec,</span><br><span class="line">                   max_position_sec,</span><br><span class="line">                   during_sec</span><br><span class="line">            from aggred</span><br><span class="line">                     left join dim_video</span><br><span class="line">                               on aggred.video_id = dim_video.id</span><br><span class="line">           ) joined) calculated</span><br><span class="line">         left join curpos</span><br><span class="line">                   on calculated.user_id = curpos.user_id</span><br><span class="line">                       and calculated.video_id = curpos.video_id;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_learn_play_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_learn_play_inc</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select mid,</span><br><span class="line">       province_id,</span><br><span class="line">       brand,</span><br><span class="line">       is_new,</span><br><span class="line">       model,</span><br><span class="line">       os,</span><br><span class="line">       session_id,</span><br><span class="line">       user_id,</span><br><span class="line">       version_code,</span><br><span class="line">       sc,</span><br><span class="line">       video_id,</span><br><span class="line">       video_name,</span><br><span class="line">       chapter_id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       play_sec,</span><br><span class="line">       ts</span><br><span class="line">from (select common.mid,</span><br><span class="line">             common.ar              province_id,</span><br><span class="line">             common.ba              brand,</span><br><span class="line">             common.is_new,</span><br><span class="line">             common.md              model,</span><br><span class="line">             common.os,</span><br><span class="line">             common.sid             session_id,</span><br><span class="line">             common.uid             user_id,</span><br><span class="line">             common.vc              version_code,</span><br><span class="line">             common.sc,</span><br><span class="line">             appvideo.video_id,</span><br><span class="line">             sum(appvideo.play_sec) play_sec,</span><br><span class="line">             max(ts)                ts</span><br><span class="line">      from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">      where dt = &#x27;$do_date&#x27;</span><br><span class="line">        and appvideo is not null</span><br><span class="line">      group by common.mid,</span><br><span class="line">               common.ar,</span><br><span class="line">               common.ba,</span><br><span class="line">               common.is_new,</span><br><span class="line">               common.md,</span><br><span class="line">               common.os,</span><br><span class="line">               common.sid,</span><br><span class="line">               common.uid,</span><br><span class="line">               common.vc,</span><br><span class="line">               common.sc,</span><br><span class="line">               appvideo.video_id) aggred</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             video_name,</span><br><span class="line">             chapter_id,</span><br><span class="line">             chapter_name,</span><br><span class="line">             course_id</span><br><span class="line">      from $&#123;APP&#125;.dim_video_full</span><br><span class="line">      where dt = &#x27;$do_date&#x27;) dim_video</span><br><span class="line">     on aggred.video_id = dim_video.id</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             course_name</span><br><span class="line">      from $&#123;APP&#125;.dim_course_full</span><br><span class="line">      where dt = &#x27;$do_date&#x27;) dim_course</span><br><span class="line">     on course_id = dim_course.id;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_user_register_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_user_register_inc</span><br><span class="line">select register.id,</span><br><span class="line">       register_time,</span><br><span class="line">       register_date,</span><br><span class="line">       province_id,</span><br><span class="line">       version_code,</span><br><span class="line">       mid_id,</span><br><span class="line">       brand,</span><br><span class="line">       model,</span><br><span class="line">       operate_system,</span><br><span class="line">       register_date</span><br><span class="line">from (</span><br><span class="line">select data.id,</span><br><span class="line">             data.create_time register_time,</span><br><span class="line">             date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) register_date</span><br><span class="line">      from $&#123;APP&#125;.ods_user_info_inc</span><br><span class="line">      where type = &#x27;bootstrap-insert&#x27;</span><br><span class="line">        and dt = &#x27;$do_date&#x27;</span><br><span class="line">) register</span><br><span class="line">         left join</span><br><span class="line">     (select common.uid      user_id,</span><br><span class="line">             min(common.ar)  province_id,</span><br><span class="line">             min(common.vc)  version_code,</span><br><span class="line">             min(common.mid) mid_id,</span><br><span class="line">             min(common.ba)  brand,</span><br><span class="line">             min(common.md)  model,</span><br><span class="line">             min(common.os)  operate_system</span><br><span class="line">      from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">      where dt = &#x27;$do_date&#x27;</span><br><span class="line">        and \`start\` is not null</span><br><span class="line">      group by common.uid) log_dim</span><br><span class="line">     on register.id = log_dim.user_id;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_user_login_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_user_login_inc</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;) login_time,</span><br><span class="line">       province_id,</span><br><span class="line">       version_code,</span><br><span class="line">       mid_id,</span><br><span class="line">       brand,</span><br><span class="line">       model,</span><br><span class="line">       operate_system</span><br><span class="line">from (select user_id,</span><br><span class="line">             province_id,</span><br><span class="line">             version_code,</span><br><span class="line">             mid_id,</span><br><span class="line">             brand,</span><br><span class="line">             model,</span><br><span class="line">             operate_system,</span><br><span class="line">             row_number() over (partition by session_id order by ts) rk,</span><br><span class="line">             ts</span><br><span class="line">      from (select common.ar  province_id,</span><br><span class="line">                   common.ba  brand,</span><br><span class="line">                   common.md  model,</span><br><span class="line">                   common.mid mid_id,</span><br><span class="line">                   common.os  operate_system,</span><br><span class="line">                   common.sid session_id,</span><br><span class="line">                   common.vc  version_code,</span><br><span class="line">                   common.uid user_id,</span><br><span class="line">                   ts</span><br><span class="line">            from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">            where page is not null</span><br><span class="line">   				and common.uid is not null</span><br><span class="line">              and dt = &#x27;$do_date&#x27;) log</span><br><span class="line">     ) rkt</span><br><span class="line">where rk = 1;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    dwd_trade_cart_add_inc|dwd_trade_cart_full|dwd_trade_course_order_inc|dwd_trade_order_detail_inc|dwd_trade_pay_detail_suc_inc|dwd_traffic_page_view_inc|dwd_traffic_start_inc|dwd_traffic_action_inc|dwd_traffic_display_inc|dwd_traffic_error_inc|dwd_interaction_favor_add_inc|dwd_interaction_comment_inc|dwd_interaction_review_inc|dwd_examination_test_paper_inc|dwd_examination_test_question_inc|dwd_learn_play_stats_full|dwd_learn_play_inc|dwd_user_register_inc|dwd_user_login_inc)</span><br><span class="line">        eval &quot;hive -e \&quot;\$$1\&quot;&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot; )</span><br><span class="line">        hive -e &quot;$&#123;dwd_trade_cart_add_inc&#125;$&#123;dwd_trade_cart_full&#125;$&#123;dwd_trade_course_order_inc&#125;$&#123;dwd_trade_order_detail_inc&#125;$&#123;dwd_trade_pay_detail_suc_inc&#125;$&#123;dwd_traffic_page_view_inc&#125;$&#123;dwd_traffic_start_inc&#125;$&#123;dwd_traffic_action_inc&#125;$&#123;dwd_traffic_display_inc&#125;$&#123;dwd_traffic_error_inc&#125;$&#123;dwd_interaction_favor_add_inc&#125;$&#123;dwd_interaction_comment_inc&#125;$&#123;dwd_interaction_review_inc&#125;$&#123;dwd_examination_test_paper_inc&#125;$&#123;dwd_examination_test_question_inc&#125;$&#123;dwd_learn_play_stats_full&#125;$&#123;dwd_learn_play_inc&#125;$&#123;dwd_user_register_inc&#125;$&#123;dwd_user_login_inc&#125;&quot;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）增加权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 ods_to_dwd_init.sh </span><br></pre></td></tr></table></figure>

<p>（3）用法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# ods_to_dwd_init.sh all 2022-02-21 </span><br></pre></td></tr></table></figure>

<h4 id="9-20-2-每日装载脚本"><a href="#9-20-2-每日装载脚本" class="headerlink" title="9.20.2 每日装载脚本"></a>9.20.2 每日装载脚本</h4><p>（1）在hadoop102的~&#x2F;bin目录下创建ods_to_dwd.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim ods_to_dwd.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">APP=edu</span><br><span class="line"></span><br><span class="line">dwd_trade_cart_add_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_trade_cart_add_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       data.session_id,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.cart_price</span><br><span class="line">from $&#123;APP&#125;.ods_cart_info_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27; and type = &#x27;insert&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_trade_cart_full=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_trade_cart_full partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       date_format(create_time, &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       session_id,</span><br><span class="line">       course_name,</span><br><span class="line">       create_time,</span><br><span class="line">       cart_price</span><br><span class="line">from $&#123;APP&#125;.ods_cart_info_full</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and deleted = &#x27;0&#x27; and sold = &#x27;0&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_trade_course_order_inc=&quot;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">with play as</span><br><span class="line">         (select id,</span><br><span class="line">                 user_id,</span><br><span class="line">                 course_id,</span><br><span class="line">                 play_time,</span><br><span class="line">                 play_date</span><br><span class="line">          from $&#123;APP&#125;.dwd_trade_course_order_inc</span><br><span class="line">          where dt = &#x27;9999-12-31&#x27;</span><br><span class="line">          union</span><br><span class="line">          select min(id)                                     id,</span><br><span class="line">                 user_id,</span><br><span class="line">                 course_id,</span><br><span class="line">                 min(create_time),</span><br><span class="line">                 date_format(min(create_time), &#x27;yyyy-MM-dd&#x27;) play_date</span><br><span class="line">          from $&#123;APP&#125;.ods_user_chapter_process_full</span><br><span class="line">          where dt = &#x27;$do_date&#x27;</span><br><span class="line">          group by user_id, course_id</span><br><span class="line">          having date_format(min(create_time), &#x27;yyyy-MM-dd&#x27;) = &#x27;$do_date&#x27;),</span><br><span class="line">     oi as</span><br><span class="line">         (</span><br><span class="line">             select data.id,</span><br><span class="line">                    data.province_id,</span><br><span class="line">                    data.session_id,</span><br><span class="line">                    data.create_time order_time</span><br><span class="line">             from $&#123;APP&#125;.ods_order_info_inc</span><br><span class="line">             where dt = &#x27;$do_date&#x27;</span><br><span class="line">               and type = &#x27;insert&#x27;</span><br><span class="line">         ),</span><br><span class="line">     od as</span><br><span class="line">         (</span><br><span class="line">             select data.id,</span><br><span class="line">                    data.course_id,</span><br><span class="line">                    data.order_id,</span><br><span class="line">                    data.user_id,</span><br><span class="line">                    data.origin_amount,</span><br><span class="line">                    data.coupon_reduce,</span><br><span class="line">                    data.final_amount,</span><br><span class="line">                    date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) order_date</span><br><span class="line">             from $&#123;APP&#125;.ods_order_detail_inc</span><br><span class="line">             where dt = &#x27;$do_date&#x27;</span><br><span class="line">               and type = &#x27;insert&#x27;</span><br><span class="line">         ),</span><br><span class="line">     dim_course as</span><br><span class="line">         (</span><br><span class="line">             select id,</span><br><span class="line">                    course_name,</span><br><span class="line">                    category_id,</span><br><span class="line">                    category_name,</span><br><span class="line">                    subject_id,</span><br><span class="line">                    subject_name</span><br><span class="line">             from $&#123;APP&#125;.dim_course_full</span><br><span class="line">             where dt = &#x27;$do_date&#x27;</span><br><span class="line">         )</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_trade_course_order_inc</span><br><span class="line">partition (dt)</span><br><span class="line">select final.id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       order_id,</span><br><span class="line">       province_id,</span><br><span class="line">       play_time,</span><br><span class="line">       play_date,</span><br><span class="line">       order_time,</span><br><span class="line">       order_date,</span><br><span class="line">       end_date,</span><br><span class="line">       session_id,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount,</span><br><span class="line">       case</span><br><span class="line">           when end_date is not null then end_date</span><br><span class="line">           when order_date is not null then order_date</span><br><span class="line">           else &#x27;9999-12-31&#x27; end dt</span><br><span class="line">from (select play.id,</span><br><span class="line">             play.user_id,</span><br><span class="line">             play.course_id,</span><br><span class="line">             od.order_id,</span><br><span class="line">             oi.province_id,</span><br><span class="line">             play.play_time,</span><br><span class="line">             play.play_date,</span><br><span class="line">             oi.order_time,</span><br><span class="line">             od.order_date,</span><br><span class="line">             if(order_date is null and date_add(play_date, 7) = &#x27;$do_date&#x27;, &#x27;$do_date&#x27;, null) end_date,</span><br><span class="line">             oi.session_id,</span><br><span class="line">             od.origin_amount,</span><br><span class="line">             od.coupon_reduce,</span><br><span class="line">             od.final_amount</span><br><span class="line">      from play</span><br><span class="line">               left join od on play.user_id = od.user_id and play.course_id = od.course_id</span><br><span class="line">               left join oi on od.order_id = oi.id</span><br><span class="line">where order_time is null </span><br><span class="line">or order_time &gt; play_time </span><br><span class="line">) final</span><br><span class="line">         left join dim_course on course_id = dim_course.id;&quot;</span><br><span class="line"></span><br><span class="line">dwd_trade_order_detail_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_trade_order_detail_inc </span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select odt.id,</span><br><span class="line">       order_id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       province_id,</span><br><span class="line">       date_id,</span><br><span class="line">       session_id,</span><br><span class="line">       source_id,</span><br><span class="line">       create_time,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount,</span><br><span class="line">       out_trade_no,</span><br><span class="line">       trade_body</span><br><span class="line">from (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.user_id,</span><br><span class="line">                data.course_id,</span><br><span class="line">                date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">                data.create_time,</span><br><span class="line">                data.origin_amount,</span><br><span class="line">                data.coupon_reduce,</span><br><span class="line">                data.final_amount</span><br><span class="line">         from $&#123;APP&#125;.ods_order_detail_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">           and type = &#x27;insert&#x27;</span><br><span class="line">     ) odt</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.province_id,</span><br><span class="line">                data.session_id,</span><br><span class="line">                data.out_trade_no,</span><br><span class="line">                data.trade_body</span><br><span class="line">         from $&#123;APP&#125;.ods_order_info_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">           and type = &#x27;insert&#x27;</span><br><span class="line">     ) od</span><br><span class="line">     on odt.order_id = od.id</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select distinct common.sid,</span><br><span class="line">                         common.sc source_id</span><br><span class="line">         from $&#123;APP&#125;.ods_log_inc oli</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) log</span><br><span class="line">     on od.session_id = log.sid</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                course_name,</span><br><span class="line">                category_id,</span><br><span class="line">                category_name,</span><br><span class="line">                subject_id,</span><br><span class="line">                subject_name</span><br><span class="line">         from $&#123;APP&#125;.dim_course_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) dim_course</span><br><span class="line">     on course_id = dim_course.id;&quot;</span><br><span class="line"></span><br><span class="line">dwd_trade_pay_detail_suc_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_trade_pay_detail_suc_inc</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select</span><br><span class="line"> odt.id,</span><br><span class="line">       od.id,</span><br><span class="line">       user_id,</span><br><span class="line">       course_id,</span><br><span class="line">       province_id,</span><br><span class="line">       date_format(create_time, &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       alipay_trade_no,</span><br><span class="line">       trade_body,</span><br><span class="line">       payment_type,</span><br><span class="line">       payment_status,</span><br><span class="line">       callback_time,</span><br><span class="line">       callback_content,</span><br><span class="line">       origin_amount,</span><br><span class="line">       coupon_reduce,</span><br><span class="line">       final_amount</span><br><span class="line">from (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.user_id,</span><br><span class="line">                data.course_id,</span><br><span class="line">                data.origin_amount,</span><br><span class="line">                data.coupon_reduce,</span><br><span class="line">                data.final_amount,</span><br><span class="line">                data.create_time</span><br><span class="line">         from $&#123;APP&#125;.ods_order_detail_inc</span><br><span class="line">         where (dt = &#x27;$do_date&#x27; or dt = date_add(&#x27;$do_date&#x27;, -1))</span><br><span class="line">           and (type = &#x27;insert&#x27; or type = &#x27;bootstrap-insert&#x27;)</span><br><span class="line">     ) odt</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select data.id,</span><br><span class="line">                data.province_id</span><br><span class="line">         from $&#123;APP&#125;.ods_order_info_inc</span><br><span class="line">         where (dt = &#x27;$do_date&#x27; or dt = date_add(&#x27;$do_date&#x27;, -1))</span><br><span class="line">           and (type = &#x27;insert&#x27; or type = &#x27;bootstrap-insert&#x27;)</span><br><span class="line">     ) od</span><br><span class="line">     on odt.order_id = od.id</span><br><span class="line">         join</span><br><span class="line">     (</span><br><span class="line">         select data.alipay_trade_no,</span><br><span class="line">                data.trade_body,</span><br><span class="line">                data.order_id,</span><br><span class="line">                data.payment_type,</span><br><span class="line">                data.payment_status,</span><br><span class="line">                data.callback_time,</span><br><span class="line">                data.callback_content</span><br><span class="line">         from $&#123;APP&#125;.ods_payment_info_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">           and type = &#x27;update&#x27;</span><br><span class="line">           and array_contains(map_keys(old), &#x27;callback_time&#x27;)</span><br><span class="line">     ) pi</span><br><span class="line">     on od.id = pi.order_id;&quot;</span><br><span class="line"></span><br><span class="line">dwd_traffic_page_view_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_traffic_page_view_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar      province_id,</span><br><span class="line">       common.ba      brand,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md      model,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid     session_id,</span><br><span class="line">       common.uid     user_id,</span><br><span class="line">       common.vc      version_code,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item      page_item,</span><br><span class="line">       page.item_type page_item_type,</span><br><span class="line">       page.page_id,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       ts</span><br><span class="line">from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and page is not null;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_traffic_start_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_traffic_start_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar            province_id,</span><br><span class="line">       common.ba            brand,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md            model,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid           session_id,</span><br><span class="line">       common.uid           user_id,</span><br><span class="line">       common.vc            version_code,</span><br><span class="line">       common.sc,</span><br><span class="line">       \`start\`.entry,</span><br><span class="line">       \`start\`.open_ad_id,</span><br><span class="line">       \`start\`.first_open,</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd HH:mm:ss&#x27;),</span><br><span class="line">       \`start\`.loading_time loading_time_ms,</span><br><span class="line">       \`start\`.open_ad_ms,</span><br><span class="line">       \`start\`.open_ad_skip_ms</span><br><span class="line">from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and \`start\` is not null;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_traffic_action_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_traffic_action_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar,</span><br><span class="line">       common.ba,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid,</span><br><span class="line">       common.uid,</span><br><span class="line">       common.vc,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item,</span><br><span class="line">       page.item_type,</span><br><span class="line">       page.page_id,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       action.action_id,</span><br><span class="line">       action.item,</span><br><span class="line">       action.item_type,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       action.ts</span><br><span class="line">from $&#123;APP&#125;.ods_log_inc oli lateral view explode(actions) tmp as action</span><br><span class="line">where oli.dt = &#x27;$do_date&#x27;</span><br><span class="line">  and actions is not null;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_traffic_display_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_traffic_display_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar,</span><br><span class="line">       common.ba,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid,</span><br><span class="line">       common.uid,</span><br><span class="line">       common.vc,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item,</span><br><span class="line">       page.item_type,</span><br><span class="line">       page.page_id,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       ts,</span><br><span class="line">       display.display_type,</span><br><span class="line">       display.item,</span><br><span class="line">       display.item_type,</span><br><span class="line">       display.\`order\`,</span><br><span class="line">       display.pos_id</span><br><span class="line">from $&#123;APP&#125;.ods_log_inc oli lateral view explode(displays) tmp as display</span><br><span class="line">where oli.dt = &#x27;$do_date&#x27;</span><br><span class="line">  and displays is not null;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_traffic_error_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_traffic_error_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select common.mid,</span><br><span class="line">       common.ar,</span><br><span class="line">       common.ba,</span><br><span class="line">       common.is_new,</span><br><span class="line">       common.md,</span><br><span class="line">       common.os,</span><br><span class="line">       common.sid,</span><br><span class="line">       common.uid,</span><br><span class="line">       common.vc,</span><br><span class="line">       common.sc,</span><br><span class="line">       page.during_time,</span><br><span class="line">       page.item,</span><br><span class="line">       page.item_type,</span><br><span class="line">       page.last_page_id,</span><br><span class="line">       page.page_id,</span><br><span class="line">       \`start\`.entry,</span><br><span class="line">       \`start\`.loading_time,</span><br><span class="line">       \`start\`.open_ad_id,</span><br><span class="line">       \`start\`.open_ad_ms,</span><br><span class="line">       \`start\`.open_ad_skip_ms,</span><br><span class="line">       actions,</span><br><span class="line">       displays,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       ts,</span><br><span class="line">       err.error_code,</span><br><span class="line">       err.msg</span><br><span class="line">from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and err is not null;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_interaction_favor_add_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_interaction_favor_add_inc</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.create_time</span><br><span class="line">from $&#123;APP&#125;.ods_favor_info_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and type = &#x27;insert&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_interaction_comment_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_interaction_comment_inc</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.chapter_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.comment_txt</span><br><span class="line">from $&#123;APP&#125;.ods_comment_info_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and type = &#x27;insert&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_interaction_review_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_interaction_review_inc</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.course_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts * 1000, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.review_txt,</span><br><span class="line">       data.review_stars,</span><br><span class="line">       data.create_time</span><br><span class="line">from $&#123;APP&#125;.ods_review_info_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and type = &#x27;insert&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_examination_test_paper_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_examination_test_paper_inc</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.paper_id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.score,</span><br><span class="line">       data.duration_sec,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.submit_time,</span><br><span class="line">       data.update_time</span><br><span class="line">from $&#123;APP&#125;.ods_test_exam_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and type = &#x27;insert&#x27;</span><br><span class="line">  and data.deleted = &#x27;0&#x27;;&quot;</span><br><span class="line"></span><br><span class="line">dwd_examination_test_question_inc=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_examination_test_question_inc</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select data.id,</span><br><span class="line">       data.user_id,</span><br><span class="line">       data.paper_id,</span><br><span class="line">       data.question_id,</span><br><span class="line">       date_format(from_utc_timestamp(data.create_time, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;),</span><br><span class="line">       data.answer,</span><br><span class="line">       data.is_correct,</span><br><span class="line">       data.score,</span><br><span class="line">       data.create_time,</span><br><span class="line">       data.update_time</span><br><span class="line">from $&#123;APP&#125;.ods_test_exam_question_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">  and type = &#x27;insert&#x27;</span><br><span class="line">  and data.deleted = &#x27;0&#x27;;&quot;</span><br><span class="line"> </span><br><span class="line">dwd_learn_play_stats_full=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">with curpos as (</span><br><span class="line">    select user_id,</span><br><span class="line">           video_id,</span><br><span class="line">           position_sec</span><br><span class="line">    from (select common.uid            user_id,</span><br><span class="line">                 appvideo.video_id,</span><br><span class="line">                 appvideo.position_sec,</span><br><span class="line">                 row_number() over (partition by common.uid, appvideo.video_id</span><br><span class="line">                     order by ts desc) rk</span><br><span class="line">          from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">          where appvideo is not null</span><br><span class="line">            and dt = &#x27;$do_date&#x27;) origin</span><br><span class="line">    where rk = 1</span><br><span class="line">),</span><br><span class="line">     aggred as (</span><br><span class="line">         select common.uid                 user_id,</span><br><span class="line">                appvideo.video_id,</span><br><span class="line">                sum(appvideo.play_sec)     total_play_sec,</span><br><span class="line">                max(appvideo.position_sec) max_position_sec</span><br><span class="line">         from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">         where appvideo is not null</span><br><span class="line">           and dt = &#x27;$do_date&#x27;</span><br><span class="line">         group by common.uid, appvideo.video_id</span><br><span class="line">     ),</span><br><span class="line">     dim_video as (</span><br><span class="line">         select id,</span><br><span class="line">                video_name,</span><br><span class="line">                chapter_id,</span><br><span class="line">                chapter_name,</span><br><span class="line">                course_id,</span><br><span class="line">                during_sec</span><br><span class="line">         from $&#123;APP&#125;.dim_video_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     )</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_learn_play_stats_full partition(dt = &#x27;$do_date&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       video_id,</span><br><span class="line">       video_name,</span><br><span class="line">       chapter_id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       course_id,</span><br><span class="line">       total_play_sec,</span><br><span class="line">       position_sec,</span><br><span class="line">       max_position_sec,</span><br><span class="line">       nvl(first_sec_complete_date,</span><br><span class="line">           if(total_play_sec / during_sec &gt;= 0.9, &#x27;$do_date&#x27;, null))   first_sec_complete_date,</span><br><span class="line">       nvl(first_process_complete_date,</span><br><span class="line">           if(max_position_sec / during_sec &gt;= 0.9, &#x27;$do_date&#x27;, null)) first_process_complete_date,</span><br><span class="line">       nvl(first_complete_date,</span><br><span class="line">           if(total_play_sec / during_sec &gt;= 0.9 and</span><br><span class="line">              max_position_sec / during_sec &gt;= 0.9, &#x27;$do_date&#x27;, null)) first_complete_date</span><br><span class="line">from (</span><br><span class="line">         select nvl(new.user_id, old.user_id)                             user_id,</span><br><span class="line">                nvl(new.video_id, old.video_id)                           video_id,</span><br><span class="line">                nvl(new.video_name, old.video_name)                       video_name,</span><br><span class="line">                nvl(new.chapter_id, old.chapter_id)                       chapter_id,</span><br><span class="line">                nvl(new.chapter_name, old.chapter_name)                   chapter_name,</span><br><span class="line">                nvl(new.course_id, old.course_id)                         course_id,</span><br><span class="line">                nvl(new.total_play_sec, 0L) + nvl(old.total_play_sec, 0L) total_play_sec,</span><br><span class="line">                nvl(new.position_sec, old.position_sec)                   position_sec,</span><br><span class="line">                if(new.max_position_sec is null, old.max_position_sec,</span><br><span class="line">                   if(old.max_position_sec is null, new.max_position_sec,</span><br><span class="line">                      if(new.max_position_sec &gt; old.max_position_sec,</span><br><span class="line">                         new.max_position_sec, old.max_position_sec)))    max_position_sec,</span><br><span class="line">                old.first_sec_complete_date,</span><br><span class="line">                old.first_process_complete_date,</span><br><span class="line">                old.first_complete_date,</span><br><span class="line">                during_sec</span><br><span class="line">         from (select calculated.user_id  user_id,</span><br><span class="line">                      calculated.video_id video_id,</span><br><span class="line">                      video_name,</span><br><span class="line">                      chapter_id,</span><br><span class="line">                      chapter_name,</span><br><span class="line">                      course_id,</span><br><span class="line">                      total_play_sec,</span><br><span class="line">                      position_sec,</span><br><span class="line">                      max_position_sec,</span><br><span class="line">                      during_sec</span><br><span class="line">               from (select user_id,</span><br><span class="line">                            video_id,</span><br><span class="line">                            video_name,</span><br><span class="line">                            chapter_id,</span><br><span class="line">                            chapter_name,</span><br><span class="line">                            course_id,</span><br><span class="line">                            total_play_sec,</span><br><span class="line">                            max_position_sec,</span><br><span class="line">                            during_sec</span><br><span class="line">                     from (select user_id,</span><br><span class="line">                                  video_id,</span><br><span class="line">                                  video_name,</span><br><span class="line">                                  chapter_id,</span><br><span class="line">                                  chapter_name,</span><br><span class="line">                                  course_id,</span><br><span class="line">                                  total_play_sec,</span><br><span class="line">                                  max_position_sec,</span><br><span class="line">                                  during_sec</span><br><span class="line">                           from aggred</span><br><span class="line">                                    left join dim_video</span><br><span class="line">                                              on aggred.video_id = dim_video.id</span><br><span class="line">                          ) joined) calculated</span><br><span class="line">                        left join curpos</span><br><span class="line">                                  on calculated.user_id = curpos.user_id</span><br><span class="line">                                      and calculated.video_id = curpos.video_id) new</span><br><span class="line">                  full outer join</span><br><span class="line">              (</span><br><span class="line">                  select user_id,</span><br><span class="line">                         video_id,</span><br><span class="line">                         video_name,</span><br><span class="line">                         chapter_id,</span><br><span class="line">                         chapter_name,</span><br><span class="line">                         course_id,</span><br><span class="line">                         total_play_sec,</span><br><span class="line">                         position_sec,</span><br><span class="line">                         max_position_sec,</span><br><span class="line">                         first_sec_complete_date,</span><br><span class="line">                         first_process_complete_date,</span><br><span class="line">                         first_complete_date</span><br><span class="line">                  from $&#123;APP&#125;.dwd_learn_play_stats_full</span><br><span class="line">                  where dt = date_add(&#x27;$do_date&#x27;, -1)</span><br><span class="line">              ) old</span><br><span class="line">              on new.user_id = old.user_id</span><br><span class="line">                  and new.video_id = old.video_id</span><br><span class="line">     ) final;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_learn_play_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_learn_play_inc</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select mid,</span><br><span class="line">       province_id,</span><br><span class="line">       brand,</span><br><span class="line">       is_new,</span><br><span class="line">       model,</span><br><span class="line">       os,</span><br><span class="line">       session_id,</span><br><span class="line">       user_id,</span><br><span class="line">       version_code,</span><br><span class="line">       sc,</span><br><span class="line">       video_id,</span><br><span class="line">       video_name,</span><br><span class="line">       chapter_id,</span><br><span class="line">       chapter_name,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       play_sec,</span><br><span class="line">       ts</span><br><span class="line">from (select common.mid,</span><br><span class="line">             common.ar              province_id,</span><br><span class="line">             common.ba              brand,</span><br><span class="line">             common.is_new,</span><br><span class="line">             common.md              model,</span><br><span class="line">             common.os,</span><br><span class="line">             common.sid             session_id,</span><br><span class="line">             common.uid             user_id,</span><br><span class="line">             common.vc              version_code,</span><br><span class="line">             common.sc,</span><br><span class="line">             appvideo.video_id,</span><br><span class="line">             sum(appvideo.play_sec) play_sec,</span><br><span class="line">             max(ts)                ts</span><br><span class="line">      from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">      where dt = &#x27;$do_date&#x27;</span><br><span class="line">        and appvideo is not null</span><br><span class="line">      group by common.mid,</span><br><span class="line">               common.ar,</span><br><span class="line">               common.ba,</span><br><span class="line">               common.is_new,</span><br><span class="line">               common.md,</span><br><span class="line">               common.os,</span><br><span class="line">               common.sid,</span><br><span class="line">               common.uid,</span><br><span class="line">               common.vc,</span><br><span class="line">               common.sc,</span><br><span class="line">               appvideo.video_id) aggred</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             video_name,</span><br><span class="line">             chapter_id,</span><br><span class="line">             chapter_name,</span><br><span class="line">             course_id</span><br><span class="line">      from $&#123;APP&#125;.dim_video_full</span><br><span class="line">      where dt = &#x27;$do_date&#x27;) dim_video</span><br><span class="line">     on aggred.video_id = dim_video.id</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             course_name</span><br><span class="line">      from $&#123;APP&#125;.dim_course_full</span><br><span class="line">      where dt = &#x27;$do_date&#x27;) dim_course</span><br><span class="line">     on course_id = dim_course.id;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_user_register_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_user_register_inc partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select </span><br><span class="line">	register.id,</span><br><span class="line">	register_time,</span><br><span class="line">	register_date,</span><br><span class="line">	province_id,</span><br><span class="line">	version_code,</span><br><span class="line">	mid_id,</span><br><span class="line">	brand,</span><br><span class="line">	model,</span><br><span class="line">	operate_system</span><br><span class="line">from </span><br><span class="line">	(</span><br><span class="line">	select </span><br><span class="line">		data.id,</span><br><span class="line">		data.create_time register_time,</span><br><span class="line">		date_format(data.create_time, &#x27;yyyy-MM-dd&#x27;) register_date</span><br><span class="line">	from $&#123;APP&#125;.ods_user_info_inc</span><br><span class="line">	where type = &#x27;insert&#x27; and dt = &#x27;$do_date&#x27;</span><br><span class="line">	) register</span><br><span class="line">left join</span><br><span class="line">	(</span><br><span class="line">	select</span><br><span class="line">		common.uid      user_id,</span><br><span class="line">		min(common.ar)  province_id,</span><br><span class="line">		min(common.vc)  version_code,</span><br><span class="line">		min(common.mid) mid_id,</span><br><span class="line">		min(common.ba)  brand,</span><br><span class="line">		min(common.md)  model,</span><br><span class="line">		min(common.os)  operate_system</span><br><span class="line">	from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">	where dt = &#x27;$do_date&#x27; and \`start\` is not null</span><br><span class="line">	group by common.uid</span><br><span class="line">	) log_dim</span><br><span class="line">	on register.id = log_dim.user_id;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">dwd_user_login_inc=&quot;</span><br><span class="line">set hive.cbo.enable=false;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dwd_user_login_inc</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;) date_id,</span><br><span class="line">       date_format(from_utc_timestamp(ts, &#x27;GMT+8&#x27;), &#x27;yyyy-MM-dd&#x27;) login_time,</span><br><span class="line">       province_id,</span><br><span class="line">       version_code,</span><br><span class="line">       mid_id,</span><br><span class="line">       brand,</span><br><span class="line">       model,</span><br><span class="line">       operate_system</span><br><span class="line">from (select user_id,</span><br><span class="line">             province_id,</span><br><span class="line">             version_code,</span><br><span class="line">             mid_id,</span><br><span class="line">             brand,</span><br><span class="line">             model,</span><br><span class="line">             operate_system,</span><br><span class="line">             row_number() over (partition by session_id order by ts) rk,</span><br><span class="line">             ts</span><br><span class="line">      from (select common.ar  province_id,</span><br><span class="line">                   common.ba  brand,</span><br><span class="line">                   common.md  model,</span><br><span class="line">                   common.mid mid_id,</span><br><span class="line">                   common.os  operate_system,</span><br><span class="line">                   common.sid session_id,</span><br><span class="line">                   common.vc  version_code,</span><br><span class="line">                   common.uid user_id,</span><br><span class="line">                   ts</span><br><span class="line">            from $&#123;APP&#125;.ods_log_inc</span><br><span class="line">            where page is not null</span><br><span class="line">   				and common.uid is not null</span><br><span class="line">              and dt = &#x27;$do_date&#x27;) log</span><br><span class="line">     ) rkt</span><br><span class="line">where rk = 1;</span><br><span class="line">set hive.cbo.enable=true;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    dwd_trade_cart_add_inc|dwd_trade_cart_full|dwd_trade_course_order_inc|dwd_trade_order_detail_inc|dwd_trade_pay_detail_suc_inc|dwd_traffic_page_view_inc|dwd_traffic_start_inc|dwd_traffic_action_inc|dwd_traffic_display_inc|dwd_traffic_error_inc|dwd_interaction_favor_add_inc|dwd_interaction_comment_inc|dwd_interaction_review_inc|dwd_examination_test_paper_inc|dwd_examination_test_question_inc|dwd_learn_play_stats_full|dwd_learn_play_inc|dwd_user_register_inc|dwd_user_login_inc)</span><br><span class="line">        eval &quot;hive -e \&quot;\$$1\&quot;&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot; )</span><br><span class="line">        hive -e &quot;$&#123;dwd_trade_cart_add_inc&#125;$&#123;dwd_trade_cart_full&#125;$&#123;dwd_trade_course_order_inc&#125;$&#123;dwd_trade_order_detail_inc&#125;$&#123;dwd_trade_pay_detail_suc_inc&#125;$&#123;dwd_traffic_page_view_inc&#125;$&#123;dwd_traffic_start_inc&#125;$&#123;dwd_traffic_action_inc&#125;$&#123;dwd_traffic_display_inc&#125;$&#123;dwd_traffic_error_inc&#125;$&#123;dwd_interaction_favor_add_inc&#125;$&#123;dwd_interaction_comment_inc&#125;$&#123;dwd_interaction_review_inc&#125;$&#123;dwd_examination_test_paper_inc&#125;$&#123;dwd_examination_test_question_inc&#125;$&#123;dwd_learn_play_stats_full&#125;$&#123;dwd_learn_play_inc&#125;$&#123;dwd_user_register_inc&#125;$&#123;dwd_user_login_inc&#125;&quot;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）增加权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 ods_to_dwd.sh</span><br></pre></td></tr></table></figure>

<p>（3）用法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# ods_to_dwd.sh all 2022-02-22</span><br></pre></td></tr></table></figure>

<h2 id="第十章-数仓开发之DWS层"><a href="#第十章-数仓开发之DWS层" class="headerlink" title="第十章 数仓开发之DWS层"></a>第十章 数仓开发之DWS层</h2><p>dws层是为ads层服务的，也可以说给ads层提供提前的计算，提供给ads层多个需求共同使用的相同子查询，在ads层我们需要计算各项业务指标，比如交易主题的</p>
<ul>
<li>最近1&#x2F;7&#x2F;30日下单总额</li>
<li>最近1&#x2F;7&#x2F;30日下单次数</li>
<li>最近1&#x2F;7&#x2F;30日下单人数</li>
<li>最近1&#x2F;7&#x2F;30日各省份下单次数</li>
<li>最近1&#x2F;7&#x2F;30日各省份下单人数</li>
<li>最近1&#x2F;7&#x2F;30日各省份下单金额</li>
<li>最近1&#x2F;7&#x2F;30日各来源销售额</li>
</ul>
<p>都是由<strong>最近1日的各会话下单次数</strong>和<strong>最近1日的各会话下单金额</strong>这个两个同粒度指标派生出来的，所以我们对下单次数、下单金额等度量按照会话粒度进行汇总计算，设计dws表“交易域会话粒度用户下单最近1日汇总表”，以此类推，将这些业务指标共同依赖的粒度最大的派生指标做（最大公约数）成表格即为dws表。</p>
<p><img src="Snipaste_2023-11-22_10-47-27.png" alt="Snipaste_2023-11-22_10-47-27"></p>
<p>设计要点：</p>
<p>（1）DWS层的设计参考指标体系</p>
<p>（2）DWS层的数据存储格式为orc列式存储+snappy压缩。</p>
<p>（3）DWS层表名的命名规范为dws_数据域_统计粒度_业务过程_统计周期（1d&#x2F;nd&#x2F;td）</p>
<p>注：1d标识最近1日，nd表示最近n日，td表示历史至今。</p>
<h3 id="10-1-最近1日汇总表"><a href="#10-1-最近1日汇总表" class="headerlink" title="10.1 最近1日汇总表"></a>10.1 最近1日汇总表</h3><h4 id="10-1-1-交易域用户粒度用户加购最近1日汇总表"><a href="#10-1-1-交易域用户粒度用户加购最近1日汇总表" class="headerlink" title="10.1.1 交易域用户粒度用户加购最近1日汇总表"></a>10.1.1 交易域用户粒度用户加购最近1日汇总表</h4><p><img src="Snipaste_2023-11-22_10-56-45.png" alt="Snipaste_2023-11-22_10-56-45"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_trade_user_cart_add_1d;</span><br><span class="line">CREATE EXTERNAL TABLE dws_trade_user_cart_add_1d</span><br><span class="line">(</span><br><span class="line">    `user_id`      STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `course_count` BIGINT COMMENT &#x27;加购课程数&#x27;</span><br><span class="line">) COMMENT &#x27;交易域用户粒度用户加购最近1日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_trade_user_cart_add_1d/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table edu.dws_trade_user_cart_add_1d</span><br><span class="line">	partition(dt)</span><br><span class="line">select user_id,</span><br><span class="line">       count(course_id) course_count,</span><br><span class="line">       dt</span><br><span class="line">from edu.dwd_trade_cart_add_inc</span><br><span class="line">group by user_id, dt;</span><br></pre></td></tr></table></figure>

<p>每日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_trade_user_cart_add_1d</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       count(course_id) course_count</span><br><span class="line">from edu.dwd_trade_cart_add_inc</span><br><span class="line">where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">group by user_id;</span><br></pre></td></tr></table></figure>

<h4 id="10-1-2-交易域用户粒度用户支付最近1日汇总表"><a href="#10-1-2-交易域用户粒度用户支付最近1日汇总表" class="headerlink" title="10.1.2 交易域用户粒度用户支付最近1日汇总表"></a>10.1.2 交易域用户粒度用户支付最近1日汇总表</h4><p><img src="Snipaste_2023-11-22_10-59-42.png" alt="Snipaste_2023-11-22_10-59-42"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_trade_user_payment_1d;</span><br><span class="line">CREATE EXTERNAL TABLE dws_trade_user_payment_1d</span><br><span class="line">(</span><br><span class="line">    `user_id`       STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `payment_count` BIGINT COMMENT &#x27;支付次数&#x27;</span><br><span class="line">) COMMENT &#x27;交易域用户粒度用户支付最近1日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_trade_user_payment_1d/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table edu.dws_trade_user_payment_1d</span><br><span class="line">partition (dt)</span><br><span class="line">select user_id,</span><br><span class="line">       count(distinct order_id) payment_count,</span><br><span class="line">       dt</span><br><span class="line">from edu.dwd_trade_pay_detail_suc_inc</span><br><span class="line">group by user_id, dt;</span><br></pre></td></tr></table></figure>

<p>每日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_trade_user_payment_1d</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       count(distinct order_id) payment_count</span><br><span class="line">from edu.dwd_trade_pay_detail_suc_inc</span><br><span class="line">where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">group by user_id;</span><br></pre></td></tr></table></figure>

<h4 id="10-1-3-交易域会话粒度用户下单最近一日汇总表"><a href="#10-1-3-交易域会话粒度用户下单最近一日汇总表" class="headerlink" title="10.1.3 交易域会话粒度用户下单最近一日汇总表"></a>10.1.3 交易域会话粒度用户下单最近一日汇总表</h4><p><img src="Snipaste_2023-11-22_11-04-10.png" alt="Snipaste_2023-11-22_11-04-10"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_trade_session_order_1d;</span><br><span class="line">CREATE EXTERNAL TABLE dws_trade_session_order_1d</span><br><span class="line">(</span><br><span class="line">    `session_id`   STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `user_id`      STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `province_id`  STRING COMMENT &#x27;省份id&#x27;,</span><br><span class="line">    `source_id`    STRING COMMENT &#x27;引流来源id&#x27;,</span><br><span class="line">    `source_site`  STRING COMMENT &#x27;引流名称&#x27;,</span><br><span class="line">    `order_count`  BIGINT COMMENT &#x27;下单次数&#x27;,</span><br><span class="line">    `order_amount` DECIMAL(16, 2) COMMENT &#x27;下单金额&#x27;</span><br><span class="line">) COMMENT &#x27;交易域会话粒度用户下单最近1日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_trade_session_order_1d/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table  edu.dws_trade_session_order_1d</span><br><span class="line">partition (dt)</span><br><span class="line">select session_id,</span><br><span class="line">       user_id,</span><br><span class="line">       province_id,</span><br><span class="line">       source_id,</span><br><span class="line">       source_site,</span><br><span class="line">       order_count,</span><br><span class="line">       order_amount,</span><br><span class="line">       dt</span><br><span class="line">from (</span><br><span class="line">         select session_id,</span><br><span class="line">                user_id,</span><br><span class="line">                province_id,</span><br><span class="line">                source_id,</span><br><span class="line">                count(distinct order_id) order_count,</span><br><span class="line">                sum(final_amount)        order_amount,</span><br><span class="line">                dt</span><br><span class="line">         from  edu.dwd_trade_order_detail_inc</span><br><span class="line">         group by session_id, user_id, province_id, source_id, dt</span><br><span class="line">     ) ag_t</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                source_site</span><br><span class="line">         from edu.dim_source_full</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">     ) d_source</span><br><span class="line">     on ag_t.source_id = d_source.id;</span><br></pre></td></tr></table></figure>

<p>每日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table  edu.dws_trade_session_order_1d</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select session_id,</span><br><span class="line">       user_id,</span><br><span class="line">       province_id,</span><br><span class="line">       source_id,</span><br><span class="line">       source_site,</span><br><span class="line">       order_count,</span><br><span class="line">       order_amount</span><br><span class="line">from (select session_id,</span><br><span class="line">             user_id,</span><br><span class="line">             province_id,</span><br><span class="line">             source_id,</span><br><span class="line">             count(distinct order_id) order_count,</span><br><span class="line">             sum(final_amount)        order_amount</span><br><span class="line">      from  edu.dwd_trade_order_detail_inc</span><br><span class="line">      where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">      group by session_id, user_id, province_id, source_id) ag_t</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             source_site</span><br><span class="line">      from edu.dim_source_full</span><br><span class="line">      where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">     ) d_source</span><br><span class="line">     on ag_t.source_id = d_source.id;</span><br></pre></td></tr></table></figure>

<h4 id="10-1-4-考试域试卷粒度考试最近1日汇总表"><a href="#10-1-4-考试域试卷粒度考试最近1日汇总表" class="headerlink" title="10.1.4 考试域试卷粒度考试最近1日汇总表"></a>10.1.4 考试域试卷粒度考试最近1日汇总表</h4><p><img src="Snipaste_2023-11-22_11-07-46.png" alt="Snipaste_2023-11-22_11-07-46"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_examination_paper_exam_1d;</span><br><span class="line">CREATE EXTERNAL TABLE dws_examination_paper_exam_1d</span><br><span class="line">(</span><br><span class="line">    `paper_id`         STRING COMMENT &#x27;试卷id&#x27;,</span><br><span class="line">    `paper_title`      STRING COMMENT &#x27;试卷标题&#x27;,</span><br><span class="line">    `course_id`        STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `course_name`      STRING COMMENT &#x27;课程名称&#x27;,</span><br><span class="line">    `subject_id`       STRING COMMENT &#x27;学科id&#x27;,</span><br><span class="line">    `subject_name`     STRING COMMENT &#x27;学科名称&#x27;,</span><br><span class="line">    `category_id`      STRING COMMENT &#x27;分类id&#x27;,</span><br><span class="line">    `category_name`    STRING COMMENT &#x27;分类名称&#x27;,</span><br><span class="line">    `avg_score`        DECIMAL(16, 2) COMMENT &#x27;平均分&#x27;,</span><br><span class="line">    `avg_during_sec`   BIGINT COMMENT &#x27;平均时长&#x27;,</span><br><span class="line">    `total_score`      BIGINT COMMENT &#x27;总分&#x27;,</span><br><span class="line">    `total_during_sec` BIGINT COMMENT &#x27;总时长&#x27;,</span><br><span class="line">    `user_count`       BIGINT COMMENT &#x27;用户数&#x27;</span><br><span class="line">) COMMENT &#x27;考试域试卷粒度考试最近1日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_examination_paper_exam_1d/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table edu.dws_examination_paper_exam_1d</span><br><span class="line">    partition (dt)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       avg_score,</span><br><span class="line">       avg_during_sec,</span><br><span class="line">       total_score,</span><br><span class="line">       total_during_sec,</span><br><span class="line">       user_count,</span><br><span class="line">       dt</span><br><span class="line">from (select paper_id,</span><br><span class="line">             avg(score)        avg_score,</span><br><span class="line">             avg(duration_sec) avg_during_sec,</span><br><span class="line">             sum(score)        total_score,</span><br><span class="line">             sum(duration_sec) total_during_sec,</span><br><span class="line">             count(user_id)    user_count,</span><br><span class="line">             dt</span><br><span class="line">      from edu.dwd_examination_test_paper_inc</span><br><span class="line">      group by paper_id, dt) ex</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             paper_title,</span><br><span class="line">             course_id</span><br><span class="line">      from edu.dim_paper_full</span><br><span class="line">      where dt = &#x27;2022-02-21&#x27;) paper</span><br><span class="line">     on ex.paper_id = paper.id</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             course_name,</span><br><span class="line">             subject_id,</span><br><span class="line">             subject_name,</span><br><span class="line">             category_id,</span><br><span class="line">             category_name</span><br><span class="line">      from edu.dim_course_full</span><br><span class="line">      where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">     ) dim_course</span><br><span class="line">     on paper.course_id = dim_course.id;</span><br></pre></td></tr></table></figure>

<p>每日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_examination_paper_exam_1d</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       avg_score,</span><br><span class="line">       avg_during_sec,</span><br><span class="line">       total_score,</span><br><span class="line">       total_during_sec,</span><br><span class="line">       user_count</span><br><span class="line">from (select paper_id,</span><br><span class="line">             avg(score)        avg_score,</span><br><span class="line">             avg(duration_sec) avg_during_sec,</span><br><span class="line">             sum(score)        total_score,</span><br><span class="line">             sum(duration_sec) total_during_sec,</span><br><span class="line">             count(user_id)    user_count</span><br><span class="line">      from edu.dwd_examination_test_paper_inc</span><br><span class="line">      where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">      group by paper_id) exa</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             paper_title,</span><br><span class="line">             course_id</span><br><span class="line">      from edu.dim_paper_full</span><br><span class="line">      where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">     ) paper</span><br><span class="line">     on exa.paper_id = paper.id</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             course_name,</span><br><span class="line">             subject_id,</span><br><span class="line">             subject_name,</span><br><span class="line">             category_id,</span><br><span class="line">             category_name</span><br><span class="line">      from edu.dim_course_full</span><br><span class="line">      where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">     ) dim_course</span><br><span class="line">     on paper.course_id = dim_course.id;</span><br></pre></td></tr></table></figure>

<h4 id="10-1-5-考试域试卷分数段粒度考试最近1日汇总表"><a href="#10-1-5-考试域试卷分数段粒度考试最近1日汇总表" class="headerlink" title="10.1.5 考试域试卷分数段粒度考试最近1日汇总表"></a>10.1.5 考试域试卷分数段粒度考试最近1日汇总表</h4><p><img src="Snipaste_2023-11-22_11-10-35.png" alt="Snipaste_2023-11-22_11-10-35"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_examination_paper_duration_exam_1d;</span><br><span class="line">CREATE EXTERNAL TABLE dws_examination_paper_duration_exam_1d</span><br><span class="line">(</span><br><span class="line">    `paper_id`      STRING COMMENT &#x27;试卷id&#x27;,</span><br><span class="line">    `paper_title`   STRING COMMENT &#x27;试卷名称&#x27;,</span><br><span class="line">    `duration_name` STRING COMMENT &#x27;分数区间&#x27;,</span><br><span class="line">    `user_count`    BIGINT COMMENT &#x27;用户数&#x27;</span><br><span class="line">) COMMENT &#x27;考试域试卷-分数段粒度最近1日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_examination_paper_duration_exam_1d/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table edu.dws_examination_paper_duration_exam_1d</span><br><span class="line">    partition (dt)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       duration_name,</span><br><span class="line">       user_count,</span><br><span class="line">       dt</span><br><span class="line">from (select paper_id,</span><br><span class="line">             duration_name,</span><br><span class="line">             count(user_id) user_count,</span><br><span class="line">             dt</span><br><span class="line">      from (select paper_id,</span><br><span class="line">                   case</span><br><span class="line">                       when score &gt;= 0 and score &lt; 60</span><br><span class="line">                           then &#x27;[0, 60)&#x27;</span><br><span class="line">                       when score &gt;= 60 and score &lt; 70</span><br><span class="line">                           then &#x27;[60, 70)&#x27;</span><br><span class="line">                       when score &gt;= 70 and score &lt; 80</span><br><span class="line">                           then &#x27;[70, 80)&#x27;</span><br><span class="line">                       when score &gt;= 80 and score &lt; 90</span><br><span class="line">                           then &#x27;[80, 90)&#x27;</span><br><span class="line">                       when score &gt;= 90 and score &lt;= 100</span><br><span class="line">                           then &#x27;[90, 100]&#x27;</span><br><span class="line">                       end duration_name,</span><br><span class="line">                   user_id,</span><br><span class="line">                   dt</span><br><span class="line">            from edu.dwd_examination_test_paper_inc) origin</span><br><span class="line">      group by paper_id, duration_name, dt) dur</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             paper_title</span><br><span class="line">      from edu.dim_paper_full</span><br><span class="line">      where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">     ) paper</span><br><span class="line">     on dur.paper_id = paper.id;</span><br></pre></td></tr></table></figure>

<p>每日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_examination_paper_duration_exam_1d</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       duration_name,</span><br><span class="line">       user_count</span><br><span class="line">from (</span><br><span class="line">         select paper_id,</span><br><span class="line">                duration_name,</span><br><span class="line">                count(user_id) user_count</span><br><span class="line">         from (select paper_id,</span><br><span class="line">                      case</span><br><span class="line">                          when score &gt;= 0 and score &lt; 60 then &#x27;[0, 60)&#x27;</span><br><span class="line">                          when score &gt;= 60 and score &lt; 70 then &#x27;[60, 70)&#x27;</span><br><span class="line">                          when score &gt;= 70 and score &lt; 80 then &#x27;[70, 80)&#x27;</span><br><span class="line">                          when score &gt;= 80 and score &lt; 90 then &#x27;[80, 90)&#x27;</span><br><span class="line">                          when score &gt;= 90 and score &lt;= 100 then &#x27;[90, 100]&#x27;</span><br><span class="line">                          end duration_name,</span><br><span class="line">                      user_id</span><br><span class="line">               from edu.dwd_examination_test_paper_inc</span><br><span class="line">               where dt = &#x27;2022-02-22&#x27;) orgin</span><br><span class="line">         group by paper_id, duration_name</span><br><span class="line">     ) dur</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                paper_title</span><br><span class="line">         from edu.dim_paper_full</span><br><span class="line">         where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">     ) paper</span><br><span class="line">     on dur.paper_id = paper.id;</span><br></pre></td></tr></table></figure>

<h4 id="10-1-6-考试域题目粒度考试最近1日汇总表"><a href="#10-1-6-考试域题目粒度考试最近1日汇总表" class="headerlink" title="10.1.6 考试域题目粒度考试最近1日汇总表"></a>10.1.6 考试域题目粒度考试最近1日汇总表</h4><p><img src="Snipaste_2023-11-22_11-13-46.png" alt="Snipaste_2023-11-22_11-13-46"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_examination_question_exam_1d;</span><br><span class="line">CREATE EXTERNAL TABLE dws_examination_question_exam_1d</span><br><span class="line">(</span><br><span class="line">    `question_id`   STRING COMMENT &#x27;题目id&#x27;,</span><br><span class="line">    `correct_count` BIGINT COMMENT &#x27;正确答题次数&#x27;,</span><br><span class="line">    `answer_count`  BIGINT COMMENT &#x27;答题次数&#x27;</span><br><span class="line">) COMMENT &#x27;考试域题目粒度考试最近1日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_examination_question_exam_1d/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table edu.dws_examination_question_exam_1d</span><br><span class="line">    partition (dt)</span><br><span class="line">select question_id,</span><br><span class="line">             sum(if(is_correct = 1, 1, 0))          correct_count,</span><br><span class="line">             count(*)                               answer_count,</span><br><span class="line">             date_format(create_time, &#x27;yyyy-MM-dd&#x27;) dt</span><br><span class="line">      from edu.dwd_examination_test_question_inc</span><br><span class="line">      group by question_id, date_format(create_time, &#x27;yyyy-MM-dd&#x27;);</span><br></pre></td></tr></table></figure>

<p>每日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_examination_question_exam_1d</span><br><span class="line">    partition (dt = &#x27;2022-02-22&#x27;)</span><br><span class="line">select question_id,</span><br><span class="line">             sum(if(is_correct = 1, 1, 0))          correct_count,</span><br><span class="line">             count(*)                               answer_count</span><br><span class="line">      from edu.dwd_examination_test_question_inc</span><br><span class="line">      where dt = &#x27;2022-02-22&#x27;</span><br><span class="line">      group by question_id;</span><br></pre></td></tr></table></figure>

<h4 id="10-1-7-流量域会话粒度页面浏览最近1日汇总表"><a href="#10-1-7-流量域会话粒度页面浏览最近1日汇总表" class="headerlink" title="10.1.7 流量域会话粒度页面浏览最近1日汇总表"></a>10.1.7 流量域会话粒度页面浏览最近1日汇总表</h4><p><img src="Snipaste_2023-11-22_14-27-21.png" alt="Snipaste_2023-11-22_14-27-21"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_traffic_session_page_view_1d;</span><br><span class="line">CREATE EXTERNAL TABLE dws_traffic_session_page_view_1d</span><br><span class="line">(</span><br><span class="line">    `session_id`  STRING COMMENT &#x27;会话id&#x27;,</span><br><span class="line">    `mid_id`      STRING COMMENT &#x27;设备id&#x27;,</span><br><span class="line">    `user_id`     STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `source_id`   STRING COMMENT &#x27;引流来源id&#x27;,</span><br><span class="line">    `source_site` STRING COMMENT &#x27;引流来源名称&#x27;,</span><br><span class="line">    `page_count`  BIGINT COMMENT &#x27;页面总数&#x27;,</span><br><span class="line">    `during_time` BIGINT COMMENT &#x27;停留时长,单位: 毫秒&#x27;</span><br><span class="line">) COMMENT &#x27;流量域会话粒度页面浏览最近1日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_traffic_session_page_view_1d/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_traffic_session_page_view_1d</span><br><span class="line">  partition(dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select session_id,</span><br><span class="line">       mid_id,</span><br><span class="line">       user_id,</span><br><span class="line">       source_id,</span><br><span class="line">       source_site,</span><br><span class="line">       page_count,</span><br><span class="line">       during_time</span><br><span class="line">from (</span><br><span class="line">         select session_id,</span><br><span class="line">                max(mid_id)      mid_id,</span><br><span class="line">                max(user_id)     user_id,</span><br><span class="line">                max(source_id)   source_id,</span><br><span class="line">                count(*)         page_count,</span><br><span class="line">                sum(during_time) during_time,</span><br><span class="line">                dt</span><br><span class="line">         from edu.dwd_traffic_page_view_inc</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">         group by session_id, dt</span><br><span class="line">     ) pv</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                source_site</span><br><span class="line">         from edu.dim_source_full</span><br><span class="line">         where dt = &#x27;2022-02-21&#x27;</span><br><span class="line">     ) dim</span><br><span class="line">     on pv.source_id = dim.id;</span><br></pre></td></tr></table></figure>

<h4 id="10-1-8-数据装载脚本"><a href="#10-1-8-数据装载脚本" class="headerlink" title="10.1.8 数据装载脚本"></a>10.1.8 数据装载脚本</h4><p>（1）首日数据装载脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim dwd_to_dws_1d_init.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    echo &quot;请传入日期参数&quot;</span><br><span class="line">    exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">APP=edu</span><br><span class="line"></span><br><span class="line">dws_trade_user_cart_add_1d=&quot;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_trade_user_cart_add_1d</span><br><span class="line">	partition(dt)</span><br><span class="line">select user_id,</span><br><span class="line">       count(course_id) course_count,</span><br><span class="line">       dt</span><br><span class="line">from $&#123;APP&#125;.dwd_trade_cart_add_inc</span><br><span class="line">group by user_id, dt;&quot;</span><br><span class="line"></span><br><span class="line">dws_trade_user_payment_1d=&quot;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_trade_user_payment_1d</span><br><span class="line">partition (dt)</span><br><span class="line">select user_id,</span><br><span class="line">       count(distinct order_id) payment_count,</span><br><span class="line">       dt</span><br><span class="line">from $&#123;APP&#125;.dwd_trade_pay_detail_suc_inc</span><br><span class="line">group by user_id, dt;&quot;</span><br><span class="line"></span><br><span class="line">dws_trade_session_order_1d=&quot;</span><br><span class="line">insert overwrite table  $&#123;APP&#125;.dws_trade_session_order_1d</span><br><span class="line">partition (dt)</span><br><span class="line">select session_id,</span><br><span class="line">       user_id,</span><br><span class="line">       province_id,</span><br><span class="line">       source_id,</span><br><span class="line">       source_site,</span><br><span class="line">       order_count,</span><br><span class="line">       order_amount,</span><br><span class="line">       dt</span><br><span class="line">from (</span><br><span class="line">         select session_id,</span><br><span class="line">                user_id,</span><br><span class="line">                province_id,</span><br><span class="line">                source_id,</span><br><span class="line">                count(distinct order_id) order_count,</span><br><span class="line">                sum(final_amount)        order_amount,</span><br><span class="line">                dt</span><br><span class="line">         from  $&#123;APP&#125;.dwd_trade_order_detail_inc</span><br><span class="line">         group by session_id, user_id, province_id, source_id, dt</span><br><span class="line">     ) ag_t</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                source_site</span><br><span class="line">         from $&#123;APP&#125;.dim_source_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) d_source</span><br><span class="line">     on ag_t.source_id = d_source.id;&quot;</span><br><span class="line"></span><br><span class="line">dws_examination_paper_exam_1d=&quot;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_examination_paper_exam_1d</span><br><span class="line">    partition (dt)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       avg_score,</span><br><span class="line">       avg_during_sec,</span><br><span class="line">       total_score,</span><br><span class="line">       total_during_sec,</span><br><span class="line">       user_count,</span><br><span class="line">       dt</span><br><span class="line">from (select paper_id,</span><br><span class="line">             avg(score)        avg_score,</span><br><span class="line">             avg(duration_sec) avg_during_sec,</span><br><span class="line">             sum(score)        total_score,</span><br><span class="line">             sum(duration_sec) total_during_sec,</span><br><span class="line">             count(user_id)    user_count,</span><br><span class="line">             dt</span><br><span class="line">      from $&#123;APP&#125;.dwd_examination_test_paper_inc</span><br><span class="line">      group by paper_id, dt) ex</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             paper_title,</span><br><span class="line">             course_id</span><br><span class="line">      from $&#123;APP&#125;.dim_paper_full</span><br><span class="line">      where dt = &#x27;$do_date&#x27;) paper</span><br><span class="line">     on ex.paper_id = paper.id</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             course_name,</span><br><span class="line">             subject_id,</span><br><span class="line">             subject_name,</span><br><span class="line">             category_id,</span><br><span class="line">             category_name</span><br><span class="line">      from $&#123;APP&#125;.dim_course_full</span><br><span class="line">      where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) dim_course</span><br><span class="line">     on paper.course_id = dim_course.id;&quot;</span><br><span class="line"></span><br><span class="line">dws_examination_paper_duration_exam_1d=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_examination_paper_duration_exam_1d</span><br><span class="line">    partition (dt)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       duration_name,</span><br><span class="line">       user_count,</span><br><span class="line">       dt</span><br><span class="line">from (select paper_id,</span><br><span class="line">             duration_name,</span><br><span class="line">             count(user_id) user_count,</span><br><span class="line">             dt</span><br><span class="line">      from (select paper_id,</span><br><span class="line">                   case</span><br><span class="line">                       when score &gt;= 0 and score &lt; 60</span><br><span class="line">                           then &#x27;[0, 60)&#x27;</span><br><span class="line">                       when score &gt;= 60 and score &lt; 70</span><br><span class="line">                           then &#x27;[60, 70)&#x27;</span><br><span class="line">                       when score &gt;= 70 and score &lt; 80</span><br><span class="line">                           then &#x27;[70, 80)&#x27;</span><br><span class="line">                       when score &gt;= 80 and score &lt; 90</span><br><span class="line">                           then &#x27;[80, 90)&#x27;</span><br><span class="line">                       when score &gt;= 90 and score &lt;= 100</span><br><span class="line">                           then &#x27;[90, 100]&#x27;</span><br><span class="line">                       end duration_name,</span><br><span class="line">                   user_id,</span><br><span class="line">                   dt</span><br><span class="line">            from $&#123;APP&#125;.dwd_examination_test_paper_inc) origin</span><br><span class="line">      group by paper_id, duration_name, dt) dur</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             paper_title</span><br><span class="line">      from $&#123;APP&#125;.dim_paper_full</span><br><span class="line">      where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) paper</span><br><span class="line">     on dur.paper_id = paper.id;&quot;</span><br><span class="line"></span><br><span class="line">dws_examination_question_exam_1d=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_examination_question_exam_1d</span><br><span class="line">    partition (dt)</span><br><span class="line">select question_id,</span><br><span class="line">             sum(if(is_correct = 1, 1, 0))          correct_count,</span><br><span class="line">             count(*)                               answer_count,</span><br><span class="line">             date_format(create_time, &#x27;yyyy-MM-dd&#x27;) dt</span><br><span class="line">      from $&#123;APP&#125;.dwd_examination_test_question_inc</span><br><span class="line">      group by question_id, date_format(create_time, &#x27;yyyy-MM-dd&#x27;);&quot;</span><br><span class="line"></span><br><span class="line">dws_traffic_session_page_view_1d=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_traffic_session_page_view_1d</span><br><span class="line">  partition(dt = &#x27;$do_date&#x27;)</span><br><span class="line">select session_id,</span><br><span class="line">       mid_id,</span><br><span class="line">       user_id,</span><br><span class="line">       source_id,</span><br><span class="line">       source_site,</span><br><span class="line">       page_count,</span><br><span class="line">       during_time</span><br><span class="line">from (</span><br><span class="line">         select session_id,</span><br><span class="line">                max(mid_id)      mid_id,</span><br><span class="line">                max(user_id)     user_id,</span><br><span class="line">                max(source_id)   source_id,</span><br><span class="line">                count(*)         page_count,</span><br><span class="line">                sum(during_time) during_time,</span><br><span class="line">                dt</span><br><span class="line">         from $&#123;APP&#125;.dwd_traffic_page_view_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">         group by session_id, dt</span><br><span class="line">     ) pv</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                source_site</span><br><span class="line">         from $&#123;APP&#125;.dim_source_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) dim</span><br><span class="line">     on pv.source_id = dim.id;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    dws_trade_user_cart_add_1d|dws_trade_user_payment_1d|dws_trade_session_order_1d|dws_examination_paper_exam_1d|dws_examination_paper_duration_exam_1d|dws_examination_question_exam_1d|dws_traffic_session_page_view_1d)</span><br><span class="line">        eval &quot;hive -e \&quot;\$$1\&quot;&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot; )</span><br><span class="line">        hive -e &quot;$&#123;dws_trade_user_cart_add_1d&#125;$&#123;dws_trade_user_payment_1d&#125;$&#123;dws_trade_session_order_1d&#125;$&#123;dws_examination_paper_exam_1d&#125;$&#123;dws_examination_paper_duration_exam_1d&#125;$&#123;dws_examination_question_exam_1d&#125;$&#123;dws_traffic_session_page_view_1d&#125;&quot;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 dwd_to_dws_1d_init.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# dwd_to_dws_1d_init.sh all 2022-02-21</span><br></pre></td></tr></table></figure>

<p>（2）每日数据装载脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim dwd_to_dws_1d.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">APP=edu</span><br><span class="line"></span><br><span class="line">dws_trade_user_cart_add_1d=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_trade_user_cart_add_1d</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       count(course_id) course_count</span><br><span class="line">from $&#123;APP&#125;.dwd_trade_cart_add_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">group by user_id;&quot;</span><br><span class="line"></span><br><span class="line">dws_trade_user_payment_1d=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_trade_user_payment_1d</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       count(distinct order_id) payment_count</span><br><span class="line">from $&#123;APP&#125;.dwd_trade_pay_detail_suc_inc</span><br><span class="line">where dt = &#x27;$do_date&#x27;</span><br><span class="line">group by user_id;&quot;</span><br><span class="line"></span><br><span class="line">dws_trade_session_order_1d=&quot;</span><br><span class="line">insert overwrite table  $&#123;APP&#125;.dws_trade_session_order_1d</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select session_id,</span><br><span class="line">       user_id,</span><br><span class="line">       province_id,</span><br><span class="line">       source_id,</span><br><span class="line">       source_site,</span><br><span class="line">       order_count,</span><br><span class="line">       order_amount</span><br><span class="line">from (select session_id,</span><br><span class="line">             user_id,</span><br><span class="line">             province_id,</span><br><span class="line">             source_id,</span><br><span class="line">             count(distinct order_id) order_count,</span><br><span class="line">             sum(final_amount)        order_amount</span><br><span class="line">      from  $&#123;APP&#125;.dwd_trade_order_detail_inc</span><br><span class="line">      where dt = &#x27;$do_date&#x27;</span><br><span class="line">      group by session_id, user_id, province_id, source_id) ag_t</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             source_site</span><br><span class="line">      from $&#123;APP&#125;.dim_source_full</span><br><span class="line">      where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) d_source</span><br><span class="line">     on ag_t.source_id = d_source.id;&quot;</span><br><span class="line"></span><br><span class="line">dws_examination_paper_exam_1d=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_examination_paper_exam_1d</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       avg_score,</span><br><span class="line">       avg_during_sec,</span><br><span class="line">       total_score,</span><br><span class="line">       total_during_sec,</span><br><span class="line">       user_count</span><br><span class="line">from (select paper_id,</span><br><span class="line">             avg(score)        avg_score,</span><br><span class="line">             avg(duration_sec) avg_during_sec,</span><br><span class="line">             sum(score)        total_score,</span><br><span class="line">             sum(duration_sec) total_during_sec,</span><br><span class="line">             count(user_id)    user_count</span><br><span class="line">      from $&#123;APP&#125;.dwd_examination_test_paper_inc</span><br><span class="line">      where dt = &#x27;$do_date&#x27;</span><br><span class="line">      group by paper_id) exa</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             paper_title,</span><br><span class="line">             course_id</span><br><span class="line">      from $&#123;APP&#125;.dim_paper_full</span><br><span class="line">      where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) paper</span><br><span class="line">     on exa.paper_id = paper.id</span><br><span class="line">         left join</span><br><span class="line">     (select id,</span><br><span class="line">             course_name,</span><br><span class="line">             subject_id,</span><br><span class="line">             subject_name,</span><br><span class="line">             category_id,</span><br><span class="line">             category_name</span><br><span class="line">      from $&#123;APP&#125;.dim_course_full</span><br><span class="line">      where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) dim_course</span><br><span class="line">     on paper.course_id = dim_course.id;&quot;</span><br><span class="line"></span><br><span class="line">dws_examination_paper_duration_exam_1d=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_examination_paper_duration_exam_1d</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       duration_name,</span><br><span class="line">       user_count</span><br><span class="line">from (</span><br><span class="line">         select paper_id,</span><br><span class="line">                duration_name,</span><br><span class="line">                count(user_id) user_count</span><br><span class="line">         from (select paper_id,</span><br><span class="line">                      case</span><br><span class="line">                          when score &gt;= 0 and score &lt; 60 then &#x27;[0, 60)&#x27;</span><br><span class="line">                          when score &gt;= 60 and score &lt; 70 then &#x27;[60, 70)&#x27;</span><br><span class="line">                          when score &gt;= 70 and score &lt; 80 then &#x27;[70, 80)&#x27;</span><br><span class="line">                          when score &gt;= 80 and score &lt; 90 then &#x27;[80, 90)&#x27;</span><br><span class="line">                          when score &gt;= 90 and score &lt;= 100 then &#x27;[90, 100]&#x27;</span><br><span class="line">                          end duration_name,</span><br><span class="line">                      user_id</span><br><span class="line">               from $&#123;APP&#125;.dwd_examination_test_paper_inc</span><br><span class="line">               where dt = &#x27;$do_date&#x27;) orgin</span><br><span class="line">         group by paper_id, duration_name</span><br><span class="line">     ) dur</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                paper_title</span><br><span class="line">         from $&#123;APP&#125;.dim_paper_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) paper</span><br><span class="line">     on dur.paper_id = paper.id;&quot;</span><br><span class="line"></span><br><span class="line">dws_examination_question_exam_1d=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_examination_question_exam_1d</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select question_id,</span><br><span class="line">             sum(if(is_correct = 1, 1, 0))          correct_count,</span><br><span class="line">             count(*)                               answer_count</span><br><span class="line">      from $&#123;APP&#125;.dwd_examination_test_question_inc</span><br><span class="line">      where dt = &#x27;$do_date&#x27;</span><br><span class="line">      group by question_id;&quot;</span><br><span class="line"></span><br><span class="line">dws_traffic_session_page_view_1d=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_traffic_session_page_view_1d</span><br><span class="line">  partition(dt = &#x27;$do_date&#x27;)</span><br><span class="line">select session_id,</span><br><span class="line">       mid_id,</span><br><span class="line">       user_id,</span><br><span class="line">       source_id,</span><br><span class="line">       source_site,</span><br><span class="line">       page_count,</span><br><span class="line">       during_time</span><br><span class="line">from (</span><br><span class="line">         select session_id,</span><br><span class="line">                max(mid_id)      mid_id,</span><br><span class="line">                max(user_id)     user_id,</span><br><span class="line">                max(source_id)   source_id,</span><br><span class="line">                count(*)         page_count,</span><br><span class="line">                sum(during_time) during_time,</span><br><span class="line">                dt</span><br><span class="line">         from $&#123;APP&#125;.dwd_traffic_page_view_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">         group by session_id, dt</span><br><span class="line">     ) pv</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select id,</span><br><span class="line">                source_site</span><br><span class="line">         from $&#123;APP&#125;.dim_source_full</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">     ) dim</span><br><span class="line">     on pv.source_id = dim.id;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    dws_trade_user_cart_add_1d|dws_trade_user_payment_1d|dws_trade_session_order_1d|dws_examination_paper_exam_1d|dws_examination_paper_duration_exam_1d|dws_examination_question_exam_1d|dws_traffic_session_page_view_1d)</span><br><span class="line">        eval &quot;hive -e \&quot;\$$1\&quot;&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot; )</span><br><span class="line">        hive -e &quot;$&#123;dws_trade_user_cart_add_1d&#125;$&#123;dws_trade_user_payment_1d&#125;$&#123;dws_trade_session_order_1d&#125;$&#123;dws_examination_paper_exam_1d&#125;$&#123;dws_examination_paper_duration_exam_1d&#125;$&#123;dws_examination_question_exam_1d&#125;$&#123;dws_traffic_session_page_view_1d&#125;&quot;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 dwd_to_dws_1d.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# dwd_to_dws_1d.sh all 2022-02-22</span><br></pre></td></tr></table></figure>

<h3 id="10-2-最近n日汇总表"><a href="#10-2-最近n日汇总表" class="headerlink" title="10.2 最近n日汇总表"></a>10.2 最近n日汇总表</h3><h4 id="10-2-1-交易域用户粒度用户加购最近n日汇总表"><a href="#10-2-1-交易域用户粒度用户加购最近n日汇总表" class="headerlink" title="10.2.1 交易域用户粒度用户加购最近n日汇总表"></a>10.2.1 交易域用户粒度用户加购最近n日汇总表</h4><p><img src="Snipaste_2023-11-22_10-56-45.png" alt="Snipaste_2023-11-22_10-56-45"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_trade_user_cart_add_nd;</span><br><span class="line">CREATE EXTERNAL TABLE dws_trade_user_cart_add_nd</span><br><span class="line">(</span><br><span class="line">    `user_id`          STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `course_count_7d`  BIGINT COMMENT &#x27;最近7日加购课程数&#x27;,</span><br><span class="line">    `course_count_30d` BIGINT COMMENT &#x27;最近30日加购课程数&#x27;</span><br><span class="line">) COMMENT &#x27;交易域用户粒度用户加购最近n日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_trade_user_cart_add_nd/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<p>通过交易域用户粒度用户加购最近1日汇总表，对筛选日期为最近30日的数据进行进一步汇总，在进行汇总求和时，通过if函数判断日期为7日来产生最近7日的汇总数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_trade_user_cart_add_nd</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), course_count, 0)) course_count_7d,</span><br><span class="line">       sum(course_count)                                          course_count_30d</span><br><span class="line">from edu.dws_trade_user_cart_add_1d</span><br><span class="line">where dt &gt;= date_add(&#x27;2022-02-21&#x27;, -29)</span><br><span class="line">  and dt &lt;= &#x27;2022-02-21&#x27;</span><br><span class="line">group by user_id;</span><br></pre></td></tr></table></figure>

<h4 id="10-2-2-交易域用户粒度用户支付最近n日汇总表"><a href="#10-2-2-交易域用户粒度用户支付最近n日汇总表" class="headerlink" title="10.2.2 交易域用户粒度用户支付最近n日汇总表"></a>10.2.2 交易域用户粒度用户支付最近n日汇总表</h4><p><img src="Snipaste_2023-11-22_10-59-42.png" alt="Snipaste_2023-11-22_10-59-42"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_trade_user_payment_nd;</span><br><span class="line">CREATE EXTERNAL TABLE dws_trade_user_payment_nd</span><br><span class="line">(</span><br><span class="line">    `user_id`           STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `payment_count_7d`  BIGINT COMMENT &#x27;最近7日支付次数&#x27;,</span><br><span class="line">    `payment_count_30d` BIGINT COMMENT &#x27;最近30日支付次数&#x27;</span><br><span class="line">) COMMENT &#x27;交易域用户粒度用户支付最近n日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_trade_user_payment_nd/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_trade_user_payment_nd</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), payment_count, 0)) payment_count_7d,</span><br><span class="line">       sum(payment_count)                                          payment_count_30d</span><br><span class="line">from edu.dws_trade_user_payment_1d</span><br><span class="line">where dt &gt;= date_add(&#x27;2022-02-21&#x27;, -29)</span><br><span class="line">  and dt &lt;= &#x27;2022-02-21&#x27;</span><br><span class="line">group by user_id;</span><br></pre></td></tr></table></figure>

<h4 id="10-2-3-考试域试卷粒度考试最近n日汇总表"><a href="#10-2-3-考试域试卷粒度考试最近n日汇总表" class="headerlink" title="10.2.3 考试域试卷粒度考试最近n日汇总表"></a>10.2.3 考试域试卷粒度考试最近n日汇总表</h4><p><img src="Snipaste_2023-11-22_11-07-46.png" alt="Snipaste_2023-11-22_11-07-46"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_examination_paper_exam_nd;</span><br><span class="line">CREATE EXTERNAL TABLE dws_examination_paper_exam_nd</span><br><span class="line">(</span><br><span class="line">    `paper_id`             STRING COMMENT &#x27;试卷id&#x27;,</span><br><span class="line">    `paper_title`          STRING COMMENT &#x27;试卷标题&#x27;,</span><br><span class="line">    `course_id`            STRING COMMENT &#x27;课程id&#x27;,</span><br><span class="line">    `course_name`          STRING COMMENT &#x27;课程名称&#x27;,</span><br><span class="line">    `subject_id`           STRING COMMENT &#x27;学科id&#x27;,</span><br><span class="line">    `subject_name`         STRING COMMENT &#x27;学科名称&#x27;,</span><br><span class="line">    `category_id`          STRING COMMENT &#x27;分类id&#x27;,</span><br><span class="line">    `category_name`        STRING COMMENT &#x27;分类名称&#x27;,</span><br><span class="line">    `avg_score_7d`         DECIMAL(16, 2) COMMENT &#x27;最近7日平均分&#x27;,</span><br><span class="line">    `avg_during_sec_7d`    BIGINT COMMENT &#x27;最近7日平均时长&#x27;,</span><br><span class="line">    `total_score_7d`       BIGINT COMMENT &#x27;最近7日总分&#x27;,</span><br><span class="line">    `total_during_sec_7d`  BIGINT COMMENT &#x27;最近7日总时长&#x27;,</span><br><span class="line">    `user_count_7d`        BIGINT COMMENT &#x27;最近7日用户数&#x27;,</span><br><span class="line">    `avg_score_30d`        DECIMAL(16, 2) COMMENT &#x27;最近30日平均分&#x27;,</span><br><span class="line">    `avg_during_sec_30d`   BIGINT COMMENT &#x27;最近30日平均时长&#x27;,</span><br><span class="line">    `total_score_30d`      BIGINT COMMENT &#x27;最近30日总分&#x27;,</span><br><span class="line">    `total_during_sec_30d` BIGINT COMMENT &#x27;最近30日总时长&#x27;,</span><br><span class="line">    `user_count_30d`       BIGINT COMMENT &#x27;最近30日用户数&#x27;</span><br><span class="line">) COMMENT &#x27;考试域试卷粒度考试最近n日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_examination_paper_exam_nd/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据加载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_examination_paper_exam_nd</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), total_score, 0)) /</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), user_count, 0))       avg_score_7d,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), total_during_sec, 0)) /</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), user_count, 0))       avg_during_sec_7d,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), total_score, 0))      total_score_7d,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), total_during_sec, 0)) total_during_sec_7d,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), user_count, 0))       user_count_7d,</span><br><span class="line">       sum(total_score) / sum(user_count)                             avg_score_30d,</span><br><span class="line">       sum(total_during_sec) / sum(user_count)                        avg_during_sec_30d,</span><br><span class="line">       sum(total_score)                                               total_score_30d,</span><br><span class="line">       sum(total_during_sec)                                          total_during_sec_30d,</span><br><span class="line">       sum(user_count)                                                user_count_30d</span><br><span class="line">from edu.dws_examination_paper_exam_1d</span><br><span class="line">where dt &gt;= date_add(&#x27;2022-02-21&#x27;, -29)</span><br><span class="line">  and dt &lt;= &#x27;2022-02-21&#x27;</span><br><span class="line">group by paper_id,</span><br><span class="line">         paper_title,</span><br><span class="line">         course_id,</span><br><span class="line">         course_name,</span><br><span class="line">         subject_id,</span><br><span class="line">         subject_name,</span><br><span class="line">         category_id,</span><br><span class="line">         category_name;</span><br></pre></td></tr></table></figure>

<h4 id="10-2-4-考试域试卷分数段粒度最近n日汇总表"><a href="#10-2-4-考试域试卷分数段粒度最近n日汇总表" class="headerlink" title="10.2.4 考试域试卷分数段粒度最近n日汇总表"></a>10.2.4 考试域试卷分数段粒度最近n日汇总表</h4><p><img src="Snipaste_2023-11-22_11-10-35.png" alt="Snipaste_2023-11-22_11-10-35"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_examination_paper_duration_exam_nd;</span><br><span class="line">CREATE EXTERNAL TABLE dws_examination_paper_duration_exam_nd</span><br><span class="line">(</span><br><span class="line">    `paper_id`       STRING COMMENT &#x27;试卷id&#x27;,</span><br><span class="line">    `paper_title`    STRING COMMENT &#x27;试卷名称&#x27;,</span><br><span class="line">    `duration_name`  STRING COMMENT &#x27;分数区间&#x27;,</span><br><span class="line">    `user_count_7d`  BIGINT COMMENT &#x27;最近7日用户数&#x27;,</span><br><span class="line">    `user_count_30d` BIGINT COMMENT &#x27;最近30日用户数&#x27;</span><br><span class="line">) COMMENT &#x27;考试域试卷-分数段粒度最近n日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_examination_paper_duration_exam_nd/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_examination_paper_duration_exam_nd</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       duration_name,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), user_count, 0)) user_count_7d,</span><br><span class="line">       sum(user_count)                                          user_count_30d</span><br><span class="line">from edu.dws_examination_paper_duration_exam_1d</span><br><span class="line">where dt &gt;= date_add(&#x27;2022-02-21&#x27;, -29)</span><br><span class="line">  and dt &lt;= &#x27;2022-02-21&#x27;</span><br><span class="line">group by paper_id,</span><br><span class="line">         paper_title,</span><br><span class="line">         duration_name;</span><br></pre></td></tr></table></figure>

<h4 id="10-2-5-考试域题目粒度考试最近n日汇总表"><a href="#10-2-5-考试域题目粒度考试最近n日汇总表" class="headerlink" title="10.2.5 考试域题目粒度考试最近n日汇总表"></a>10.2.5 考试域题目粒度考试最近n日汇总表</h4><p><img src="Snipaste_2023-11-22_11-13-46.png" alt="Snipaste_2023-11-22_11-13-46"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_examination_question_exam_nd;</span><br><span class="line">CREATE EXTERNAL TABLE dws_examination_question_exam_nd</span><br><span class="line">(</span><br><span class="line">    `question_id`       STRING COMMENT &#x27;题目id&#x27;,</span><br><span class="line">    `correct_count_7d`  BIGINT COMMENT &#x27;最近7日正确答题次数&#x27;,</span><br><span class="line">    `answer_count_7d`   BIGINT COMMENT &#x27;最近7日答题次数&#x27;,</span><br><span class="line">    `correct_count_30d` BIGINT COMMENT &#x27;最近30日正确答题次数&#x27;,</span><br><span class="line">    `answer_count_30d`  BIGINT COMMENT &#x27;最近30日答题次数&#x27;</span><br><span class="line">) COMMENT &#x27;考试域题目粒度考试最近n日汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_examination_question_exam_nd/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_examination_question_exam_nd</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select question_id,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), correct_count, 0)) correct_count_7d,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;2022-02-21&#x27;, -6), answer_count, 0))  answer_count_7d,</span><br><span class="line">       sum(correct_count)                                          correct_count_30d,</span><br><span class="line">       sum(answer_count)                                           answer_count_30d</span><br><span class="line">from edu.dws_examination_question_exam_1d</span><br><span class="line">where dt &gt;= date_add(&#x27;2022-02-21&#x27;, -29)</span><br><span class="line">  and dt &lt;= &#x27;2022-02-21&#x27;</span><br><span class="line">group by question_id;</span><br></pre></td></tr></table></figure>

<h4 id="10-2-6-数据装载脚本"><a href="#10-2-6-数据装载脚本" class="headerlink" title="10.2.6 数据装载脚本"></a>10.2.6 数据装载脚本</h4><p>（1）每日数据装载脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim dws_1d_to_dws_nd.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">APP=edu</span><br><span class="line"></span><br><span class="line">dws_trade_user_cart_add_nd=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_trade_user_cart_add_nd</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), course_count, 0)) course_count_7d,</span><br><span class="line">       sum(course_count)                                          course_count_30d</span><br><span class="line">from $&#123;APP&#125;.dws_trade_user_cart_add_1d</span><br><span class="line">where dt &gt;= date_add(&#x27;$do_date&#x27;, -29)</span><br><span class="line">  and dt &lt;= &#x27;$do_date&#x27;</span><br><span class="line">group by user_id;&quot;</span><br><span class="line"></span><br><span class="line">dws_trade_user_payment_nd=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_trade_user_payment_nd</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), payment_count, 0)) payment_count_7d,</span><br><span class="line">       sum(payment_count)                                          payment_count_30d</span><br><span class="line">from $&#123;APP&#125;.dws_trade_user_payment_1d</span><br><span class="line">where dt &gt;= date_add(&#x27;$do_date&#x27;, -29)</span><br><span class="line">  and dt &lt;= &#x27;$do_date&#x27;</span><br><span class="line">group by user_id;&quot;</span><br><span class="line"></span><br><span class="line">dws_examination_paper_exam_nd=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_examination_paper_exam_nd</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       course_id,</span><br><span class="line">       course_name,</span><br><span class="line">       subject_id,</span><br><span class="line">       subject_name,</span><br><span class="line">       category_id,</span><br><span class="line">       category_name,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), total_score, 0)) /</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), user_count, 0))       avg_score_7d,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), total_during_sec, 0)) /</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), user_count, 0))       avg_during_sec_7d,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), total_score, 0))      total_score_7d,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), total_during_sec, 0)) total_during_sec_7d,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), user_count, 0))       user_count_7d,</span><br><span class="line">       sum(total_score) / sum(user_count)                             avg_score_30d,</span><br><span class="line">       sum(total_during_sec) / sum(user_count)                        avg_during_sec_30d,</span><br><span class="line">       sum(total_score)                                               total_score_30d,</span><br><span class="line">       sum(total_during_sec)                                          total_during_sec_30d,</span><br><span class="line">       sum(user_count)                                                user_count_30d</span><br><span class="line">from $&#123;APP&#125;.dws_examination_paper_exam_1d</span><br><span class="line">where dt &gt;= date_add(&#x27;$do_date&#x27;, -29)</span><br><span class="line">  and dt &lt;= &#x27;$do_date&#x27;</span><br><span class="line">group by paper_id,</span><br><span class="line">         paper_title,</span><br><span class="line">         course_id,</span><br><span class="line">         course_name,</span><br><span class="line">         subject_id,</span><br><span class="line">         subject_name,</span><br><span class="line">         category_id,</span><br><span class="line">         category_name;&quot;</span><br><span class="line"></span><br><span class="line">dws_examination_paper_duration_exam_nd=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_examination_paper_duration_exam_nd</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select paper_id,</span><br><span class="line">       paper_title,</span><br><span class="line">       duration_name,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), user_count, 0)) user_count_7d,</span><br><span class="line">       sum(user_count)                                          user_count_30d</span><br><span class="line">from $&#123;APP&#125;.dws_examination_paper_duration_exam_1d</span><br><span class="line">where dt &gt;= date_add(&#x27;$do_date&#x27;, -29)</span><br><span class="line">  and dt &lt;= &#x27;$do_date&#x27;</span><br><span class="line">group by paper_id,</span><br><span class="line">         paper_title,</span><br><span class="line">         duration_name;&quot;</span><br><span class="line"></span><br><span class="line">dws_examination_question_exam_nd=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_examination_question_exam_nd</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select question_id,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), correct_count, 0)) correct_count_7d,</span><br><span class="line">       sum(if(dt &gt;= date_add(&#x27;$do_date&#x27;, -6), answer_count, 0))  answer_count_7d,</span><br><span class="line">       sum(correct_count)                                          correct_count_30d,</span><br><span class="line">       sum(answer_count)                                           answer_count_30d</span><br><span class="line">from $&#123;APP&#125;.dws_examination_question_exam_1d</span><br><span class="line">where dt &gt;= date_add(&#x27;$do_date&#x27;, -29)</span><br><span class="line">  and dt &lt;= &#x27;$do_date&#x27;</span><br><span class="line">group by question_id;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    dws_trade_user_cart_add_nd|dws_trade_user_payment_nd|dws_examination_paper_exam_nd|dws_examination_paper_duration_exam_nd|dws_examination_question_exam_nd)</span><br><span class="line">        eval &quot;hive -e \&quot;\$$1\&quot;&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot; )</span><br><span class="line">        hive -e &quot;$&#123;dws_trade_user_cart_add_nd&#125;$&#123;dws_trade_user_payment_nd&#125;$&#123;dws_examination_paper_exam_nd&#125;$&#123;dws_examination_paper_duration_exam_nd&#125;$&#123;dws_examination_question_exam_nd&#125;&quot;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 dws_1d_to_dws_nd.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# dws_1d_to_dws_nd.sh all 2022-02-21</span><br></pre></td></tr></table></figure>

<h3 id="10-3-历史至今汇总表"><a href="#10-3-历史至今汇总表" class="headerlink" title="10.3 历史至今汇总表"></a>10.3 历史至今汇总表</h3><h4 id="10-3-1-交易域用户粒度下单历史至今汇总表"><a href="#10-3-1-交易域用户粒度下单历史至今汇总表" class="headerlink" title="10.3.1 交易域用户粒度下单历史至今汇总表"></a>10.3.1 交易域用户粒度下单历史至今汇总表</h4><p><img src="Snipaste_2023-11-22_15-48-20.png" alt="Snipaste_2023-11-22_15-48-20"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_trade_user_order_td;</span><br><span class="line">CREATE EXTERNAL TABLE dws_trade_user_order_td</span><br><span class="line">(</span><br><span class="line">    `user_id`        STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `order_dt_first` STRING COMMENT &#x27;首次下单日期&#x27;,</span><br><span class="line">    `order_dt_last`  STRING COMMENT &#x27;末次下单日期&#x27;</span><br><span class="line">) COMMENT &#x27;交易域用户粒度用户下单历史至今汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_trade_user_order_td/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_trade_user_order_td</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       min(create_time) order_dt_first,</span><br><span class="line">       max(create_time) order_dt_last</span><br><span class="line">from edu.dwd_trade_order_detail_inc</span><br><span class="line">group by user_id;</span><br></pre></td></tr></table></figure>

<p>每日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table dws_trade_user_order_td partition (dt=&#x27;2022-02-22&#x27;)</span><br><span class="line">select  nvl(old.user_id,nod.user_id) user_id,</span><br><span class="line">        `if`(old.order_dt_first is null,nod.create_time,old.order_dt_first) order_dt_first,</span><br><span class="line">        `if`(nod.create_time is null,old.order_dt_last,nod.create_time) order_dt_last</span><br><span class="line">from (</span><br><span class="line">     select user_id,</span><br><span class="line">            max(create_time) create_time</span><br><span class="line">    from dwd_trade_order_detail_inc</span><br><span class="line">    where dt=&#x27;2022-02-22&#x27;</span><br><span class="line">    group by user_id</span><br><span class="line">)nod</span><br><span class="line">full outer join (</span><br><span class="line">    select user_id,</span><br><span class="line">           order_dt_first,</span><br><span class="line">           order_dt_last</span><br><span class="line">    from dws_trade_user_order_td</span><br><span class="line">    where dt=date_sub(&#x27;2022-02-22&#x27;,1)</span><br><span class="line">)old</span><br><span class="line">on nod.user_id=old.user_id;</span><br></pre></td></tr></table></figure>

<h4 id="10-3-2-交易域用户粒度用户支付历史至今汇总表"><a href="#10-3-2-交易域用户粒度用户支付历史至今汇总表" class="headerlink" title="10.3.2 交易域用户粒度用户支付历史至今汇总表"></a>10.3.2 交易域用户粒度用户支付历史至今汇总表</h4><p><img src="Snipaste_2023-11-22_15-54-22.png" alt="Snipaste_2023-11-22_15-54-22"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_trade_user_payment_td;</span><br><span class="line">CREATE EXTERNAL TABLE dws_trade_user_payment_td</span><br><span class="line">(</span><br><span class="line">    `user_id`          STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `payment_dt_first` STRING COMMENT &#x27;首次支付日期&#x27;</span><br><span class="line">) COMMENT &#x27;交易域用户粒度用户支付历史至今汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_trade_user_payment_td/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_trade_user_payment_td</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       min(callback_time) payment_dt_first</span><br><span class="line">from edu.dwd_trade_pay_detail_suc_inc</span><br><span class="line">group by user_id;</span><br></pre></td></tr></table></figure>

<p>每日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table dws_trade_user_payment_td partition (dt=&#x27;2022-02-22&#x27;)</span><br><span class="line">select nvl(old.user_id,npd.user_id) user_id,</span><br><span class="line">       `if`(payment_dt_first is null ,callback_time,payment_dt_first ) payment_dt_first</span><br><span class="line">from (</span><br><span class="line">     select user_id,</span><br><span class="line">           payment_dt_first</span><br><span class="line">    from dws_trade_user_payment_td</span><br><span class="line">    where dt=date_sub(&#x27;2022-02-22&#x27;,1)</span><br><span class="line">)old</span><br><span class="line">full outer join (</span><br><span class="line">    select user_id,</span><br><span class="line">           max(callback_time) callback_time</span><br><span class="line">    from dwd_trade_pay_detail_suc_inc</span><br><span class="line">    where dt=&#x27;2022-02-22&#x27;</span><br><span class="line">    group by user_id</span><br><span class="line">)npd</span><br><span class="line">on old.user_id=npd.user_id;</span><br></pre></td></tr></table></figure>

<h4 id="10-3-3-用户域用户粒度用户登录历史至今汇总表"><a href="#10-3-3-用户域用户粒度用户登录历史至今汇总表" class="headerlink" title="10.3.3 用户域用户粒度用户登录历史至今汇总表"></a>10.3.3 用户域用户粒度用户登录历史至今汇总表</h4><p><img src="Snipaste_2023-11-22_16-35-13.png" alt="Snipaste_2023-11-22_16-35-13"></p>
<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS dws_user_user_login_td;</span><br><span class="line">CREATE EXTERNAL TABLE dws_user_user_login_td</span><br><span class="line">(</span><br><span class="line">    `user_id`          STRING COMMENT &#x27;用户id&#x27;,</span><br><span class="line">    `login_last_date`  STRING COMMENT &#x27;末次登录日期&#x27;,</span><br><span class="line">    `user_login_count` STRING COMMENT &#x27;用户登录次数&#x27;</span><br><span class="line">) COMMENT &#x27;用户域用户粒度用户登录历史至今汇总表&#x27;</span><br><span class="line">    PARTITIONED BY (`dt` STRING)</span><br><span class="line">    STORED AS ORC</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/dws/dws_user_user_login_td/&#x27;</span><br><span class="line">    TBLPROPERTIES (&#x27;orc.compress&#x27; = &#x27;snappy&#x27;);</span><br></pre></td></tr></table></figure>

<p>首日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.dws_user_user_login_td</span><br><span class="line">    partition (dt = &#x27;2022-02-21&#x27;)</span><br><span class="line">select ui.id,</span><br><span class="line">       nvl(login_last_date, register_date) login_last_date,</span><br><span class="line">       nvl(user_login_count, 1)            user_login_count</span><br><span class="line">from (select id,</span><br><span class="line">             date_format(create_time, &#x27;yyyy-MM-dd&#x27;) register_date</span><br><span class="line">      from edu.dim_user_zip</span><br><span class="line">      where dt = &#x27;9999-12-31&#x27;) ui</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select user_id,</span><br><span class="line">                max(date_id) login_last_date,</span><br><span class="line">                count(*)     user_login_count</span><br><span class="line">         from edu.dwd_user_login_inc</span><br><span class="line">         group by user_id</span><br><span class="line">     ) login_td</span><br><span class="line">     on ui.id = login_td.user_id;</span><br></pre></td></tr></table></figure>

<p>每日数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table dws_user_user_login_td partition (dt=&#x27;2022-02-22&#x27;)</span><br><span class="line">select nvl(old.user_id,nl.user_id) user_id,</span><br><span class="line">       nvl(nl.date_id,old.login_last_date) login_last_date,</span><br><span class="line">       nvl(old.user_login_count,0) + nvl(nl.counts,0) user_login_count</span><br><span class="line">from (</span><br><span class="line">     select user_id,</span><br><span class="line">           login_last_date,</span><br><span class="line">           user_login_count</span><br><span class="line">    from dws_user_user_login_td</span><br><span class="line">    where dt=date_sub(&#x27;2022-02-22&#x27;,1)</span><br><span class="line">)old</span><br><span class="line">full outer join (</span><br><span class="line">    select user_id,</span><br><span class="line">           max(date_id) date_id,</span><br><span class="line">           count(*) counts</span><br><span class="line">    from dwd_user_login_inc</span><br><span class="line">    where dt=&#x27;2022-02-22&#x27;</span><br><span class="line">    group by user_id</span><br><span class="line">)nl</span><br><span class="line">on old.user_id=nl.user_id;</span><br></pre></td></tr></table></figure>

<h4 id="10-3-4-数据装载脚本"><a href="#10-3-4-数据装载脚本" class="headerlink" title="10.3.4 数据装载脚本"></a>10.3.4 数据装载脚本</h4><p>（1）首日数据装载</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim dws_1d_to_dws_td_init.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    echo &quot;请传入日期参数&quot;</span><br><span class="line">    exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">APP=edu</span><br><span class="line"></span><br><span class="line">dws_trade_user_order_td=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_trade_user_order_td</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       min(create_time) order_dt_first,</span><br><span class="line">       max(create_time) order_dt_last</span><br><span class="line">from $&#123;APP&#125;.dwd_trade_order_detail_inc</span><br><span class="line">group by user_id;&quot;</span><br><span class="line"></span><br><span class="line">dws_trade_user_payment_td=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_trade_user_payment_td</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select user_id,</span><br><span class="line">       min(callback_time) payment_dt_first</span><br><span class="line">from $&#123;APP&#125;.dwd_trade_pay_detail_suc_inc</span><br><span class="line">group by user_id;&quot;</span><br><span class="line"></span><br><span class="line">dws_user_user_login_td=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_user_user_login_td</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select ui.id,</span><br><span class="line">       nvl(login_last_date, register_date) login_last_date,</span><br><span class="line">       nvl(user_login_count, 1)            user_login_count</span><br><span class="line">from (select id,</span><br><span class="line">             date_format(create_time, &#x27;yyyy-MM-dd&#x27;) register_date</span><br><span class="line">      from $&#123;APP&#125;.dim_user_zip</span><br><span class="line">      where dt = &#x27;9999-12-31&#x27;) ui</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select user_id,</span><br><span class="line">                max(date_id) login_last_date,</span><br><span class="line">                count(*)     user_login_count</span><br><span class="line">         from $&#123;APP&#125;.dwd_user_login_inc</span><br><span class="line">         group by user_id</span><br><span class="line">     ) login_td</span><br><span class="line">     on ui.id = login_td.user_id;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    dws_trade_user_order_td|dws_trade_user_payment_td|dws_user_user_login_td)</span><br><span class="line">        eval &quot;hive -e \&quot;\$$1\&quot;&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot; )</span><br><span class="line">        hive -e &quot;$&#123;dws_trade_user_order_td&#125;$&#123;dws_trade_user_payment_td&#125;$&#123;dws_user_user_login_td&#125;&quot;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 dws_1d_to_dws_td_init.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# dws_1d_to_dws_td_init.sh all 2022-02-21</span><br></pre></td></tr></table></figure>

<p>（2）每日数据装载</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim dws_1d_to_dws_td.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ -n &quot;$2&quot; ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    do_date=`date -d &quot;-1 day&quot; +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">APP=edu</span><br><span class="line"></span><br><span class="line">dws_trade_user_order_td=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_trade_user_order_td partition (dt=&#x27;$do_date&#x27;)</span><br><span class="line">select  nvl(old.user_id,nod.user_id) user_id,</span><br><span class="line">        if(old.order_dt_first is null,nod.create_time,old.order_dt_first) order_dt_first,</span><br><span class="line">        if(nod.create_time is null,old.order_dt_last,nod.create_time) order_dt_last</span><br><span class="line">from (</span><br><span class="line">     select user_id,</span><br><span class="line">            max(create_time) create_time</span><br><span class="line">    from $&#123;APP&#125;.dwd_trade_order_detail_inc</span><br><span class="line">    where dt=&#x27;$do_date&#x27;</span><br><span class="line">    group by user_id</span><br><span class="line">)nod</span><br><span class="line">full outer join (</span><br><span class="line">    select user_id,</span><br><span class="line">           order_dt_first,</span><br><span class="line">           order_dt_last</span><br><span class="line">    from $&#123;APP&#125;.dws_trade_user_order_td</span><br><span class="line">    where dt=date_sub(&#x27;$do_date&#x27;,1)</span><br><span class="line">)old</span><br><span class="line">on nod.user_id=old.user_id;&quot;</span><br><span class="line"></span><br><span class="line">dws_trade_user_payment_td=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_trade_user_payment_td</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select nvl(new.user_id, old.user_id),</span><br><span class="line">       if((payment_dt_first is null) or (new_payment_dt_first &lt; payment_dt_first), new_payment_dt_first,</span><br><span class="line">          payment_dt_first)</span><br><span class="line">from (select *</span><br><span class="line">      from $&#123;APP&#125;.dws_trade_user_payment_td</span><br><span class="line">      where dt = date_add(&#x27;$do_date&#x27;, -1)) old</span><br><span class="line">         full outer join</span><br><span class="line">     (</span><br><span class="line">         select user_id,</span><br><span class="line">                min(callback_time) new_payment_dt_first</span><br><span class="line">         from $&#123;APP&#125;.dwd_trade_pay_detail_suc_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">         group by user_id</span><br><span class="line">     ) new</span><br><span class="line">     on old.user_id = new.user_id;&quot;</span><br><span class="line"></span><br><span class="line">dws_user_user_login_td=&quot;</span><br><span class="line">insert overwrite table $&#123;APP&#125;.dws_user_user_login_td</span><br><span class="line">    partition (dt = &#x27;$do_date&#x27;)</span><br><span class="line">select nvl(old_user_id, new_user_id)         user_id,</span><br><span class="line">       nvl(new_date, old_date)               login_last_date,</span><br><span class="line">       nvl(old_count, 0) + nvl(new_count, 0) user_login_count</span><br><span class="line">from (select user_id          old_user_id,</span><br><span class="line">             login_last_date  old_date,</span><br><span class="line">             user_login_count old_count</span><br><span class="line">      from $&#123;APP&#125;.dws_user_user_login_td</span><br><span class="line">      where dt = date_add(&#x27;$do_date&#x27;, -1)) old</span><br><span class="line">         full outer join</span><br><span class="line">     (</span><br><span class="line">         select user_id      new_user_id,</span><br><span class="line">                &#x27;$do_date&#x27; new_date,</span><br><span class="line">                count(*)     new_count</span><br><span class="line">         from $&#123;APP&#125;.dwd_user_login_inc</span><br><span class="line">         where dt = &#x27;$do_date&#x27;</span><br><span class="line">         group by user_id</span><br><span class="line">     ) new</span><br><span class="line">     on old_user_id = new_user_id;&quot;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    dws_trade_user_order_td|dws_trade_user_payment_td|dws_user_user_login_td)</span><br><span class="line">        eval &quot;hive -e \&quot;\$$1\&quot;&quot;</span><br><span class="line">    ;;</span><br><span class="line">    &quot;all&quot; )</span><br><span class="line">        hive -e &quot;$&#123;dws_trade_user_order_td&#125;$&#123;dws_trade_user_payment_td&#125;$&#123;dws_user_user_login_td&#125;&quot;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 dws_1d_to_dws_td.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# dws_1d_to_dws_td.sh all 2022-02-22</span><br></pre></td></tr></table></figure>

<h2 id="第十一章-数仓开发之ADS层"><a href="#第十一章-数仓开发之ADS层" class="headerlink" title="第十一章  数仓开发之ADS层"></a>第十一章  数仓开发之ADS层</h2><p>前面已经完成了ODS、DIM、DWD、DWS层数据仓库的搭建，本节主要实现具体需求（不同的业务有不同的需求，这里面的在线教育业务需要了解，并不一定熟悉，可以把装载数据部分的SQL变成一道SQL题，读懂其中的逻辑即可）</p>
<h3 id="11-1-流量主题指标"><a href="#11-1-流量主题指标" class="headerlink" title="11.1 流量主题指标"></a>11.1 流量主题指标</h3><h4 id="11-1-1-各来源流量统计"><a href="#11-1-1-各来源流量统计" class="headerlink" title="11.1.1 各来源流量统计"></a>11.1.1 各来源流量统计</h4><p>引流就是通过各种渠道吸引顾客的工作，统计不同引流来源的访客可用于分析各来源的引流效果，对企业推广策略的制定提供参考。</p>
<p>访客数是指通过不同来源进入应用的访客人数。会话平均停留时长是指访客进入应用后，在每个会话的平均停留时间，以秒为单位。会话平均浏览页面数是每个会话平均访问的页面数量。会话数是不同来源进入应用后打开的会话总个数。跳出率是指只访问一个页面就退出的会话个数占总会话数量的比率。</p>
<img src="Snipaste_2023-11-22_20-39-49.png" alt="Snipaste_2023-11-22_20-39-49" style="zoom:50%;">

<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ads_traffic_stats_by_source;</span><br><span class="line">CREATE EXTERNAL TABLE ads_traffic_stats_by_source</span><br><span class="line">(</span><br><span class="line">    `dt`               STRING COMMENT &#x27;统计日期&#x27;,</span><br><span class="line">    `recent_days`      BIGINT COMMENT &#x27;最近天数,1:最近1天,7:最近7天,30:最近30天&#x27;,</span><br><span class="line">    `source_id`           STRING COMMENT &#x27;引流来源id&#x27;,</span><br><span class="line">    `source_site`      STRING COMMENT &#x27;引流来源名称&#x27;,</span><br><span class="line">    `uv_count`         BIGINT COMMENT &#x27;访客人数&#x27;,</span><br><span class="line">    `avg_duration_sec` BIGINT COMMENT &#x27;会话平均停留时长，单位为秒&#x27;,</span><br><span class="line">    `avg_page_count`   BIGINT COMMENT &#x27;会话平均浏览页面数&#x27;,</span><br><span class="line">    `sv_count`         BIGINT COMMENT &#x27;会话数&#x27;,</span><br><span class="line">    `bounce_rate`      DECIMAL(16, 2) COMMENT &#x27;跳出率&#x27;</span><br><span class="line">) COMMENT &#x27;各引流来源流量统计&#x27;</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ads/ads_traffic_stats_by_source/&#x27;;</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.ads_traffic_stats_by_source</span><br><span class="line">select dt,</span><br><span class="line">       recent_days,</span><br><span class="line">       source_id,</span><br><span class="line">       source_site,</span><br><span class="line">       uv_count,</span><br><span class="line">       avg_duration_sec,</span><br><span class="line">       avg_page_count,</span><br><span class="line">       sv_count,</span><br><span class="line">       bounce_rate</span><br><span class="line">from edu.ads_traffic_stats_by_source</span><br><span class="line">union</span><br><span class="line">select &#x27;2022-02-21&#x27;                                      dt,</span><br><span class="line">       recent_days,</span><br><span class="line">       source_id,</span><br><span class="line">       source_site,</span><br><span class="line">       count(distinct user_id)                           uv_count,</span><br><span class="line">       cast(avg(during_time / 1000) as BIGINT)           avg_duration_sec,</span><br><span class="line">       cast(avg(page_count) as BIGINT)                   avg_page_count,</span><br><span class="line">       count(session_id)                                 sv_count,</span><br><span class="line">       sum(if(page_count = 1, 1, 0)) / count(session_id) bounce_rate</span><br><span class="line">from edu.dws_traffic_session_page_view_1d</span><br><span class="line">         lateral view explode(array(1, 7, 30)) tmp as recent_days</span><br><span class="line">where dt &gt;= date_add(&#x27;2022-02-21&#x27;, -recent_days + 1)</span><br><span class="line">  and dt &lt;= &#x27;2022-02-21&#x27;</span><br><span class="line">group by recent_days, source_id, source_site;</span><br></pre></td></tr></table></figure>

<h4 id="11-1-2-路径分析"><a href="#11-1-2-路径分析" class="headerlink" title="11.1.2 路径分析"></a>11.1.2 路径分析</h4><p>用户路径分析，顾名思义，就是指用户在APP或网站中的访问路径。为了衡量网站优化的效果或营销推广的效果，以及了解用户行为偏好，时常要对访问路径进行分析。</p>
<p>用户访问路径的可视化通常使用桑基图。如下图所示，该图可真实还原用户的访问路径，包括页面跳转和页面访问次序。</p>
<p>桑基图需要我们提供每种页面跳转的次数，每个跳转由source&#x2F;target表示，source指跳转起始页面，target表示跳转终到页面。</p>
<p>用户访问路径分析的关键是梳理出用户在同一个会话种访问的全部页面，然后按照访问页面的时间戳，对同一会话中的页面访问数据进行排序，即可得到用户同一个会话访问页面的完整路径。</p>
<img src="Snipaste_2023-11-22_20-53-26.png" alt="Snipaste_2023-11-22_20-53-26" style="zoom:50%;">

<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ads_traffic_page_path;</span><br><span class="line">CREATE EXTERNAL TABLE ads_traffic_page_path</span><br><span class="line">(</span><br><span class="line">    `dt`          STRING COMMENT &#x27;统计日期&#x27;,</span><br><span class="line">    `recent_days` BIGINT COMMENT &#x27;最近天数,1:最近1天,7:最近7天,30:最近30天&#x27;,</span><br><span class="line">    `source`      STRING COMMENT &#x27;跳转起始页面id&#x27;,</span><br><span class="line">    `target`      STRING COMMENT &#x27;跳转终到页面id&#x27;,</span><br><span class="line">    `path_count`  BIGINT COMMENT &#x27;跳转次数&#x27;</span><br><span class="line">) COMMENT &#x27;页面浏览路径分析&#x27;</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ads/ads_traffic_page_path/&#x27;;</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.ads_traffic_page_path</span><br><span class="line">select dt, recent_days, source, target, path_count</span><br><span class="line">from edu.ads_traffic_page_path</span><br><span class="line">union</span><br><span class="line">select &#x27;2022-02-21&#x27; dt,</span><br><span class="line">       recent_days,</span><br><span class="line">       source,</span><br><span class="line">       nvl(target, &#x27;null&#x27;),</span><br><span class="line">       count(*)     path_count</span><br><span class="line">from (select recent_days,</span><br><span class="line">             concat(&#x27;step-&#x27;, rk, &#x27;:&#x27;, page_id)          source,</span><br><span class="line">             concat(&#x27;step-&#x27;, rk + 1, &#x27;:&#x27;, next_page_id) target</span><br><span class="line">      from (select recent_days,</span><br><span class="line">                   page_id,</span><br><span class="line">                   lead(page_id) over (partition by recent_days, session_id order by ts) next_page_id,</span><br><span class="line">                   row_number() over (partition by recent_days, session_id order by ts)  rk</span><br><span class="line">            from edu.dwd_traffic_page_view_inc</span><br><span class="line">                     lateral view explode(array(1, 7, 30)) tmp as recent_days</span><br><span class="line">            where dt &gt;= date_add(&#x27;2022-02-21&#x27;, -recent_days + 1)</span><br><span class="line">              and dt &lt;= &#x27;2022-02-21&#x27;) t1) t2</span><br><span class="line">group by recent_days, source, target;</span><br></pre></td></tr></table></figure>

<h4 id="11-1-3-各来源下单统计"><a href="#11-1-3-各来源下单统计" class="headerlink" title="11.1.3 各来源下单统计"></a>11.1.3 各来源下单统计</h4><p>统计不同来源用户的下单总下单金额，以及不同来源访客最终转化为下单用户的转化率，各来源下单统计指标如下：</p>
<img src="Snipaste_2023-11-22_21-02-55.png" alt="Snipaste_2023-11-22_21-02-55" style="zoom:50%;">

<p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS ads_traffic_sale_stats_by_source;</span><br><span class="line">CREATE EXTERNAL TABLE ads_traffic_sale_stats_by_source</span><br><span class="line">(</span><br><span class="line">    `dt` STRING COMMENT &#x27;统计日期&#x27;,</span><br><span class="line">    `recent_days` BIGINT COMMENT &#x27;最近天数,1:最近1天,7:最近7天,30:最近30天&#x27;,</span><br><span class="line">    `source_id`          STRING COMMENT &#x27;引流来源id&#x27;,</span><br><span class="line">    `source_site`        STRING COMMENT &#x27;引流来源名称&#x27;,</span><br><span class="line">    `order_total_amount` DECIMAL(16, 2) COMMENT &#x27;销售额&#x27;,</span><br><span class="line">    `order_user_count`  BIGINT COMMENT &#x27;下单用户数&#x27;,</span><br><span class="line">    `pv_visitor_count`         BIGINT COMMENT &#x27;引流访客数&#x27;,</span><br><span class="line">    `convert_rate`       DECIMAL(16, 2) COMMENT &#x27;转化率&#x27;</span><br><span class="line">) COMMENT &#x27;各引流来源销售状况统计&#x27;</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">    LOCATION &#x27;/warehouse/edu/ads/ads_traffic_sale_stats_by_source&#x27;;</span><br></pre></td></tr></table></figure>

<p>数据装载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table edu.ads_traffic_sale_stats_by_source</span><br><span class="line">select dt,</span><br><span class="line">       recent_days,</span><br><span class="line">       source_id,</span><br><span class="line">       source_site,</span><br><span class="line">       order_total_amount,</span><br><span class="line">       order_user_count,</span><br><span class="line">       pv_visitor_count,</span><br><span class="line">       convert_rate</span><br><span class="line">from edu.ads_traffic_sale_stats_by_source</span><br><span class="line">union</span><br><span class="line">select &#x27;2022-02-21&#x27;                     dt,</span><br><span class="line">       t_u_count.recent_days,</span><br><span class="line">       t_u_count.source_id,</span><br><span class="line">       t_u_count.source_site,</span><br><span class="line">       order_total_amount,</span><br><span class="line">       order_user_count,</span><br><span class="line">       pv_visitor_count,</span><br><span class="line">       order_user_count / pv_visitor_count convert_rate</span><br><span class="line">from (</span><br><span class="line">         select recent_days,</span><br><span class="line">                source_id,</span><br><span class="line">                source_site,</span><br><span class="line">                count(distinct mid_id) pv_visitor_count</span><br><span class="line">         from edu.dws_traffic_session_page_view_1d</span><br><span class="line">                  lateral view explode(array(1, 7, 30)) tmp as recent_days</span><br><span class="line">         where dt &gt;= date_add(&#x27;2022-02-21&#x27;, -recent_days + 1)</span><br><span class="line">         group by recent_days,</span><br><span class="line">                  source_id,</span><br><span class="line">                  source_site</span><br><span class="line">     ) t_u_count</span><br><span class="line">         left join</span><br><span class="line">     (</span><br><span class="line">         select recent_days,</span><br><span class="line">                source_id,</span><br><span class="line">                source_site,</span><br><span class="line">                sum(order_amount)       order_total_amount,</span><br><span class="line">                count(distinct user_id) order_user_count</span><br><span class="line">         from edu.dws_trade_session_order_1d</span><br><span class="line">                  lateral view explode(array(1, 7, 30)) tmp as recent_days</span><br><span class="line">         where dt &gt;= date_add(&#x27;2022-02-21&#x27;, -recent_days + 1)</span><br><span class="line">           and order_amount is not null</span><br><span class="line">           and order_amount &gt; 0</span><br><span class="line">         group by recent_days,</span><br><span class="line">                  source_id,</span><br><span class="line">                  source_site</span><br><span class="line">     ) t_amount</span><br><span class="line">     on t_u_count.recent_days = t_amount.recent_days</span><br><span class="line">         and t_u_count.source_id = t_amount.source_id;</span><br></pre></td></tr></table></figure>

<p>剩余的表格没必要写了，知道就可以了。</p>
<p>。。。。。。</p>
<h3 id="11-8-数据装载脚本"><a href="#11-8-数据装载脚本" class="headerlink" title="11.8 数据装载脚本"></a>11.8 数据装载脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim dws_to_ads.sh</span><br><span class="line">[root@hadoop102 bin]# chmod 777 dws_to_ads.sh </span><br></pre></td></tr></table></figure>

<p>脚本使用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# dws_to_ads.sh all 2022-02-21</span><br></pre></td></tr></table></figure>

<h3 id="11-9-数据模型评价及优化"><a href="#11-9-数据模型评价及优化" class="headerlink" title="11.9 数据模型评价及优化"></a>11.9 数据模型评价及优化</h3><p>在数据仓库搭建完成后，需要对数据仓库的数据模型进行评估，根据评估结果对数据模型做出优化，评估从以下几个方面展开：</p>
<ol>
<li><p>完善度<br>汇总数据能<strong>直接满足</strong>多少查询需求，即应用层（ADS层）访问汇总层（DWD层）就能直接得出结果的查询占所有指标查询的比例。<br><strong>跨层引用率</strong>：ODS层直接被中间数据层引用的表占所有ODS层表的比例<br>是否可以快速相应业务方的需求</p>
<p>对于比较好的模型，使用方可以直接从该模型获取所有想要的数据，若跨层引用率太高（DWS层和ADS层直接引用ODS层的表比例太大），该模型不是最优，需要继续优化</p>
</li>
<li><p>复用度<br>模型引用系数：模型被读取并产出下游模型的平均数量<br>DWD、DWS层下游直接产出表的数量</p>
</li>
<li><p>规范度</p>
</li>
<li><p>稳定性</p>
</li>
<li><p>准确性和一致性</p>
</li>
<li><p>健壮性</p>
</li>
<li><p>成本</p>
</li>
</ol>
<h2 id="第十二章-报表数据导出"><a href="#第十二章-报表数据导出" class="headerlink" title="第十二章 报表数据导出"></a>第十二章 报表数据导出</h2><p>在ADS层实现具体需求之后，还需要将结果数据导出至关系型数据库（MySQL）中，可以方便后期对结果数据进行可视化展示，使用DataX将ADS层中的结果数据导出至MySQL。</p>
<h3 id="12-1-创建MySQL数据库和表"><a href="#12-1-创建MySQL数据库和表" class="headerlink" title="12.1 创建MySQL数据库和表"></a>12.1 创建MySQL数据库和表</h3><ol>
<li>创建edu_report数据库</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> edu_report <span class="keyword">DEFAULT</span> CHARSET utf8 <span class="keyword">COLLATE</span> utf8_general_ci;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>创建表（表语句略）（和ads表对应，27张）</li>
</ol>
<p>建表成功：</p>
<img src="Snipaste_2023-11-23_13-28-53.png" alt="Snipaste_2023-11-23_13-28-53" style="zoom:50%;">

<h3 id="12-2-DataX数据导出"><a href="#12-2-DataX数据导出" class="headerlink" title="12.2 DataX数据导出"></a>12.2 DataX数据导出</h3><p>选用HDFSReader和MySQLWriter。</p>
<h4 id="12-2-1-编写DataX配置文件"><a href="#12-2-1-编写DataX配置文件" class="headerlink" title="12.2.1 编写DataX配置文件"></a>12.2.1 编写DataX配置文件</h4><p>我们需要为<strong>每个张表</strong>编写一个DataX配置文件（故27张表需要编写27个配置文件），此处以ads_traffic_stats_by_source为例，配置文件内容如下：（仅供参考）</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;reader&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfsreader&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="string">&quot;*&quot;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;defaultFS&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfs://hadoop102:8020&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;encoding&quot;</span><span class="punctuation">:</span> <span class="string">&quot;UTF-8&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fieldDelimiter&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\t&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;fileType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;nullFormat&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\\N&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;exportdir&#125;&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;writer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mysqlwriter&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="string">&quot;dt&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;recent_days&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;channel&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;uv_count&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;avg_duration_sec&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;avg_page_count&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;sv_count&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="string">&quot;bounce_rate&quot;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;connection&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;jdbcUrl&quot;</span><span class="punctuation">:</span> <span class="string">&quot;jdbc:mysql://hadoop102:3306/edu_report?useUnicode=true&amp;characterEncoding=utf-8&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;table&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                    <span class="string">&quot;ads_traffic_stats_by_source&quot;</span></span><br><span class="line">                                <span class="punctuation">]</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;password&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wyhdhr19980418&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;root&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;writeMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;replace&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;errorLimit&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;percentage&quot;</span><span class="punctuation">:</span> <span class="number">0.02</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;record&quot;</span><span class="punctuation">:</span> <span class="number">0</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="number">3</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h4 id="12-2-2-DataX配置文件生成脚本"><a href="#12-2-2-DataX配置文件生成脚本" class="headerlink" title="12.2.2 DataX配置文件生成脚本"></a>12.2.2 DataX配置文件生成脚本</h4><p>方便起见，此处提供了DataX配置文件批量生成脚本，脚本内容及使用方式如下。</p>
<p>思路还是通过gen_export_config.py批量生成DataX配置文件，执行DataX配置文件完成数据从HDFS到MySQL的同步</p>
<p>（1）在~&#x2F;bin目录下创建gen_export_config.py脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim gen_export_config.py</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> getopt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> MySQLdb</span><br><span class="line"></span><br><span class="line"><span class="comment">#MySQL相关配置，需根据实际情况作出修改</span></span><br><span class="line">mysql_host = <span class="string">&quot;hadoop102&quot;</span></span><br><span class="line">mysql_port = <span class="string">&quot;3306&quot;</span></span><br><span class="line">mysql_user = <span class="string">&quot;root&quot;</span></span><br><span class="line">mysql_passwd = <span class="string">&quot;wyhdhr19980418&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#HDFS NameNode相关配置，需根据实际情况作出修改</span></span><br><span class="line">hdfs_nn_host = <span class="string">&quot;hadoop102&quot;</span></span><br><span class="line">hdfs_nn_port = <span class="string">&quot;8020&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成配置文件的目标路径，可根据实际情况作出修改</span></span><br><span class="line">output_path = <span class="string">&quot;/opt/module/datax/job/export&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_connection</span>():</span><br><span class="line">    <span class="keyword">return</span> MySQLdb.connect(host=mysql_host, port=<span class="built_in">int</span>(mysql_port), user=mysql_user, passwd=mysql_passwd)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_mysql_meta</span>(<span class="params">database, table</span>):</span><br><span class="line">    connection = get_connection()</span><br><span class="line">    cursor = connection.cursor()</span><br><span class="line">    sql = <span class="string">&quot;SELECT COLUMN_NAME,DATA_TYPE from information_schema.COLUMNS WHERE TABLE_SCHEMA=%s AND TABLE_NAME=%s ORDER BY ORDINAL_POSITION&quot;</span></span><br><span class="line">    cursor.execute(sql, [database, table])</span><br><span class="line">    fetchall = cursor.fetchall()</span><br><span class="line">    cursor.close()</span><br><span class="line">    connection.close()</span><br><span class="line">    <span class="keyword">return</span> fetchall</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_mysql_columns</span>(<span class="params">database, table</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], get_mysql_meta(database, table))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_json</span>(<span class="params">target_database, target_table</span>):</span><br><span class="line">    job = &#123;</span><br><span class="line">        <span class="string">&quot;job&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;setting&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;speed&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;channel&quot;</span>: <span class="number">3</span></span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;errorLimit&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;record&quot;</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">&quot;percentage&quot;</span>: <span class="number">0.02</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: [&#123;</span><br><span class="line">                <span class="string">&quot;reader&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;hdfsreader&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;parameter&quot;</span>: &#123;</span><br><span class="line">                        <span class="string">&quot;path&quot;</span>: <span class="string">&quot;$&#123;exportdir&#125;&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;defaultFS&quot;</span>: <span class="string">&quot;hdfs://&quot;</span> + hdfs_nn_host + <span class="string">&quot;:&quot;</span> + hdfs_nn_port,</span><br><span class="line">                        <span class="string">&quot;column&quot;</span>: [<span class="string">&quot;*&quot;</span>],</span><br><span class="line">                        <span class="string">&quot;fileType&quot;</span>: <span class="string">&quot;text&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;encoding&quot;</span>: <span class="string">&quot;UTF-8&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;fieldDelimiter&quot;</span>: <span class="string">&quot;\t&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;nullFormat&quot;</span>: <span class="string">&quot;\\N&quot;</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;writer&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;mysqlwriter&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;parameter&quot;</span>: &#123;</span><br><span class="line">                        <span class="string">&quot;writeMode&quot;</span>: <span class="string">&quot;replace&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;username&quot;</span>: mysql_user,</span><br><span class="line">                        <span class="string">&quot;password&quot;</span>: mysql_passwd,</span><br><span class="line">                        <span class="string">&quot;column&quot;</span>: get_mysql_columns(target_database, target_table),</span><br><span class="line">                        <span class="string">&quot;connection&quot;</span>: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="string">&quot;jdbcUrl&quot;</span>:</span><br><span class="line">                                    <span class="string">&quot;jdbc:mysql://&quot;</span> + mysql_host + <span class="string">&quot;:&quot;</span> + mysql_port + <span class="string">&quot;/&quot;</span> + target_database + <span class="string">&quot;?useUnicode=true&amp;characterEncoding=utf-8&quot;</span>,</span><br><span class="line">                                <span class="string">&quot;table&quot;</span>: [target_table]</span><br><span class="line">                            &#125;</span><br><span class="line">                        ]</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_path):</span><br><span class="line">        os.makedirs(output_path)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(output_path, <span class="string">&quot;.&quot;</span>.join([target_database, target_table, <span class="string">&quot;json&quot;</span>])), <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(job, f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    target_database = <span class="string">&quot;&quot;</span></span><br><span class="line">    target_table = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    options, arguments = getopt.getopt(args, <span class="string">&#x27;-d:-t:&#x27;</span>, [<span class="string">&#x27;targetdb=&#x27;</span>, <span class="string">&#x27;targettbl=&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> opt_name, opt_value <span class="keyword">in</span> options:</span><br><span class="line">        <span class="keyword">if</span> opt_name <span class="keyword">in</span> (<span class="string">&#x27;-d&#x27;</span>, <span class="string">&#x27;--targetdb&#x27;</span>):</span><br><span class="line">            target_database = opt_value</span><br><span class="line">        <span class="keyword">if</span> opt_name <span class="keyword">in</span> (<span class="string">&#x27;-t&#x27;</span>, <span class="string">&#x27;--targettbl&#x27;</span>):</span><br><span class="line">            target_table = opt_value</span><br><span class="line"></span><br><span class="line">    generate_json(target_database, target_table)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main(sys.argv[<span class="number">1</span>:])</span><br></pre></td></tr></table></figure>

<p>（2）在~&#x2F;bin目录下创建gen_export_config.sh脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim gen_export_config.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_traffic_stats_by_source;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_traffic_page_path;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_traffic_sale_stats_by_source;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_user_user_change;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_user_user_retention;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_user_user_stats;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_user_user_action;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_user_new_buyer_stats;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_user_order_user_count_by_age_group;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_course_trade_stats_by_category;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_course_trade_stats_by_subject;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_course_trade_stats_by_course;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_course_review_stats_by_course;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_sample_retention_stats_by_category;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_sample_retention_stats_by_subject;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_sample_retention_stats_by_course;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_trade_stats;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_trade_order_by_province;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_examination_paper_avg_stats;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_examination_course_avg_stats;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_examination_user_count_by_score_duration;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_examination_question_accuracy;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_learn_play_stats_by_chapter;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_learn_play_stats_by_course;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_complete_complete_user_count_per_course;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_complete_complete_stats;</span><br><span class="line">python /bin/gen_export_config.py -d edu_report -t ads_complete_complete_chapter_count_per_course;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 gen_export_config.sh</span><br></pre></td></tr></table></figure>

<p>执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# gen_export_config.sh</span><br></pre></td></tr></table></figure>

<p>配置文件生成成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 export]# ll</span><br><span class="line">总用量 108</span><br><span class="line">-rw-r--r-- 1 root root 693 11月 23 13:50 edu_report.ads_complete_complete_chapter_count_per_course.json</span><br><span class="line">-rw-r--r-- 1 root root 688 11月 23 13:50 edu_report.ads_complete_complete_stats.json</span><br><span class="line">-rw-r--r-- 1 root root 678 11月 23 13:50 edu_report.ads_complete_complete_user_count_per_course.json</span><br><span class="line">-rw-r--r-- 1 root root 718 11月 23 13:50 edu_report.ads_course_review_stats_by_course.json</span><br><span class="line">-rw-r--r-- 1 root root 725 11月 23 13:50 edu_report.ads_course_trade_stats_by_category.json</span><br><span class="line">-rw-r--r-- 1 root root 719 11月 23 13:50 edu_report.ads_course_trade_stats_by_course.json</span><br><span class="line">-rw-r--r-- 1 root root 722 11月 23 13:50 edu_report.ads_course_trade_stats_by_subject.json</span><br><span class="line">-rw-r--r-- 1 root root 713 11月 23 13:50 edu_report.ads_examination_course_avg_stats.json</span><br><span class="line">-rw-r--r-- 1 root root 711 11月 23 13:50 edu_report.ads_examination_paper_avg_stats.json</span><br><span class="line">-rw-r--r-- 1 root root 668 11月 23 13:50 edu_report.ads_examination_question_accuracy.json</span><br><span class="line">-rw-r--r-- 1 root root 696 11月 23 13:50 edu_report.ads_examination_user_count_by_score_duration.json</span><br><span class="line">-rw-r--r-- 1 root root 739 11月 23 13:50 edu_report.ads_learn_play_stats_by_chapter.json</span><br><span class="line">-rw-r--r-- 1 root root 710 11月 23 13:50 edu_report.ads_learn_play_stats_by_course.json</span><br><span class="line">-rw-r--r-- 1 root root 720 11月 23 13:50 edu_report.ads_sample_retention_stats_by_category.json</span><br><span class="line">-rw-r--r-- 1 root root 714 11月 23 13:50 edu_report.ads_sample_retention_stats_by_course.json</span><br><span class="line">-rw-r--r-- 1 root root 717 11月 23 13:50 edu_report.ads_sample_retention_stats_by_subject.json</span><br><span class="line">-rw-r--r-- 1 root root 781 11月 23 13:50 edu_report.ads_trade_order_by_province.json</span><br><span class="line">-rw-r--r-- 1 root root 680 11月 23 13:50 edu_report.ads_trade_stats.json</span><br><span class="line">-rw-r--r-- 1 root root 663 11月 23 13:50 edu_report.ads_traffic_page_path.json</span><br><span class="line">-rw-r--r-- 1 root root 746 11月 23 13:50 edu_report.ads_traffic_sale_stats_by_source.json</span><br><span class="line">-rw-r--r-- 1 root root 740 11月 23 13:50 edu_report.ads_traffic_stats_by_source.json</span><br><span class="line">-rw-r--r-- 1 root root 682 11月 23 13:50 edu_report.ads_user_new_buyer_stats.json</span><br><span class="line">-rw-r--r-- 1 root root 679 11月 23 13:50 edu_report.ads_user_order_user_count_by_age_group.json</span><br><span class="line">-rw-r--r-- 1 root root 709 11月 23 13:50 edu_report.ads_user_user_action.json</span><br><span class="line">-rw-r--r-- 1 root root 652 11月 23 13:50 edu_report.ads_user_user_change.json</span><br><span class="line">-rw-r--r-- 1 root root 703 11月 23 13:50 edu_report.ads_user_user_retention.json</span><br><span class="line">-rw-r--r-- 1 root root 666 11月 23 13:50 edu_report.ads_user_user_stats.json</span><br></pre></td></tr></table></figure>

<h4 id="12-2-3-测试生成的DataX配置文件"><a href="#12-2-3-测试生成的DataX配置文件" class="headerlink" title="12.2.3 测试生成的DataX配置文件"></a>12.2.3 测试生成的DataX配置文件</h4><p>略</p>
<h4 id="12-2-4-编写每日导出脚本"><a href="#12-2-4-编写每日导出脚本" class="headerlink" title="12.2.4 编写每日导出脚本"></a>12.2.4 编写每日导出脚本</h4><p>（1）在~&#x2F;bin目录下创建hdfs_to_mysql.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim hdfs_to_mysql.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">DATAX_HOME=/opt/module/datax</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">DataX导出路径不允许存在空文件，该函数作用为清理空文件</span></span><br><span class="line">handle_export_path()&#123;</span><br><span class="line">  target_file=$1</span><br><span class="line">  for i in `hadoop fs -ls -R $target_file | awk &#x27;&#123;print $8&#125;&#x27;`; do</span><br><span class="line">    hadoop fs -test -z $i</span><br><span class="line">    if [[ $? -eq 0 ]]; then</span><br><span class="line">      echo &quot;$i文件大小为0，正在删除&quot;</span><br><span class="line">      hadoop fs -rm -r -f $i</span><br><span class="line">    fi</span><br><span class="line">  done</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">数据导出</span></span><br><span class="line">export_data() &#123;</span><br><span class="line">  datax_config=$1</span><br><span class="line">  export_dir=$2</span><br><span class="line">  hadoop fs -test -e $export_dir</span><br><span class="line">  if [[ $? -eq 0 ]]</span><br><span class="line">  then</span><br><span class="line">    handle_export_path $export_dir</span><br><span class="line">    file_count=$(hadoop fs -ls $export_dir | wc -l)</span><br><span class="line">    if [ $file_count -gt 0 ]</span><br><span class="line">    then</span><br><span class="line">      set -e;</span><br><span class="line">      $DATAX_HOME/bin/datax.py -p&quot;-Dexportdir=$export_dir&quot; $datax_config</span><br><span class="line">      set +e;</span><br><span class="line">    else </span><br><span class="line">      echo &quot;$export_dir 目录为空，跳过~&quot;</span><br><span class="line">    fi</span><br><span class="line">  else</span><br><span class="line">    echo &quot;路径 $export_dir 不存在，跳过~&quot;</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">  &quot;ads_complete_complete_chapter_count_per_course&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_complete_complete_chapter_count_per_course.json /warehouse/edu/ads/ads_complete_complete_chapter_count_per_course</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_complete_complete_stats&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_complete_complete_stats.json /warehouse/edu/ads/ads_complete_complete_stats</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_complete_complete_user_count_per_course&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_complete_complete_user_count_per_course.json /warehouse/edu/ads/ads_complete_complete_user_count_per_course</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_course_review_stats_by_course&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_course_review_stats_by_course.json /warehouse/edu/ads/ads_course_review_stats_by_course</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_course_trade_stats_by_category&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_course_trade_stats_by_category.json /warehouse/edu/ads/ads_course_trade_stats_by_category</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_course_trade_stats_by_course&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_course_trade_stats_by_course.json /warehouse/edu/ads/ads_course_trade_stats_by_course</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_course_trade_stats_by_subject&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_course_trade_stats_by_subject.json /warehouse/edu/ads/ads_course_trade_stats_by_subject</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_examination_course_avg_stats&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_examination_course_avg_stats.json /warehouse/edu/ads/ads_examination_course_avg_stats</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_examination_paper_avg_stats&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_examination_paper_avg_stats.json /warehouse/edu/ads/ads_examination_paper_avg_stats</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_examination_question_accuracy&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_examination_question_accuracy.json /warehouse/edu/ads/ads_examination_question_accuracy</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_examination_user_count_by_score_duration&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_examination_user_count_by_score_duration.json /warehouse/edu/ads/ads_examination_user_count_by_score_duration</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_learn_play_stats_by_chapter&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_learn_play_stats_by_chapter.json /warehouse/edu/ads/ads_learn_play_stats_by_chapter</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_learn_play_stats_by_course&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_learn_play_stats_by_course.json /warehouse/edu/ads/ads_learn_play_stats_by_course</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_sample_retention_stats_by_category&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_sample_retention_stats_by_category.json /warehouse/edu/ads/ads_sample_retention_stats_by_category</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_sample_retention_stats_by_course&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_sample_retention_stats_by_course.json /warehouse/edu/ads/ads_sample_retention_stats_by_course</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_sample_retention_stats_by_subject&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_sample_retention_stats_by_subject.json /warehouse/edu/ads/ads_sample_retention_stats_by_subject</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_trade_order_by_province&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_trade_order_by_province.json /warehouse/edu/ads/ads_trade_order_by_province</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_trade_stats&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_trade_stats.json /warehouse/edu/ads/ads_trade_stats</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_traffic_page_path&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_traffic_page_path.json /warehouse/edu/ads/ads_traffic_page_path</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_traffic_sale_stats_by_source&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_traffic_sale_stats_by_source.json /warehouse/edu/ads/ads_traffic_sale_stats_by_source</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_traffic_stats_by_source&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_traffic_stats_by_source.json /warehouse/edu/ads/ads_traffic_stats_by_source</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_user_new_buyer_stats&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_new_buyer_stats.json /warehouse/edu/ads/ads_user_new_buyer_stats</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_user_order_user_count_by_age_group&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_order_user_count_by_age_group.json /warehouse/edu/ads/ads_user_order_user_count_by_age_group</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_user_user_action&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_user_action.json /warehouse/edu/ads/ads_user_user_action</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_user_user_change&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_user_change.json /warehouse/edu/ads/ads_user_user_change</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_user_user_retention&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_user_retention.json /warehouse/edu/ads/ads_user_user_retention</span><br><span class="line">  ;;</span><br><span class="line">  &quot;ads_user_user_stats&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_user_stats.json /warehouse/edu/ads/ads_user_user_stats</span><br><span class="line">  ;;</span><br><span class="line">  </span><br><span class="line">  &quot;all&quot;)</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_complete_complete_chapter_count_per_course.json /warehouse/edu/ads/ads_complete_complete_chapter_count_per_course</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_complete_complete_stats.json /warehouse/edu/ads/ads_complete_complete_stats</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_complete_complete_user_count_per_course.json /warehouse/edu/ads/ads_complete_complete_user_count_per_course</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_course_review_stats_by_course.json /warehouse/edu/ads/ads_course_review_stats_by_course</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_course_trade_stats_by_category.json /warehouse/edu/ads/ads_course_trade_stats_by_category</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_course_trade_stats_by_course.json /warehouse/edu/ads/ads_course_trade_stats_by_course</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_course_trade_stats_by_subject.json /warehouse/edu/ads/ads_course_trade_stats_by_subject</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_examination_course_avg_stats.json /warehouse/edu/ads/ads_examination_course_avg_stats</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_examination_paper_avg_stats.json /warehouse/edu/ads/ads_examination_paper_avg_stats</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_examination_question_accuracy.json /warehouse/edu/ads/ads_examination_question_accuracy</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_examination_user_count_by_score_duration.json /warehouse/edu/ads/ads_examination_user_count_by_score_duration</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_learn_play_stats_by_chapter.json /warehouse/edu/ads/ads_learn_play_stats_by_chapter</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_learn_play_stats_by_course.json /warehouse/edu/ads/ads_learn_play_stats_by_course</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_sample_retention_stats_by_category.json /warehouse/edu/ads/ads_sample_retention_stats_by_category</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_sample_retention_stats_by_course.json /warehouse/edu/ads/ads_sample_retention_stats_by_course</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_sample_retention_stats_by_subject.json /warehouse/edu/ads/ads_sample_retention_stats_by_subject</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_trade_order_by_province.json /warehouse/edu/ads/ads_trade_order_by_province</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_trade_stats.json /warehouse/edu/ads/ads_trade_stats</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_traffic_page_path.json /warehouse/edu/ads/ads_traffic_page_path</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_traffic_sale_stats_by_source.json /warehouse/edu/ads/ads_traffic_sale_stats_by_source</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_traffic_stats_by_source.json /warehouse/edu/ads/ads_traffic_stats_by_source</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_new_buyer_stats.json /warehouse/edu/ads/ads_user_new_buyer_stats</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_order_user_count_by_age_group.json /warehouse/edu/ads/ads_user_order_user_count_by_age_group</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_user_action.json /warehouse/edu/ads/ads_user_user_action</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_user_change.json /warehouse/edu/ads/ads_user_user_change</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_user_retention.json /warehouse/edu/ads/ads_user_user_retention</span><br><span class="line">    export_data /opt/module/datax/job/export/edu_report.ads_user_user_stats.json /warehouse/edu/ads/ads_user_user_stats </span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 hdfs_to_mysql.sh</span><br></pre></td></tr></table></figure>

<p>执行脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# hdfs_to_mysql.sh all</span><br></pre></td></tr></table></figure>

<p>此时HDFS上的数据全部同步到了MySQL上</p>
<h2 id="第十三章-数据仓库工作流调度"><a href="#第十三章-数据仓库工作流调度" class="headerlink" title="第十三章 数据仓库工作流调度"></a>第十三章 数据仓库工作流调度</h2><p>为了方便脚本正确有规律执行，需要完善的工作流调度系统。使用DolphinScheduler实现全流程调度，以及邮件报警。</p>
<h3 id="13-1-DolphinScheduler概述"><a href="#13-1-DolphinScheduler概述" class="headerlink" title="13.1 DolphinScheduler概述"></a>13.1 DolphinScheduler概述</h3><p>Apache DolphinScheduler是一个分布式、易扩展的可视化DAG工作流任务调度平台。致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用。</p>
<p>核心架构：</p>
<p>MasterServer采用分布式无中心设计理念，MasterServer主要负责 DAG 任务切分、任务提交、任务监控，并同时监听其它MasterServer和WorkerServer的健康状态。</p>
<p>WorkerServer也采用分布式无中心设计理念，WorkerServer主要负责任务的执行和提供日志服务。</p>
<p>ZooKeeper服务，系统中的MasterServer和WorkerServer节点都通过ZooKeeper来进行集群管理和容错。</p>
<p>Alert服务，提供告警相关服务。</p>
<p>API接口层，主要负责处理前端UI层的请求。</p>
<p>UI，系统的前端页面，提供系统的各种可视化操作界面。</p>
<img src="Snipaste_2023-11-23_16-02-19.png" alt="Snipaste_2023-11-23_16-02-19" style="zoom:50%;">

<p>一句话概括海豚调度器的执行过程：</p>
<p>每一个节点都启动一个MasterServerhe，WorkerServer和LoggerServer（无中心分布式就是每个节点都有），MasterServerhe和WorkerServer通过Zookeeper进行集群管理和容错。元数据会存在Database（MySQL）中，当有任务从UI下达时，调用API接口将工作流元数据写到数据库中，MasterServer进行抢占，用数据库锁锁住，只有成功抢占的节点才能操作。</p>
<h3 id="13-2-部署说明"><a href="#13-2-部署说明" class="headerlink" title="13.2 部署说明"></a>13.2 部署说明</h3><h4 id="13-2-1-服务器硬件要求"><a href="#13-2-1-服务器硬件要求" class="headerlink" title="13.2.1 服务器硬件要求"></a>13.2.1 服务器硬件要求</h4><img src="Snipaste_2023-11-23_16-14-28.png" alt="Snipaste_2023-11-23_16-14-28" style="zoom:50%;">

<h4 id="13-2-2-部署模式"><a href="#13-2-2-部署模式" class="headerlink" title="13.2.2 部署模式"></a>13.2.2 部署模式</h4><p>DolphinScheduler支持多种部署模式，包括单机模式（Standalone）、伪集群模式（Pseudo-Cluster）、集群模式（Cluster）等。</p>
<p>（1）单机模式</p>
<p>单机模式（standalone）模式下，所有服务均集中于一个StandaloneServer进程中，并且其中内置了注册中心Zookeeper和数据库H2。只需配置JDK环境，就可一键启动DolphinScheduler，快速体验其功能。</p>
<p>（2）伪集群模式</p>
<p>伪集群模式（Pseudo-Cluster）是在单台机器部署 DolphinScheduler 各项服务，该模式下master、worker、api server、logger server等服务都只在同一台机器上。Zookeeper和数据库需单独安装并进行相应配置。</p>
<p>（3）集群模式</p>
<p>集群模式（Cluster）与伪集群模式的区别就是在多台机器部署 DolphinScheduler各项服务，并且可以配置多个Master及多个Worker。</p>
<h3 id="13-3-集群模式部署"><a href="#13-3-集群模式部署" class="headerlink" title="13.3 集群模式部署"></a>13.3 集群模式部署</h3><p>集群模式下，可配置多个Master及多个Worker。通常可配置2~3个Master，若干个Worker。由于集群资源有限，此处配置一个Master，三个Worker，集群规划如下。</p>
<img src="Snipaste_2023-11-23_16-17-42.png" alt="Snipaste_2023-11-23_16-17-42" style="zoom:50%;">

<p>（1）前置准备工作</p>
<p>jdk，MySQL，Zookeeper</p>
<p>3台节点服务器安装进程管理工具包psmisc</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# yum install -y psmisc</span><br><span class="line">[root@hadoop103 bin]# yum install -y psmisc</span><br><span class="line">[root@hadoop104 bin]# yum install -y psmisc</span><br></pre></td></tr></table></figure>

<p>（2）安装解压包</p>
<p>上传DolphinScheduler安装包和MySQL驱动器jar包至hadoop102的&#x2F;opt&#x2F;software目录下，解压安装包到当前目录（不是最终的安装目录）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf apache-dolphinscheduler-2.0.3-bin.tar.gz </span><br></pre></td></tr></table></figure>

<p>（3）配置一键部署脚本</p>
<p>修改解压目录下的conf&#x2F;config目录下的install_config.conf文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# cd apache-dolphinscheduler-2.0.3-bin/conf/config/</span><br><span class="line">[root@hadoop102 config]# vim install_config.conf </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">ips=&quot;hadoop102,hadoop103,hadoop104&quot; </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将要部署任一 DolphinScheduler 服务的服务器主机名或 ip 列表</span></span><br><span class="line">masters=&quot;hadoop102&quot; </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">master 所在主机名列表，必须是 ips 的子集</span></span><br><span class="line">workers=&quot;hadoop102:default,hadoop103:default,hadoop104:default&quot; </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker主机名及队列，此处的 ip 必须在 ips 列表中</span></span><br><span class="line">alertServer=&quot;hadoop102&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">告警服务所在服务器主机名</span></span><br><span class="line">apiServers=&quot;hadoop102&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">api服务所在服务器主机名</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">pythonGatewayServers=<span class="string">&quot;ds1&quot;</span></span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">不需要的配置项，可以保留默认值，也可以用 <span class="comment"># 注释</span></span></span><br><span class="line">installPath=&quot;/opt/module/dolphinscheduler&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">DS 安装路径，如果不存在会创建</span></span><br><span class="line">deployUser=&quot;root&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">部署用户，任务执行服务是以 sudo -u &#123;linux-user&#125; 切换不同 Linux 用户的方式来实现多租户运行作业，因此该用户必须有免密的 sudo 权限。</span></span><br><span class="line">javaHome=&quot;/opt/module/jdk1.8.0_212&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">JAVA_HOME 路径</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注意：数据库相关配置的 value 必须加引号，否则配置无法生效</span></span><br><span class="line"></span><br><span class="line">DATABASE_TYPE=&quot;mysql&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">数据库类型</span></span><br><span class="line">SPRING_DATASOURCE_URL=&quot;jdbc:mysql://hadoop102:3306/dolphinscheduler?useUnicode=true&amp;characterEncoding=UTF-8&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">数据库 URL</span></span><br><span class="line">SPRING_DATASOURCE_USERNAME=&quot;dolphinscheduler&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">数据库用户名</span></span><br><span class="line">SPRING_DATASOURCE_PASSWORD=&quot;dolphinscheduler&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">数据库密码</span></span><br><span class="line">registryPluginName=&quot;zookeeper&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注册中心插件名称，DS 通过注册中心来确保集群配置的一致性</span></span><br><span class="line">registryServers=&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注册中心地址，即 Zookeeper 集群的地址</span></span><br><span class="line">registryNamespace=&quot;dolphinscheduler&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">DS 在 Zookeeper 的结点名称</span></span><br><span class="line">resourceStorageType=&quot;HDFS&quot;	</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">资源存储类型</span></span><br><span class="line">resourceUploadPath=&quot;/dolphinscheduler&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">资源上传路径</span></span><br><span class="line">defaultFS=&quot;hdfs://hadoop102:8020&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">默认文件系统</span></span><br><span class="line">resourceManagerHttpAddressPort=&quot;8088&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">yarn RM http 访问端口</span></span><br><span class="line">yarnHaIps=</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Yarn RM 高可用 ip，若未启用 RM 高可用，则将该值置空</span></span><br><span class="line">singleYarnIp=&quot;hadoop103&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Yarn RM 主机名，若启用了 HA 或未启用 RM，保留默认值</span></span><br><span class="line">hdfsRootUser=&quot;atguigu&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">拥有 HDFS 根目录操作权限的用户</span></span><br></pre></td></tr></table></figure>

<p>（4）初始化数据库</p>
<p>DolphinScheduler 元数据存储在关系型数据库中，故需创建相应的数据库和用户</p>
<p>①创建dolphinScheduler 数据库</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> DATABASE dolphinscheduler <span class="keyword">DEFAULT</span> <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">DEFAULT</span> <span class="keyword">COLLATE</span> utf8_general_ci;</span><br></pre></td></tr></table></figure>

<p>②创建dolphinScheduler用户</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> validate_password_length<span class="operator">=</span><span class="number">4</span>;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> validate_password_policy<span class="operator">=</span><span class="number">0</span>;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">&#x27;dolphinscheduler&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">&#x27;dolphinscheduler&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>③赋予用户相应权限</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">GRANT</span> <span class="keyword">ALL</span> PRIVILEGES <span class="keyword">ON</span> dolphinscheduler.<span class="operator">*</span> <span class="keyword">TO</span> <span class="string">&#x27;dolphinscheduler&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br></pre></td></tr></table></figure>

<p>④拷贝MySQL驱动到DolphinScheduler的解压目录下的lib中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 apache-dolphinscheduler-2.0.3-bin]# cp /opt/software/mysql-connector-java-8.0.16.jar lib/</span><br></pre></td></tr></table></figure>

<p>⑤执行数据库初始化脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 apache-dolphinscheduler-2.0.3-bin]# script/create-dolphinscheduler.sh</span><br></pre></td></tr></table></figure>

<p>（5）一键部署Dolphinscheduler</p>
<p>①启动zookeeper集群</p>
<p>②一键部署并启动DolphinScheduler</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 apache-dolphinscheduler-2.0.3-bin]# ./install.sh</span><br></pre></td></tr></table></figure>

<p>③查看DolphinScheduler进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 apache-dolphinscheduler-2.0.3-bin]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">3584 JobHistoryServer</span><br><span class="line">2883 NameNode</span><br><span class="line">3399 NodeManager</span><br><span class="line">3688 RunJar</span><br><span class="line">20136 WorkerServer       ###</span><br><span class="line">20235 AlertServer        ###</span><br><span class="line">19437 QuorumPeerMain</span><br><span class="line">20271 ApiApplicationServer   ###</span><br><span class="line">3668 RunJar</span><br><span class="line">3030 DataNode</span><br><span class="line">20183 LoggerServer        ###</span><br><span class="line">20089 MasterServer      ###</span><br><span class="line">20733 Jps</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">7169 LoggerServer      ###</span><br><span class="line">7122 WorkerServer      ###</span><br><span class="line">3044 NodeManager</span><br><span class="line">2889 ResourceManager</span><br><span class="line">6778 QuorumPeerMain</span><br><span class="line">2670 DataNode</span><br><span class="line">7327 Jps</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">7536 Jps</span><br><span class="line">2657 DataNode</span><br><span class="line">2873 NodeManager</span><br><span class="line">7005 QuorumPeerMain</span><br><span class="line">2782 SecondaryNameNode</span><br><span class="line">7390 LoggerServer       ###</span><br><span class="line">7343 WorkerServer       ###</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>④访问DolphinScheduler UI</p>
<p>DolphinScheduler UI地址为<a target="_blank" rel="noopener" href="http://hadoop102:12345/dolphinscheduler">http://hadoop102:12345/dolphinscheduler</a></p>
<p>初始用户的用户名为：admin，密码为dolphinscheduler123</p>
<p><img src="Snipaste_2023-11-23_17-04-33.png" alt="Snipaste_2023-11-23_17-04-33"></p>
<h3 id="13-4-Dolphinscheduler启停命令"><a href="#13-4-Dolphinscheduler启停命令" class="headerlink" title="13.4 Dolphinscheduler启停命令"></a>13.4 Dolphinscheduler启停命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/module/dolphinscheduler/</span><br><span class="line">[root@hadoop102 dolphinscheduler]# ./bin/start-all.sh    # 启动所有服务</span><br><span class="line">[root@hadoop102 dolphinscheduler]# ./bin/stop-all.sh     # 停止所有服务</span><br></pre></td></tr></table></figure>

<h3 id="13-5-Dolphinscheduler入门"><a href="#13-5-Dolphinscheduler入门" class="headerlink" title="13.5 Dolphinscheduler入门"></a>13.5 Dolphinscheduler入门</h3><h4 id="13-5-1-创建租户"><a href="#13-5-1-创建租户" class="headerlink" title="13.5.1 创建租户"></a>13.5.1 创建租户</h4><p>租户对应的时Linux的系统用户</p>
<img src="Snipaste_2023-11-23_17-14-52.png" alt="Snipaste_2023-11-23_17-14-52" style="zoom:50%;">

<h4 id="13-5-2-创建用户"><a href="#13-5-2-创建用户" class="headerlink" title="13.5.2 创建用户"></a>13.5.2 创建用户</h4><p>创建一个普通用户root，普通用户没有安全中心的权限，密码：wyhdhr19980418</p>
<p><img src="Snipaste_2023-11-23_17-17-31.png" alt="Snipaste_2023-11-23_17-17-31"></p>
<p><img src="Snipaste_2023-11-23_17-18-29.png" alt="Snipaste_2023-11-23_17-18-29"></p>
<p>退出管理员用户，使用普通用户登录，后续所有操作都使用普通用户完成</p>
<p><img src="Snipaste_2023-11-23_17-20-30.png" alt="Snipaste_2023-11-23_17-20-30"></p>
<h4 id="13-5-3-工作流基础配置"><a href="#13-5-3-工作流基础配置" class="headerlink" title="13.5.3 工作流基础配置"></a>13.5.3 工作流基础配置</h4><p>首先在项目管理中点“创建项目”</p>
<p><img src="Snipaste_2023-11-23_17-32-46.png" alt="Snipaste_2023-11-23_17-32-46"></p>
<p>点击项目，进入项目工作台</p>
<p>下图为工作流配置页面，共包含三个模快，分别为工作流定义、工作流实例和任务实例。</p>
<p>工作流定义：用于定义工作流，包括工作流各节点任务详情及各节点依赖关系等。</p>
<p>工作流实例：工作流每执行一次就会生成一个工作流实例。此处可查看正在运行的工作流以及已经完成的工作流。</p>
<p>任务实例：工作流中的一个节点任务，每执行一次就会生成一个任务实例。此处可用于查看正在执行的节点任务以及已经完成的节点任务。</p>
<p><img src="Snipaste_2023-11-23_17-33-48.png" alt="Snipaste_2023-11-23_17-33-48"></p>
<p>点击“创建工作流”，创建3个工作节点</p>
<img src="Snipaste_2023-11-23_17-36-22.png" alt="Snipaste_2023-11-23_17-36-22" style="zoom:33%;">

<img src="Snipaste_2023-11-23_17-37-16.png" alt="Snipaste_2023-11-23_17-37-16" style="zoom:33%;">

<img src="Snipaste_2023-11-23_17-37-50.png" alt="Snipaste_2023-11-23_17-37-50" style="zoom:33%;">

<p>对DAG图直接操作</p>
<img src="Snipaste_2023-11-23_17-38-57.png" alt="Snipaste_2023-11-23_17-38-57" style="zoom:33%;">

<p>点击保存，工作流创建并保存成功：</p>
<p><img src="Snipaste_2023-11-23_17-39-59.png" alt="Snipaste_2023-11-23_17-39-59"></p>
<p>点击“上线”—&gt;“运行”</p>
<p>查看所有工作流实例：</p>
<p><img src="Snipaste_2023-11-23_17-48-20.png" alt="Snipaste_2023-11-23_17-48-20"></p>
<p>查看所有任务实例</p>
<p><img src="Snipaste_2023-11-23_17-45-59.png" alt="Snipaste_2023-11-23_17-45-59"></p>
<h3 id="13-6-Dolphinscheduler进阶"><a href="#13-6-Dolphinscheduler进阶" class="headerlink" title="13.6 Dolphinscheduler进阶"></a>13.6 Dolphinscheduler进阶</h3><h4 id="13-6-1-工作流传参"><a href="#13-6-1-工作流传参" class="headerlink" title="13.6.1 工作流传参"></a>13.6.1 工作流传参</h4><p>略，看文档</p>
<h4 id="13-6-2-引用依赖资源"><a href="#13-6-2-引用依赖资源" class="headerlink" title="13.6.2 引用依赖资源"></a>13.6.2 引用依赖资源</h4><p>略，看文档</p>
<h4 id="13-6-3-数据源配置"><a href="#13-6-3-数据源配置" class="headerlink" title="13.6.3 数据源配置"></a>13.6.3 数据源配置</h4><p>略，看文档</p>
<h4 id="13-6-4-邮箱告警实例配置"><a href="#13-6-4-邮箱告警实例配置" class="headerlink" title="13.6.4 邮箱告警实例配置"></a>13.6.4 邮箱告警实例配置</h4><p>如需使用DolphinScheduler的邮件告警通知功能，需要准备一个电子邮箱账号，并启用SMTP服务。此处以 QQ 邮箱为例。</p>
<p>（1）POP3，IMAP，SMTP</p>
<p>POP3是Post Office Protocol 3的简称，即邮局协议的第3个版本,它规定怎样将个人计算机连接到Internet的邮件服务器和下载电子邮件的电子协议。它是因特网电子邮件的第一个离线协议标准,POP3允许用户从服务器上把邮件存储到本地主机（即自己的计算机）上,同时删除保存在邮件服务器上的邮件，而POP3服务器则是遵循POP3协议的接收邮件服务器，用来接收电子邮件的。(与IMAP有什么区别？)</p>
<p>SMTP 的全称是“Simple Mail Transfer Protocol”，即简单邮件传输协议。它是一组用于从源地址到目的地址传输邮件的规范，通过它来控制邮件的中转方式。SMTP 协议属于 TCP&#x2F;IP 协议簇，它帮助每台计算机在发送或中转信件时找到下一个目的地。SMTP 服务器就是遵循 SMTP 协议的发送邮件服务器。</p>
<p>SMTP 认证，简单地说就是要求必须在提供了账户名和密码之后才可以登录 SMTP 服务器，这就使得那些垃圾邮件的散播者无可乘之机。</p>
<p>增加 SMTP 认证的目的是为了使用户避免受到垃圾邮件的侵扰。</p>
<p>IMAP全称是Internet Mail Access Protocol，即交互式邮件存取协议，它是跟POP3类似邮件访问标准协议之一。不同的是，开启了IMAP后，您在电子邮件客户端收取的邮件仍然保留在服务器上，同时在客户端上的操作都会反馈到服务器上，如：删除邮件，标记已读等，服务器上的邮件也会做相应的动作。所以无论从浏览器登录邮箱或者客户端软件登录邮箱，看到的邮件以及状态都是一致的。</p>
<img src="Snipaste_2023-11-24_11-42-38.png" alt="Snipaste_2023-11-24_11-42-38" style="zoom:50%;">

<p>i）DS 使用 mail.sender 指定的邮箱发送邮件到 SMTP 服务器，要求此邮箱开启 SMTP 服务；</p>
<p>ii）SMTP 服务将邮件转交给 POP3 或 IMAP 服务，经测试，通常SMTP 服务和 POP3 或 IMAP 服务处于同一台服务器；</p>
<p>iii）收件邮箱客户端从 IMAP&#x2F;POP3 服务器拉取邮件，某些邮箱可以设置邮件刷新时间，以此来控制客户端从服务端拉取邮件的频率。</p>
<p>iv）需要注意：此处的邮件客户端均为第三方右键客户端，登陆邮箱输入的密码为授权码，使用 web 端收发邮件的流程可能有所不同。</p>
<p>（2）获取授权码</p>
<p>①开启SMTP服务</p>
<img src="Snipaste_2023-11-24_11-45-52.png" alt="Snipaste_2023-11-24_11-45-52" style="zoom: 50%;">

<p>拖动进度条在页面下方找到下图所示内容，开启 POP3&#x2F;SMTP | IMAP&#x2F;SMTP 任一服务即可。</p>
<p><img src="Snipaste_2023-11-24_11-51-18.png" alt="Snipaste_2023-11-24_11-51-18"></p>
<p>记住授权码：ozlhealjkxmbdach（忘记可重新生成）</p>
<p>（3）DolphinScheduler配置</p>
<p>用admin登录，点击告警实例管理</p>
<img src="Snipaste_2023-11-24_12-11-25.png" alt="Snipaste_2023-11-24_12-11-25" style="zoom:50%;">

<ul>
<li><p>告警实例名称：在告警组配置时可以选择的告警插件实例名称，用户自定义。</p>
</li>
<li><p>选择插件：选择 Email 则为邮箱告警实例</p>
</li>
<li><p>收件人：接收方邮箱地址，收件人不需要开启 SMTP 服务。</p>
</li>
<li><p>抄送人：抄送是指用户给收件人发出邮件的同时把该邮件发送给另外的人，收件人之外的收件方都是抄送人，“收件人”可以获知该邮件的所有抄送人。抄送人可以为空。</p>
</li>
<li><p>mail.smtp.host：邮箱的 SMTP 服务器域名，对于 QQ 邮箱，为 smtp.qq.com。各邮箱的 SMTP 服务器见此链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/wustzjf/article/details/52481309">https://blog.csdn.net/wustzjf/article/details/52481309</a></p>
</li>
<li><p>mail.smtp.port：邮箱的 SMTP 服务端口号，主流邮箱均为 25 端口，使用默认值即可。</p>
</li>
<li><p>mail.sender：发件方邮箱地址，需要开启 SMTP 服务。</p>
</li>
<li><p>mail.user：与 mail.sender 保持一致即可。</p>
</li>
<li><p>mail.password：上文获取的邮箱授权码。未列出的选项保留默认值或默认选项即可。</p>
</li>
</ul>
<p>再点击告警组管理</p>
<img src="Snipaste_2023-11-24_12-03-28.png" alt="Snipaste_2023-11-24_12-03-28" style="zoom:50%;">

<p>测试：</p>
<p>①在工作流的其中一个节点添加运行一个不存在的脚本：</p>
<img src="Snipaste_2023-11-24_12-05-28.png" alt="Snipaste_2023-11-24_12-05-28" style="zoom: 33%;">

<p>②将工作流上线、运行</p>
<img src="Snipaste_2023-11-24_12-07-31.png" alt="Snipaste_2023-11-24_12-07-31" style="zoom:33%;">

<p>③可以看到工作流运行失败</p>
<p><img src="Snipaste_2023-11-24_12-14-09.png" alt="Snipaste_2023-11-24_12-14-09"></p>
<p><a href="mailto:&#x31;&#55;&#x37;&#52;&#53;&#x31;&#56;&#x32;&#54;&#x30;&#x35;&#x40;&#x31;&#x36;&#x33;&#46;&#x63;&#111;&#109;">&#x31;&#55;&#x37;&#52;&#53;&#x31;&#56;&#x32;&#54;&#x30;&#x35;&#x40;&#x31;&#x36;&#x33;&#46;&#x63;&#111;&#109;</a>收到邮件：</p>
<img src="Snipaste_2023-11-24_12-15-50.png" alt="Snipaste_2023-11-24_12-15-50" style="zoom:50%;">

<p><a href="mailto:&#x33;&#x35;&#x35;&#x38;&#x30;&#x38;&#52;&#x37;&#x32;&#54;&#x40;&#x71;&#x71;&#x2e;&#x63;&#x6f;&#x6d;">&#x33;&#x35;&#x35;&#x38;&#x30;&#x38;&#52;&#x37;&#x32;&#54;&#x40;&#x71;&#x71;&#x2e;&#x63;&#x6f;&#x6d;</a>收到抄送：</p>
<p><img src="Snipaste_2023-11-24_12-17-20.png" alt="Snipaste_2023-11-24_12-17-20"></p>
<h3 id="13-7-全流程调度"><a href="#13-7-全流程调度" class="headerlink" title="13.7 全流程调度"></a>13.7 全流程调度</h3><h4 id="13-7-1-数据准备"><a href="#13-7-1-数据准备" class="headerlink" title="13.7.1 数据准备"></a>13.7.1 数据准备</h4><p>此处模拟生成一天的新数据，作为全流程调度的测试数据</p>
<p>（1）执行集群启动脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cluster.sh start</span><br></pre></td></tr></table></figure>

<p>（2）启动maxwell</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# mxw.sh start</span><br></pre></td></tr></table></figure>

<p>（3）启动业务数据采集Flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# f3.sh start</span><br></pre></td></tr></table></figure>

<p>（4）模拟生成2022-02-22的数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# mock.sh 2022-02-22</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-11-24_12-39-38.png" alt="Snipaste_2023-11-24_12-39-38" style="zoom:33%;">

<p>为了节省资源，将采集数据的相关进程关掉</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# kf.sh stop</span><br><span class="line"> --------停止 hadoop102 Kafka-------</span><br><span class="line"> --------停止 hadoop103 Kafka-------</span><br><span class="line"> --------停止 hadoop104 Kafka-------</span><br><span class="line">[root@hadoop102 ~]# f3.sh stop</span><br><span class="line"> --------停止 hadoop104 业务数据flume-------</span><br><span class="line">[root@hadoop102 ~]# f2.sh stop</span><br><span class="line"> --------停止 hadoop104 消费flume-------</span><br><span class="line">[root@hadoop102 ~]# f1.sh stop</span><br><span class="line"> --------停止 hadoop102 采集flume-------</span><br><span class="line">[root@hadoop102 ~]# mxw.sh stop</span><br><span class="line">停止Maxwell</span><br></pre></td></tr></table></figure>

<h4 id="17-7-2-全流程调度配置"><a href="#17-7-2-全流程调度配置" class="headerlink" title="17.7.2 全流程调度配置"></a>17.7.2 全流程调度配置</h4><p>（1）启动dolphinScheduler</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 dolphinscheduler]# ./bin/start-all.sh</span><br></pre></td></tr></table></figure>

<p>（2）向DolphinSecheduler资源中心上传工作流所需要的脚本</p>
<p>①在文件管理中创建edu文件夹</p>
<p>②将工作流所需要的所有脚本上传至资源中心的edu文件夹下</p>
<p><img src="Snipaste_2023-11-24_12-56-35.png" alt="Snipaste_2023-11-24_12-56-35"></p>
<p>（3）向每个节点分发脚本依赖的组件hive和dataX</p>
<p>（4）切换至admin用户，在环境管理下点击“创建环境”选项</p>
<img src="Snipaste_2023-11-24_13-17-10.png" alt="Snipaste_2023-11-24_13-17-10" style="zoom: 50%;">

<p>（5）创建项目edu，工作流定义—-&gt;创建工作流</p>
<img src="Snipaste_2023-11-24_13-19-17.png" alt="Snipaste_2023-11-24_13-19-17" style="zoom: 50%;">

<p>依次类推，创建10个脚本的工作节点</p>
<img src="Snipaste_2023-11-24_13-29-20.png" alt="Snipaste_2023-11-24_13-29-20" style="zoom: 50%;">

<p>（6）各节点依赖关系</p>
<img src="chart.png" alt="chart" style="zoom: 67%;">

<p>（7）保存工作流</p>
<img src="Snipaste_2023-11-24_13-38-23.png" alt="Snipaste_2023-11-24_13-38-23" style="zoom:33%;">

<p>点击上线，运行，由于运行时间过长，在此不运行了</p>
<h2 id="第十四章-数据可视化模块"><a href="#第十四章-数据可视化模块" class="headerlink" title="第十四章 数据可视化模块"></a>第十四章 数据可视化模块</h2><h3 id="14-1-Superset部署"><a href="#14-1-Superset部署" class="headerlink" title="14.1 Superset部署"></a>14.1 Superset部署</h3><h4 id="14-1-1-环境准备"><a href="#14-1-1-环境准备" class="headerlink" title="14.1.1 环境准备"></a>14.1.1 环境准备</h4><p>Superset是由Python语言编写的Web应用，要求Python3.7以上的环境。</p>
<p>（1）安装Miniconda</p>
<p>将下载的Miniconda包上传至&#x2F;opt&#x2F;software&#x2F;路径，然后执行命令安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# bash Miniconda3-latest-Linux-x86_64.sh </span><br></pre></td></tr></table></figure>

<p>一直按回车，点yes</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Please answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; <span class="built_in">yes</span></span></span><br></pre></td></tr></table></figure>

<p>指定安装路径</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[/root/miniconda3] &gt;&gt;&gt; /opt/module/miniconda3</span><br></pre></td></tr></table></figure>

<p>是否初始化点yes</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">by running conda init? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes</span><br></pre></td></tr></table></figure>

<p>（2）加载环境变量配置文件，使之生效</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# source ~/.bashrc</span><br><span class="line">(base) [root@hadoop102 software]# </span><br></pre></td></tr></table></figure>

<p>（3）禁止激活默认的base环境</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@hadoop102 software]# conda config --set auto_activate_base false</span><br></pre></td></tr></table></figure>

<p>（4）创建python3.8环境</p>
<p>配置Conda国内镜像</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@hadoop102 software]# conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span><br><span class="line">(base) [root@hadoop102 software]# conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">(base) [root@hadoop102 software]# conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure>

<p>创建python3.8环境</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@hadoop102 software]# conda create --name superset python=3.8.16</span><br></pre></td></tr></table></figure>

<ul>
<li>说明：conda环境管理常用命令</li>
<li>创建环境：conda create -n env_name</li>
<li>查看所有环境：conda info –envs</li>
<li>删除一个环境：conda remove -n env_name –all</li>
</ul>
<p>激活superset环境</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@hadoop102 software]# conda activate superset</span><br><span class="line">(superset) [root@hadoop102 software]# </span><br></pre></td></tr></table></figure>

<p>查看python及其版本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# python</span><br><span class="line">Python 3.8.16 (default, Jun 12 2023, 18:09:05) </span><br><span class="line">[GCC 11.2.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; quit()</span></span><br></pre></td></tr></table></figure>

<p>退出当前环境</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# conda deactivate</span><br></pre></td></tr></table></figure>

<h4 id="14-1-2-Superset安装"><a href="#14-1-2-Superset安装" class="headerlink" title="14.1.2 Superset安装"></a>14.1.2 Superset安装</h4><p>（1）安装依赖</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# yum install -y gcc gcc-c++ libffi-devel python-devel python-pip python-wheel python-setuptools openssl-devel cyrus-sasl-devel openldap-devel</span><br></pre></td></tr></table></figure>

<p>（2）安装Superset</p>
<p>①执行以下命令，安装setuotools和pip</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# pip install --upgrade setuptools pip -i https://pypi.douban.com/simple/</span><br></pre></td></tr></table></figure>

<p>②安装superset</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# pip install apache-superset==2.0.0 -i https://pypi.tuna.tsinghua.edu.cn/simple -r base.txt</span><br></pre></td></tr></table></figure>

<h4 id="14-1-3-配置Superset元数据库"><a href="#14-1-3-配置Superset元数据库" class="headerlink" title="14.1.3 配置Superset元数据库"></a>14.1.3 配置Superset元数据库</h4><p>（1）在MySQL中创建superset元数据库</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> DATABASE superset <span class="keyword">DEFAULT</span> <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">DEFAULT</span> <span class="keyword">COLLATE</span> utf8_general_ci;</span><br></pre></td></tr></table></figure>

<p>（2）创建superset用户</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> validate_password_length<span class="operator">=</span><span class="number">4</span>;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> validate_password_policy<span class="operator">=</span><span class="number">0</span>;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> superset@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">WITH</span> mysql_native_password <span class="keyword">BY</span> <span class="string">&#x27;superset&#x27;</span>;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">all</span> privileges <span class="keyword">on</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">to</span> superset@<span class="string">&#x27;%&#x27;</span> <span class="keyword">with</span> <span class="keyword">grant</span> option;</span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br></pre></td></tr></table></figure>

<p>（3）修改superset配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# vim /opt/module/miniconda3/envs/superset/lib/python3.8/site-packages/superset/config.py</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SQLALCHEMY_DATABASE_URI = &quot;sqlite:///&quot; + os.path.join(DATA_DIR, &quot;superset.db&quot;)</span></span><br><span class="line">SQLALCHEMY_DATABASE_URI = <span class="string">&#x27;mysql://superset:superset@hadoop102:3306/superset?charset=utf8&#x27;</span></span><br></pre></td></tr></table></figure>

<p>（4）安装python mysql驱动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# conda install mysqlclient</span><br></pre></td></tr></table></figure>

<p>（5）初始化superset元数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# export FLASK_APP=superset</span><br></pre></td></tr></table></figure>

<h4 id="14-1-4-superset初始化"><a href="#14-1-4-superset初始化" class="headerlink" title="14.1.4 superset初始化"></a>14.1.4 superset初始化</h4><p>（1）创建管理员用户</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# superset fab create-admin</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Username [admin]: atguigu</span><br><span class="line">User first name [admin]: at</span><br><span class="line">User last name [user]: guigu</span><br><span class="line">Email [admin@fab.org]: </span><br><span class="line">Password: </span><br><span class="line">Repeat for confirmation: </span><br><span class="line">Recognized Database Authentications.</span><br><span class="line">Admin User atguigu created.</span><br></pre></td></tr></table></figure>

<p>用户名：atguigu，密码：123456</p>
<p>（2）初始化superset</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# superset init</span><br></pre></td></tr></table></figure>

<h4 id="14-1-5-启动superset"><a href="#14-1-5-启动superset" class="headerlink" title="14.1.5 启动superset"></a>14.1.5 启动superset</h4><p>（1）安装gunicorn</p>
<p>gunicorn是一个Python Web Server，可以和java中的TomCat类比</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# pip install gunicorn -i https://pypi.douban.com/simple/</span><br></pre></td></tr></table></figure>

<p>（2）启动superset</p>
<p>确保当前conda环境为supersetm，启动：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# gunicorn --workers 5 --timeout 120 --bind hadoop102:8787  &quot;superset.app:create_app()&quot; --daemon</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ul>
<li>–workers：指定进程个数</li>
<li>–timeout：worker进程超时时间，超时会自动重启</li>
<li>–bind：绑定本机地址，即为Superset访问地址</li>
<li>–daemon：后台运行</li>
</ul>
<p>（3）登录Superset</p>
<p>访问<a href="http://hadoop102:8787，并使用atguigu+123456登录">http://hadoop102:8787，并使用atguigu+123456登录</a></p>
<p><img src="Snipaste_2023-11-24_15-02-08.png" alt="Snipaste_2023-11-24_15-02-08"></p>
<img src="Snipaste_2023-11-24_15-02-46.png" alt="Snipaste_2023-11-24_15-02-46" style="zoom:50%;">

<h4 id="14-1-6-Superset启停脚本"><a href="#14-1-6-Superset启停脚本" class="headerlink" title="14.1.6 Superset启停脚本"></a>14.1.6 Superset启停脚本</h4><p>（1）创建superset.sh文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim superset.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">superset_status()&#123;</span><br><span class="line">    result=`ps -ef | awk &#x27;/gunicorn/ &amp;&amp; !/awk/&#123;print $2&#125;&#x27; | wc -l`</span><br><span class="line">    if [[ $result -eq 0 ]]; then</span><br><span class="line">        return 0</span><br><span class="line">    else</span><br><span class="line">        return 1</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line">superset_start()&#123;</span><br><span class="line">        source ~/.bashrc</span><br><span class="line">        superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">        if [[ $? -eq 0 ]]; then</span><br><span class="line">            conda activate superset ; gunicorn --workers 5 --timeout 120 --bind hadoop102:8787 --daemon &#x27;superset.app:create_app()&#x27;</span><br><span class="line">        else</span><br><span class="line">            echo &quot;superset正在运行&quot;</span><br><span class="line">        fi</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">superset_stop()&#123;</span><br><span class="line">    superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">    if [[ $? -eq 0 ]]; then</span><br><span class="line">        echo &quot;superset未在运行&quot;</span><br><span class="line">    else</span><br><span class="line">        ps -ef | awk &#x27;/gunicorn/ &amp;&amp; !/awk/&#123;print $2&#125;&#x27; | xargs kill -9</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">    start )</span><br><span class="line">        echo &quot;启动Superset&quot;</span><br><span class="line">        superset_start</span><br><span class="line">    ;;</span><br><span class="line">    stop )</span><br><span class="line">        echo &quot;停止Superset&quot;</span><br><span class="line">        superset_stop</span><br><span class="line">    ;;</span><br><span class="line">    restart )</span><br><span class="line">        echo &quot;重启Superset&quot;</span><br><span class="line">        superset_stop</span><br><span class="line">        superset_start</span><br><span class="line">    ;;</span><br><span class="line">    status )</span><br><span class="line">        superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">        if [[ $? -eq 0 ]]; then</span><br><span class="line">            echo &quot;superset未在运行&quot;</span><br><span class="line">        else</span><br><span class="line">            echo &quot;superset正在运行&quot;</span><br><span class="line">        fi</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 superset.sh</span><br></pre></td></tr></table></figure>

<h3 id="14-2-Superset使用"><a href="#14-2-Superset使用" class="headerlink" title="14.2 Superset使用"></a>14.2 Superset使用</h3><h4 id="14-2-1-对接MySQL数据库"><a href="#14-2-1-对接MySQL数据库" class="headerlink" title="14.2.1 对接MySQL数据库"></a>14.2.1 对接MySQL数据库</h4><p>（1）安装依赖（已安装）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(superset) [root@hadoop102 software]# conda install mysqlclient</span><br></pre></td></tr></table></figure>

<p>（2）数据源配置</p>
<p>①Database配置</p>
<img src="Snipaste_2023-11-24_15-19-39.png" alt="Snipaste_2023-11-24_15-19-39" style="zoom:50%;">

<p>点击添加数据库图标，选择MySQL，使用URI方式配置MySQL数据库</p>
<img src="Snipaste_2023-11-24_15-20-55.png" alt="Snipaste_2023-11-24_15-20-55" style="zoom: 33%;">

<img src="Snipaste_2023-11-24_15-24-02.png" alt="Snipaste_2023-11-24_15-24-02" style="zoom:33%;">

<p>出现“Connection looks good!”提示即表示连接成功，点击CONNECT即可</p>
<p>②Table配置</p>
<p><img src="Snipaste_2023-11-24_15-25-59.png" alt="Snipaste_2023-11-24_15-25-59"></p>
<p>点击添加数据表图标，并选择相应的表</p>
<img src="Snipaste_2023-11-24_15-27-44.png" alt="Snipaste_2023-11-24_15-27-44" style="zoom:33%;">

<h4 id="14-2-2-制作仪表盘"><a href="#14-2-2-制作仪表盘" class="headerlink" title="14.2.2 制作仪表盘"></a>14.2.2 制作仪表盘</h4><p>（1）创建空白仪表盘</p>
<p>点击Dashboards</p>
<p><img src="Snipaste_2023-11-24_15-39-58.png" alt="Snipaste_2023-11-24_15-39-58"></p>
<p>把绘制的地图添加进来，仪表盘也可以调整各个图像的布局，大小和自动刷新时间</p>
<p><img src="Snipaste_2023-11-24_15-44-47.png" alt="Snipaste_2023-11-24_15-44-47"></p>
<h4 id="14-2-3-创建地图"><a href="#14-2-3-创建地图" class="headerlink" title="14.2.3 创建地图"></a>14.2.3 创建地图</h4><p><img src="Snipaste_2023-11-24_15-30-31.png" alt="Snipaste_2023-11-24_15-30-31"></p>
<img src="Snipaste_2023-11-24_15-31-34.png" alt="Snipaste_2023-11-24_15-31-34" style="zoom: 33%;">

<img src="Snipaste_2023-11-24_15-32-43.png" alt="Snipaste_2023-11-24_15-32-43" style="zoom:33%;">

<img src="Snipaste_2023-11-24_15-35-20.png" alt="Snipaste_2023-11-24_15-35-20" style="zoom:33%;">

<img src="Snipaste_2023-11-24_15-36-31.png" alt="Snipaste_2023-11-24_15-36-31" style="zoom:33%;">

<p><img src="Snipaste_2023-11-24_15-37-27.png" alt="Snipaste_2023-11-24_15-37-27"></p>
<h4 id="14-2-4-创建桑基图"><a href="#14-2-4-创建桑基图" class="headerlink" title="14.2.4 创建桑基图"></a>14.2.4 创建桑基图</h4><img src="Snipaste_2023-11-24_15-48-26.png" alt="Snipaste_2023-11-24_15-48-26" style="zoom:33%;">

<img src="Snipaste_2023-11-24_15-49-29.png" alt="Snipaste_2023-11-24_15-49-29" style="zoom:33%;">

<img src="Snipaste_2023-11-24_15-50-50.png" alt="Snipaste_2023-11-24_15-50-50" style="zoom: 33%;">

<h4 id="14-2-5-创建柱状图"><a href="#14-2-5-创建柱状图" class="headerlink" title="14.2.5 创建柱状图"></a>14.2.5 创建柱状图</h4><img src="Snipaste_2023-11-24_15-52-49.png" alt="Snipaste_2023-11-24_15-52-49" style="zoom:33%;">

<img src="Snipaste_2023-11-24_15-53-52.png" alt="Snipaste_2023-11-24_15-53-52" style="zoom:33%;">

<img src="Snipaste_2023-11-24_15-56-28.png" alt="Snipaste_2023-11-24_15-56-28" style="zoom:33%;">

<h4 id="14-2-6-创建大数字表"><a href="#14-2-6-创建大数字表" class="headerlink" title="14.2.6 创建大数字表"></a>14.2.6 创建大数字表</h4><img src="Snipaste_2023-11-24_15-59-30.png" alt="Snipaste_2023-11-24_15-59-30" style="zoom:33%;">

<img src="Snipaste_2023-11-24_16-00-14.png" alt="Snipaste_2023-11-24_16-00-14" style="zoom:33%;">

<img src="Snipaste_2023-11-24_16-01-58.png" alt="Snipaste_2023-11-24_16-01-58" style="zoom:33%;">

<p>最终在仪表盘上合理展示：</p>
<p><img src="Snipaste_2023-11-24_16-04-16.png" alt="Snipaste_2023-11-24_16-04-16"></p>
<h2 id="做项目中遇到的BUG"><a href="#做项目中遇到的BUG" class="headerlink" title="做项目中遇到的BUG"></a>做项目中遇到的BUG</h2><p>在搭建数仓工作流程调度器Dolphinscheduler时，发现登录UI页面输入用户名和密码后不能登录</p>
<p>（1）首先查看海豚调度器各项服务的状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 dolphinscheduler]# ./bin/status-all.sh</span><br></pre></td></tr></table></figure>

<p>发现alert-server是STOP的状态</p>
<p>（2）此时我们去查看海豚调度器的日志</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 dolphinscheduler]# cd logs/</span><br><span class="line">[root@hadoop102 logs]# ll</span><br><span class="line">总用量 42116</span><br><span class="line">-rw-r--r-- 1 root root        0 11月 23 16:57 dolphinscheduler-alert.log</span><br><span class="line">-rw-r--r-- 1 root root    15330 11月 23 22:24 dolphinscheduler-alert-server-hadoop102.out</span><br><span class="line">-rw-r--r-- 1 root root    25303 11月 23 16:57 dolphinscheduler-api.2023-11-23_16.0.log</span><br><span class="line">-rw-r--r-- 1 root root    38903 11月 23 17:51 dolphinscheduler-api.2023-11-23_17.0.log</span><br><span class="line">-rw-r--r-- 1 root root    28699 11月 23 20:57 dolphinscheduler-api.2023-11-23_20.0.log</span><br><span class="line">-rw-r--r-- 1 root root   578481 11月 23 21:41 dolphinscheduler-api.2023-11-23_21.0.log</span><br><span class="line">-rw-r--r-- 1 root root    29513 11月 23 22:25 dolphinscheduler-api.log</span><br><span class="line">-rw-r--r-- 1 root root      823 11月 23 22:24 dolphinscheduler-api-server-hadoop102.out</span><br><span class="line">-rw-r--r-- 1 root root    16938 11月 23 22:24 dolphinscheduler-logger-server-hadoop102.out</span><br><span class="line">-rw-r--r-- 1 root root   144357 11月 23 16:59 dolphinscheduler-master.2023-11-23_16.0.log</span><br><span class="line">-rw-r--r-- 1 root root   425153 11月 23 17:50 dolphinscheduler-master.2023-11-23_17.0.log</span><br><span class="line">-rw-r--r-- 1 root root    97497 11月 23 20:59 dolphinscheduler-master.2023-11-23_20.0.log</span><br><span class="line">-rw-r--r-- 1 root root 32347547 11月 23 21:59 dolphinscheduler-master.2023-11-23_21.0.log</span><br><span class="line">-rw-r--r-- 1 root root  8564475 11月 23 22:28 dolphinscheduler-master.log</span><br><span class="line">-rw-r--r-- 1 root root      823 11月 23 22:24 dolphinscheduler-master-server-hadoop102.out</span><br><span class="line">-rw-r--r-- 1 root root    73998 11月 23 16:59 dolphinscheduler-worker.2023-11-23_16.0.log</span><br><span class="line">-rw-r--r-- 1 root root    10999 11月 23 17:50 dolphinscheduler-worker.2023-11-23_17.0.log</span><br><span class="line">-rw-r--r-- 1 root root    73685 11月 23 20:59 dolphinscheduler-worker.2023-11-23_20.0.log</span><br><span class="line">-rw-r--r-- 1 root root   322045 11月 23 21:57 dolphinscheduler-worker.2023-11-23_21.0.log</span><br><span class="line">-rw-r--r-- 1 root root    91550 11月 23 22:25 dolphinscheduler-worker.log</span><br><span class="line">-rw-r--r-- 1 root root      823 11月 23 22:25 dolphinscheduler-worker-server-hadoop102.out</span><br><span class="line">-rw-r--r-- 1 root root     1581 11月 23 22:25 gc.log</span><br><span class="line">[root@hadoop102 logs]# cat dolphinscheduler-alert-server-hadoop102.out</span><br></pre></td></tr></table></figure>

<p>发现其中的错误为java.net.ConnectException Connection refusedconnect，也就是hadoop102节点的MySQL服务器连接失败。</p>
<p>（3）此时我们再去查看mysql服务器的状态是failed的，登录时发现：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# mysql -uroot -p&quot;wyhdhr19980418&quot;</span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">ERROR 2002 (HY000): Can&#x27;t connect to local MySQL server through socket &#x27;/var/lib/mysql/mysql.sock&#x27; (111)</span><br></pre></td></tr></table></figure>

<p>（4）再去查看mysql日志</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# tail -n 20 /var/log/mysqld.log </span><br><span class="line">2023-11-23T14:14:58.093935Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier</span><br><span class="line">2023-11-23T14:14:58.093937Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11</span><br><span class="line">2023-11-23T14:14:58.093939Z 0 [Note] InnoDB: Using Linux native AIO</span><br><span class="line">2023-11-23T14:14:58.094088Z 0 [Note] InnoDB: Number of pools: 1</span><br><span class="line">2023-11-23T14:14:58.094150Z 0 [Note] InnoDB: Using CPU crc32 instructions</span><br><span class="line">2023-11-23T14:14:58.094993Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk</span><br><span class="line">2023-11-23T14:14:58.099334Z 0 [Note] InnoDB: Completed initialization of buffer pool</span><br><span class="line">2023-11-23T14:14:58.100680Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread panged. See the man page of setpriority().</span><br><span class="line">2023-11-23T14:14:58.112744Z 0 [Note] InnoDB: Highest supported file format is Barracuda.</span><br><span class="line">2023-11-23T14:14:58.113537Z 0 [ERROR] InnoDB: Ignoring the redo log due to missing MLOG_CHECKPOINT between the8141 and the end 362697728.</span><br><span class="line">2023-11-23T14:14:58.113557Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error</span><br><span class="line">2023-11-23T14:14:58.716506Z 0 [ERROR] Plugin &#x27;InnoDB&#x27; init function returned error.</span><br><span class="line">2023-11-23T14:14:58.716589Z 0 [ERROR] Plugin &#x27;InnoDB&#x27; registration as a STORAGE ENGINE failed.</span><br><span class="line">2023-11-23T14:14:58.716611Z 0 [ERROR] Failed to initialize builtin plugins.</span><br><span class="line">2023-11-23T14:14:58.716613Z 0 [ERROR] Aborting</span><br><span class="line"></span><br><span class="line">2023-11-23T14:14:58.716659Z 0 [Note] Binlog end</span><br><span class="line">2023-11-23T14:14:58.716768Z 0 [Note] Shutting down plugin &#x27;CSV&#x27;</span><br><span class="line">2023-11-23T14:14:58.717111Z 0 [Note] /usr/sbin/mysqld: Shutdown complete</span><br></pre></td></tr></table></figure>

<p>可以看到：MySQL 容器报错 Ignoring the redo log due to missing MLOG_CHECKPOINT between the checkpoint xxxxxxxxx and the end xxxxxxxxx.\n</p>
<p>查网络得知，当有一些用户的机器突然断电关机或者不正常关机之后，DataEase 服务中的 MySQL 容器无法启动，之前确实因为在操作海豚调度器的时候电脑蓝屏重启了。报错原因，一般是服务器断电或者 MySQL 服务不正常结束导致的，和 MySQL redo log 有关。因为ib_logfile文件中记录些innodb引擎非常有用的信息比如说默认的innodb默认的配置信息，在未正常关闭server情况下，重启后的server不支持innodb引擎。</p>
<p>（5）解决办法：</p>
<p>进入如下目录，将ib_logfile0和ib_logfile1文件重命名（变为备份文件）或者删除</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /var/lib/mysql</span><br><span class="line">[root@hadoop102 mysql]# ll</span><br><span class="line">总用量 245616</span><br><span class="line">-rw-r-----. 1 mysql mysql       56 10月  7 15:42 auto.cnf</span><br><span class="line">-rw-------. 1 mysql mysql     1676 10月  7 15:42 ca-key.pem</span><br><span class="line">-rw-r--r--. 1 mysql mysql     1112 10月  7 15:42 ca.pem</span><br><span class="line">-rw-r--r--. 1 mysql mysql     1112 10月  7 15:42 client-cert.pem</span><br><span class="line">-rw-------. 1 mysql mysql     1676 10月  7 15:42 client-key.pem</span><br><span class="line">drwxr-x---  2 mysql mysql     4096 11月 23 16:54 dolphinscheduler</span><br><span class="line">drwxr-x---  2 mysql mysql     4096 11月 13 13:23 edu</span><br><span class="line">drwxr-x---  2 mysql mysql     4096 11月 23 13:28 edu_report</span><br><span class="line">-rw-r-----  1 mysql mysql     1204 11月 22 22:07 ib_buffer_pool</span><br><span class="line">-rw-r-----. 1 mysql mysql 79691776 11月 23 21:04 ibdata1</span><br><span class="line">-rw-r-----. 1 mysql mysql 50331648 11月 23 21:04 ib_logfile0</span><br><span class="line">-rw-r-----. 1 mysql mysql 50331648 11月 23 21:04 ib_logfile1</span><br><span class="line">-rw-r-----  1 mysql mysql 12582912 11月 23 17:48 ibtmp1</span><br><span class="line">drwxr-x---  2 mysql mysql     4096 10月 30 15:35 ke</span><br><span class="line">drwxr-x---  2 mysql mysql      302 11月 13 15:46 maxwell</span><br><span class="line">drwxr-x---. 2 mysql mysql     8192 10月  7 16:16 metastore</span><br><span class="line">drwxr-x---. 2 mysql mysql     4096 10月  7 15:42 mysql</span><br><span class="line">-rw-r-----  1 mysql mysql  2188508 11月 13 22:01 mysql-bin.000001</span><br><span class="line">-rw-r-----  1 mysql mysql 39952457 11月 14 21:46 mysql-bin.000002</span><br><span class="line">-rw-r-----  1 mysql mysql      177 11月 15 21:06 mysql-bin.000003</span><br><span class="line">-rw-r-----  1 mysql mysql      154 11月 16 19:12 mysql-bin.000004</span><br><span class="line">-rw-r-----  1 mysql mysql      177 11月 16 19:12 mysql-bin.000005</span><br><span class="line">-rw-r-----  1 mysql mysql 16285621 11月 16 22:15 mysql-bin.000006</span><br><span class="line">-rw-r-----  1 mysql mysql      177 11月 17 21:45 mysql-bin.000007</span><br><span class="line">-rw-r-----  1 mysql mysql      177 11月 21 22:24 mysql-bin.000008</span><br><span class="line">-rw-r-----  1 mysql mysql      177 11月 22 22:07 mysql-bin.000009</span><br><span class="line">-rw-r-----  1 mysql mysql      794 11月 23 16:51 mysql-bin.000010</span><br><span class="line">-rw-r-----  1 mysql mysql      190 11月 23 12:34 mysql-bin.index</span><br><span class="line">srwxrwxrwx  1 mysql mysql        0 11月 23 12:34 mysql.sock</span><br><span class="line">-rw-------  1 mysql mysql        5 11月 23 12:34 mysql.sock.lock</span><br><span class="line">drwxr-x---. 2 mysql mysql     8192 10月  7 15:42 performance_schema</span><br><span class="line">-rw-------. 1 mysql mysql     1676 10月  7 15:42 private_key.pem</span><br><span class="line">-rw-r--r--. 1 mysql mysql      452 10月  7 15:42 public_key.pem</span><br><span class="line">-rw-r--r--. 1 mysql mysql     1112 10月  7 15:42 server-cert.pem</span><br><span class="line">-rw-------. 1 mysql mysql     1680 10月  7 15:42 server-key.pem</span><br><span class="line">drwxr-x---. 2 mysql mysql     8192 10月  7 15:42 sys</span><br><span class="line">[root@hadoop102 mysql]# mv ib_logfile0 ib_logfile0.bak</span><br><span class="line">[root@hadoop102 mysql]# mv ib_logfile1 ib_logfile1.bak</span><br></pre></td></tr></table></figure>

<p>（6）启动mysql服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql]# systemctl start mysqld</span><br></pre></td></tr></table></figure>

<p>（7）此时mysql服务为active状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql]# systemctl status mysqld</span><br><span class="line">● mysqld.service - MySQL Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 四 2023-11-23 22:20:41 CST; 15min ago</span><br><span class="line">     Docs: man:mysqld(8)</span><br><span class="line">           http://dev.mysql.com/doc/refman/en/using-systemd.html</span><br><span class="line">  Process: 2837 ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid $MYSQLD_OPTS (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 2813 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 2839 (mysqld)</span><br><span class="line">    Tasks: 57</span><br><span class="line">   CGroup: /system.slice/mysqld.service</span><br><span class="line">           └─2839 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid</span><br><span class="line"></span><br><span class="line">11月 23 22:20:40 hadoop102 systemd[1]: Starting MySQL Server...</span><br><span class="line">11月 23 22:20:41 hadoop102 systemd[1]: Started MySQL Server.</span><br></pre></td></tr></table></figure>

<p>（8）接着启动并查看海豚调度器各项服务，都是RUNNING状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 dolphinscheduler]# ./bin/status-all.sh </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">====================== dolphinscheduler server config =============================</span><br><span class="line">1.dolphinscheduler server node config hosts:[  hadoop102,hadoop103,hadoop104  ]</span><br><span class="line">2.master server node config hosts:[  hadoop102  ]</span><br><span class="line">3.worker server node config hosts:[  hadoop102:default,hadoop103:default,hadoop104:default  ]</span><br><span class="line">4.alert server node config hosts:[  hadoop102  ]</span><br><span class="line">5.api server node config hosts:[  hadoop102  ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">====================== dolphinscheduler server status =============================</span><br><span class="line">node server state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hadoop102  Begin status master-server......</span><br><span class="line">master-server  [  RUNNING  ]</span><br><span class="line">End status master-server.</span><br><span class="line">hadoop102  Begin status worker-server......</span><br><span class="line">worker-server  [  RUNNING  ]</span><br><span class="line">End status worker-server.</span><br><span class="line">hadoop102  Begin status logger-server......</span><br><span class="line">logger-server  [  RUNNING  ]</span><br><span class="line">End status logger-server.</span><br><span class="line">hadoop103  Begin status worker-server......</span><br><span class="line">worker-server  [  RUNNING  ]</span><br><span class="line">End status worker-server.</span><br><span class="line">hadoop103  Begin status logger-server......</span><br><span class="line">logger-server  [  RUNNING  ]</span><br><span class="line">End status logger-server.</span><br><span class="line">hadoop104  Begin status worker-server......</span><br><span class="line">worker-server  [  RUNNING  ]</span><br><span class="line">End status worker-server.</span><br><span class="line">hadoop104  Begin status logger-server......</span><br><span class="line">logger-server  [  RUNNING  ]</span><br><span class="line">End status logger-server.</span><br><span class="line">hadoop102  Begin status alert-server......</span><br><span class="line">alert-server  [  RUNNING  ]</span><br><span class="line">End status alert-server.</span><br><span class="line">hadoop102  Begin status api-server......</span><br><span class="line">api-server  [  RUNNING  ]</span><br><span class="line">End status api-server.</span><br></pre></td></tr></table></figure>


      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">项目</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/11/07/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE%E2%80%94%E2%80%94%E5%9C%A8%E7%BA%BF%E6%95%99%E8%82%B2/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-腾讯云EMR离线数仓" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/01/%E8%85%BE%E8%AE%AF%E4%BA%91EMR%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/">腾讯云EMR离线数仓</a>
    </h1>
  

        
        <a href="/2023/11/01/%E8%85%BE%E8%AE%AF%E4%BA%91EMR%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/" class="archive-article-date">
  	<time datetime="2023-11-01T06:05:31.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-11-01</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>基于腾讯云EMR搭建离线数据仓库（需求及架构设计）</p>
</blockquote>
<h1 id="第一章-序言"><a href="#第一章-序言" class="headerlink" title="第一章 序言"></a>第一章 序言</h1><h2 id="1-1-目标"><a href="#1-1-目标" class="headerlink" title="1.1 目标"></a>1.1 目标</h2><p>在生产环境中，我们的数据来源于java后台，我们要从业务库中将这些数据采集到大数据集群中，这样才能做分析处理。</p>
<p>按照数仓建模理论搭建一个完整的数据仓库。包括：需求分析、架构设计、数据存储、建模、计算、输出、展示等所有流程。</p>
<h2 id="1-2-数据仓库概念"><a href="#1-2-数据仓库概念" class="headerlink" title="1.2 数据仓库概念"></a>1.2 数据仓库概念</h2><p><img src="Snipaste_2023-11-02_21-09-44.png" alt="Snipaste_2023-11-02_21-09-44"></p>
<p>ODS：原始数据层</p>
<p>DWD：明细数据层</p>
<p>DWS：汇总数据层</p>
<p>ADS：数据应用层</p>
<p>Oozie：定时调度</p>
<h1 id="第二章-项目需求及架构设计"><a href="#第二章-项目需求及架构设计" class="headerlink" title="第二章 项目需求及架构设计"></a>第二章 项目需求及架构设计</h1><h2 id="2-1-项目需求分析"><a href="#2-1-项目需求分析" class="headerlink" title="2.1 项目需求分析"></a>2.1 项目需求分析</h2><img src="Snipaste_2023-11-02_21-16-10.png" alt="Snipaste_2023-11-02_21-16-10" style="zoom:50%;">

<h2 id="2-2-项目框架"><a href="#2-2-项目框架" class="headerlink" title="2.2 项目框架"></a>2.2 项目框架</h2><h3 id="2-2-1-技术选型"><a href="#2-2-1-技术选型" class="headerlink" title="2.2.1 技术选型"></a>2.2.1 技术选型</h3><img src="Snipaste_2023-11-02_21-16-43.png" alt="Snipaste_2023-11-02_21-16-43" style="zoom:50%;">

<h3 id="2-2-2-系统数据流程设计"><a href="#2-2-2-系统数据流程设计" class="headerlink" title="2.2.2 系统数据流程设计"></a>2.2.2 系统数据流程设计</h3><img src="Snipaste_2023-11-02_21-17-15.png" alt="Snipaste_2023-11-02_21-17-15" style="zoom:50%;">

<h3 id="2-2-3-框架版本选择"><a href="#2-2-3-框架版本选择" class="headerlink" title="2.2.3 框架版本选择"></a>2.2.3 框架版本选择</h3><img src="Snipaste_2023-11-02_21-40-23.png" alt="Snipaste_2023-11-02_21-40-23" style="zoom:43%;">

<img src="Snipaste_2023-11-02_21-41-39.png" alt="Snipaste_2023-11-02_21-41-39" style="zoom:43%;">

<h3 id="2-2-4-服务器选型"><a href="#2-2-4-服务器选型" class="headerlink" title="2.2.4 服务器选型"></a>2.2.4 服务器选型</h3><img src="Snipaste_2023-11-02_21-42-52.png" alt="Snipaste_2023-11-02_21-42-52" style="zoom:43%;">

<h3 id="2-2-5-集群规模"><a href="#2-2-5-集群规模" class="headerlink" title="2.2.5 集群规模"></a>2.2.5 集群规模</h3><img src="Snipaste_2023-11-02_21-45-48.png" alt="Snipaste_2023-11-02_21-45-48" style="zoom:50%;">

<h3 id="2-2-6-集群资源规划设计"><a href="#2-2-6-集群资源规划设计" class="headerlink" title="2.2.6 集群资源规划设计"></a>2.2.6 集群资源规划设计</h3><p>在企业中通常会搭建一套生产集群和一套测试集群。生产集群运行生产任务，测试集群用于上线前代码编写和测试。</p>
<img src="Snipaste_2023-11-02_21-51-20.png" alt="Snipaste_2023-11-02_21-51-20" style="zoom:50%;">

<h1 id="第三章-电商业务简介"><a href="#第三章-电商业务简介" class="headerlink" title="第三章 电商业务简介"></a>第三章 电商业务简介</h1><h2 id="3-1-电商业务流程"><a href="#3-1-电商业务流程" class="headerlink" title="3.1 电商业务流程"></a>3.1 电商业务流程</h2><p>电商的业务流程可以以一个普通用户的浏览足迹为例进行说明，用户点开电商首页开始浏览，可能会通过分类查询也可能通过全文搜索寻找自己中意的商品，这些商品无疑都是存储在后台的管理系统中的。</p>
<p>当用户寻找到自己中意的商品，可能会想要购买，将商品添加到购物车后发现需要登录，登录后对商品进行结算，这时候购物车的管理和商品订单信息的生成都会对业务数据库产生影响，会生成相应的订单数据和支付数据。</p>
<p>订单正式生成之后，还会对订单进行跟踪处理，直到订单全部完成。</p>
<p>电商的主要业务流程包括用户前台浏览商品时的商品详情的管理，用户商品加入购物车进行支付时用户个人中心&amp;支付服务的管理，用户支付完成后订单后台服务的管理，这些流程涉及到了十几个甚至几十个业务数据表，甚至更多。</p>
<h2 id="3-2-电商常识"><a href="#3-2-电商常识" class="headerlink" title="3.2 电商常识"></a>3.2 电商常识</h2><h3 id="3-2-1-SKU和SPU"><a href="#3-2-1-SKU和SPU" class="headerlink" title="3.2.1 SKU和SPU"></a>3.2.1 SKU和SPU</h3><p>SKU&#x3D;Stock Keeping Unit（库存量基本单位）。现在已经被引申为<strong>产品统一编号</strong>的简称，每种产品均对应有唯一的SKU号。</p>
<p> SPU（Standard Product Unit）：是<strong>商品信息聚合的最小单位</strong>，是一组可复用、易检索的标准化信息集合。</p>
<p>例如：iPhoneX手机就是SPU。一台银色、128G内存的、支持联通网络的iPhoneX，就是SKU。</p>
<p><img src="%E5%9B%BE%E7%89%871.png" alt="图片1"></p>
<p>SPU表示一类商品。同一SPU的商品可以共用商品图片、海报、销售属性等。</p>
<h1 id="第四章-选购腾讯云EMR"><a href="#第四章-选购腾讯云EMR" class="headerlink" title="第四章 选购腾讯云EMR"></a>第四章 选购腾讯云EMR</h1><h2 id="4-1-注册腾讯云账户"><a href="#4-1-注册腾讯云账户" class="headerlink" title="4.1 注册腾讯云账户"></a>4.1 注册腾讯云账户</h2><h2 id="4-2-购买弹性MapReduce"><a href="#4-2-购买弹性MapReduce" class="headerlink" title="4.2 购买弹性MapReduce"></a>4.2 购买弹性MapReduce</h2><p><img src="Snipaste_2023-11-05_21-23-35.png" alt="Snipaste_2023-11-05_21-23-35"></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">项目</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/11/01/%E8%85%BE%E8%AE%AF%E4%BA%91EMR%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-kafka框架学习笔记" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/20/kafka%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">kafka框架学习笔记</a>
    </h1>
  

        
        <a href="/2023/10/20/kafka%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="archive-article-date">
  	<time datetime="2023-10-20T13:37:36.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-10-20</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第一章-Kafka概述"><a href="#第一章-Kafka概述" class="headerlink" title="第一章 Kafka概述"></a>第一章 Kafka概述</h1><h2 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h2><p><img src="Snipaste_2023-10-23_12-21-51.png" alt="Snipaste_2023-10-23_12-21-51"></p>
<h2 id="1-2-消息队列"><a href="#1-2-消息队列" class="headerlink" title="1.2 消息队列"></a>1.2 消息队列</h2><p>目 前企 业中比 较常 见的 消息 队列产 品主 要有 Kafka、ActiveMQ 、RabbitMQ 、RocketMQ 等。</p>
<p>在<strong>大数据</strong>场景主要采用 <strong>Kafka</strong> 作为消息队列。在 JavaEE 开发中主要采用 ActiveMQ、RabbitMQ、RocketMQ。</p>
<h3 id="1-2-1-传统消息队列的应用场景"><a href="#1-2-1-传统消息队列的应用场景" class="headerlink" title="1.2.1 传统消息队列的应用场景"></a>1.2.1 传统消息队列的应用场景</h3><p>传统的消息队列的主要应用场景包括：<strong>缓存&#x2F;消峰</strong>、<strong>解耦</strong>和<strong>异步通信</strong></p>
<img src="Snipaste_2023-10-23_12-28-55.png" alt="Snipaste_2023-10-23_12-28-55" style="zoom:43%;">

<img src="Snipaste_2023-10-23_12-29-14.png" alt="Snipaste_2023-10-23_12-29-14" style="zoom:43%;">

<img src="Snipaste_2023-10-23_12-31-49.png" alt="Snipaste_2023-10-23_12-31-49" style="zoom:43%;">

<h3 id="1-2-2-消息队列的两种模式"><a href="#1-2-2-消息队列的两种模式" class="headerlink" title="1.2.2 消息队列的两种模式"></a>1.2.2 消息队列的两种模式</h3><p>（1）点对点模式</p>
<img src="Snipaste_2023-10-23_12-39-38.png" alt="Snipaste_2023-10-23_12-39-38" style="zoom:50%;">

<p>（2）发布&#x2F;订阅模式</p>
<img src="Snipaste_2023-10-23_12-40-01.png" alt="Snipaste_2023-10-23_12-40-01" style="zoom:50%;">

<h2 id="1-3-Kafka基础架构"><a href="#1-3-Kafka基础架构" class="headerlink" title="1.3 Kafka基础架构"></a>1.3 Kafka基础架构</h2><p><img src="Snipaste_2023-10-23_13-10-47.png" alt="Snipaste_2023-10-23_13-10-47"></p>
<p>（1）Producer：消息生产者，就是向 Kafka broker 发消息的客户端。</p>
<p>（2）Consumer：消息消费者，向 Kafka broker 取消息的客户端。</p>
<p>（3）Consumer Group（CG）：消费者组，由多个 consumer 组成。<strong>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。</strong>所有的消费者都属于某个消费者组，即<strong>消费者组是逻辑上的一个订阅者</strong>。</p>
<p>也就是说：一个分区如果被两个消费者消费，那么这两个消费者必定不在同一个组；</p>
<p>（4）Broker：一台Kafka服务器就是一个broker。一个集群由多个 broker （kafka）组成。一个broker （kafka）可以容纳多个 topic。</p>
<p>（5）Topic：可以理解为一个队列，<strong>生产者和消费者面向的都是一个</strong> <strong>topic</strong>。</p>
<p>（6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，<strong>一个</strong> <strong>topic</strong> <strong>可以分为多个</strong> <strong>partition</strong>，每个 partition 是一个有序的队列</p>
<p>（7）Replica：副本。一个 topic 的每个分区都有若干个副本，一个 <strong>Leader</strong> 和若干个<strong>Follower</strong></p>
<p>（8）Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 Leader。</p>
<p>（9）Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和Leader 数据的同步。Leader 发生故障时，某个 Follower 会成为新的 Leader。</p>
<h1 id="第二章-Kafka快速入门"><a href="#第二章-Kafka快速入门" class="headerlink" title="第二章 Kafka快速入门"></a>第二章 Kafka快速入门</h1><h2 id="2-1-安装部署"><a href="#2-1-安装部署" class="headerlink" title="2.1 安装部署"></a>2.1 安装部署</h2><h3 id="2-1-1-集群规划"><a href="#2-1-1-集群规划" class="headerlink" title="2.1.1 集群规划"></a>2.1.1 集群规划</h3><p><img src="Snipaste_2023-10-23_13-33-36.png" alt="Snipaste_2023-10-23_13-33-36"></p>
<h3 id="2-1-2-集群部署"><a href="#2-1-2-集群部署" class="headerlink" title="2.1.2 集群部署"></a>2.1.2 集群部署</h3><p>（0）将kafka的安装包拖拽到&#x2F;opt&#x2F;software&#x2F;</p>
<p>（1）解压安装包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf kafka_2.12-3.0.0.tgz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>（2）修改解压后的文件名称</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# mv kafka_2.12-3.0.0/ kafka</span><br><span class="line">[root@hadoop102 module]# ll</span><br><span class="line">总用量 8</span><br><span class="line">drwxr-xr-x.  2 root root   99 10月 10 13:38 data</span><br><span class="line">drwxr-xr-x.  4 root root   76 10月 21 20:31 datas</span><br><span class="line">drwxr-xr-x. 12 root root  272 10月 21 17:08 flume</span><br><span class="line">-rw-r--r--.  1 root root    9 10月 22 14:08 group.log</span><br><span class="line">drwxr-xr-x. 13 wyh  wyh  4096 9月  26 12:55 hadoop-3.1.3</span><br><span class="line">drwxr-xr-x. 12 root root  243 10月  8 13:38 hive</span><br><span class="line">drwxr-xr-x.  7   10  143  245 4月   2 2019 jdk1.8.0_212</span><br><span class="line">drwxr-xr-x   7 root root  105 9月   9 2021 kafka       # 在这里</span><br><span class="line">drwxr-xr-x.  8 root root  160 10月 19 12:57 zookeeper-3.5.7</span><br></pre></td></tr></table></figure>

<p>（3）进入到&#x2F;opt&#x2F;module&#x2F;kafka 目录，修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# cd config/</span><br><span class="line">[root@hadoop102 config]# vim server.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">broker 的全局唯一编号，不能重复，只能是数字。</span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">处理网络请求的线程数量</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">用来处理磁盘 IO 的线程数量</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">发送套接字的缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">接收套接字的缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">请求套接字的缓冲区大小</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka 运行日志(数据)存放的路径，路径不需要提前创建，kafka 自动帮你创建，可以配置多个磁盘路径，路径与路径之间可以用<span class="string">&quot;，&quot;</span>分隔</span></span><br><span class="line">log.dirs=/opt/module/kafka/datas</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">topic 在当前 broker 上的分区个数</span></span><br><span class="line">num.partitions=1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">用来恢复和清理 data 下数据的线程数量</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">每个 topic 创建时的副本数，默认时 1 个副本</span></span><br><span class="line">offsets.topic.replication.factor=1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">segment 文件保留的最长时间，超时将被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">每个 segment 文件的大小，默认最大 1G</span></span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">检查过期数据的时间，默认 5 分钟检查一次是否数据过期</span></span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">配置连接 Zookeeper 集群地址（在 zk 根目录下创建/kafka，方便管理）</span></span><br><span class="line">zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</span><br></pre></td></tr></table></figure>

<p>（4）分发安装包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# xsync kafka/</span><br></pre></td></tr></table></figure>

<p>（5）分别在hadoop103和hadoop104上修改配置文件&#x2F;opt&#x2F;module&#x2F;kafka&#x2F;config&#x2F;server.properties中的 broker.id&#x3D;1、broker.id&#x3D;2</p>
<p><strong>注：broker.id 不得重复，整个集群中唯一。</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 module]#  vim kafka/config/server.properties</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">The <span class="built_in">id</span> of the broker. This must be <span class="built_in">set</span> to a unique <span class="built_in">integer</span> <span class="keyword">for</span></span> </span><br><span class="line">each broker.</span><br><span class="line">broker.id=1</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 module]# vim kafka/config/server.properties</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">The <span class="built_in">id</span> of the broker. This must be <span class="built_in">set</span> to a unique <span class="built_in">integer</span> <span class="keyword">for</span></span> </span><br><span class="line">each broker.</span><br><span class="line">broker.id=2</span><br></pre></td></tr></table></figure>

<p>（6）配置环境变量</p>
<p>在&#x2F;etc&#x2F;profile.d&#x2F;my_env.sh 文件中增加 kafka 环境变量配置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>刷新新一下环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# source /etc/profile</span><br></pre></td></tr></table></figure>

<p>分发环境变量文件到其他节点，并 source。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 profile.d]# xsync my_env.sh</span><br><span class="line"></span><br><span class="line">[root@hadoop103 ~]# source /etc/profile</span><br><span class="line">[root@hadoop104 module]# source /etc/profile</span><br></pre></td></tr></table></figure>

<p>（7）启动集群</p>
<p>必须先启动Zookeeper集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 profile.d]# zk.sh start</span><br><span class="line"></span><br><span class="line">[root@hadoop102 profile.d]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">4788 QuorumPeerMain</span><br><span class="line">4863 Jps</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">4449 Jps</span><br><span class="line">4377 QuorumPeerMain</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">4336 QuorumPeerMain</span><br><span class="line">4401 Jps</span><br></pre></td></tr></table></figure>

<p>然后依次在102，103，104节点上启动kafka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line">[root@hadoop103 kafka]# bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line">[root@hadoop104 kafka]# bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动成功</span></span><br><span class="line">[root@hadoop102 kafka]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">5392 Jps</span><br><span class="line">4788 QuorumPeerMain</span><br><span class="line">5252 Kafka</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">4949 Jps</span><br><span class="line">4840 Kafka</span><br><span class="line">4377 QuorumPeerMain</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">4336 QuorumPeerMain</span><br><span class="line">4896 Jps</span><br><span class="line">4807 Kafka</span><br></pre></td></tr></table></figure>

<p>（8）关闭集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-server-stop.sh </span><br><span class="line">[root@hadoop103 kafka]# bin/kafka-server-stop.sh </span><br><span class="line">[root@hadoop104 kafka]# bin/kafka-server-stop.sh</span><br><span class="line"></span><br><span class="line">[root@hadoop102 kafka]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">5553 Jps</span><br><span class="line">4788 QuorumPeerMain</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">5121 Jps</span><br><span class="line">4377 QuorumPeerMain</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">4336 QuorumPeerMain</span><br><span class="line">5093 Jps</span><br></pre></td></tr></table></figure>

<h3 id="2-1-3-集群启停脚本"><a href="#2-1-3-集群启停脚本" class="headerlink" title="2.1.3 集群启停脚本"></a>2.1.3 集群启停脚本</h3><p>（1）在&#x2F;bin 目录下创建文件 kf.sh 脚本文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim kf.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">     for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">     do</span><br><span class="line">         echo &quot; --------启动 $i Kafka-------&quot;</span><br><span class="line">         ssh $i &quot;/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties&quot;</span><br><span class="line">     done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">     for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">     do</span><br><span class="line">         echo &quot; --------停止 $i Kafka-------&quot;</span><br><span class="line">         ssh $i &quot;/opt/module/kafka/bin/kafka-server-stop.sh &quot;</span><br><span class="line">     done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）添加执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 kf.sh</span><br></pre></td></tr></table></figure>

<p>（3）启动集群命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# kf.sh start</span><br><span class="line"> --------启动 hadoop102 Kafka-------</span><br><span class="line"> --------启动 hadoop103 Kafka-------</span><br><span class="line"> --------启动 hadoop104 Kafka-------</span><br><span class="line">[root@hadoop102 bin]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">4788 QuorumPeerMain</span><br><span class="line">6004 Kafka</span><br><span class="line">6100 Jps</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">5554 Kafka</span><br><span class="line">4377 QuorumPeerMain</span><br><span class="line">5647 Jps</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">4336 QuorumPeerMain</span><br><span class="line">5623 Jps</span><br><span class="line">5535 Kafka</span><br></pre></td></tr></table></figure>

<p><strong>注意zookeeper和kafa集群的启停顺序：</strong></p>
<p>停止 Kafka 集群时，一定要等 Kafka 所有节点进程全部停止后再停止 Zookeeper集群。因为 Zookeeper 集群当中记录着 Kafka 集群相关信息，Zookeeper 集群一旦先停止，Kafka 集群就没有办法再获取停止进程的信息，只能手动杀死 Kafka 进程了。</p>
<p>①zookeeper启动     zk.sh start</p>
<p>②kafka启动            kf.sh start</p>
<p>③kafka停止            kf.sh stop</p>
<p>④zookeeper停止    zk.sh stop</p>
<h2 id="2-2-Kafka命令行操作"><a href="#2-2-Kafka命令行操作" class="headerlink" title="2.2 Kafka命令行操作"></a>2.2 Kafka命令行操作</h2><img src="Snipaste_2023-10-23_14-24-00.png" alt="Snipaste_2023-10-23_14-24-00" style="zoom:50%;">

<h3 id="2-2-1-主题命令行操作"><a href="#2-2-1-主题命令行操作" class="headerlink" title="2.2.1 主题命令行操作"></a>2.2.1 主题命令行操作</h3><p>（1）查看操作主题命令参数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-10-23_14-52-01.png" alt="Snipaste_2023-10-23_14-52-01" style="zoom:43%;">

<p>（2）查看当前服务器中的所有 topic（当前为空）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>（3）创建first topic</p>
<p>topic名称为first，分区数为1，分区副本为3</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --topic first --partitions 1 --replication-factor 3</span><br><span class="line">Created topic first.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看副本</span></span><br><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list</span><br><span class="line">first</span><br></pre></td></tr></table></figure>

<p>（4）查看first主题的详情</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first</span><br><span class="line">Topic: first	TopicId: VV7ddVy4SKyTQauf1IHvag	PartitionCount: 1	ReplicationFactor: 3	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: first	Partition: 0	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0</span><br></pre></td></tr></table></figure>

<p>（5）修改分区数（注意：分区数只能增加，不能减少）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --topic first --alter --partitions 3</span><br></pre></td></tr></table></figure>

<p>（6）再次查看详情</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first</span><br><span class="line">Topic: first	TopicId: VV7ddVy4SKyTQauf1IHvag	PartitionCount: 3	ReplicationFactor: 3	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: first	Partition: 0	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0</span><br><span class="line">	Topic: first	Partition: 1	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2</span><br><span class="line">	Topic: first	Partition: 2	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br></pre></td></tr></table></figure>

<h3 id="2-2-2-生产者命令行操作"><a href="#2-2-2-生产者命令行操作" class="headerlink" title="2.2.2 生产者命令行操作"></a>2.2.2 生产者命令行操作</h3><p>（1）查看操作生产者命令参数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-producer.sh</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-10-23_15-12-51.png" alt="Snipaste_2023-10-23_15-12-51" style="zoom:43%;">

<p>（2）发送消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">atguigu</span></span><br></pre></td></tr></table></figure>

<h3 id="2-2-3-消费者命令行操作"><a href="#2-2-3-消费者命令行操作" class="headerlink" title="2.2.3 消费者命令行操作"></a>2.2.3 消费者命令行操作</h3><p>（1）查看操作消费者命令参数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-consumer.sh</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-10-23_15-18-16.png" alt="Snipaste_2023-10-23_15-18-16" style="zoom:43%;">

<p>（2）消费数据</p>
<p>消费 first 主题中的数据（只是增量数据，历史数据不显示）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first </span><br></pre></td></tr></table></figure>

<p>把主题中所有的数据都读取出来（包括历史数据）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first --from-beginning</span><br><span class="line">hello</span><br><span class="line">atguigu</span><br></pre></td></tr></table></figure>

<p><strong>此时在生产者端输入一条数据，消费者端就能在对应的topic下收到一条数据。</strong></p>
<h1 id="第三章-Kafka的生产者"><a href="#第三章-Kafka的生产者" class="headerlink" title="第三章 Kafka的生产者"></a>第三章 Kafka的生产者</h1><h2 id="3-1-生产者消息发送流程"><a href="#3-1-生产者消息发送流程" class="headerlink" title="3.1 生产者消息发送流程"></a>3.1 生产者消息发送流程</h2><h3 id="3-1-1-发送原理"><a href="#3-1-1-发送原理" class="headerlink" title="3.1.1 发送原理"></a>3.1.1 发送原理</h3><p>在消息发送的过程中，涉及到了<strong>两个线程——****main</strong> <strong>线程和</strong> <strong>Sender</strong> <strong>线程</strong>。在 main 线程中创建了<strong>一个双端队列</strong> <strong>RecordAccumulator</strong>。main 线程将消息发送给 RecordAccumulator，Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker。</p>
<p><img src="Snipaste_2023-10-23_15-38-44.png" alt="Snipaste_2023-10-23_15-38-44"></p>
<h3 id="3-1-2-生产者重要参数列表"><a href="#3-1-2-生产者重要参数列表" class="headerlink" title="3.1.2 生产者重要参数列表"></a>3.1.2 生产者重要参数列表</h3><table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>生产者连接集群所需的 broker 地址清单 。 例 如hadoop102:9092,hadoop103:9092,hadoop104:9092，可以设置 1 个或者多个，中间用逗号隔开。注意这里并非需要所有的 broker 地址，因为生产者从给定的 broker里查找到其他 broker 信息</td>
</tr>
<tr>
<td>key.serializer 和 value.serializer</td>
<td>指定发送消息的 key 和 value 的序列化类型。一定要写全类名</td>
</tr>
<tr>
<td>buffer.memory</td>
<td>RecordAccumulator 缓冲区总大小，默认 32m</td>
</tr>
<tr>
<td>batch.size</td>
<td>缓冲区一批数据最大值，默认 16k。适当增加该值，可以提高吞吐量，但是如果该值设置太大，会导致数据传输延迟增加</td>
</tr>
<tr>
<td>linger.ms</td>
<td>如果数据迟迟未达到 batch.size，sender 等待 linger.time之后就会发送数据。单位 ms，默认值是 0ms，表示没有延迟。生产环境建议该值大小为 5-100ms 之间</td>
</tr>
<tr>
<td>acks</td>
<td>0：生产者发送过来的数据，不需要等数据落盘应答<br>1：生产者发送过来的数据，Leader 收到数据后应答<br>-1（all）：生产者发送过来的数据，Leader+和 isr 队列里面的所有节点收齐数据后应答。默认值是-1，-1 和all 是等价的。</td>
</tr>
<tr>
<td>max.in.flight.requests.per.connection</td>
<td>允许最多没有返回 ack 的次数，默认为 5，开启幂等性要保证该值是 1-5 的数字</td>
</tr>
<tr>
<td>retries</td>
<td>当消息发送出现错误的时候，系统会重发消息。retries表示重试次数。默认是 int 最大值，2147483647。如果设置了重试，还想保证消息的有序性，需要设置MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION&#x3D;1否则在重试此失败消息的时候，其他的消息可能发送成功了</td>
</tr>
<tr>
<td>retry.backoff.ms</td>
<td>两次重试之间的时间间隔，默认是 100ms</td>
</tr>
<tr>
<td>enable.idempotence</td>
<td>是否开启幂等性，默认 true，开启幂等性</td>
</tr>
<tr>
<td>compression.type</td>
<td>生产者发送的所有数据的压缩方式。默认是 none，也就是不压缩。支持压缩类型：none、gzip、snappy、lz4 和 zstd。</td>
</tr>
</tbody></table>
<h2 id="3-2-异步发送API"><a href="#3-2-异步发送API" class="headerlink" title="3.2 异步发送API"></a>3.2 异步发送API</h2><h3 id="3-2-1-普通异步发送"><a href="#3-2-1-普通异步发送" class="headerlink" title="3.2.1 普通异步发送"></a>3.2.1 普通异步发送</h3><p>（1）需求：创建 Kafka 生产者，采用异步的方式发送到 Kafka Broker，也就是从kafka Producer到RecordAccumulator实现异步发送。</p>
<p>（2）代码编写</p>
<p>①创建maven工程kafka</p>
<p>②导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>③创建包名：com.atguigu.kafka.producer</p>
<p>④编写不带回调函数的 API 代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定key-value序列化类型</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用 send 方法,发送消息，不带回调信息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;atguigu &quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        往first主题中发送：</span></span><br><span class="line"><span class="comment">        atguigu0</span></span><br><span class="line"><span class="comment">        atguigu1</span></span><br><span class="line"><span class="comment">        atguigu2</span></span><br><span class="line"><span class="comment">        atguigu3</span></span><br><span class="line"><span class="comment">        atguigu4</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<p>在102上开启kafka消费者，运行java代码，观察hadoop102上的kafka消费者能否接收到消息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first </span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 4</span><br></pre></td></tr></table></figure>

<h3 id="3-2-2-带回调函数的异步发送"><a href="#3-2-2-带回调函数的异步发送" class="headerlink" title="3.2.2 带回调函数的异步发送"></a>3.2.2 带回调函数的异步发送</h3><p>回调函数会在 producer 收到 ack 时调用，为异步调用，该方法有两个参数，分别是元数据信息（RecordMetadata）和异常信息（Exception），如果 Exception 为 null，说明消息发送成功，如果 Exception 不为 null，说明消息发送失败。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallback</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定key-value序列化类型</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用 send 方法,发送消息，带回调信息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;atguigu &quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="comment">// 该方法在 Producer 收到 ack 时调用，为异步调用</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="literal">null</span>) &#123;</span><br><span class="line">                        <span class="comment">// 没有异常,输出信息到控制台</span></span><br><span class="line">                        System.out.println(<span class="string">&quot; 主题： &quot;</span> +</span><br><span class="line">                                metadata.topic() + <span class="string">&quot;-&gt;&quot;</span> + <span class="string">&quot;分区：&quot;</span> + metadata.partition());</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 出现异常打印</span></span><br><span class="line">                        exception.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="comment">// 延迟一会会看到数据发往不同分区</span></span><br><span class="line">            Thread.sleep(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        往first主题中发送：</span></span><br><span class="line"><span class="comment">        atguigu0</span></span><br><span class="line"><span class="comment">        atguigu1</span></span><br><span class="line"><span class="comment">        atguigu2</span></span><br><span class="line"><span class="comment">        atguigu3</span></span><br><span class="line"><span class="comment">        atguigu4</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<p>①在hadoop102上开启kafka消费者</p>
<p>②在IDEA中执行代码，观察102控制台是否收到消息，因为异步发送且设定了延迟，所以数据不一定按顺序到达消费者端</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first </span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 4</span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 2</span><br></pre></td></tr></table></figure>

<p>③在IDEA控制台观察回调信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主题： first-&gt;分区：0</span><br><span class="line">主题： first-&gt;分区：0</span><br><span class="line">主题： first-&gt;分区：2</span><br><span class="line">主题： first-&gt;分区：2</span><br><span class="line">主题： first-&gt;分区：2</span><br></pre></td></tr></table></figure>

<h2 id="3-3-同步发送API"><a href="#3-3-同步发送API" class="headerlink" title="3.3 同步发送API"></a>3.3 同步发送API</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerSync</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定key-value序列化类型</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用 send.get()方法,发送消息，采用的就是同步发送</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;atguigu &quot;</span> + i)).get();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        往first主题中发送：</span></span><br><span class="line"><span class="comment">        atguigu0</span></span><br><span class="line"><span class="comment">        atguigu1</span></span><br><span class="line"><span class="comment">        atguigu2</span></span><br><span class="line"><span class="comment">        atguigu3</span></span><br><span class="line"><span class="comment">        atguigu4</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<p>在102上开启kafka消费者，运行java代码，观察hadoop102上的kafka消费者能否接收到消息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first </span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 4</span><br></pre></td></tr></table></figure>

<h2 id="3-4-生产者分区"><a href="#3-4-生产者分区" class="headerlink" title="3.4 生产者分区"></a>3.4 生产者分区</h2><h3 id="3-4-1-分区好处"><a href="#3-4-1-分区好处" class="headerlink" title="3.4.1 分区好处"></a>3.4.1 分区好处</h3><p>（1）<strong>便于合理使用存储资源</strong>，每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果。</p>
<p>（2）<strong>提高并行度</strong>，生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据。</p>
<img src="Snipaste_2023-10-23_19-58-33.png" alt="Snipaste_2023-10-23_19-58-33" style="zoom:43%;">

<h3 id="3-4-2-生产者发送消息的分区策略"><a href="#3-4-2-生产者发送消息的分区策略" class="headerlink" title="3.4.2 生产者发送消息的分区策略"></a>3.4.2 生产者发送消息的分区策略</h3><p>（1）默认的分区器DefaultPartitioner</p>
<img src="Snipaste_2023-10-23_20-07-52.png" alt="Snipaste_2023-10-23_20-07-52" style="zoom:50%;">

<p><img src="Snipaste_2023-10-23_20-11-26.png" alt="Snipaste_2023-10-23_20-11-26"></p>
<p>（2）案例1：</p>
<p>将数据发往指定 partition 的情况下，例如，将所有数据发往分区 1 中。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallbackPartitions</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定key-value序列化类型</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用 send 方法,发送消息，带回调信息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="comment">// 指定数据发送到 1 号分区，key 为空</span></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="number">1</span>, <span class="string">&quot;&quot;</span>, <span class="string">&quot;atguigu &quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="comment">// 该方法在 Producer 收到 ack 时调用，为异步调用</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="literal">null</span>) &#123;</span><br><span class="line">                        <span class="comment">// 没有异常,输出信息到控制台</span></span><br><span class="line">                        System.out.println(<span class="string">&quot; 主题： &quot;</span> +</span><br><span class="line">                                metadata.topic() + <span class="string">&quot;-&gt;&quot;</span> + <span class="string">&quot;分区：&quot;</span> + metadata.partition());</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 出现异常打印</span></span><br><span class="line">                        exception.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="comment">// 延迟一会会看到数据发往不同分区</span></span><br><span class="line"><span class="comment">//            Thread.sleep(2);</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        往first主题中发送：</span></span><br><span class="line"><span class="comment">        atguigu0</span></span><br><span class="line"><span class="comment">        atguigu1</span></span><br><span class="line"><span class="comment">        atguigu2</span></span><br><span class="line"><span class="comment">        atguigu3</span></span><br><span class="line"><span class="comment">        atguigu4</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<p>在hadoop102上开启kafka消费者并观察其控制台：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first </span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 4</span><br></pre></td></tr></table></figure>

<p>在IDEA控制台观察回调信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主题： first-&gt;分区：1</span><br><span class="line">主题： first-&gt;分区：1</span><br><span class="line">主题： first-&gt;分区：1</span><br><span class="line">主题： first-&gt;分区：1</span><br><span class="line">主题： first-&gt;分区：1</span><br></pre></td></tr></table></figure>

<p>（3）案例2：</p>
<p>没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallback</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定key-value序列化类型</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用 send 方法,发送消息，带回调信息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="comment">// 依次指定 key 值为 a,b,f ，数据 key 的 hash 值与 3 个分区求余，分别发往 1、2、0</span></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;a&quot;</span>,<span class="string">&quot;atguigu &quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="comment">// 该方法在 Producer 收到 ack 时调用，为异步调用</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="literal">null</span>) &#123;</span><br><span class="line">                        <span class="comment">// 没有异常,输出信息到控制台</span></span><br><span class="line">                        System.out.println(<span class="string">&quot; 主题： &quot;</span> +</span><br><span class="line">                                metadata.topic() + <span class="string">&quot;-&gt;&quot;</span> + <span class="string">&quot;分区：&quot;</span> + metadata.partition());</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 出现异常打印</span></span><br><span class="line">                        exception.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="comment">// 延迟一会会看到数据发往不同分区</span></span><br><span class="line">            Thread.sleep(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        往first主题中发送：</span></span><br><span class="line"><span class="comment">        atguigu0</span></span><br><span class="line"><span class="comment">        atguigu1</span></span><br><span class="line"><span class="comment">        atguigu2</span></span><br><span class="line"><span class="comment">        atguigu3</span></span><br><span class="line"><span class="comment">        atguigu4</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<p>①key&#x3D;”a”时，在控制台查看结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主题：first-&gt;分区：1</span><br><span class="line">主题：first-&gt;分区：1</span><br><span class="line">主题：first-&gt;分区：1</span><br><span class="line">主题：first-&gt;分区：1</span><br><span class="line">主题：first-&gt;分区：1</span><br></pre></td></tr></table></figure>

<p>②key&#x3D;”b”时，在控制台查看结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主题：first-&gt;分区：2</span><br><span class="line">主题：first-&gt;分区：2</span><br><span class="line">主题：first-&gt;分区：2</span><br><span class="line">主题：first-&gt;分区：2</span><br><span class="line">主题：first-&gt;分区：2</span><br></pre></td></tr></table></figure>

<p>③key&#x3D;”f”时，在控制台查看结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主题：first-&gt;分区：0</span><br><span class="line">主题：first-&gt;分区：0</span><br><span class="line">主题：first-&gt;分区：0</span><br><span class="line">主题：first-&gt;分区：0</span><br><span class="line">主题：first-&gt;分区：0</span><br></pre></td></tr></table></figure>

<h3 id="3-4-3-自定义分区器"><a href="#3-4-3-自定义分区器" class="headerlink" title="3.4.3 自定义分区器"></a>3.4.3 自定义分区器</h3><p>如果研发人员可以根据企业需求，自己重新实现分区器。</p>
<p>（1）需求</p>
<p>例如我们实现一个分区器实现，发送过来的数据中如果包含 atguigu，就发往 0 号分区，不包含 atguigu，就发往 1 号分区。</p>
<p>（2）实现步骤</p>
<p>编写MyPartitioner类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyPartitioner</span> <span class="keyword">implements</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 返回信息对应的分区</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic 主题</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key 消息的 key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyBytes 消息的 key 序列化后的字节数组</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value 消息的 value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueBytes 消息的 value 序列化后的字节数组</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster 集群元数据可以查看分区信息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(String topic, Object key, <span class="type">byte</span>[] keyBytes, Object value, <span class="type">byte</span>[] valueBytes, Cluster cluster)</span> &#123;</span><br><span class="line">        <span class="comment">// 获取消息</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">msgValue</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 partition</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断消息是否包含 atguigu</span></span><br><span class="line">        <span class="keyword">if</span> (msgValue.contains(<span class="string">&quot;atguigu&quot;</span>))&#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回分区号</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用分区器的方法，在生产者的配置中添加分区器参数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallbackPartitions</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定key-value序列化类型</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//添加自定义分区器</span></span><br><span class="line">        properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG,<span class="string">&quot;com.atguigu.kafka.producer.MyPartitioner&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用 send 方法,发送消息，带回调信息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;atguigu &quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="comment">// 该方法在 Producer 收到 ack 时调用，为异步调用</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="literal">null</span>) &#123;</span><br><span class="line">                        <span class="comment">// 没有异常,输出信息到控制台</span></span><br><span class="line">                        System.out.println(<span class="string">&quot; 主题： &quot;</span> +</span><br><span class="line">                                metadata.topic() + <span class="string">&quot;-&gt;&quot;</span> + <span class="string">&quot;分区：&quot;</span> + metadata.partition());</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 出现异常打印</span></span><br><span class="line">                        exception.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="comment">// 延迟一会会看到数据发往不同分区</span></span><br><span class="line"><span class="comment">//            Thread.sleep(2);</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        往first主题中发送：</span></span><br><span class="line"><span class="comment">        atguigu0</span></span><br><span class="line"><span class="comment">        atguigu1</span></span><br><span class="line"><span class="comment">        atguigu2</span></span><br><span class="line"><span class="comment">        atguigu3</span></span><br><span class="line"><span class="comment">        atguigu4</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<p>在hadoop102上开启kafka消费者，在IDEA控制台观察回调信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主题： first-&gt;分区：0</span><br><span class="line">主题： first-&gt;分区：0</span><br><span class="line">主题： first-&gt;分区：0</span><br><span class="line">主题： first-&gt;分区：0</span><br><span class="line">主题： first-&gt;分区：0</span><br></pre></td></tr></table></figure>

<h2 id="3-5-生产经验——生产者如何提高吞吐量"><a href="#3-5-生产经验——生产者如何提高吞吐量" class="headerlink" title="3.5 生产经验——生产者如何提高吞吐量"></a>3.5 生产经验——生产者如何提高吞吐量</h2><img src="Snipaste_2023-10-23_21-08-48.png" alt="Snipaste_2023-10-23_21-08-48" style="zoom: 50%;">

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerParameters</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//以上4个是必须配置的</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// batch.size：批次大小，默认 16K</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// linger.ms：等待时间，默认 0</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RecordAccumulator：缓冲区大小，默认 32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// compression.type：压缩，默认 none，可配置值 gzip、snappy、lz4 和 zstd</span></span><br><span class="line">        properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG,<span class="string">&quot;snappy&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用 send 方法,发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;atguigu &quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试：</p>
<p>在hadoop102上开启kafka消费者，在IDEA中执行java代码，观察hadoop102控制台中是否收到消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first</span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 4</span><br></pre></td></tr></table></figure>

<h2 id="3-6-生产经验——数据可靠性"><a href="#3-6-生产经验——数据可靠性" class="headerlink" title="3.6 生产经验——数据可靠性"></a>3.6 生产经验——数据可靠性</h2><p>（0）回顾</p>
<p><img src="Snipaste_2023-10-23_15-38-44.png" alt="Snipaste_2023-10-23_15-38-44"></p>
<p>（1）ack应答原理</p>
<p><img src="Snipaste_2023-10-23_21-24-39.png" alt="Snipaste_2023-10-23_21-24-39"></p>
<p><img src="Snipaste_2023-10-23_21-25-48.png" alt="Snipaste_2023-10-23_21-25-48"></p>
<p><img src="Snipaste_2023-10-23_21-28-28.png" alt="Snipaste_2023-10-23_21-28-28"></p>
<p>（2）代码配置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerAck</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定key-value序列化类型</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 acks</span></span><br><span class="line">        properties.put(ProducerConfig.ACKS_CONFIG, <span class="string">&quot;all&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 重试次数 retries，默认是 int 最大值，2147483647</span></span><br><span class="line">        properties.put(ProducerConfig.RETRIES_CONFIG, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用 send 方法,发送消息，不带回调信息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;atguigu &quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        往first主题中发送：</span></span><br><span class="line"><span class="comment">        atguigu0</span></span><br><span class="line"><span class="comment">        atguigu1</span></span><br><span class="line"><span class="comment">        atguigu2</span></span><br><span class="line"><span class="comment">        atguigu3</span></span><br><span class="line"><span class="comment">        atguigu4</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-7-生产经验——数据去重"><a href="#3-7-生产经验——数据去重" class="headerlink" title="3.7 生产经验——数据去重"></a>3.7 生产经验——数据去重</h2><h3 id="3-7-1-数据传递语义"><a href="#3-7-1-数据传递语义" class="headerlink" title="3.7.1 数据传递语义"></a>3.7.1 数据传递语义</h3><p><img src="Snipaste_2023-10-23_21-38-00.png" alt="Snipaste_2023-10-23_21-38-00"></p>
<h3 id="3-7-2-幂等性"><a href="#3-7-2-幂等性" class="headerlink" title="3.7.2 幂等性"></a>3.7.2 幂等性</h3><p>（1）幂等性原理</p>
<p><img src="Snipaste_2023-10-23_21-41-19.png" alt="Snipaste_2023-10-23_21-41-19"></p>
<p>（2）如何使用幂等性</p>
<p>开启参数 <strong>enable.idempotence</strong> 默认为 true，false 关闭</p>
<h3 id="3-7-3-生产者事务"><a href="#3-7-3-生产者事务" class="headerlink" title="3.7.3 生产者事务"></a>3.7.3 生产者事务</h3><p>（1）kafka事务原理</p>
<p><img src="Snipaste_2023-10-23_21-43-04.png" alt="Snipaste_2023-10-23_21-43-04"></p>
<p>（2）kafaka的事务一共有如下5个API</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 初始化事务</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">initTransactions</span><span class="params">()</span>;</span><br><span class="line"><span class="comment">// 2 开启事务</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException;</span><br><span class="line"><span class="comment">// 3 在事务内提交已经消费的偏移量（主要用于消费者）</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,String consumerGroupId)</span> <span class="keyword">throws</span> </span><br><span class="line">ProducerFencedException;</span><br><span class="line"><span class="comment">// 4 提交事务</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException;</span><br><span class="line"><span class="comment">// 5 放弃事务（类似于回滚事务的操作）</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException;</span><br></pre></td></tr></table></figure>

<p>（3）代码实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerTransactions</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定key-value序列化类型</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置事务 id（必须），事务 id 任意起名</span></span><br><span class="line">        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, <span class="string">&quot;transaction_id_0&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化事务</span></span><br><span class="line">        kafkaProducer.initTransactions();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开启事务</span></span><br><span class="line">        kafkaProducer.beginTransaction();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 4. 调用 send 方法,发送消息，不带回调信息</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">                kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;atguigu &quot;</span> + i));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 提交事务</span></span><br><span class="line">            kafkaProducer.commitTransaction();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            <span class="comment">// 终止事务</span></span><br><span class="line">            kafkaProducer.abortTransaction();</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 5. 关闭资源</span></span><br><span class="line">            kafkaProducer.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>102的控制台成功收到消息。</p>
<h2 id="3-8-生产经验——数据乱序"><a href="#3-8-生产经验——数据乱序" class="headerlink" title="3.8 生产经验——数据乱序"></a>3.8 生产经验——数据乱序</h2><p><img src="Snipaste_2023-10-23_21-58-03.png" alt="Snipaste_2023-10-23_21-58-03"></p>
<p><img src="Snipaste_2023-10-23_21-58-32.png" alt="Snipaste_2023-10-23_21-58-32"></p>
<p><strong>max.in.flight.requests.per.connection</strong>就是上图Request的个数，如果没有开启幂等性，设置为1说明缓存只有一个请求，只有该请求成功成功ack之后才能处理下一个请求，所以强制保证了请求按顺序到达，数据也是按顺序提交给消费者的。</p>
<p>如果开启了幂等性，幂等性中有SeqNumber，它是单调递增的，集群收到的数据如果SeqNumber非单调递增，它会在服务端重新排序，保证单调递增再传递，所以这也保证了数据的按序传递。而为什么<strong>max.in.flight.requests.per.connection</strong>最大设置成5，是因为启用幂等性后最多在kafka集群缓存生产者发送的5个request，也就是保证了最近5个request数据一定有序，超过5个就不保证了。</p>
<h1 id="第四章-Kafka-Broker"><a href="#第四章-Kafka-Broker" class="headerlink" title="第四章 Kafka Broker"></a>第四章 Kafka Broker</h1><h2 id="4-1-Kafka-Broker工作流程"><a href="#4-1-Kafka-Broker工作流程" class="headerlink" title="4.1 Kafka Broker工作流程"></a>4.1 Kafka Broker工作流程</h2><h3 id="4-1-1-Zookeeper存储的Kafka信息"><a href="#4-1-1-Zookeeper存储的Kafka信息" class="headerlink" title="4.1.1 Zookeeper存储的Kafka信息"></a>4.1.1 Zookeeper存储的Kafka信息</h3><p>（1）启动 Zookeeper 客户端</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/module/zookeeper-3.5.7/</span><br><span class="line">[root@hadoop102 zookeeper-3.5.7]# bin/zkCli.sh</span><br></pre></td></tr></table></figure>

<p>（2）通过 ls 命令可以查看 kafka 相关信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /kafka</span><br><span class="line">[admin, brokers, cluster, config, consumers, controller, controller_epoch, feature, isr_change_notification, latest_producer_id_block, log_dir_event_notification]</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] ls /kafka/brokers/ids</span><br><span class="line">[0, 1, 2]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">以上0，1，2表示brokerid，分别代表hadoop102,hadoop103,hadoop104</span></span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-10-24_12-42-44.png" alt="Snipaste_2023-10-24_12-42-44"></p>
<p>也可以使用prettyZoo更方便地查看Zookeeper的节点信息：</p>
<img src="Snipaste_2023-10-24_12-30-53.png" alt="Snipaste_2023-10-24_12-30-53" style="zoom: 33%;">

<img src="Snipaste_2023-10-24_12-33-09.png" alt="Snipaste_2023-10-24_12-33-09" style="zoom:43%;">

<img src="Snipaste_2023-10-24_12-37-56.png" alt="Snipaste_2023-10-24_12-37-56" style="zoom: 40%;">

<img src="Snipaste_2023-10-24_12-42-13.png" alt="Snipaste_2023-10-24_12-42-13" style="zoom:40%;">

<h3 id="4-1-2-Kafka-Broker总体工作流程"><a href="#4-1-2-Kafka-Broker总体工作流程" class="headerlink" title="4.1.2 Kafka Broker总体工作流程"></a>4.1.2 Kafka Broker总体工作流程</h3><p><img src="Snipaste_2023-10-24_12-50-24.png" alt="Snipaste_2023-10-24_12-50-24"></p>
<p>（1）模拟Kafka上下线，Zookeeper中数据变化</p>
<p>①目前看&#x2F;kafka&#x2F;brokers&#x2F;ids 路径上的节点：</p>
<img src="Snipaste_2023-10-24_12-53-35.png" alt="Snipaste_2023-10-24_12-53-35" style="zoom:43%;">

<p>②查看&#x2F;kafka&#x2F;controller 路径上的数据：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span> <span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;brokerid&quot;</span> <span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;1698120266977&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>③查看&#x2F;kafka&#x2F;brokers&#x2F;topics&#x2F;first&#x2F;partitions&#x2F;0&#x2F;state 路径上的数据：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;controller_epoch&quot;</span> <span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;leader&quot;</span> <span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span> <span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;leader_epoch&quot;</span> <span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;isr&quot;</span> <span class="punctuation">:</span> <span class="punctuation">[</span> <span class="number">2</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span> <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>④停止 hadoop104 上的 kafka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 kafka]# bin/kafka-server-stop.sh</span><br><span class="line">[root@hadoop104 kafka]# jps</span><br><span class="line">3856 Jps</span><br><span class="line">2737 QuorumPeerMain</span><br></pre></td></tr></table></figure>

<p>⑤再次查看&#x2F;kafka&#x2F;brokers&#x2F;ids 路径上的节点（Hadoop04对应ids&#x3D;2，已经没了）</p>
<img src="Snipaste_2023-10-24_12-57-31.png" alt="Snipaste_2023-10-24_12-57-31" style="zoom:43%;">

<p>⑥再次查看&#x2F;kafka&#x2F;controller 路径上的数据</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span> <span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;brokerid&quot;</span> <span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;1698120266977&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>⑦再次查看&#x2F;kafka&#x2F;brokers&#x2F;topics&#x2F;first&#x2F;partitions&#x2F;0&#x2F;state 路径上的数据。此时leader已经变成1了</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;controller_epoch&quot;</span> <span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;leader&quot;</span> <span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span> <span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;leader_epoch&quot;</span> <span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;isr&quot;</span> <span class="punctuation">:</span> <span class="punctuation">[</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span> <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>⑧启动hadoop104上的Kafka，再次观察①②③步骤中的内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 kafka]# bin/kafka-server-start.sh -daemon ./config/server.properties</span><br><span class="line">[root@hadoop104 kafka]# jps</span><br><span class="line">2737 QuorumPeerMain</span><br><span class="line">4273 Kafka</span><br><span class="line">4361 Jps</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-10-24_12-53-35.png" alt="Snipaste_2023-10-24_12-53-35" style="zoom:43%;">

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span> <span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;brokerid&quot;</span> <span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;1698120266977&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;controller_epoch&quot;</span> <span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;leader&quot;</span> <span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span> <span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;leader_epoch&quot;</span> <span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;isr&quot;</span> <span class="punctuation">:</span> <span class="punctuation">[</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> <span class="number">2</span> <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="4-1-3-Broker重要参数"><a href="#4-1-3-Broker重要参数" class="headerlink" title="4.1.3 Broker重要参数"></a>4.1.3 Broker重要参数</h3><table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>replica.lag.time.max.ms</td>
<td>ISR 中，如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值，默认 30s。</td>
</tr>
<tr>
<td>auto.leader.rebalance.enable</td>
<td>默认是 true。 自动 Leader Partition 平衡。</td>
</tr>
<tr>
<td>leader.imbalance.per.broker.percentage</td>
<td>默认是 10%。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器会触发 leader 的平衡。</td>
</tr>
<tr>
<td>leader.imbalance.check.interval.seconds</td>
<td>默认值 300 秒。检查 leader 负载是否平衡的间隔时间。</td>
</tr>
<tr>
<td>log.segment.bytes</td>
<td>Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分 成块的大小，默认值 1G。</td>
</tr>
<tr>
<td>log.index.interval.bytes</td>
<td>默认 4kb，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。</td>
</tr>
<tr>
<td>log.retention.hours</td>
<td>Kafka 中数据保存的时间，默认 7 天。</td>
</tr>
<tr>
<td>log.retention.minutes</td>
<td>Kafka 中数据保存的时间，分钟级别，默认关闭。</td>
</tr>
<tr>
<td>log.retention.ms</td>
<td>Kafka 中数据保存的时间，毫秒级别，默认关闭。</td>
</tr>
<tr>
<td>log.retention.check.interval.ms</td>
<td>检查数据是否保存超时的间隔，默认是 5 分钟。</td>
</tr>
<tr>
<td>log.retention.bytes</td>
<td>默认等于-1，表示无穷大。超过设置的所有日志总大小，删除最早的 segment。</td>
</tr>
<tr>
<td>log.cleanup.policy</td>
<td>默认是 delete，表示所有数据启用删除策略；如果设置值为 compact，表示所有数据启用压缩策略</td>
</tr>
<tr>
<td>num.io.threads</td>
<td>默认是 8。负责写磁盘的线程数。整个参数值要占总核数的 50%。</td>
</tr>
<tr>
<td>num.replica.fetchers</td>
<td>副本拉取线程数，这个参数占总核数的 50%的 1&#x2F;3</td>
</tr>
<tr>
<td>num.network.threads</td>
<td>默认是 3。数据传输线程数，这个参数占总核数的50%的 2&#x2F;3 。</td>
</tr>
<tr>
<td>log.flush.interval.messages</td>
<td>强制页缓存刷写到磁盘的条数，默认是 long 的最大值，9223372036854775807。一般不建议修改，交给系统自己管理。</td>
</tr>
<tr>
<td>log.flush.interval.ms</td>
<td>每隔多久，刷数据到磁盘，默认是 null。一般不建议修改，交给系统自己管理。</td>
</tr>
</tbody></table>
<h2 id="4-2-生产经验——节点服役和退役"><a href="#4-2-生产经验——节点服役和退役" class="headerlink" title="4.2 生产经验——节点服役和退役"></a>4.2 生产经验——节点服役和退役</h2><h3 id="4-2-1-服役新节点"><a href="#4-2-1-服役新节点" class="headerlink" title="4.2.1 服役新节点"></a>4.2.1 服役新节点</h3><p>（1）新节点准备</p>
<p>①关机hadoop104，右键执行克隆操作</p>
<p>②开启hadoop105，并修改IP地址</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">IP地址</span></span><br><span class="line">IPADDR=192.168.255.105</span><br></pre></td></tr></table></figure>

<p>③在hadoop105上，修改主机名称为hadoop105</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 ~]# vim /etc/hostname</span><br><span class="line">hadoop105</span><br></pre></td></tr></table></figure>

<p>④重启104，105</p>
<p>⑤删除 hadoop105 中 kafka 下的 datas 和 logs。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop105 kafka]# rm -rf datas/* logs/*</span><br></pre></td></tr></table></figure>

<p>⑥修改hadoop105中kafka的broker.id 为 3</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop105 config]# vim server.properties</span><br></pre></td></tr></table></figure>

<p>⑦启动102，103，104上的kafka集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 zookeeper-3.5.7]# zk.sh start</span><br><span class="line">[root@hadoop102 zookeeper-3.5.7]# kf.sh start</span><br><span class="line">[root@hadoop102 zookeeper-3.5.7]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">5032 Kafka</span><br><span class="line">4618 QuorumPeerMain</span><br><span class="line">5131 Jps</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">4977 Jps</span><br><span class="line">4469 QuorumPeerMain</span><br><span class="line">4861 Kafka</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">3345 Jps</span><br><span class="line">2852 QuorumPeerMain</span><br><span class="line">3239 Kafka</span><br></pre></td></tr></table></figure>

<p>⑧单独启动105中的kafka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop105 kafka]# bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line">[root@hadoop105 kafka]# jps</span><br><span class="line">3187 Jps</span><br><span class="line">3102 Kafka</span><br></pre></td></tr></table></figure>

<p>（2）执行负载均衡操作</p>
<p>①创建一个要均衡的主题</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# vim topics-to-move.json</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">     <span class="attr">&quot;topics&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">     	<span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span> <span class="string">&quot;first&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">     <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">     <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>②生成一个负载均衡的计划：【–broker-list “0,1,2,3”】</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2,3&quot; --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,1,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,3,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,0,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,1,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure>

<p>③创建副本存储计划（所有副本存储在 broker0、broker1、broker2、broker3 中）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]#  vim increase-replication-factor.json</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;version&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;partitions&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">0</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span><span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span><span class="punctuation">,</span><span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">]</span><span class="punctuation">,</span><span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>④执行副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,1,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started partition reassignments for first-0,first-1,first-2</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>⑤验证副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition first-0 is complete.</span><br><span class="line">Reassignment of partition first-1 is complete.</span><br><span class="line">Reassignment of partition first-2 is complete.</span><br><span class="line"></span><br><span class="line">Clearing broker-level throttles on brokers 0,1,2,3</span><br><span class="line">Clearing topic-level throttles on topic first</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --topic first --describe</span><br><span class="line">Topic: first	TopicId: VV7ddVy4SKyTQauf1IHvag	PartitionCount: 3	ReplicationFactor: Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: first	Partition: 0	Leader: 1	Replicas: 0,3,1	Isr: 1,0,3</span><br><span class="line">	Topic: first	Partition: 1	Leader: 1	Replicas: 1,0,2	Isr: 1,2,0</span><br><span class="line">	Topic: first	Partition: 2	Leader: 1	Replicas: 2,1,3	Isr: 1,2,3</span><br></pre></td></tr></table></figure>

<h3 id="4-2-2-退役旧节点"><a href="#4-2-2-退役旧节点" class="headerlink" title="4.2.2 退役旧节点"></a>4.2.2 退役旧节点</h3><p>（1）执行负载均衡操作</p>
<p>①创建一个要均衡的主题</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# vim topics-to-move.json</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">     <span class="attr">&quot;topics&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">     	<span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span> <span class="string">&quot;first&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">     <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">     <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>②创建执行计划：【–broker-list “0,1,2”】</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2&quot; --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,3,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,0,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,1,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,2,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure>

<p>③创建副本存储计划（所有副本存储在 broker0、broker1、broker2 中）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]#  vim increase-replication-factor.json</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;version&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;partitions&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">0</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span><span class="punctuation">,</span><span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">]</span><span class="punctuation">,</span><span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span><span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">,</span><span class="string">&quot;any&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>④执行副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,3,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,0,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,1,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started partition reassignments for first-0,first-1,first-2</span><br></pre></td></tr></table></figure>

<p>⑤验证副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition first-0 is complete.</span><br><span class="line">Reassignment of partition first-1 is complete.</span><br><span class="line">Reassignment of partition first-2 is complete.</span><br><span class="line"></span><br><span class="line">Clearing broker-level throttles on brokers 0,1,2,3</span><br><span class="line">Clearing topic-level throttles on topic first</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --topic first --describe</span><br><span class="line">Topic: first	TopicId: VV7ddVy4SKyTQauf1IHvag	PartitionCount: 3	ReplicationFactor: Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: first	Partition: 0	Leader: 1	Replicas: 0,1,2	Isr: 1,0,2</span><br><span class="line">	Topic: first	Partition: 1	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br><span class="line">	Topic: first	Partition: 2	Leader: 1	Replicas: 2,0,1	Isr: 1,2,0</span><br></pre></td></tr></table></figure>

<p>（2）在105上执行停止命令，退役105节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop105 kafka]# bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<h2 id="4-3-Kafka副本"><a href="#4-3-Kafka副本" class="headerlink" title="4.3 Kafka副本"></a>4.3 Kafka副本</h2><h3 id="4-3-1-副本基本信息"><a href="#4-3-1-副本基本信息" class="headerlink" title="4.3.1 副本基本信息"></a>4.3.1 副本基本信息</h3><ul>
<li><p>Kafka 副本作用：提高数据可靠性</p>
</li>
<li><p>Kafka 默认副本 1 个，生产环境一般配置为 2 个，保证数据可靠性；太多副本会增加磁盘存储空间，增加网络上数据传输，降低效率</p>
</li>
<li><p>Kafka 中副本分为：Leader 和 Follower。Kafka 生产者只会把数据发往 Leader，然后 Follower 找 Leader 进行同步数据</p>
</li>
<li><p>Kafka 分区中的所有副本统称为 AR（Assigned Repllicas）。</p>
<p>AR &#x3D; ISR + OSR</p>
<p><strong>ISR</strong>，表示和 Leader 保持同步的 Follower 集合。如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值由 <strong>replica.lag.time.max.ms</strong>参数设定，默认 30s。Leader 发生故障之后，就会从 ISR 中选举新的 Leader</p>
<p><strong>OSR</strong>，表示 Follower 与 Leader 副本同步时，延迟过多的副本</p>
</li>
</ul>
<h3 id="4-3-2-Leader选举流程"><a href="#4-3-2-Leader选举流程" class="headerlink" title="4.3.2 Leader选举流程"></a>4.3.2 Leader选举流程</h3><p>Kafka 集群中有一个 broker 的 Controller 会被选举为 Controller Leader，负责管理集群broker 的上下线，所有 topic 的分区副本分配和 Leader 选举等工作。</p>
<p>Controller 的信息同步工作是依赖于 Zookeeper 的。</p>
<p><img src="Snipaste_2023-10-24_15-14-44.png" alt="Snipaste_2023-10-24_15-14-44"></p>
<p>（1）创建创建一个新的 topic  atguigu1，4 个分区，4 个副本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --topic atguigu1 --partitions 4 --replication-factor 4</span><br><span class="line">Created topic atguigu1.</span><br></pre></td></tr></table></figure>

<p>（2）查看Leader分布情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic atguigu1</span><br><span class="line">Topic: atguigu1	TopicId: orp06E3cQHGvR4FFHYcjJg	PartitionCount: 4	ReplicationFactor: Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: atguigu1	Partition: 0	Leader: 1	Replicas: 1,2,3,0	Isr: 1,2,3,0</span><br><span class="line">	Topic: atguigu1	Partition: 1	Leader: 0	Replicas: 0,3,1,2	Isr: 0,3,1,2</span><br><span class="line">	Topic: atguigu1	Partition: 2	Leader: 2	Replicas: 2,1,0,3	Isr: 2,1,0,3</span><br><span class="line">	Topic: atguigu1	Partition: 3	Leader: 3	Replicas: 3,0,2,1	Isr: 3,0,2,1</span><br></pre></td></tr></table></figure>

<p>我们发现：在3分区中，Leader为3，如果停掉hadoop105的进程，那么Leader应该变为0（按照Replicas顺序）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Topic: atguigu1	Partition: 3	Leader: 3	Replicas: 3,0,2,1	Isr: 3,0,2,1</span><br></pre></td></tr></table></figure>

<p>（3）停止掉hadoop105的kafka进程，并查看Leader分区情况，和预想的一致</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop105 kafka]# bin/kafka-server-stop.sh</span><br><span class="line"></span><br><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic atguigu1</span><br><span class="line">Topic: atguigu1	TopicId: orp06E3cQHGvR4FFHYcjJg	PartitionCount: 4	ReplicationFactor: 4	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: atguigu1	Partition: 0	Leader: 1	Replicas: 1,2,3,0	Isr: 1,2,0</span><br><span class="line">	Topic: atguigu1	Partition: 1	Leader: 0	Replicas: 0,3,1,2	Isr: 0,1,2</span><br><span class="line">	Topic: atguigu1	Partition: 2	Leader: 2	Replicas: 2,1,0,3	Isr: 2,1,0</span><br><span class="line">	Topic: atguigu1	Partition: 3	Leader: 0	Replicas: 3,0,2,1	Isr: 0,2,1</span><br></pre></td></tr></table></figure>

<p>我们发现：在2分区中，Leader为2，如果停掉hadoop104的进程，那么Leader应该变为1（按照Replicas顺序）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Topic: atguigu1	Partition: 2	Leader: 2	Replicas: 2,1,0,3	Isr: 2,1,0</span><br></pre></td></tr></table></figure>

<p>（4）停止掉 hadoop104 的 kafka 进程，并查看 Leader 分区情况，和预想的一致</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[rootu@hadoop104 kafka]# bin/kafka-server-stop.sh</span><br><span class="line"></span><br><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic atguigu1</span><br><span class="line">Topic: atguigu1	TopicId: orp06E3cQHGvR4FFHYcjJg	PartitionCount: 4	ReplicationFactor: 4	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: atguigu1	Partition: 0	Leader: 1	Replicas: 1,2,3,0	Isr: 1,0</span><br><span class="line">	Topic: atguigu1	Partition: 1	Leader: 0	Replicas: 0,3,1,2	Isr: 0,1</span><br><span class="line">	Topic: atguigu1	Partition: 2	Leader: 1	Replicas: 2,1,0,3	Isr: 1,0</span><br><span class="line">	Topic: atguigu1	Partition: 3	Leader: 0	Replicas: 3,0,2,1	Isr: 0,1</span><br></pre></td></tr></table></figure>

<p>（5）启动 hadoop105 的 kafka 进程，并查看 Leader 分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop105 kafka]# bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line"></span><br><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic atguigu1</span><br><span class="line">Topic: atguigu1	TopicId: orp06E3cQHGvR4FFHYcjJg	PartitionCount: 4	ReplicationFactor: 4	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: atguigu1	Partition: 0	Leader: 1	Replicas: 1,2,3,0	Isr: 1,0,3</span><br><span class="line">	Topic: atguigu1	Partition: 1	Leader: 0	Replicas: 0,3,1,2	Isr: 0,1,3</span><br><span class="line">	Topic: atguigu1	Partition: 2	Leader: 1	Replicas: 2,1,0,3	Isr: 1,0,3</span><br><span class="line">	Topic: atguigu1	Partition: 3	Leader: 0	Replicas: 3,0,2,1	Isr: 0,1,3</span><br></pre></td></tr></table></figure>

<p>（6）启动 hadoop104 的 kafka 进程，并查看 Leader 分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 kafka]# bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line"></span><br><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic atguigu1</span><br><span class="line">Topic: atguigu1	TopicId: orp06E3cQHGvR4FFHYcjJg	PartitionCount: 4	ReplicationFactor: 4	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: atguigu1	Partition: 0	Leader: 1	Replicas: 1,2,3,0	Isr: 1,0,3,2</span><br><span class="line">	Topic: atguigu1	Partition: 1	Leader: 0	Replicas: 0,3,1,2	Isr: 0,1,3,2</span><br><span class="line">	Topic: atguigu1	Partition: 2	Leader: 1	Replicas: 2,1,0,3	Isr: 1,0,3,2</span><br><span class="line">	Topic: atguigu1	Partition: 3	Leader: 0	Replicas: 3,0,2,1	Isr: 0,1,3,2</span><br></pre></td></tr></table></figure>

<p>我们发现：在0分区中，Leader为1，如果停掉hadoop103的进程，那么Leader应该变为2（按照Replicas顺序）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Topic: atguigu1	Partition: 0	Leader: 1	Replicas: 1,2,3,0	Isr: 1,0,3,2</span><br></pre></td></tr></table></figure>

<p>我们发现：在2分区中，Leader为1，如果停掉hadoop103的进程，那么Leader应该变为2（按照Replicas顺序）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Topic: atguigu1	Partition: 2	Leader: 1	Replicas: 2,1,0,3	Isr: 1,0,3,2</span><br></pre></td></tr></table></figure>

<p>（7）停止掉 hadoop103 的 kafka 进程，并查看 Leader 分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 kafka]# bin/kafka-server-stop.sh</span><br><span class="line"></span><br><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic atguigu1</span><br><span class="line">Topic: atguigu1	TopicId: orp06E3cQHGvR4FFHYcjJg	PartitionCount: 4	ReplicationFactor: 4	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: atguigu1	Partition: 0	Leader: 2	Replicas: 1,2,3,0	Isr: 0,3,2</span><br><span class="line">	Topic: atguigu1	Partition: 1	Leader: 0	Replicas: 0,3,1,2	Isr: 0,3,2</span><br><span class="line">	Topic: atguigu1	Partition: 2	Leader: 2	Replicas: 2,1,0,3	Isr: 0,3,2</span><br><span class="line">	Topic: atguigu1	Partition: 3	Leader: 3	Replicas: 3,0,2,1	Isr: 0,3,2</span><br></pre></td></tr></table></figure>

<h3 id="4-3-3-Leader和Follower故障处理细节"><a href="#4-3-3-Leader和Follower故障处理细节" class="headerlink" title="4.3.3 Leader和Follower故障处理细节"></a>4.3.3 Leader和Follower故障处理细节</h3><p>Leader是先收到数据的，其余Follower对Leader中的数据进行拉取，所以Follower中的数据在某一时刻可能比Leader中要少</p>
<img src="Snipaste_2023-10-24_15-53-26.png" alt="Snipaste_2023-10-24_15-53-26" style="zoom: 67%;">

<p>（1）Follower故障处理细节：</p>
<p>①当Follower没出现故障时，Leader和Follower们的LEO和HW：</p>
<img src="webwxgetmsgimg (3).jpg" alt="webwxgetmsgimg (3)" style="zoom: 25%;">

<p>②当Follower发生故障后会被踢出ISR：</p>
<img src="webwxgetmsgimg.jpg" alt="webwxgetmsgimg" style="zoom: 25%;">

<p>③这个期间Leader和Follower继续接收数据，LEO和HW也不断更新：</p>
<img src="webwxgetmsgimg (1).jpg" alt="webwxgetmsgimg (1)" style="zoom:25%;">

<p>④待该Follower恢复后，Follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向Leader进行同步。</p>
<img src="webwxgetmsgimg (2).jpg" alt="webwxgetmsgimg (2)" style="zoom:25%;">

<p>⑤等该Follower的LEO大于等于该Partition的HW，即Follower追上Leader之后，就可以重新加入ISR了。 </p>
<img src="webwxgetmsgimg (4).jpg" alt="webwxgetmsgimg (4)" style="zoom:25%;">

<p>（2）Leader故障处理细节</p>
<p>①当Leader没出现故障时，Leader和Follower们的LEO和HW：</p>
<img src="webwxgetmsgimg (3).jpg" alt="webwxgetmsgimg (3)" style="zoom:25%;">

<p>②Leader发生故障之后，会从ISR中选出一个新的Leader</p>
<img src="webwxgetmsgimg (5).jpg" alt="webwxgetmsgimg (5)" style="zoom:25%;">

<img src="webwxgetmsgimg (6).jpg" alt="webwxgetmsgimg (6)" style="zoom:25%;">

<p>③为保证多个副本之间的数据一致性，其余的Follower会先将各自的log文件高于HW的部分截掉，然后从新的Leader同步数据</p>
<img src="webwxgetmsgimg (7).jpg" alt="webwxgetmsgimg (7)" style="zoom:25%;">

<p><strong>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</strong></p>
<h3 id="4-3-4-分区副本分配"><a href="#4-3-4-分区副本分配" class="headerlink" title="4.3.4 分区副本分配"></a>4.3.4 分区副本分配</h3><p>如果 kafka 服务器只有 4 个节点，那么设置 kafka 的分区数大于服务器台数，在 kafka底层如何分配存储副本呢？</p>
<p>（1）创建16个分区，3个副本</p>
<p>①创建一个新的topic，名称为second</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 16 --replication-factor 3 --topic second</span><br><span class="line">Created topic second.</span><br></pre></td></tr></table></figure>

<p>②查看分区和副本情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic second</span><br><span class="line">Topic: second	TopicId: ENu4i6cnTIKB4cEs8Caskw	PartitionCount: 16	ReplicationFactor: 3	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: second	Partition: 0	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</span><br><span class="line">	Topic: second	Partition: 1	Leader: 0	Replicas: 0,2,3	Isr: 0,2,3</span><br><span class="line">	Topic: second	Partition: 2	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1</span><br><span class="line">	Topic: second	Partition: 3	Leader: 3	Replicas: 3,1,0	Isr: 3,1,0</span><br><span class="line">	Topic: second	Partition: 4	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3</span><br><span class="line">	Topic: second	Partition: 5	Leader: 0	Replicas: 0,3,1	Isr: 0,3,1</span><br><span class="line">	Topic: second	Partition: 6	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0</span><br><span class="line">	Topic: second	Partition: 7	Leader: 3	Replicas: 3,0,2	Isr: 3,0,2</span><br><span class="line">	Topic: second	Partition: 8	Leader: 1	Replicas: 1,3,0	Isr: 1,3,0</span><br><span class="line">	Topic: second	Partition: 9	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2</span><br><span class="line">	Topic: second	Partition: 10	Leader: 2	Replicas: 2,0,3	Isr: 2,0,3</span><br><span class="line">	Topic: second	Partition: 11	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1</span><br><span class="line">	Topic: second	Partition: 12	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</span><br><span class="line">	Topic: second	Partition: 13	Leader: 0	Replicas: 0,2,3	Isr: 0,2,3</span><br><span class="line">	Topic: second	Partition: 14	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1</span><br><span class="line">	Topic: second	Partition: 15	Leader: 3	Replicas: 3,1,0	Isr: 3,1,0</span><br></pre></td></tr></table></figure>

<p>这个图，broker0~3代表的是hadoop102-105服务器（或者说kafka集群），横向每一行代表一个分区，一共16行，就代表16个分区。</p>
<img src="Snipaste_2023-10-24_21-07-22.png" alt="Snipaste_2023-10-24_21-07-22" style="zoom:50%;">



<h3 id="4-3-5-生产经验——手动调整分区副本存储"><a href="#4-3-5-生产经验——手动调整分区副本存储" class="headerlink" title="4.3.5 生产经验——手动调整分区副本存储"></a>4.3.5 生产经验——手动调整分区副本存储</h3><img src="Snipaste_2023-10-24_21-13-39.png" alt="Snipaste_2023-10-24_21-13-39" style="zoom:50%;">

<p>步骤：</p>
<p>（1）创建一个新的 topic，名称为 three</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 4 --replication-factor 2 --topic three</span><br><span class="line">Created topic three.</span><br></pre></td></tr></table></figure>

<p>（2）查看分区副本存储情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic three</span><br><span class="line">Topic: three	TopicId: CeDwRMeHQQCkTeD5H1x9Zw	PartitionCount: 4	ReplicationFactor: 2	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: three	Partition: 0	Leader: 3	Replicas: 3,1	Isr: 3,1</span><br><span class="line">	Topic: three	Partition: 1	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br><span class="line">	Topic: three	Partition: 2	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br><span class="line">	Topic: three	Partition: 3	Leader: 2	Replicas: 2,3	Isr: 2,3</span><br></pre></td></tr></table></figure>

<p>（3）创建副本存储计划（所有副本都指定存储在 broker0、broker1 中）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# vim increase-replication-factor.json</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;partitions&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;three&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">0</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                  <span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;three&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                  <span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;three&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                  <span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;three&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>（4）执行副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[rootu@hadoop102 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[2,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started partition reassignments for three-0,three-1,three-2,three-3</span><br></pre></td></tr></table></figure>

<p>（5）验证副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition three-0 is complete.</span><br><span class="line">Reassignment of partition three-1 is complete.</span><br><span class="line">Reassignment of partition three-2 is complete.</span><br><span class="line">Reassignment of partition three-3 is complete.</span><br><span class="line"></span><br><span class="line">Clearing broker-level throttles on brokers 0,1,2,3</span><br><span class="line">Clearing topic-level throttles on topic three</span><br></pre></td></tr></table></figure>

<p>（6）查看分区副本存储情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic three</span><br><span class="line">Topic: three	TopicId: CeDwRMeHQQCkTeD5H1x9Zw	PartitionCount: 4	ReplicationFactor: 2	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: three	Partition: 0	Leader: 0	Replicas: 0,1	Isr: 1,0</span><br><span class="line">	Topic: three	Partition: 1	Leader: 1	Replicas: 0,1	Isr: 1,0</span><br><span class="line">	Topic: three	Partition: 2	Leader: 0	Replicas: 1,0	Isr: 0,1</span><br><span class="line">	Topic: three	Partition: 3	Leader: 1	Replicas: 1,0	Isr: 1,0</span><br></pre></td></tr></table></figure>

<h3 id="4-3-6-生产经验——Leader-Partition负载平衡"><a href="#4-3-6-生产经验——Leader-Partition负载平衡" class="headerlink" title="4.3.6 生产经验——Leader Partition负载平衡"></a>4.3.6 生产经验——Leader Partition负载平衡</h3><p><img src="Snipaste_2023-10-25_16-10-37.png" alt="Snipaste_2023-10-25_16-10-37"></p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>auto.leader.rebalance.enable</td>
<td>默认是 true。 自动 Leader Partition 平衡。生产环境中，leader 重选举的代价比较大，可能会带来性能影响，建议设置为 false 关闭</td>
</tr>
<tr>
<td>leader.imbalance.per.broker.percentage</td>
<td>默认是 10%。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器会触发 leader 的平衡</td>
</tr>
<tr>
<td>leader.imbalance.check.interval.seconds</td>
<td>默认值 300 秒。检查 leader 负载是否平衡的间隔时间</td>
</tr>
</tbody></table>
<h3 id="4-3-7-生产经验——增加副本因子"><a href="#4-3-7-生产经验——增加副本因子" class="headerlink" title="4.3.7 生产经验——增加副本因子"></a>4.3.7 生产经验——增加副本因子</h3><p>在生产环境当中，由于某个主题的重要等级需要提升，我们考虑<strong>增加副本</strong>。副本数的增加需要先制定计划，然后根据计划执行</p>
<p>（1）创建topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 3 --replication-factor 1 --topic four</span><br><span class="line">Created topic four.</span><br></pre></td></tr></table></figure>

<p>（2）手动增加副本存储</p>
<p>①创建副本存储计划（所有副本都指定存储在 broker0、broker1、broker2 中）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# vim increase-replication-factor.json</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;version&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;partitions&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;four&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">0</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                           <span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;four&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                           <span class="punctuation">&#123;</span><span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;four&quot;</span><span class="punctuation">,</span><span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span><span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>②执行副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started partition reassignments for four-0,four-1,four-2</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic four</span><br><span class="line">Topic: four	TopicId: DMmey5uUSL6J5ue3twq3Yw	PartitionCount: 3	ReplicationFactor: 3	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: four	Partition: 0	Leader: 1	Replicas: 0,1,2	Isr: 1,0,2</span><br><span class="line">	Topic: four	Partition: 1	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2</span><br><span class="line">	Topic: four	Partition: 2	Leader: 2	Replicas: 0,1,2	Isr: 2,1,0</span><br></pre></td></tr></table></figure>

<h2 id="4-4-文件存储"><a href="#4-4-文件存储" class="headerlink" title="4.4 文件存储"></a>4.4 文件存储</h2><h3 id="4-4-1-文件存储机制"><a href="#4-4-1-文件存储机制" class="headerlink" title="4.4.1 文件存储机制"></a>4.4.1 文件存储机制</h3><p>（1）Topicc数据的存储机制</p>
<p><img src="Snipaste_2023-10-25_21-36-21.png" alt="Snipaste_2023-10-25_21-36-21"></p>
<p>（2）Topic数据到底存储在什么位置？</p>
<p>①启动生产者，并发送消息（之前发过了）</p>
<p>②查看 hadoop102（或者 hadoop103、hadoop104）的&#x2F;opt&#x2F;module&#x2F;kafka&#x2F;datas&#x2F;first-1（first-0、first-2）路径上的文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datas]# cd first-0</span><br><span class="line">[root@hadoop102 first-0]# ll</span><br><span class="line">总用量 20</span><br><span class="line">-rw-r--r-- 1 root root 10485760 10月 25 16:21 00000000000000000000.index</span><br><span class="line">-rw-r--r-- 1 root root     1126 10月 23 21:14 00000000000000000000.log</span><br><span class="line">-rw-r--r-- 1 root root 10485756 10月 25 16:21 00000000000000000000.timeindex</span><br><span class="line">-rw-r--r-- 1 root root       10 10月 23 21:51 00000000000000000032.snapshot</span><br><span class="line">-rw-r--r-- 1 root root       14 10月 25 16:21 leader-epoch-checkpoint</span><br><span class="line">-rw-r--r-- 1 root root       43 10月 23 15:04 partition.metadata</span><br></pre></td></tr></table></figure>

<p>③直接查看日志，发现是乱码</p>
<p>④通过工具查看 index 和 log 信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 first-0]# kafka-run-class.sh kafka.tools.DumpLogSegments --files ./00000000000000000000.index</span><br><span class="line">Dumping ./00000000000000000000.index</span><br><span class="line">offset: 0 position: 0</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 first-0]# kafka-run-class.sh kafka.tools.DumpLogSegments --files ./00000000000000000000.log</span><br><span class="line">Dumping ./00000000000000000000.log</span><br><span class="line">Starting offset: 0</span><br><span class="line">baseOffset: 0 lastOffset: 4 count: 5 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 0 CreateTime: 1698048686162 size: 141 magic: 2 compresscodec: none crc: 1451661340 isvalid: true</span><br><span class="line">baseOffset: 5 lastOffset: 6 count: 2 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 141 CreateTime: 1698049194013 size: 93 magic: 2 compresscodec: none crc: 2165397900 isvalid: true</span><br><span class="line">baseOffset: 7 lastOffset: 8 count: 2 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 234 CreateTime: 1698063818922 size: 95 magic: 2 compresscodec: none crc: 3599054527 isvalid: true</span><br><span class="line">baseOffset: 9 lastOffset: 9 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 329 CreateTime: 1698063818925 size: 78 magic: 2 compresscodec: none crc: 1140168620 isvalid: true</span><br><span class="line">baseOffset: 10 lastOffset: 10 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 407 CreateTime: 1698063818929 size: 78 magic: 2 compresscodec: none crc: 2810004441 isvalid: true</span><br><span class="line">baseOffset: 11 lastOffset: 11 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 485 CreateTime: 1698063818933 size: 78 magic: 2 compresscodec: none crc: 2525659745 isvalid: true</span><br><span class="line">baseOffset: 12 lastOffset: 16 count: 5 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 563 CreateTime: 1698065665463 size: 141 magic: 2 compresscodec: none crc: 962796909 isvalid: true</span><br><span class="line">baseOffset: 17 lastOffset: 21 count: 5 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 704 CreateTime: 1698067107691 size: 141 magic: 2 compresscodec: snappy crc: 1908828542 isvalid: true</span><br><span class="line">baseOffset: 22 lastOffset: 26 count: 5 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 845 CreateTime: 1698067116133 size: 140 magic: 2 compresscodec: snappy crc: 980480848 isvalid: true</span><br><span class="line">baseOffset: 27 lastOffset: 31 count: 5 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 985 CreateTime: 1698068119235 size: 141 magic: 2 compresscodec: none crc: 2727579545 isvalid: true</span><br></pre></td></tr></table></figure>

<p>（3）index文件和log文件详解</p>
<p><img src="Snipaste_2023-10-25_22-00-46.png" alt="Snipaste_2023-10-25_22-00-46"></p>
<p>说明：日志存储参数配置</p>
<img src="Snipaste_2023-10-25_22-02-02.png" alt="Snipaste_2023-10-25_22-02-02" style="zoom: 50%;">

<h3 id="4-4-2-文件清理策略"><a href="#4-4-2-文件清理策略" class="headerlink" title="4.4.2 文件清理策略"></a>4.4.2 文件清理策略</h3><p>Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。</p>
<ul>
<li>log.retention.hours，最低优先级小时，默认 7 天。</li>
<li>log.retention.minutes，分钟。</li>
<li>log.retention.ms，最高优先级毫秒。</li>
<li>log.retention.check.interval.ms，负责设置检查周期，默认 5 分钟。</li>
</ul>
<p>那么日志一旦超过了设置的时间，怎么处理呢？</p>
<p>Kafka 中提供的日志清理策略有 delete 和 compact 两种。</p>
<p><strong>（1）delete 日志删除：将过期数据删除</strong></p>
<p>log.cleanup.policy &#x3D; delete 所有数据启用删除策略</p>
<p>①基于时间：默认打开。以 segment 中所有记录中的最大时间戳作为该文件时间戳</p>
<p>②基于大小：默认关闭。超过设置的所有日志总大小，删除最早的 segment。log.retention.bytes，默认等于-1，表示无穷大</p>
<img src="Snipaste_2023-10-26_14-41-42.png" alt="Snipaste_2023-10-26_14-41-42" style="zoom: 50%;">

<p><strong>（2）compact 日志压缩（用的少）</strong></p>
<img src="Snipaste_2023-10-26_14-42-31.png" alt="Snipaste_2023-10-26_14-42-31" style="zoom:50%;">

<h2 id="4-5-高效读写数据（高频面试题，必会）"><a href="#4-5-高效读写数据（高频面试题，必会）" class="headerlink" title="4.5 高效读写数据（高频面试题，必会）"></a>4.5 高效读写数据（高频面试题，必会）</h2><p>（1）kafka本身是分布式集群，可以采用分区技术，并行度高</p>
<p>（2）读数据采用稀疏索引，可以快速定位要消费的数据</p>
<p>（3）顺序写磁盘</p>
<img src="Snipaste_2023-10-26_14-49-11.png" alt="Snipaste_2023-10-26_14-49-11" style="zoom:50%;">

<p>（4）页缓存+零拷贝技术</p>
<img src="Snipaste_2023-10-26_14-53-55.png" alt="Snipaste_2023-10-26_14-53-55" style="zoom: 50%;">

<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>log.flush.interval.messages</td>
<td>强制页缓存刷写到磁盘的条数，默认是 long 的最大值，9223372036854775807。一般不建议修改，交给系统自己管理。</td>
</tr>
<tr>
<td>log.flush.interval.ms</td>
<td>每隔多久，刷数据到磁盘，默认是 null。一般不建议修改，交给系统自己管理。</td>
</tr>
</tbody></table>
<h1 id="第五章-Kafka消费者"><a href="#第五章-Kafka消费者" class="headerlink" title="第五章 Kafka消费者"></a>第五章 Kafka消费者</h1><h2 id="5-1-kafka消费方式"><a href="#5-1-kafka消费方式" class="headerlink" title="5.1 kafka消费方式"></a>5.1 kafka消费方式</h2><img src="Snipaste_2023-10-26_15-29-33.png" alt="Snipaste_2023-10-26_15-29-33" style="zoom:50%;">

<h2 id="5-2-kafka消费者工作流程"><a href="#5-2-kafka消费者工作流程" class="headerlink" title="5.2 kafka消费者工作流程"></a>5.2 kafka消费者工作流程</h2><h3 id="5-2-1-消费者总体工作流程"><a href="#5-2-1-消费者总体工作流程" class="headerlink" title="5.2.1 消费者总体工作流程"></a>5.2.1 消费者总体工作流程</h3><img src="Snipaste_2023-10-26_15-37-22.png" alt="Snipaste_2023-10-26_15-37-22" style="zoom:50%;">

<p>可以一对一，多对一（无条件），如果一对多，对应的消费者一定是分属于不同消费者组的</p>
<h3 id="5-2-2-消费者组原理"><a href="#5-2-2-消费者组原理" class="headerlink" title="5.2.2 消费者组原理"></a>5.2.2 消费者组原理</h3><p>可以一对一，多对一（无条件）</p>
<img src="Snipaste_2023-10-26_15-43-19.png" alt="Snipaste_2023-10-26_15-43-19" style="zoom:50%;">

<p>如果一对多，对应的消费者一定是分属于不同的消费者组的</p>
<img src="Snipaste_2023-10-26_15-44-51.png" alt="Snipaste_2023-10-26_15-44-51" style="zoom:50%;">

<img src="webwxgetmsgimg (8).jpg" alt="webwxgetmsgimg (8)" style="zoom:50%;">

<img src="webwxgetmsgimg (9).jpg" alt="webwxgetmsgimg (9)" style="zoom:50%;">

<p>这里面的消费计划就是消费者组中的各个消费者拉取Partition分区的方案，谁拉取0号分区，谁拉取1号分区，谁拉取2号分区</p>
<img src="webwxgetmsgimg (10).jpg" alt="webwxgetmsgimg (10)" style="zoom:50%;">

<p>再平衡就是其中某个消费者挂了之后，该消费者组内的其余消费者重新分配消费方案</p>
<p><img src="Snipaste_2023-10-26_16-13-12.png" alt="Snipaste_2023-10-26_16-13-12"></p>
<h3 id="5-2-3-消费者重要参数"><a href="#5-2-3-消费者重要参数" class="headerlink" title="5.2.3 消费者重要参数"></a>5.2.3 消费者重要参数</h3><table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>向 Kafka 集群建立初始连接用到的 host&#x2F;port 列表。</td>
</tr>
<tr>
<td>key.deserializer 和value.deserializer</td>
<td>指定接收消息的 key 和 value 的反序列化类型。一定要写全类名。</td>
</tr>
<tr>
<td>group.id</td>
<td>标记消费者所属的消费者组。</td>
</tr>
<tr>
<td>enable.auto.commit</td>
<td>默认值为 true，消费者会自动周期性地向服务器提交偏移量</td>
</tr>
<tr>
<td>auto.commit.interval.ms</td>
<td>如果设置了 enable.auto.commit 的值为 true， 则该值定义了消费者偏移量向 Kafka 提交的频率，默认 5s。</td>
</tr>
<tr>
<td>auto.offset.reset</td>
<td>当 Kafka 中没有初始偏移量或当前偏移量在服务器中不存在（如，数据被删除了），该如何处理？ earliest：自动重置偏移量到最早的偏移量。 latest：默认，自动重置偏移量为最新的偏移量。 none：如果消费组原来的（previous）偏移量不存在，则向消费者抛异常。 anything：向消费者抛异常。</td>
</tr>
<tr>
<td>offsets.topic.num.partitions</td>
<td>__consumer_offsets 的分区数，默认是 50 个分区。</td>
</tr>
<tr>
<td>heartbeat.interval.ms</td>
<td>Kafka 消费者和 coordinator 之间的心跳时间，默认 3s。该条目的值必须小于 session.timeout.ms ，也不应该高于session.timeout.ms 的 1&#x2F;3。</td>
</tr>
<tr>
<td>session.timeout.ms</td>
<td>Kafka 消费者和 coordinator 之间连接超时时间，默认 45s。超过该值，该消费者被移除，消费者组执行再平衡</td>
</tr>
<tr>
<td>max.poll.interval.ms</td>
<td>消费者处理消息的最大时长，默认是 5 分钟。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>fetch.min.bytes</td>
<td>默认 1 个字节。消费者获取服务器端一批消息最小的字节数</td>
</tr>
<tr>
<td>fetch.max.wait.ms</td>
<td>默认 500ms。如果没有从服务器端获取到一批数据的最小字节数。该时间到，仍然会返回数据。</td>
</tr>
<tr>
<td>fetch.max.bytes</td>
<td>默认 Default: 52428800（50 m）。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值（50m）仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受 message.max.bytes （brokerconfig）or max.message.bytes （topic config）影响</td>
</tr>
<tr>
<td>max.poll.records</td>
<td>一次 poll 拉取数据返回消息的最大条数，默认是 500 条。</td>
</tr>
</tbody></table>
<h3 id="5-3-消费者API"><a href="#5-3-消费者API" class="headerlink" title="5.3 消费者API"></a>5.3 消费者API</h3><h3 id="5-3-1-独立消费者案例（订阅主题）"><a href="#5-3-1-独立消费者案例（订阅主题）" class="headerlink" title="5.3.1 独立消费者案例（订阅主题）"></a>5.3.1 独立消费者案例（订阅主题）</h3><p>（1）需求：创建一个独立消费者，消费 first 主题（所有分区）中数据</p>
<img src="Snipaste_2023-10-26_16-21-19.png" alt="Snipaste_2023-10-26_16-21-19" style="zoom:50%;">

<p><strong>注意：</strong>在消费者 API 代码中必须配置消费者组 id。命令行启动消费者不填写消费者组id 会被自动填写随机的消费者组 id。</p>
<p>（2）实现步骤：</p>
<p>①创建包名：com.atguigu.kafka.consumer</p>
<p>②编写代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.创建消费者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给消费者配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置反序列化 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意起名） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 订阅要消费的主题的所有分区（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置 1s 中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 打印消费到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）测试</p>
<p>①在IDEA中执行消费者程序</p>
<p>②创建kafka生产者，并输入数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello</span></span><br></pre></td></tr></table></figure>

<p>③在IDEA的控制台观察到接收到的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 26, offset = 32, CreateTime = 1698322534687, serialized key size = -1, serialized value size = 5, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = hello)</span><br></pre></td></tr></table></figure>

<h3 id="5-3-2-独立消费者案例（订阅分区）"><a href="#5-3-2-独立消费者案例（订阅分区）" class="headerlink" title="5.3.2 独立消费者案例（订阅分区）"></a>5.3.2 独立消费者案例（订阅分区）</h3><p>（1）需求：创建一个独立消费者，消费first主题0号分区的数据</p>
<img src="Snipaste_2023-10-26_21-02-00.png" alt="Snipaste_2023-10-26_21-02-00" style="zoom:50%;">

<p>（2）代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerPartition</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置反序列化 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（必须），名字可以任意起</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费某个主题的某个分区数据</span></span><br><span class="line">        ArrayList&lt;TopicPartition&gt; topicPartitions = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topicPartitions.add(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;first&quot;</span>, <span class="number">0</span>));</span><br><span class="line">        kafkaConsumer.assign(topicPartitions);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）测试</p>
<p>①在IDEA中执行消费者程序</p>
<p>②在 IDEA 中执行生产者程序 CustomProducerCallbackPartitions()，往first主题的分区0发送五条数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主题： first-&gt;分区：0</span><br><span class="line">主题： first-&gt;分区：0</span><br><span class="line">主题： first-&gt;分区：0</span><br><span class="line">主题： first-&gt;分区：0</span><br><span class="line">主题： first-&gt;分区：0</span><br></pre></td></tr></table></figure>

<p>③可以看到收到的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 26, offset = 33, CreateTime = 1698326266217, serialized key size = 0, serialized value size = 9, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu 0)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 26, offset = 34, CreateTime = 1698326266217, serialized key size = 0, serialized value size = 9, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu 1)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 26, offset = 35, CreateTime = 1698326266217, serialized key size = 0, serialized value size = 9, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu 2)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 26, offset = 36, CreateTime = 1698326266217, serialized key size = 0, serialized value size = 9, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu 3)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 26, offset = 37, CreateTime = 1698326266217, serialized key size = 0, serialized value size = 9, headers = RecordHeaders(headers = [], isReadOnly = false), key = , value = atguigu 4)</span><br></pre></td></tr></table></figure>

<h3 id="5-3-3-消费者组案例"><a href="#5-3-3-消费者组案例" class="headerlink" title="5.3.3 消费者组案例"></a>5.3.3 消费者组案例</h3><p>（1）需求：测试同一个主题的分区数据，只能由一个消费者组中的一个消费</p>
<img src="Snipaste_2023-10-26_21-21-56.png" alt="Snipaste_2023-10-26_21-21-56" style="zoom:50%;">

<p>（2）实操</p>
<p>①复制两份 CustomConsumer代码，分别命名为 CustomConsumer1和 CustomConsumer2，在IDEA中同时启动，这三个消费者同属于一个消费者组</p>
<p>②启动代码中的生产者发送消息，可以看到每个消费者只能收到特定某个分区的数据</p>
<p><img src="Snipaste_2023-10-26_21-35-37.png" alt="Snipaste_2023-10-26_21-35-37"></p>
<p><img src="Snipaste_2023-10-26_21-35-54.png" alt="Snipaste_2023-10-26_21-35-54"></p>
<p><img src="Snipaste_2023-10-26_21-36-09.png" alt="Snipaste_2023-10-26_21-36-09"></p>
<h2 id="5-4-生产经验——分区的分配以及再平衡"><a href="#5-4-生产经验——分区的分配以及再平衡" class="headerlink" title="5.4 生产经验——分区的分配以及再平衡"></a>5.4 生产经验——分区的分配以及再平衡</h2><p><img src="Snipaste_2023-10-28_12-43-01.png" alt="Snipaste_2023-10-28_12-43-01"></p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>heartbeat.interval.ms</td>
<td>Kafka 消费者和 coordinator 之间的心跳时间，默认 3s。该条目的值必须小于 session.timeout.ms，也不应该高于session.timeout.ms 的 1&#x2F;3。</td>
</tr>
<tr>
<td>session.timeout.ms</td>
<td>Kafka 消费者和 coordinator 之间连接超时时间，默认 45s。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>max.poll.interval.ms</td>
<td>消费者处理消息的最大时长，默认是 5 分钟。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>partition.assignment.strategy</td>
<td>消费者分区分配策略 ， 默认策略是Range + CooperativeSticky。Kafka 可以同时使用多个分区分配策略。可以选择的策略包括 ： Range 、 RoundRobin 、 Sticky 、CooperativeSticky</td>
</tr>
</tbody></table>
<h3 id="5-4-1-Range以及再平衡"><a href="#5-4-1-Range以及再平衡" class="headerlink" title="5.4.1 Range以及再平衡"></a>5.4.1 Range以及再平衡</h3><p>（1）Range分区策略原理</p>
<p><img src="Snipaste_2023-10-28_12-54-44.png" alt="Snipaste_2023-10-28_12-54-44"></p>
<p>（2）Range分区分配策略案例</p>
<p>①修改主题 first 为 7 个分区</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 7</span><br></pre></td></tr></table></figure>

<p>注意：分区数可以增加，但是不能减少</p>
<p>②构建CustomConsumer，CustomConsumer1，CustomConsumer2为一个消费者组，组名为“test”，同时启动3个消费者</p>
<p>③启动 CustomProducer 生产者，发送 500 条消息，随机发送到不同的分区</p>
<p>说明：Kafka 默认的分区分配策略就是 Range + CooperativeSticky，所以不需要修改策略。</p>
<p>④观看 3 个消费者分别消费哪些分区的数据。</p>
<p><img src="Snipaste_2023-10-28_13-02-02.png" alt="Snipaste_2023-10-28_13-02-02"></p>
<p><img src="Snipaste_2023-10-28_13-02-23.png" alt="Snipaste_2023-10-28_13-02-23"></p>
<p><img src="Snipaste_2023-10-28_13-02-44.png" alt="Snipaste_2023-10-28_13-02-44"></p>
<p>（3）Range分区分配再平衡案例</p>
<p>①停止掉0号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。</p>
<p>1 号消费者：消费到 3、4 号分区数据。</p>
<p>2 号消费者：消费到 5、6 号分区数据。</p>
<p>0 号消费者的任务<strong>会整体被分配</strong>到 1 号消费者或者 2 号消费者。</p>
<p>说明：0 号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p>
<p>②再次重新发送消息观看结果（45s 以后）。</p>
<p>1 号消费者：消费到 0、1、2、3 号分区数据。</p>
<p>2 号消费者：消费到 4、5、6 号分区数据。</p>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照 range 方式分配。</p>
<p><img src="Snipaste_2023-10-28_13-11-39.png" alt="Snipaste_2023-10-28_13-11-39"></p>
<p><img src="Snipaste_2023-10-28_13-12-05.png" alt="Snipaste_2023-10-28_13-12-05"></p>
<h3 id="5-4-2-RoundRobin以及再平衡"><a href="#5-4-2-RoundRobin以及再平衡" class="headerlink" title="5.4.2 RoundRobin以及再平衡"></a>5.4.2 RoundRobin以及再平衡</h3><p>（1）RoundRobin分区策略原理</p>
<p><img src="Snipaste_2023-10-28_19-09-50.png" alt="Snipaste_2023-10-28_19-09-50"></p>
<p>（2）RoundRobin分区分配策略案例</p>
<p>依次在 CustomConsumer、CustomConsumer1、CustomConsumer2 三个消费者代码中修改分区分配策略为 RoundRobin</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改分区分配策略</span></span><br><span class="line">        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, <span class="string">&quot;org.apache.kafka.clients.consumer.RoundRobinAssignor&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>重启 3 个消费者，重复发送消息的步骤，观看分区结果</p>
<p><img src="Snipaste_2023-10-28_19-14-32.png" alt="Snipaste_2023-10-28_19-14-32"></p>
<p><img src="Snipaste_2023-10-28_19-14-56.png" alt="Snipaste_2023-10-28_19-14-56"></p>
<p><img src="Snipaste_2023-10-28_19-15-16.png" alt="Snipaste_2023-10-28_19-15-16"></p>
<p>（3）RoundRobin分区分配再平衡案例</p>
<p>①停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。</p>
<p>1 号消费者：消费到 2、5 号分区数据</p>
<p>2 号消费者：消费到 4、1 号分区数据</p>
<p>0 号消费者的任务会按照 RoundRobin 的方式，把数据轮询分成 0 、6 和 3 号分区数据，分别由 1 号消费者或者 2 号消费者消费。**(也就是0号分区和6号分区给某个消费者，3号分区给某个消费者)**</p>
<p>说明：0 号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p>
<p>②再次重新发送消息观看结果（45s 以后）</p>
<p>1 号消费者：消费到 0、2、4、6 号分区数据</p>
<p>2 号消费者：消费到 1、3、5 号分区数据</p>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照 RoundRobin 方式分配。</p>
<p><img src="Snipaste_2023-10-28_19-22-24.png" alt="Snipaste_2023-10-28_19-22-24"></p>
<p><img src="Snipaste_2023-10-28_19-22-51.png" alt="Snipaste_2023-10-28_19-22-51"></p>
<h3 id="5-4-3-Sticky以及再平衡"><a href="#5-4-3-Sticky以及再平衡" class="headerlink" title="5.4.3 Sticky以及再平衡"></a>5.4.3 Sticky以及再平衡</h3><p>（1）<strong>粘性分区定义：</strong>可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。</p>
<p>粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化。</p>
<p>（2）①修改分区分配策略为粘性</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改分区分配策略</span></span><br><span class="line">properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, <span class="string">&quot;org.apache.kafka.clients.consumer.StickyAssignor&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>②发送数据，可以看到会尽量保持分区的个数近似划分分区。3：2：2</p>
<p><img src="Snipaste_2023-10-28_19-30-36.png" alt="Snipaste_2023-10-28_19-30-36"></p>
<p><img src="Snipaste_2023-10-28_19-30-51.png" alt="Snipaste_2023-10-28_19-30-51"></p>
<p><img src="Snipaste_2023-10-28_19-31-07.png" alt="Snipaste_2023-10-28_19-31-07"></p>
<p>（3）<strong>Sticky</strong> <strong>分区分配再平衡案例</strong></p>
<p>①停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。</p>
<p>1 号消费者：消费到 4、5 号分区数据。</p>
<p>2 号消费者：消费到 3、6 号分区数据。</p>
<p>0 号消费者的任务会按照粘性规则，尽可能均衡的随机将0，1，2 号分区分成两份数据，分别由 1 号消费者或者 2 号消费者消费。</p>
<p>说明：0 号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p>
<p>②再次重新发送消息观看结果（45s 以后）。</p>
<p>1 号消费者：消费到 0、2、4、5 号分区数据。</p>
<p>2 号消费者：消费到 1、3、6 号分区数据。</p>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照粘性方式分配。</p>
<p><img src="Snipaste_2023-10-28_19-38-30.png" alt="Snipaste_2023-10-28_19-38-30"></p>
<p><img src="Snipaste_2023-10-28_19-38-48.png" alt="Snipaste_2023-10-28_19-38-48"></p>
<h2 id="5-5-offset位移"><a href="#5-5-offset位移" class="headerlink" title="5.5 offset位移"></a>5.5 offset位移</h2><h3 id="5-5-1-offset的默认维护位置"><a href="#5-5-1-offset的默认维护位置" class="headerlink" title="5.5.1 offset的默认维护位置"></a>5.5.1 offset的默认维护位置</h3><img src="Snipaste_2023-10-30_12-28-14.png" alt="Snipaste_2023-10-30_12-28-14" style="zoom:50%;">

<p>__consumer_offsets 主题里面采用 key 和 value 的方式存储数据。key 是 group.id+topic+分区号，value 就是当前 offset 的值。每隔一段时间，kafka 内部会对这个 topic 进行compact，也就是每个 group.id+topic+分区号就保留最新数据</p>
<p>（1）消费offset案例</p>
<p>①思想：__consumer_offsets 为 Kafka 中的 topic，那就可以通过消费者进行消费。</p>
<p>②在配置文件 config&#x2F;consumer.properties 中添加配置 exclude.internal.topics&#x3D;false，默认是 true，表示不能消费系统主题。为了查看该系统主题数据，所以该参数修改为 false。</p>
<p>③采用命令行方式，创建一个新的 topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --topic atguigu --partitions 2 --replication-factor 2</span><br></pre></td></tr></table></figure>

<p>④启动生产者往 atguigu 生产数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-producer.sh --topic atguigu --bootstrap-server hadoop102:9092</span><br></pre></td></tr></table></figure>

<p>⑤启动消费者消费 atguigu 数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 kafka]# bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic atguigu --group test</span><br></pre></td></tr></table></figure>

<p>注意：指定消费者组名称，更好观察数据存储位置（key 是 group.id+topic+分区号）。</p>
<p>⑥查看消费者消费主题__consumer_offsets。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# bin/kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server hadoop102:9092 --consumer.config config/consumer.properties --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --from-beginning</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[test,atguigu,1]::OffsetAndMetadata(offset=2, leaderEpoch=Optional[0], metadata=, commitTimestamp=1698641012931, expireTimestamp=None)</span><br><span class="line">[test,atguigu,0]::OffsetAndMetadata(offset=2, leaderEpoch=Optional[0], metadata=, commitTimestamp=1698641012931, expireTimestamp=None)</span><br></pre></td></tr></table></figure>

<h3 id="5-5-2-自动提交offset"><a href="#5-5-2-自动提交offset" class="headerlink" title="5.5.2 自动提交offset"></a>5.5.2 自动提交offset</h3><img src="Snipaste_2023-10-30_12-53-15.png" alt="Snipaste_2023-10-30_12-53-15" style="zoom:50%;">

<img src="Snipaste_2023-10-30_12-53-35.png" alt="Snipaste_2023-10-30_12-53-35" style="zoom:50%;">

<p>（1）消费者自动提交offset</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerAutoOffset</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.创建消费者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给消费者配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置反序列化 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意起名） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 是否自动提交 offset</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">true</span>);</span><br><span class="line">        <span class="comment">// 提交 offset 的时间周期 1000ms，默认 5s</span></span><br><span class="line">        properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line"><span class="comment">//        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, &quot;org.apache.kafka.clients.consumer.RoundRobinAssignor&quot;);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line"><span class="comment">//        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, &quot;org.apache.kafka.clients.consumer.StickyAssignor&quot;);</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 订阅要消费的主题的所有分区（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置 1s 中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 打印消费到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-5-3-手动提交offset"><a href="#5-5-3-手动提交offset" class="headerlink" title="5.5.3 手动提交offset"></a>5.5.3 手动提交offset</h3><p><img src="Snipaste_2023-10-30_13-10-41.png" alt="Snipaste_2023-10-30_13-10-41"></p>
<p>（1）同步提交offset</p>
<p>由于同步提交 offset 有失败重试机制，故更加可靠，但是由于一直等待提交结果，提交的效率比较低。以下为同步提交 offset 的示例。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerByHandSync</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.创建消费者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给消费者配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置反序列化 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意起名） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 是否自动提交 offset</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line"><span class="comment">//        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, &quot;org.apache.kafka.clients.consumer.RoundRobinAssignor&quot;);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line"><span class="comment">//        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, &quot;org.apache.kafka.clients.consumer.StickyAssignor&quot;);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 订阅要消费的主题的所有分区（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置 1s 中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 打印消费到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 同步提交 offset</span></span><br><span class="line">            kafkaConsumer.commitSync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）异步提交pffset</p>
<p>虽然同步提交 offset 更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会受到很大的影响。因此更多的情况下，会选用异步提交 offset 的方式。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerByHandAsync</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.创建消费者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给消费者配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置反序列化 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意起名） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 是否自动提交 offset</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line"><span class="comment">//        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, &quot;org.apache.kafka.clients.consumer.RoundRobinAssignor&quot;);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line"><span class="comment">//        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, &quot;org.apache.kafka.clients.consumer.StickyAssignor&quot;);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 订阅要消费的主题的所有分区（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置 1s 中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 打印消费到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 异步提交 offset</span></span><br><span class="line">            kafkaConsumer.commitAsync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-5-4-指定Pffset消费"><a href="#5-5-4-指定Pffset消费" class="headerlink" title="5.5.4 指定Pffset消费"></a>5.5.4 指定Pffset消费</h3><p>auto.offset.reset &#x3D; earliest | latest | none 默认是 latest。</p>
<p>当 Kafka 中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量时（例如该数据已被删除），该怎么办？</p>
<p>（1）earliest：自动将偏移量重置为最早的偏移量，–from-beginning。</p>
<p>（2）latest（默认值）：自动将偏移量重置为最新偏移量</p>
<p>（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。</p>
<img src="Snipaste_2023-10-30_13-39-43.png" alt="Snipaste_2023-10-30_13-39-43" style="zoom:50%;">

<p>（4）任意指定 offset 位移开始消费</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerSeek</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 0 配置信息</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        <span class="comment">// 连接</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key value 反序列化</span></span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test3&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 创建一个消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 订阅一个主题</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定位置进行消费</span></span><br><span class="line">        Set&lt;TopicPartition&gt; assignment= <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>) &#123;</span><br><span class="line">            kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 获取消费者分区分配信息（有了分区分配信息才能开始消费）</span></span><br><span class="line">            assignment = kafkaConsumer.assignment();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 遍历所有分区，并指定 offset 从 1700 的位置开始消费</span></span><br><span class="line">        <span class="keyword">for</span> (TopicPartition tp: assignment) &#123;</span><br><span class="line">            kafkaConsumer.seek(tp, <span class="number">1700</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 消费该主题数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意：每次执行完，需要修改消费者组名</p>
<h3 id="5-5-5-指定时间消费"><a href="#5-5-5-指定时间消费" class="headerlink" title="5.5.5 指定时间消费"></a>5.5.5 指定时间消费</h3><p>需求：在生产环境中，会遇到最近消费的几个小时数据异常，想重新按照时间消费。例如要求按照时间消费前一天的数据，怎么处理？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerSeekTime</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 0 配置信息</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        <span class="comment">// 连接</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key value 反序列化</span></span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test4&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 创建一个消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 订阅一个主题</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        Set&lt;TopicPartition&gt; assignment= <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>) &#123;</span><br><span class="line">            kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 获取消费者分区分配信息（有了分区分配信息才能开始消费）</span></span><br><span class="line">            assignment = kafkaConsumer.assignment();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;TopicPartition, Long&gt; timestampToSearch = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        <span class="comment">// 封装集合存储，每个分区对应一天前的数据</span></span><br><span class="line">        <span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">            timestampToSearch.put(topicPartition, System.currentTimeMillis() - <span class="number">1</span> * <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 获取从 1 天前开始消费的每个分区的 offset</span></span><br><span class="line">        Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsets = kafkaConsumer.offsetsForTimes(timestampToSearch);</span><br><span class="line">        <span class="comment">// 遍历每个分区，对每个分区设置消费时间。</span></span><br><span class="line">        <span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">            <span class="type">OffsetAndTimestamp</span> <span class="variable">offsetAndTimestamp</span> <span class="operator">=</span> offsets.get(topicPartition);</span><br><span class="line">            <span class="comment">// 根据时间指定开始消费的位置</span></span><br><span class="line">            <span class="keyword">if</span> (offsetAndTimestamp != <span class="literal">null</span>)&#123;</span><br><span class="line">                kafkaConsumer.seek(topicPartition, offsetAndTimestamp.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 消费该主题数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-5-6-漏消费和重复消费"><a href="#5-5-6-漏消费和重复消费" class="headerlink" title="5.5.6 漏消费和重复消费"></a>5.5.6 漏消费和重复消费</h3><p><strong>重复消费：</strong>已经消费了数据，但是 offset 没提交。消费快，offset提交慢</p>
<p><strong>漏消费：</strong>先提交 offset 后消费，有可能会造成数据的漏消费。消费慢，offset提交快</p>
<img src="Snipaste_2023-10-30_14-11-06.png" alt="Snipaste_2023-10-30_14-11-06" style="zoom:50%;">

<h2 id="5-6-消费者事务"><a href="#5-6-消费者事务" class="headerlink" title="5.6 消费者事务"></a>5.6 消费者事务</h2><img src="Snipaste_2023-10-30_14-23-01.png" alt="Snipaste_2023-10-30_14-23-01" style="zoom:50%;">

<h2 id="5-7-生产经验——数据积压（消费者如何提高吞吐量）"><a href="#5-7-生产经验——数据积压（消费者如何提高吞吐量）" class="headerlink" title="5.7 生产经验——数据积压（消费者如何提高吞吐量）"></a>5.7 生产经验——数据积压（消费者如何提高吞吐量）</h2><p><img src="Snipaste_2023-10-30_14-25-03.png" alt="Snipaste_2023-10-30_14-25-03"></p>
<img src="webwxgetmsgimg (11).jpg" alt="webwxgetmsgimg (11)" style="zoom:33%;">

<img src="Snipaste_2023-10-30_14-36-56.png" alt="Snipaste_2023-10-30_14-36-56" style="zoom:50%;">

<h1 id="第六章-Kafka-Eagle监控"><a href="#第六章-Kafka-Eagle监控" class="headerlink" title="第六章 Kafka-Eagle监控"></a>第六章 Kafka-Eagle监控</h1><p>Kafka-Eagle 框架可以监控 Kafka 集群的整体运行情况，在生产环境中经常使用。</p>
<h2 id="6-1-MySQL环境准备"><a href="#6-1-MySQL环境准备" class="headerlink" title="6.1 MySQL环境准备"></a>6.1 MySQL环境准备</h2><p>之前Hive框架学习笔记中有</p>
<h2 id="6-2-Kafka环境准备"><a href="#6-2-Kafka环境准备" class="headerlink" title="6.2 Kafka环境准备"></a>6.2 Kafka环境准备</h2><p>（1）关闭Kafka集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# kf.sh stop</span><br></pre></td></tr></table></figure>

<p>（2）修改&#x2F;opt&#x2F;module&#x2F;kafka&#x2F;bin&#x2F;kafka-sever-start.sh命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka]# vim bin/kafka-server-start.sh</span><br></pre></td></tr></table></figure>

<p>修改为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then</span><br><span class="line"> 	export KAFKA_HEAP_OPTS=&quot;-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70&quot;</span><br><span class="line"> 	export JMX_PORT=&quot;9999&quot;</span><br><span class="line"><span class="meta prompt_"> 	#</span><span class="language-bash"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">&quot;-Xmx1G -Xms1G&quot;</span></span></span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>（3）分发到其他节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# xsync kafka-server-start.sh</span><br></pre></td></tr></table></figure>

<p>（4）启动Kafka集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# kf.sh start</span><br></pre></td></tr></table></figure>

<h2 id="6-3-Kafka-Eagle安装"><a href="#6-3-Kafka-Eagle安装" class="headerlink" title="6.3 Kafka-Eagle安装"></a>6.3 Kafka-Eagle安装</h2><p>（1）上传压缩包到集群&#x2F;opt&#x2F;software目录</p>
<p>（2）解压到本地</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf kafka-eagle-bin-2.0.8.tar.gz</span><br></pre></td></tr></table></figure>

<p>（3）进入刚才解压的目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka-eagle-bin-2.0.8]# ll</span><br><span class="line">总用量 79164</span><br><span class="line">-rw-rw-r-- 1 root root 81062577 10月 13 2021 efak-web-2.0.8-bin.tar.gz</span><br></pre></td></tr></table></figure>

<p>（4）将efak-web-2.0.8-bin.tar.gz解压到&#x2F;opt&#x2F;module</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka-eagle-bin-2.0.8]# tar -zxvf efak-web-2.0.8-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>（5）修改名称</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# mv efak-web-2.0.8/ efak</span><br></pre></td></tr></table></figure>

<p>（6）修改配置文件&#x2F;opt&#x2F;module&#x2F;efak&#x2F;conf&#x2F;system-config.properties</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 conf]# vim system-config.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">multi zookeeper &amp; kafka cluster list</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Settings prefixed with <span class="string">&#x27;kafka.eagle.&#x27;</span> will be deprecated, use <span class="string">&#x27;efak.&#x27;</span></span> </span><br><span class="line">instead</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">efak.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">zookeeper <span class="built_in">enable</span> acl</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">cluster1.zk.acl.enable=false</span><br><span class="line">cluster1.zk.acl.schema=digest</span><br><span class="line">cluster1.zk.acl.username=test</span><br><span class="line">cluster1.zk.acl.password=test123</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">broker size online list</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">cluster1.efak.broker.size=20</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">zk client thread <span class="built_in">limit</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">kafka.zk.limit.size=32</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">EFAK webui port</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">efak.webui.port=8048</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafka jmx acl and ssl authenticate</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">cluster1.efak.jmx.acl=false</span><br><span class="line">cluster1.efak.jmx.user=keadmin</span><br><span class="line">cluster1.efak.jmx.password=keadmin123</span><br><span class="line">cluster1.efak.jmx.ssl=false</span><br><span class="line">cluster1.efak.jmx.truststore.location=/data/ssl/certificates/kafka.truststor</span><br><span class="line">e</span><br><span class="line">cluster1.efak.jmx.truststore.password=ke123456</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafka offset storage</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">offset 保存在 kafka</span></span><br><span class="line">cluster1.efak.offset.storage=kafka</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafka jmx uri</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">cluster1.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://%s/jmxrmi</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafka metrics, 15 days by default</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">efak.metrics.charts=true</span><br><span class="line">efak.metrics.retain=15</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafka sql topic records max</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">efak.sql.topic.records.max=5000</span><br><span class="line">efak.sql.topic.preview.records.max=10</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">delete kafka topic token</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">efak.topic.token=keadmin</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafka sasl authenticate</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">cluster1.efak.sasl.enable=false</span><br><span class="line">cluster1.efak.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">cluster1.efak.sasl.mechanism=SCRAM-SHA-256</span><br><span class="line">cluster1.efak.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramL</span><br><span class="line">oginModule required username=&quot;kafka&quot; password=&quot;kafka-eagle&quot;;</span><br><span class="line">cluster1.efak.sasl.client.id=</span><br><span class="line">cluster1.efak.blacklist.topics=</span><br><span class="line">cluster1.efak.sasl.cgroup.enable=false</span><br><span class="line">cluster1.efak.sasl.cgroup.topics=</span><br><span class="line">cluster2.efak.sasl.enable=false</span><br><span class="line">cluster2.efak.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">cluster2.efak.sasl.mechanism=PLAIN</span><br><span class="line">cluster2.efak.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainL</span><br><span class="line">oginModule required username=&quot;kafka&quot; password=&quot;kafka-eagle&quot;;</span><br><span class="line">cluster2.efak.sasl.client.id=</span><br><span class="line">cluster2.efak.blacklist.topics=</span><br><span class="line">cluster2.efak.sasl.cgroup.enable=false</span><br><span class="line">cluster2.efak.sasl.cgroup.topics=</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafka ssl authenticate</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line">cluster3.efak.ssl.enable=false</span><br><span class="line">cluster3.efak.ssl.protocol=SSL</span><br><span class="line">cluster3.efak.ssl.truststore.location=</span><br><span class="line">cluster3.efak.ssl.truststore.password=</span><br><span class="line">cluster3.efak.ssl.keystore.location=</span><br><span class="line">cluster3.efak.ssl.keystore.password=</span><br><span class="line">cluster3.efak.ssl.key.password=</span><br><span class="line">cluster3.efak.ssl.endpoint.identification.algorithm=https</span><br><span class="line">cluster3.efak.blacklist.topics=</span><br><span class="line">cluster3.efak.ssl.cgroup.enable=false</span><br><span class="line">cluster3.efak.ssl.cgroup.topics=</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafka sqlite jdbc driver address</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置 mysql 连接</span></span><br><span class="line">efak.driver=com.mysql.jdbc.Driver</span><br><span class="line">efak.url=jdbc:mysql://hadoop102:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span><br><span class="line">efak.username=root</span><br><span class="line">efak.password=wy****1*****18</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafka mysql jdbc driver address</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">efak.driver=com.mysql.cj.jdbc.Driver</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">efak.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=<span class="literal">true</span>&amp;characterEncoding=U</span></span><br><span class="line">TF-8&amp;zeroDateTimeBehavior=convertToNull</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">efak.username=root</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">efak.password=123456</span></span><br></pre></td></tr></table></figure>

<p>（7）添加环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 conf]# vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafkaEFAK</span></span><br><span class="line">export KE_HOME=/opt/module/efak</span><br><span class="line">export PATH=$PATH:$KE_HOME/bin</span><br><span class="line"></span><br><span class="line">[root@hadoop102 conf]# source /etc/profile</span><br></pre></td></tr></table></figure>

<p>（8）启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 efak]# bin/ke.sh start</span><br><span class="line"></span><br><span class="line">[2023-10-30 15:34:55] INFO: [Job done!]</span><br><span class="line">Welcome to</span><br><span class="line">    ______    ______    ___     __ __</span><br><span class="line">   / ____/   / ____/   /   |   / //_/</span><br><span class="line">  / __/     / /_      / /| |  / ,&lt;   </span><br><span class="line"> / /___    / __/     / ___ | / /| |  </span><br><span class="line">/_____/   /_/       /_/  |_|/_/ |_|  </span><br><span class="line">( Eagle For Apache Kafka® )</span><br><span class="line"></span><br><span class="line">Version 2.0.8 -- Copyright 2016-2021</span><br><span class="line">*******************************************************************</span><br><span class="line">* EFAK Service has started success.</span><br><span class="line">* Welcome, Now you can visit &#x27;http://192.168.255.102:8048&#x27;</span><br><span class="line">* Account:admin ,Password:123456</span><br><span class="line">*******************************************************************</span><br><span class="line">* &lt;Usage&gt; ke.sh [start|status|stop|restart|stats] &lt;/Usage&gt;</span><br><span class="line">* &lt;Usage&gt; https://www.kafka-eagle.org/ &lt;/Usage&gt;</span><br><span class="line">*******************************************************************</span><br></pre></td></tr></table></figure>

<p>说明：如果停止 efak，执行命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 efak]# bin/ke.sh stop</span><br></pre></td></tr></table></figure>

<h2 id="6-4-Kafka-Eagle页面操作"><a href="#6-4-Kafka-Eagle页面操作" class="headerlink" title="6.4 Kafka-Eagle页面操作"></a>6.4 Kafka-Eagle页面操作</h2><p>（1）<strong>登录页面查看监控数据</strong></p>
<p><a target="_blank" rel="noopener" href="http://192.168.255.102:8048/">http://192.168.255.102:8048</a></p>
<p>Account:admin ,Password:123456</p>
<p><img src="Snipaste_2023-10-30_15-37-45.png" alt="Snipaste_2023-10-30_15-37-45"></p>
<p>查看BROKERS详情</p>
<p><img src="Snipaste_2023-10-30_15-39-12.png" alt="Snipaste_2023-10-30_15-39-12"></p>
<p><img src="Snipaste_2023-10-30_15-39-54.png" alt="Snipaste_2023-10-30_15-39-54"></p>
<p>查看TOPICS详情</p>
<p><img src="Snipaste_2023-10-30_15-40-53.png" alt="Snipaste_2023-10-30_15-40-53"></p>
<p><img src="Snipaste_2023-10-30_15-41-21.png" alt="Snipaste_2023-10-30_15-41-21"></p>
<p>查看ZOOKEEPERS详情</p>
<p><img src="Snipaste_2023-10-30_15-42-12.png" alt="Snipaste_2023-10-30_15-42-12"></p>
<p>查看CONSUMERGROUPS详情</p>
<p><img src="Snipaste_2023-10-30_15-43-13.png" alt="Snipaste_2023-10-30_15-43-13"></p>
<p>大屏展示：</p>
<p><img src="Snipaste_2023-10-30_15-46-35.png" alt="Snipaste_2023-10-30_15-46-35"></p>
<h1 id="第七章-Kafka-Kraft模式"><a href="#第七章-Kafka-Kraft模式" class="headerlink" title="第七章 Kafka-Kraft模式"></a>第七章 Kafka-Kraft模式</h1><h2 id="7-1-Kafka-Kraft架构"><a href="#7-1-Kafka-Kraft架构" class="headerlink" title="7.1 Kafka-Kraft架构"></a>7.1 Kafka-Kraft架构</h2><img src="Snipaste_2023-10-30_15-54-00.png" alt="Snipaste_2023-10-30_15-54-00" style="zoom:50%;">

<p>左图为 Kafka 现有架构，元数据在 zookeeper 中，运行时动态选举 controller，由controller 进行 Kafka 集群管理。右图为 kraft 模式架构（实验性），不再依赖 zookeeper 集群，而是用三台 controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进行 Kafka 集群管理。</p>
<p>这样做的好处有以下几个：</p>
<ul>
<li>Kafka 不再依赖外部框架，而是能够独立运行；</li>
<li>controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升；</li>
<li>由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制；</li>
<li>controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策。</li>
</ul>
<h2 id="7-2-Kafka-Kraft集群部署"><a href="#7-2-Kafka-Kraft集群部署" class="headerlink" title="7.2 Kafka-Kraft集群部署"></a>7.2 Kafka-Kraft集群部署</h2><p>（0）关闭kafka集群和zookeeper集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# kf.sh stop</span><br><span class="line">[root@hadoop102 module]# zk.sh stop</span><br></pre></td></tr></table></figure>

<p>（1）再次解压一份 kafka 安装包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf kafka_2.12-3.0.0.tgz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>（2）重命名为kafka2</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# mv kafka_2.12-3.0.0/ kafka2</span><br></pre></td></tr></table></figure>

<p>（3）在 hadoop102 上修改&#x2F;opt&#x2F;module&#x2F;kafka2&#x2F;config&#x2F;kraft&#x2F;server.properties 配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# cd kafka2/config/kraft/</span><br><span class="line">[root@hadoop102 kraft]# vim server.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka 的角色（controller 相当于主机、broker 节点相当于从机，主机类似 zk 功能）</span></span><br><span class="line">process.roles=broker, controller</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">节点 ID</span></span><br><span class="line">node.id=2</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">controller 服务协议别名</span></span><br><span class="line">controller.listener.names=CONTROLLER</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">全 Controller 列表</span></span><br><span class="line">controller.quorum.voters=2@hadoop102:9093,3@hadoop103:9093,4@hadoop104:9093</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">不同服务器绑定的端口</span></span><br><span class="line">listeners=PLAINTEXT://:9092,CONTROLLER://:9093</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">broker 服务协议别名</span></span><br><span class="line">inter.broker.listener.name=PLAINTEXT</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">broker 对外暴露的地址</span></span><br><span class="line">advertised.Listeners=PLAINTEXT://hadoop102:9092</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">协议别名到安全协议的映射</span></span><br><span class="line">listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLA</span><br><span class="line">INTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka 数据存储目录</span></span><br><span class="line">log.dirs=/opt/module/kafka2/data</span><br></pre></td></tr></table></figure>

<p>（4）分发kafka2</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# xsync kafka2/</span><br></pre></td></tr></table></figure>

<ul>
<li>在 hadoop103 和 hadoop104 上 需 要 对 node.id 相应改变 ， 值 需 要 和controller.quorum.voters 对应。</li>
<li>在 hadoop103 和 hadoop104 上需要 根据各自的主机名称，修改相应的advertised.Listeners 地址。</li>
</ul>
<p>（5）初始化集群数据目录</p>
<p>①首先生成存储目录唯一ID</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka2]# bin/kafka-storage.sh random-uuid</span><br><span class="line">fmWatWf3SPe8m3ghz_RndA</span><br></pre></td></tr></table></figure>

<p>②用该 ID 格式化 kafka 存储目录（三台节点）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[rootu@hadoop102 kafka2]# bin/kafka-storage.sh format -t fmWatWf3SPe8m3ghz_RndA -c /opt/module/kafka2/config/kraft/server.properties</span><br><span class="line">[root@hadoop103 kafka2]# bin/kafka-storage.sh format -t fmWatWf3SPe8m3ghz_RndA -c /opt/module/kafka2/config/kraft/server.properties</span><br><span class="line">[root@hadoop104 kafka2]# bin/kafka-storage.sh format -t fmWatWf3SPe8m3ghz_RndA -c /opt/module/kafka2/config/kraft/server.properties</span><br></pre></td></tr></table></figure>

<p>（6）启动kafka集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka2]# bin/kafka-server-start.sh -daemon config/kraft/server.properties</span><br><span class="line">[root@hadoop103 kafka2]# bin/kafka-server-start.sh -daemon config/kraft/server.properties</span><br><span class="line">[root@hadoop104 kafka2]# bin/kafka-server-start.sh -daemon config/kraft/server.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka2]# xcall.sh jps</span><br><span class="line">-----------hadoop102---------------</span><br><span class="line">16945 Kafka</span><br><span class="line">17080 Jps</span><br><span class="line">-----------hadoop103---------------</span><br><span class="line">8121 Kafka</span><br><span class="line">8234 Jps</span><br><span class="line">-----------hadoop104---------------</span><br><span class="line">8192 Jps</span><br><span class="line">8087 Kafka</span><br></pre></td></tr></table></figure>

<p>（7）关闭kafka集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 kafka2]# bin/kafka-server-stop.sh</span><br><span class="line">[root@hadoop103 kafka2]# bin/kafka-server-stop.sh</span><br><span class="line">[root@hadoop104 kafka2]# bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<h2 id="7-3-Kafka-Kraft集群启停脚本"><a href="#7-3-Kafka-Kraft集群启停脚本" class="headerlink" title="7.3 Kafka-Kraft集群启停脚本"></a>7.3 Kafka-Kraft集群启停脚本</h2><p>（1）~&#x2F;bin 目录下创建文件 kf2.sh 脚本文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# vim kf2.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">     for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">     do</span><br><span class="line">         echo &quot; --------启动 $i Kafka2-------&quot;</span><br><span class="line">         ssh $i &quot;/opt/module/kafka2/bin/kafka-server-start.sh -daemon /opt/module/kafka2/config/kraft/server.properties&quot;</span><br><span class="line">     done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">     for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">     do</span><br><span class="line">         echo &quot; --------停止 $i Kafka2-------&quot;</span><br><span class="line">         ssh $i &quot;/opt/module/kafka2/bin/kafka-server-stop.sh &quot;</span><br><span class="line">     done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）添加执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# chmod 777 kf2.sh</span><br></pre></td></tr></table></figure>

<p>（3）集群启动和停止</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 bin]# kf2.sh start</span><br><span class="line">[root@hadoop102 bin]# kf2.sh stop</span><br></pre></td></tr></table></figure>


      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">Linux</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/10/20/kafka%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-flume框架学习笔记" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/20/flume%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">flume框架学习笔记</a>
    </h1>
  

        
        <a href="/2023/10/20/flume%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="archive-article-date">
  	<time datetime="2023-10-20T06:31:48.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2023-10-20</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第一章-Flume概述"><a href="#第一章-Flume概述" class="headerlink" title="第一章 Flume概述"></a>第一章 Flume概述</h1><h2 id="1-1-Flume定义"><a href="#1-1-Flume定义" class="headerlink" title="1.1 Flume定义"></a>1.1 Flume定义</h2><p>Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume 基于流式架构，灵活简单。</p>
<img src="Snipaste_2023-10-20_15-24-23.png" alt="Snipaste_2023-10-20_15-24-23" style="zoom:43%;">

<h2 id="1-2-Flume基础框架"><a href="#1-2-Flume基础框架" class="headerlink" title="1.2 Flume基础框架"></a>1.2 Flume基础框架</h2><img src="Snipaste_2023-10-20_15-37-57.png" alt="Snipaste_2023-10-20_15-37-57" style="zoom:43%;">

<p>Flume整体上是<strong>Source-Channel-Sink</strong>的三层架构。</p>
<p>Flume以<strong>Agent</strong>为最小独立运行单元，一个Agent就是一个JVM，它以事件event的形式将数据从源头送至目的，单个Agent由Source、Channel、Sink三大组件组成。</p>
<h3 id="1-2-1-Source"><a href="#1-2-1-Source" class="headerlink" title="1.2.1 Source"></a>1.2.1 Source</h3><p>Source 是<strong>负责接收数据到 Flume Agent 的组件</strong>。Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、taildir、sequence generator、syslog、http、legacy。</p>
<h3 id="1-2-2-Sink"><a href="#1-2-2-Sink" class="headerlink" title="1.2.2 Sink"></a>1.2.2 Sink</h3><p>Sink <strong>不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent</strong>。</p>
<p>Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。</p>
<h3 id="1-2-3-Channel"><a href="#1-2-3-Channel" class="headerlink" title="1.2.3 Channel"></a>1.2.3 Channel</h3><p>Channel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在<strong>不同的速率</strong>上。Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个Sink 的读取操作。</p>
<p>Flume 自带两种 Channel：<strong>Memory Channel</strong> 和 <strong>File Channel</strong>。</p>
<p>Memory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p>
<p>File Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</p>
<h3 id="1-2-4-Event"><a href="#1-2-4-Event" class="headerlink" title="1.2.4 Event"></a>1.2.4 Event</h3><p>传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。Event 由 <strong>Header</strong> 和 <strong>Body</strong> 两部分组成，Header 用来存放该 event 的一些属性，为 K-V 结构，报头Header容纳的k-v信息是为了给数据增加标识，用于跟踪发送事件的优先级和重要性，用户可以通过拦截器进行修改。Body 用来存放该条数据，形式为字节数组。</p>
<img src="Snipaste_2023-10-20_15-57-12.png" alt="Snipaste_2023-10-20_15-57-12" style="zoom:43%;">

<h1 id="第二章-Flume入门"><a href="#第二章-Flume入门" class="headerlink" title="第二章 Flume入门"></a>第二章 Flume入门</h1><h2 id="2-1-Flume安装部署"><a href="#2-1-Flume安装部署" class="headerlink" title="2.1 Flume安装部署"></a>2.1 Flume安装部署</h2><h3 id="2-1-1-安装地址"><a href="#2-1-1-安装地址" class="headerlink" title="2.1.1 安装地址"></a>2.1.1 安装地址</h3><h3 id="2-1-2-安装部署"><a href="#2-1-2-安装部署" class="headerlink" title="2.1.2 安装部署"></a>2.1.2 安装部署</h3><p>（1）将 apache-flume-1.9.0-bin.tar.gz 上传到 linux 的&#x2F;opt&#x2F;software 目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# ll</span><br><span class="line">总用量 1540044</span><br><span class="line">-rw-r--r--. 1 root root  67938106 8月  13 2021 apache-flume-1.9.0-bin.tar.gz   # 在这里</span><br><span class="line">-rw-r--r--. 1 root root 356079876 12月  2 2022 apache-hive-3.1.3-bin.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root   9311744 5月  20 2021 apache-zookeeper-3.5.7-bin.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root       221 9月  19 22:17 edits.xml</span><br><span class="line">-rw-r--r--. 1 root root     19367 9月  19 20:11 fsimage.xml</span><br><span class="line">-rw-r--r--. 1 root root 338075860 8月   5 22:31 hadoop-3.1.3.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root 195013152 8月   5 22:11 jdk-8u212-linux-x64.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root 609556480 12月  2 2022 mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line">-rw-r--r--. 1 root root    985600 12月  2 2022 mysql-connector-java-5.1.37.jar</span><br><span class="line">drwxr-xr-x. 2 root root      4096 10月  7 15:30 mysql_lib</span><br></pre></td></tr></table></figure>

<p>（2）解压 apache-flume-1.9.0-bin.tar.gz 到&#x2F;opt&#x2F;module&#x2F;目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# tar -zxvf /opt/software/apache-flume-1.9.0-bin.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop102 software]# cd /opt/module/</span><br><span class="line">[root@hadoop102 module]# ll</span><br><span class="line">总用量 4</span><br><span class="line">drwxr-xr-x.  7 root root  187 10月 20 16:04 apache-flume-1.9.0-bin  # 在这里</span><br><span class="line">drwxr-xr-x.  2 root root   99 10月 10 13:38 data</span><br><span class="line">drwxr-xr-x.  3 root root   62 10月  9 13:35 datas</span><br><span class="line">drwxr-xr-x. 13 wyh  wyh  4096 9月  26 12:55 hadoop-3.1.3</span><br><span class="line">drwxr-xr-x. 12 root root  243 10月  8 13:38 hive</span><br><span class="line">drwxr-xr-x.  7   10  143  245 4月   2 2019 jdk1.8.0_212</span><br><span class="line">drwxr-xr-x.  8 root root  160 10月 19 12:57 zookeeper-3.5.7</span><br></pre></td></tr></table></figure>

<p>（3）修改 apache-flume-1.9.0-bin 的名称为 flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# mv /opt/module/apache-flume-1.9.0-bin/ /opt/module/flume</span><br><span class="line">[root@hadoop102 module]# ll</span><br><span class="line">总用量 4</span><br><span class="line">drwxr-xr-x.  2 root root   99 10月 10 13:38 data</span><br><span class="line">drwxr-xr-x.  3 root root   62 10月  9 13:35 datas</span><br><span class="line">drwxr-xr-x.  7 root root  187 10月 20 16:04 flume    # 在这里</span><br><span class="line">drwxr-xr-x. 13 wyh  wyh  4096 9月  26 12:55 hadoop-3.1.3</span><br><span class="line">drwxr-xr-x. 12 root root  243 10月  8 13:38 hive</span><br><span class="line">drwxr-xr-x.  7   10  143  245 4月   2 2019 jdk1.8.0_212</span><br><span class="line">drwxr-xr-x.  8 root root  160 10月 19 12:57 zookeeper-3.5.7</span><br></pre></td></tr></table></figure>

<p>（4）将 lib 文件夹下的 guava-11.0.2.jar 删除以兼容 Hadoop 3.1.3</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# rm /opt/module/flume/lib/guava-11.0.2.jar </span><br><span class="line">rm：是否删除普通文件 &quot;/opt/module/flume/lib/guava-11.0.2.jar&quot;？y</span><br></pre></td></tr></table></figure>

<p>（5）将flume&#x2F;conf目录下的flume-env.sh.template文件名称修改为flume-env.sh，并配置flume-env.sh文件，在配置文件中增加JAVA_HOME路径；并修改内存最大为4G</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 conf]# cp flume-env.sh.template flume-env.sh</span><br><span class="line">[root@hadoop102 conf]# vim flume-env.sh</span><br><span class="line"></span><br><span class="line">export JAVA_OPTS=&quot;-Xms100m -Xmx4000m -Dcom.sun.management.jmxremote&quot;</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br></pre></td></tr></table></figure>

<p>（6）修改logs目录，并打印到控制台一份（学习测试用）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 conf]# vim log4j.properties </span><br><span class="line"></span><br><span class="line">flume.root.logger=INFO,LOGFILE,console</span><br><span class="line">flume.log.dir=/opt/module/flume/logs # 原来是./logs</span><br></pre></td></tr></table></figure>

<p>（7）将配置好的flume文件分发到集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# xsync flume/</span><br></pre></td></tr></table></figure>

<h2 id="2-2-Flume入门案例"><a href="#2-2-Flume入门案例" class="headerlink" title="2.2 Flume入门案例"></a>2.2 Flume入门案例</h2><h3 id="2-2-1-监控端口数据官方案例"><a href="#2-2-1-监控端口数据官方案例" class="headerlink" title="2.2.1 监控端口数据官方案例"></a>2.2.1 监控端口数据官方案例</h3><p>需求：使用 Flume 监听一个端口，收集该端口数据，并打印到控制台。</p>
<p>分析：</p>
<p><img src="Snipaste_2023-10-20_16-15-53.png" alt="Snipaste_2023-10-20_16-15-53"></p>
<p>步骤：</p>
<p>（0）安装netcat工具：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# yum install -y nc</span><br></pre></td></tr></table></figure>

<p>（1）测试一下netcat</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在hadoop102上开启端口号为9999的nc服务端</span></span><br><span class="line">[root@hadoop102 flume]# nc -lk 9999</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在hadoop102的另一个窗口上开启端口号为9999的nc客户端</span></span><br><span class="line">[root@hadoop102 ~]# nc localhost 999</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在服务端输入数据在客户端就可以接收到，当然在客户端输入数据在服务端也可以接收到</span></span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-10-20_16-28-12.png" alt="Snipaste_2023-10-20_16-28-12"></p>
<p><img src="Snipaste_2023-10-20_16-29-40.png" alt="Snipaste_2023-10-20_16-29-40"></p>
<p>（2）判断 44444 端口是否被占用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]#  netstat -nlp | grep 44444</span><br><span class="line">[root@hadoop102 flume]# </span><br></pre></td></tr></table></figure>

<p>（3）在 flume 目录下创建 job 文件夹并进入 job 文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# mkdir job</span><br><span class="line">[root@hadoop102 flume]#  cd job/</span><br></pre></td></tr></table></figure>

<p>（4）在 job 文件夹下创建 Flume Agent 配置文件 flume-netcat-logger.conf。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]#  vim flume-netcat-logger.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent   	<span class="comment"># a1:表示agent的名称</span></span></span><br><span class="line">a1.sources = r1   	# r1表示a1的Source的名称</span><br><span class="line">a1.sinks = k1     	# k1表示a1的Sink的名称</span><br><span class="line">a1.channels = c1  	# c1表示a1的Channel的名称</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat		# 表示a1的输入源类型为netcat端口类型</span><br><span class="line">a1.sources.r1.bind = localhost     # 表示a1的监听的主机</span><br><span class="line">a1.sources.r1.port = 44444         # 表示a1的监听的端口号</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger         # 表示a1的输出目的地是控制台logger类型</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory      # 表示a1的channel类型是memory内存型</span><br><span class="line">a1.channels.c1.capacity = 1000    # 表示a1的channel总容量1000个event</span><br><span class="line">a1.channels.c1.transactionCapacity = 100      # 表示a1的channel传输时收集到了100条event以后再去提交事务</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1     # 表示将r1和c1连接起来</span><br><span class="line">a1.sinks.k1.channel = c1        # 表示将k1和c1连接起来</span><br></pre></td></tr></table></figure>

<p>（5）先开启flume监听端口（开启服务端）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">参数说明：</span><br><span class="line">-c：表示配置文件存储在 conf/目录</span><br><span class="line">--n：表示给 agent 起名为 a1</span><br><span class="line">-f：flume 本次启动读取的配置文件是在 job 文件夹下的 flume-telnet.conf文件。</span><br><span class="line">-Dflume.root.logger=INFO,console ：-D 表示 flume 运行时动态修改 flume.root.logger参数属性值，并将控制台日志打印级别设置为 INFO 级别。日志级别包括:log、info、warn、error。console表示将结果输出到控制台</span><br></pre></td></tr></table></figure>

<p><img src="Snipaste_2023-10-20_21-15-51.png" alt="Snipaste_2023-10-20_21-15-51"></p>
<p>（6）在开启客户端（都是在hadoop102上）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]#  nc localhost 44444</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>（7）使用netcat工具向本机的44444端口发送内容，并在flume监听页面观察接受数据情况</p>
<p><img src="Snipaste_2023-10-20_21-21-02.png" alt="Snipaste_2023-10-20_21-21-02"></p>
<h3 id="2-2-2-实时监控单个追加文件"><a href="#2-2-2-实时监控单个追加文件" class="headerlink" title="2.2.2 实时监控单个追加文件"></a>2.2.2 实时监控单个追加文件</h3><p>需求：实时监控Hive日志，并上传到HDFS中</p>
<p>分析：</p>
<p><img src="Snipaste_2023-10-21_15-01-39.png" alt="Snipaste_2023-10-21_15-01-39"></p>
<p>步骤：</p>
<p>（1）检查&#x2F;etc&#x2F;profile.d&#x2F;my_env.sh 文件，确认 Hadoop 和 Java 环境变量配置正确</p>
<p>（2）创建 flume-file-hdfs.conf 文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 profile.d]# cd /opt/module/flume/job/</span><br><span class="line">[root@hadoop102 job]# vim flume-file-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>注：要想读取 Linux 系统中的文件，就得按照 Linux 命令的规则执行命令。由于 Hive日志在 Linux 系统中所以读取文件的类型选择：exec 即 execute 执行的意思。表示执行Linux 命令来读取文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a2.sources = r2</span><br><span class="line">a2.sinks = k2</span><br><span class="line">a2.channels = c2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">定义<span class="built_in">source</span>类型为<span class="built_in">exec</span>可执行命令的</span></span><br><span class="line">a2.sources.r2.type = exec</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看文件末端10行的命令</span></span><br><span class="line">a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://hadoop102:8020/flume/%Y%m%d/%H</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">上传文件的前缀</span></span><br><span class="line">a2.sinks.k2.hdfs.filePrefix = logs-</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">是否按照时间滚动文件夹</span></span><br><span class="line">a2.sinks.k2.hdfs.round = true</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k2.hdfs.roundValue = 1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">重新定义时间单位</span></span><br><span class="line">a2.sinks.k2.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">积攒多少个 Event 才 flush 到 HDFS 一次</span></span><br><span class="line">a2.sinks.k2.hdfs.batchSize = 100</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">多久生成一个新的文件，生产环境中一般配3600</span></span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 30</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">设置每个文件的滚动大小</span></span><br><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">文件的滚动与 Event 数量无关</span></span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<p>（3）运行flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a2 -f job/flume-file-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>可以看到flume文件已经在HDFS上创建了：</p>
<img src="Snipaste_2023-10-21_16-00-33.png" alt="Snipaste_2023-10-21_16-00-33" style="zoom:43%;">

<p>（4）开启hadoop集群和hive服务</p>
<p>（5）在hive上随便执行一个语句，查看HDFS上的日志更新</p>
<img src="Snipaste_2023-10-21_16-05-08.png" alt="Snipaste_2023-10-21_16-05-08" style="zoom:43%;">

<h3 id="2-2-3-实时监控目录下多个新文件"><a href="#2-2-3-实时监控目录下多个新文件" class="headerlink" title="2.2.3 实时监控目录下多个新文件"></a>2.2.3 实时监控目录下多个新文件</h3><p>需求：使用Flume监听整个目录的文件，并上传至HDFS</p>
<p>分析：</p>
<p><img src="Snipaste_2023-10-21_16-10-48.png" alt="Snipaste_2023-10-21_16-10-48"></p>
<p>5 解释：.COMPLETED结尾表示已经上传，.tmp结尾表示还没有上传</p>
<p>步骤：</p>
<p>（1）创建配置文件flume-dir-hdfs.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# vim flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r3.type = spooldir</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED</span><br><span class="line">a3.sources.r3.fileHeader = true</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">忽略所有以.tmp 结尾的文件，不上传</span></span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://hadoop102:8020/flume/upload/%Y%m%d/%H</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">上传文件的前缀</span></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">是否按照时间滚动文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.round = true</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">重新定义时间单位</span></span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">是否使用本地时间戳</span></span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">积攒多少个 Event 才 flush 到 HDFS 一次</span></span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">设置文件类型，可支持压缩</span></span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">多久生成一个新的文件</span></span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 20</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">设置每个文件的滚动大小大概是 128M</span></span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">文件的滚动与 Event 数量无关</span></span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure>

<p>（2）在&#x2F;opt&#x2F;module&#x2F;flume 目录下创建 upload 文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]#  mkdir upload</span><br></pre></td></tr></table></figure>

<p>（3）启动监控文件夹命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a3 -f job/flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>（4）向 upload 文件夹中添加文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 upload]# touch atguigu.txt</span><br><span class="line">[root@hadoop102 upload]# touch atguigu.tmp</span><br><span class="line">[root@hadoop102 upload]# touch atguigu.log</span><br><span class="line">[root@hadoop102 upload]# ll</span><br><span class="line">总用量 0</span><br><span class="line">-rw-r--r--. 1 root root 0 10月 21 16:32 atguigu.log.COMPLETED</span><br><span class="line">-rw-r--r--. 1 root root 0 10月 21 16:31 atguigu.tmp</span><br><span class="line">-rw-r--r--. 1 root root 0 10月 21 16:29 atguigu.txt.COMPLETED</span><br></pre></td></tr></table></figure>

<p>可以发现.tmp结尾的文件是没有被上传至HDFS中的：</p>
<img src="Snipaste_2023-10-21_16-33-46.png" alt="Snipaste_2023-10-21_16-33-46" style="zoom:43%;">

<h3 id="2-2-4-实时监控目录下的多个追加文件"><a href="#2-2-4-实时监控目录下的多个追加文件" class="headerlink" title="2.2.4 实时监控目录下的多个追加文件"></a>2.2.4 实时监控目录下的多个追加文件</h3><p>Exec source 适用于监控一个实时追加的文件，不能实现断点续传；Spooldir Source适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步；而 <strong>Taildir Source</strong>适合用于监听多个实时追加的文件，并且能够实现断点续传。</p>
<p>Spooldir Source：监控同一目录的不同文件</p>
<p>Taildir Source：监控不同目录的不同文件</p>
<p>需求：使用Flume监听整个目录的实时追加文件，并上传至HDFS</p>
<p>分析：</p>
<p><img src="Snipaste_2023-10-21_16-51-57.png" alt="Snipaste_2023-10-21_16-51-57"></p>
<p>步骤：</p>
<p>（1）创建配置文件 flume-taildir-hdfs.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# vim flume-taildir-hdfs.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r3.type = TAILDIR</span><br><span class="line">a3.sources.r3.positionFile = /opt/module/flume/tail_dir.json</span><br><span class="line">a3.sources.r3.filegroups = f1 f2</span><br><span class="line">a3.sources.r3.filegroups.f1 = /opt/module/flume/files/.*file.*</span><br><span class="line">a3.sources.r3.filegroups.f2 = /opt/module/flume/files2/.*log.*</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://hadoop102:8020/flume/upload2/%Y%m%d/%H</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">上传文件的前缀</span></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">是否按照时间滚动文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.round = true</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">重新定义时间单位</span></span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">是否使用本地时间戳</span></span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">积攒多少个 Event 才 flush 到 HDFS 一次</span></span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">设置文件类型，可支持压缩</span></span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">多久生成一个新的文件</span></span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 20</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">设置每个文件的滚动大小大概是 128M</span></span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">文件的滚动与 Event 数量无关</span></span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure>

<p>（2）在&#x2F;opt&#x2F;module&#x2F;flume 目录下创建 files 文件夹和files2文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# mkdir files</span><br><span class="line">[root@hadoop102 flume]# mkdir files2</span><br></pre></td></tr></table></figure>

<p>（3）启动监控文件夹命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a3 -f job/flume-taildir-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>（4）向 files 文件夹中追加内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# cd files</span><br><span class="line">[root@hadoop102 files]# touch file1.txt</span><br><span class="line">[root@hadoop102 files]# echo hello &gt;&gt; file1.txt</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-10-21_17-11-45.png" alt="Snipaste_2023-10-21_17-11-45" style="zoom:43%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">继续追加</span></span><br><span class="line">[root@hadoop102 files]# echo atguigu &gt;&gt; file1.txt</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-10-21_17-13-49.png" alt="Snipaste_2023-10-21_17-13-49" style="zoom:43%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">向 files2 文件夹中追加内容</span></span><br><span class="line">[root@hadoop102 files2]# touch log.txt</span><br><span class="line">[root@hadoop102 files2]# echo hello2 &gt;&gt; log.txt </span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-10-21_17-16-24.png" alt="Snipaste_2023-10-21_17-16-24" style="zoom:43%;">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">继续追加</span></span><br><span class="line">[root@hadoop102 files2]# echo atguigu2 &gt;&gt; log.txt</span><br></pre></td></tr></table></figure>

<img src="Snipaste_2023-10-21_17-18-01.png" alt="Snipaste_2023-10-21_17-18-01" style="zoom:43%;">

<p>Taildir Source 维护了一个 json 格式的 position File，其会定期的往 position File中更新每个文件读取到的最新的位置，因此能够实现断点续传。Position File 的格式如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;inode&quot;</span><span class="punctuation">:</span><span class="number">104996452</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;pos&quot;</span><span class="punctuation">:</span><span class="number">14</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;file&quot;</span><span class="punctuation">:</span><span class="string">&quot;/opt/module/flume/files/file1.txt&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;inode&quot;</span><span class="punctuation">:</span><span class="number">3349704</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;pos&quot;</span><span class="punctuation">:</span><span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;file&quot;</span><span class="punctuation">:</span><span class="string">&quot;/opt/module/flume/files2/log.txt&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>注：Linux 中储存文件元数据的区域就叫做 inode，每个 inode 都有一个号码，操作系统用 inode 号码来识别不同的文件，Unix&#x2F;Linux 系统内部不使用文件名，而使用 inode 号码来识别文件。</p>
<h1 id="第三章-Flume进阶"><a href="#第三章-Flume进阶" class="headerlink" title="第三章 Flume进阶"></a>第三章 Flume进阶</h1><h2 id="3-1-Flume事务"><a href="#3-1-Flume事务" class="headerlink" title="3.1 Flume事务"></a>3.1 Flume事务</h2><p><img src="Snipaste_2023-10-21_19-47-32.png" alt="Snipaste_2023-10-21_19-47-32"></p>
<h2 id="3-2-Flume-Agent内部原理"><a href="#3-2-Flume-Agent内部原理" class="headerlink" title="3.2 Flume Agent内部原理"></a>3.2 Flume Agent内部原理</h2><p><img src="Snipaste_2023-10-21_19-52-06.png" alt="Snipaste_2023-10-21_19-52-06"></p>
<p>重要组件：</p>
<p>（1）Channel Selector</p>
<p>ChannelSelector 的作用就是选出 Event 将要被发往哪个 Channel。其共有两种类型，分别是 Replicating（复制）和 Multiplexing（多路复用）。</p>
<p>ReplicatingSelector 会将同一个 Event发往所有的 Channel，Multiplexing 会根据相应的原则，将不同的 Event 发往不同的 Channel。</p>
<p>（2）Sink Processor</p>
<p>SinkProcessor共有三种类型，分 别 是 DefaultSinkProcessor 、LoadBalancingSinkProcessor 和 FailoverSinkProcessor。</p>
<p>DefaultSinkProcessor 对 应 的 是 单 个 的 Sink ，LoadBalancingSinkProcessor 和FailoverSinkProcessor 对应的是 Sink Group，LoadBalancingSinkProcessor 可以实现负载均衡的功能，FailoverSinkProcessor 可以错误恢复的功能。</p>
<h2 id="3-3-Flume拓扑结构"><a href="#3-3-Flume拓扑结构" class="headerlink" title="3.3 Flume拓扑结构"></a>3.3 Flume拓扑结构</h2><h3 id="3-3-1-简单串联"><a href="#3-3-1-简单串联" class="headerlink" title="3.3.1 简单串联"></a>3.3.1 简单串联</h3><img src="Snipaste_2023-10-21_20-05-52.png" alt="Snipaste_2023-10-21_20-05-52" style="zoom:43%;">

<p>这种模式是将多个 flume 顺序连接起来了，从最初的 source 开始到最终 sink 传送的目的存储系统。此模式不建议桥接过多的 flume 数量， flume 数量过多不仅会影响传输速率，而且一旦传输过程中某个节点 flume 宕机，会影响整个传输系统。</p>
<h3 id="3-3-2-复制和多路复用"><a href="#3-3-2-复制和多路复用" class="headerlink" title="3.3.2 复制和多路复用"></a>3.3.2 复制和多路复用</h3><img src="Snipaste_2023-10-21_20-06-34.png" alt="Snipaste_2023-10-21_20-06-34" style="zoom:43%;">

<p>Flume 支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel 中，或者将不同数据分发到不同的 channel 中，sink 可以选择传送到不同的目的地。</p>
<h3 id="3-3-3-负载均衡和故障转移"><a href="#3-3-3-负载均衡和故障转移" class="headerlink" title="3.3.3 负载均衡和故障转移"></a>3.3.3 负载均衡和故障转移</h3><img src="Snipaste_2023-10-21_20-07-50.png" alt="Snipaste_2023-10-21_20-07-50" style="zoom:43%;">

<p>Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能。</p>
<h3 id="3-3-4-聚合"><a href="#3-3-4-聚合" class="headerlink" title="3.3.4 聚合"></a>3.3.4 聚合</h3><img src="Snipaste_2023-10-21_20-31-49.png" alt="Snipaste_2023-10-21_20-31-49" style="zoom:43%;">

<p>这种模式是我们最常见的，也非常实用，日常 web 应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用 flume 的这种组合方式能很好的解决这一问题，每台服务器部署一个 flume 采集日志，传送到一个集中收集日志的flume，再由此 flume 上传到 hdfs、hive、hbase 等，进行日志分析。</p>
<h2 id="3-4-Flume企业开发案例"><a href="#3-4-Flume企业开发案例" class="headerlink" title="3.4 Flume企业开发案例"></a>3.4 Flume企业开发案例</h2><h3 id="3-4-1-复制"><a href="#3-4-1-复制" class="headerlink" title="3.4.1 复制"></a>3.4.1 复制</h3><p>（1）需求</p>
<p>使用 Flume-1 监控文件变动，Flume-1 将变动内容传递给 Flume-2，Flume-2 负责存储到 HDFS。同时 Flume-1 将变动内容传递给 Flume-3，Flume-3 负责输出到 Local FileSystem。</p>
<p>（2）分析</p>
<p><img src="Snipaste_2023-10-21_20-45-59.png" alt="Snipaste_2023-10-21_20-45-59"></p>
<p>（3）实现步骤：</p>
<p>①在&#x2F;opt&#x2F;module&#x2F;flume&#x2F;job 目录下创建 group1 文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# mkdir group1</span><br><span class="line">[root@hadoop102 job]# cd group1/</span><br></pre></td></tr></table></figure>

<p>在&#x2F;opt&#x2F;module&#x2F;datas&#x2F;目录下创建 flume3 文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 datas]# mkdir flume3</span><br></pre></td></tr></table></figure>

<p>②创建 flume-file-flume.conf</p>
<p>配置 1 个接收日志文件的 source 和两个 channel、两个 sink，分别输送给 flume-flumehdfs 和 flume-flume-dir。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 group1]# vim flume-file-flume.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将数据流复制给所有 channel</span></span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sink 端的 avro 是一个数据发送者</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<p>③创建 flume-flume-hdfs.conf</p>
<p>配置上级 Flume 输出的 Source，输出是到 HDFS 的 Sink。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 group1]# vim flume-flume-hdfs.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">source</span> 端的 avro 是一个数据接收服务</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://hadoop102:8020/flume2/%Y%m%d/%H</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">上传文件的前缀</span></span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">是否按照时间滚动文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">重新定义时间单位</span></span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">积攒多少个 Event 才 flush 到 HDFS 一次</span></span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">多久生成一个新的文件</span></span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 30</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">设置每个文件的滚动大小大概是 128M</span></span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">文件的滚动与 Event 数量无关</span></span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>④创建 flume-flume-dir.conf</p>
<p>配置上级 Flume 输出的 Source，输出是到本地目录的 Sink。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 group1]# vim flume-flume-dir.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录</span></span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/datas/flume3</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure>

<p>⑤执行配置文件</p>
<p>分别启动对应的 flume 进程：flume-flume-dir，flume-flume-hdfs，flume-file-flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a3 -f job/group1/flume-flume-dir.conf</span><br><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a2 -f job/group1/flume-flume-hdfs.conf</span><br><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/group1/flume-file-flume.conf</span><br></pre></td></tr></table></figure>

<p>⑥启动HDFS和Hive，检查Web端</p>
<p><img src="Snipaste_2023-10-21_21-28-01.png" alt="Snipaste_2023-10-21_21-28-01"></p>
<p>⑦检查&#x2F;opt&#x2F;module&#x2F;datas&#x2F;flume3 目录中数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume3]# ll</span><br><span class="line">总用量 4</span><br><span class="line">-rw-r--r--. 1 root root    0 10月 21 20:51 1697892687504-1</span><br><span class="line">-rw-r--r--. 1 root root 1546 10月 21 20:52 1697892687504-2</span><br><span class="line">-rw-r--r--. 1 root root    0 10月 21 20:52 1697892687504-3</span><br><span class="line">-rw-r--r--. 1 root root    0 10月 21 20:53 1697892687504-4</span><br><span class="line">-rw-r--r--. 1 root root    0 10月 21 20:53 1697892687504-5</span><br><span class="line">-rw-r--r--. 1 root root    0 10月 21 20:53 1697892687504-6</span><br><span class="line">-rw-r--r--. 1 root root    0 10月 21 20:54 1697892687504-7</span><br><span class="line">-rw-r--r--. 1 root root    0 10月 21 20:55 1697892687504-8</span><br><span class="line">-rw-r--r--. 1 root root    0 10月 21 20:55 1697892687504-9</span><br></pre></td></tr></table></figure>

<h3 id="3-4-2-故障转移"><a href="#3-4-2-故障转移" class="headerlink" title="3.4.2 故障转移"></a>3.4.2 故障转移</h3><p>需求：</p>
<p>使用 Flume1 监控一个端口，其 sink 组中的 sink 分别对接 Flume2 和 Flume3，采用FailoverSinkProcessor，实现故障转移的功能。</p>
<p>分析：</p>
<img src="Snipaste_2023-10-21_21-34-27.png" alt="Snipaste_2023-10-21_21-34-27">

<p>步骤：</p>
<p>（1）准备工作：</p>
<p>在&#x2F;opt&#x2F;module&#x2F;flume&#x2F;job 目录下创建 group2 文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# mkdir group2</span><br><span class="line">[root@hadoop102 job]# cd group2/</span><br></pre></td></tr></table></figure>

<p>（2）创建 flume-netcat-flume.conf</p>
<p>配置 1 个 netcat source 和 1 个 channel、1 个 sink group（2 个 sink），分别输送给flume-flume-console1 和 flume-flume-console2。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 group2]# vim flume-netcat-flume.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 5</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 10</span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure>

<p>（3）创建 flume-flume-console1.conf</p>
<p>配置上级 Flume 输出的 Source，输出是到本地控制台。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 group2]# vim flume-flume-console1.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（4）创建 flume-flume-console2.conf</p>
<p>配置上级 Flume 输出的 Source，输出是到本地控制台。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 group2]# vim flume-flume-console2.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure>

<p>（5）执行配置文件</p>
<p>分别开启对应配置文件：flume-flume-console2，flume-flume-console1，flume-netcat-flume。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a3 -f job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a2 -f job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/group2/flume-netcat-flume.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>（6）使用netcat工具向本机的44444端口发送内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# nc localhost 44444</span><br><span class="line">hello</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>由于console2（Flume3）的优先级高，可以在其服务端看到控制台打印的日志：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2023-10-21 21:23:43,601 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F                                  hello &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>（7）将console2（Flume3）的服务停掉，再使用netcat工具发送内容，此时故障转移，可以在console1（Flume2）上看到控制台打印的日志：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2023-10-21 21:28:43,122 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 77 79 68                                        wyh &#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-4-2-负载均衡"><a href="#3-4-2-负载均衡" class="headerlink" title="3.4.2+ 负载均衡"></a>3.4.2+ 负载均衡</h3><p>（1）将group2中的flume-netcat-flume.conf配置文件复制一份，并重命名为flume-netcat-flume2.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 group2]# cp flume-netcat-flume.conf flume-netcat-flume2.conf</span><br></pre></td></tr></table></figure>

<p>（2）修改flume-netcat-flume2.conf配置文件中的内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 group2]# vim flume-netcat-flume2.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure>

<p>（3）执行配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a3 -f job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a2 -f job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/group2/flume-netcat-flume2.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>（4）使用netcat工具向本机的44444端口发送内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# nc localhost 44444</span><br><span class="line">hello    # flume3端输出日志</span><br><span class="line">OK</span><br><span class="line">atguigu  # flume2端输出日志</span><br><span class="line">OK</span><br><span class="line">123      # flume2端输出日志</span><br><span class="line">OK</span><br><span class="line">alibaba   # flume3端输出日志</span><br><span class="line">OK</span><br><span class="line">djkdjkdj   # flume2端输出日志</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>flume3端：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2023-10-22 12:48:59,423 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F                                  hello &#125;</span><br><span class="line">2023-10-22 12:51:14,806 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 6C 69 62 61 62 61                            alibaba &#125;</span><br></pre></td></tr></table></figure>

<p>flume2端：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2023-10-22 12:50:14,710 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 74 67 75 69 67 75                            atguigu &#125;</span><br><span class="line">2023-10-22 12:50:36,750 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 31 32 33                                        123 &#125;</span><br><span class="line">2023-10-22 12:51:39,836 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 64 6A 6B 64 6A 6B 64 6A                         djkdjkdj &#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-4-3-聚合"><a href="#3-4-3-聚合" class="headerlink" title="3.4.3 聚合"></a>3.4.3 聚合</h3><p>需求：</p>
<p>hadoop102 上的 Flume-1 监控文件&#x2F;opt&#x2F;module&#x2F;group.log，</p>
<p>hadoop103 上的 Flume-2 监控某一个端口的数据流，</p>
<p>Flume-1 与 Flume-2 将数据发送给 hadoop104 上的 Flume-3，Flume-3 将最终数据打印到控制台。</p>
<p>分析：</p>
<p><img src="Snipaste_2023-10-22_13-10-01.png" alt="Snipaste_2023-10-22_13-10-01"></p>
<p>步骤：</p>
<p>（1）准备工作</p>
<p>分发Flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# xsync flume</span><br></pre></td></tr></table></figure>

<p>在 hadoop102、hadoop103 以及 hadoop104 的&#x2F;opt&#x2F;module&#x2F;flume&#x2F;job 目录下创建一个group3 文件夹。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# mkdir group3</span><br><span class="line">[root@hadoop103 job]# mkdir group3</span><br><span class="line">[root@hadoop104 job]# mkdir group3</span><br></pre></td></tr></table></figure>

<p>在&#x2F;opt&#x2F;module&#x2F;上创建文件group.log</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# vim group.log</span><br></pre></td></tr></table></figure>

<p>（2）创建 flume1-logger-flume.conf</p>
<p>配置 Source 用于监控 group.log 文件，配置 Sink 输出数据到下一级 Flume。</p>
<p>在 hadoop102 上编辑配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 group3]# vim flume1-logger-flume.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/group.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop104</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（3）创建 flume2-netcat-flume.conf</p>
<p>配置 Source 监控端口 44444 数据流，配置 Sink 数据到下一级 Flume：</p>
<p>在 hadoop103 上编辑配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 group3]# vim flume2-netcat-flume.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = hadoop103</span><br><span class="line">a2.sources.r1.port = 44444</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = hadoop104</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（4）创建 flume3-flume-logger.conf</p>
<p>配置 source 用于接收 flume1 与 flume2 发送过来的数据流，最终合并后 sink 到控制台。</p>
<p>在 hadoop104 上编辑配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 group3]# vim flume3-flume-logger.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop104</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the channel</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（5）执行配置文件</p>
<p>分别开启对应配置文件：flume3-flume-logger.conf，flume2-netcat-flume.conf，flume1-logger-flume.conf。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 flume]# bin/flume-ng agent -c conf/ -n a3 -f job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[root@hadoop103 flume]# bin/flume-ng agent -c conf/ -n a2 -f job/group3/flume2-netcat-flume.conf</span><br><span class="line"></span><br><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/group3/flume1-logger-flume.conf</span><br></pre></td></tr></table></figure>

<p>（6）在hadoop103开启44444端口，并传输数据hello</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 ~]# nc hadoop103 44444</span><br><span class="line">hello</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>此时可以在hadoop104上看到由控制台输出打印的日志：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2023-10-22 14:05:59,473 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F                                  hello &#125;</span><br></pre></td></tr></table></figure>

<p>（7）在hadoop102端的&#x2F;opt&#x2F;module&#x2F;group.log中追加内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# echo &#x27;hello222&#x27; &gt;&gt; group.log</span><br></pre></td></tr></table></figure>

<p>此时也可以在hadoop104上看到由控制台输出打印的日志：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2023-10-22 14:08:35,734 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 32 32 32                         hello222 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-5-自定义拦截器（Interceptor）——多路复用（常用）"><a href="#3-5-自定义拦截器（Interceptor）——多路复用（常用）" class="headerlink" title="3.5 自定义拦截器（Interceptor）——多路复用（常用）"></a>3.5 自定义拦截器（Interceptor）——多路复用（常用）</h2><p>需求：</p>
<p>使用 Flume 采集服务器本地日志，需要按照日志<strong>类型的不同</strong>，将不同种类的日志发往不同的分析系统。</p>
<p>分析：</p>
<p>在实际的开发中，一台服务器产生的日志类型可能有很多种，不同类型的日志可能需要发送到不同的分析系统。此时会用到 Flume 拓扑结构中的 Multiplexing（多路复用） 结构，Multiplexing的原理是，根据 event 中 Header 的某个 key 的值，将不同的 event 发送到不同的 Channel中，所以我们需要自定义一个 Interceptor，为不同类型的 event 的 Header 中的 key 赋予不同的值。</p>
<p>在该案例中，我们以端口数据模拟日志，以是否包含”atguigu”模拟不同类型的日志，我们需要自定义 interceptor 区分数据中是否包含”atguigu”，将其分别发往不同的分析系统（Channel）。</p>
<p><img src="Snipaste_2023-10-22_14-20-56.png" alt="Snipaste_2023-10-22_14-20-56"></p>
<p>步骤：</p>
<p>（1）创建一个maven项目，并引入以下依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）定义TypeInterceptor类并实现Interceptor接口</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TypeInterceptor</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//声明一个存放事件的集合</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;Event&gt; addHeaderEvents;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initialize</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">//初始化存放事件的集合</span></span><br><span class="line">        addHeaderEvents = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//单个事件拦截</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Event <span class="title function_">intercept</span><span class="params">(Event event)</span> &#123;</span><br><span class="line">        <span class="comment">//1.获取事件中的头信息</span></span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.获取事件中的 body 信息</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">body</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(event.getBody());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.根据 body 中是否有&quot;atguigu&quot;来决定添加怎样的头信息</span></span><br><span class="line">        <span class="keyword">if</span> (body.contains(<span class="string">&quot;atguigu&quot;</span>)) &#123;</span><br><span class="line">            <span class="comment">//4.添加头信息</span></span><br><span class="line">            headers.put(<span class="string">&quot;type&quot;</span>, <span class="string">&quot;first&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//4.添加头信息</span></span><br><span class="line">            headers.put(<span class="string">&quot;type&quot;</span>, <span class="string">&quot;second&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//批量事件拦截</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;Event&gt; <span class="title function_">intercept</span><span class="params">(List&lt;Event&gt; events)</span> &#123;</span><br><span class="line">        <span class="comment">//1.清空集合</span></span><br><span class="line">        addHeaderEvents.clear();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.遍历 events</span></span><br><span class="line">        <span class="keyword">for</span> (Event event : events) &#123;</span><br><span class="line">            <span class="comment">//3.给每一个事件添加头信息</span></span><br><span class="line">            addHeaderEvents.add(intercept(event));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.返回结果</span></span><br><span class="line">        <span class="keyword">return</span> addHeaderEvents;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建一个静态内部类Builder，帮助我们构建拦截器对象</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Builder</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span>.Builder&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Interceptor <span class="title function_">build</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">TypeInterceptor</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）将该程序打成jar包，将jar包放到opt&#x2F;module&#x2F;flume&#x2F;lib&#x2F;下</p>
<p><img src="Snipaste_2023-10-22_14-53-38.png" alt="Snipaste_2023-10-22_14-53-38"></p>
<p>（4）编辑 flume 配置文件</p>
<p>为 hadoop102 上的 Flume1 配置 1 个 netcat source，1 个 sink group（2 个 avro sink），并配置相应的 ChannelSelector 和 interceptor。为 hadoop103 上的 Flume2 配置一个 avro source 和一个 logger sink。为 hadoop104 上的 Flume3 配置一个 avro source 和一个 logger sink。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# mkdir group4</span><br><span class="line">[root@hadoop102 job]# cd group4/</span><br><span class="line">[root@hadoop102 group4]# vim flume1.conf</span><br><span class="line">[root@hadoop102 group4]# vim flume2.conf</span><br><span class="line">[root@hadoop102 group4]# vim flume3.conf</span><br></pre></td></tr></table></figure>

<p>分别编写配置文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">flume1</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = com.atguigu.interceptor.TypeInterceptor$Builder</span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = type</span><br><span class="line">a1.sources.r1.selector.mapping.first = c1</span><br><span class="line">a1.sources.r1.selector.mapping.second = c2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop103</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type=avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop104</span><br><span class="line">a1.sinks.k2.port = 4242</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">flume2</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = hadoop103</span><br><span class="line">a1.sources.r1.port = 4141</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">flume3</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = hadoop104</span><br><span class="line">a1.sources.r1.port = 4242</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure>

<p>编写完配置文件后，分发给hadoop103和hadoop104</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# xsync group4/</span><br></pre></td></tr></table></figure>

<p>（5）分别在hadoop103，hadoop104 ，hadoop102上启动 flume 进程，注意先后顺序</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/group4/flume2.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[root@hadoop104 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/group4/flume3.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/group4/flume1.conf </span><br></pre></td></tr></table></figure>

<p>（6）在 hadoop102 使用 netcat 向 localhost:44444 发送字母和数字</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# nc localhost 44444</span><br><span class="line">atguiguaaa   # 在hadoop103上打印日志</span><br><span class="line">OK</span><br><span class="line">wyh        # 在hadoop104上打印日志</span><br><span class="line">OK</span><br><span class="line">666atguigu555   # 在hadoop103上打印日志</span><br><span class="line">OK</span><br><span class="line">alibaba      # 在hadoop104上打印日志</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>在hadoop103端：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2023-10-22 15:23:14,731 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;type=first&#125; body: 61 74 67 75 69 67 75 61 61 61                   atguiguaaa &#125;</span><br><span class="line">2023-10-22 15:27:20,998 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;type=first&#125; body: 36 36 36 61 74 67 75 69 67 75 35 35 35          666atguigu555 &#125;</span><br></pre></td></tr></table></figure>

<p>在hadoop104端：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2023-10-22 15:26:24,479 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;type=second&#125; body: 77 79 68                                        wyh &#125;</span><br><span class="line">2023-10-22 15:27:56,606 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;type=second&#125; body: 61 6C 69 62 61 62 61                            alibaba &#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-6-自定义Source（较少使用）"><a href="#3-6-自定义Source（较少使用）" class="headerlink" title="3.6 自定义Source（较少使用）"></a>3.6 自定义Source（较少使用）</h2><p>（1）介绍</p>
<p>Source 是负责接收数据到 Flume Agent 的组件。Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。官方提供的 source 类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些 source。</p>
<p>根据官方说明自定义MySource 需要继承 AbstractSource 类并实现 Configurable 和 PollableSource 接口。</p>
<img src="Snipaste_2023-10-22_15-42-23.png" alt="Snipaste_2023-10-22_15-42-23" style="zoom:50%;">

<p>（2）需求：</p>
<p>使用 flume 接收数据，并给每条数据添加前缀，输出到控制台。前缀可从 flume 配置文件中配置</p>
<img src="Snipaste_2023-10-22_15-51-12.png" alt="Snipaste_2023-10-22_15-51-12" style="zoom:50%;">

<p>（3）分析</p>
<img src="Snipaste_2023-10-22_15-52-29.png" alt="Snipaste_2023-10-22_15-52-29" style="zoom:50%;">

<p>（4）编码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MySource</span> <span class="keyword">extends</span> <span class="title class_">AbstractSource</span> <span class="keyword">implements</span> <span class="title class_">Configurable</span>, PollableSource &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义配置文件将来要读取的字段</span></span><br><span class="line">    <span class="keyword">private</span> Long delay;</span><br><span class="line">    <span class="keyword">private</span> String field;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//初始化配置信息</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line">        delay = context.getLong(<span class="string">&quot;delay&quot;</span>);</span><br><span class="line">        field = context.getString(<span class="string">&quot;field&quot;</span>, <span class="string">&quot;Hello!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Status <span class="title function_">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//创建事件头信息</span></span><br><span class="line">            HashMap&lt;String, String&gt; hearderMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">            <span class="comment">//创建事件</span></span><br><span class="line">            <span class="type">SimpleEvent</span> <span class="variable">event</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SimpleEvent</span>();</span><br><span class="line">            <span class="comment">//循环封装事件</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">                <span class="comment">//给事件设置头信息</span></span><br><span class="line">                event.setHeaders(hearderMap);</span><br><span class="line">                <span class="comment">//给事件设置内容</span></span><br><span class="line">                event.setBody((field + i).getBytes());</span><br><span class="line">                <span class="comment">//将事件写入 channel</span></span><br><span class="line">                getChannelProcessor().processEvent(event);</span><br><span class="line">                Thread.sleep(delay);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="keyword">return</span> Status.BACKOFF;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Status.READY;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getBackOffSleepIncrement</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getMaxBackOffSleepInterval</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（5）测试</p>
<p>将写好的代码打成jar包，放到&#x2F;opt&#x2F;module&#x2F;flume&#x2F;lib&#x2F;下</p>
<p>编写配置文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# vim mysoure.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = com.atguigu.source.MySource</span><br><span class="line">a1.sources.r1.delay = 2000</span><br><span class="line">a1.sources.r1.field = atguigu</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>开启任务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -f job/mysource.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>结果展示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2023-10-22 16:15:01,046 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 74 67 75 69 67 75 30                         atguigu0 &#125;</span><br><span class="line">2023-10-22 16:15:03,044 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 74 67 75 69 67 75 31                         atguigu1 &#125;</span><br><span class="line">2023-10-22 16:15:05,047 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 74 67 75 69 67 75 32                         atguigu2 &#125;</span><br><span class="line">2023-10-22 16:15:07,049 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 74 67 75 69 67 75 33                         atguigu3 &#125;</span><br><span class="line">2023-10-22 16:15:09,051 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 74 67 75 69 67 75 34                         atguigu4 &#125;</span><br><span class="line">2023-10-22 16:15:11,054 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 74 67 75 69 67 75 30                         atguigu0 &#125;</span><br><span class="line">2023-10-22 16:15:13,055 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 74 67 75 69 67 75 31                         atguigu1 &#125;</span><br><span class="line">2023-10-22 16:15:15,057 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 74 67 75 69 67 75 32                         atguigu2 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-7-自定义Sink（较少使用）"><a href="#3-7-自定义Sink（较少使用）" class="headerlink" title="3.7 自定义Sink（较少使用）"></a>3.7 自定义Sink（较少使用）</h2><p>（1）介绍</p>
<p>Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent。</p>
<p>Sink 是完全事务性的。在从 Channel 批量删除数据之前，每个 Sink 用 Channel 启动一个事务。批量事件一旦成功写出到存储系统或下一个 Flume Agent，Sink 就利用 Channel 提交事务。事务一旦被提交，该 Channel 从自己的内部缓冲区删除事件。</p>
<p>Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。官方提供的 Sink 类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些 Sink。</p>
<p>根据官方说明自定义MySink 需要继承 AbstractSink 类并实现 Configurable 接口。</p>
<img src="Snipaste_2023-10-22_16-19-36.png" alt="Snipaste_2023-10-22_16-19-36" style="zoom:50%;">

<p>（2）需求：</p>
<p>使用 flume 接收数据，并在 Sink 端给每条数据添加前缀和后缀，输出到控制台。前后缀可在 flume 任务配置文件中配置</p>
<img src="Snipaste_2023-10-22_16-21-42.png" alt="Snipaste_2023-10-22_16-21-42" style="zoom:50%;">

<p>（3）编码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MySink</span> <span class="keyword">extends</span> <span class="title class_">AbstractSink</span> <span class="keyword">implements</span> <span class="title class_">Configurable</span> &#123;</span><br><span class="line">    <span class="comment">//创建 Logger 对象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Logger</span> <span class="variable">LOG</span> <span class="operator">=</span></span><br><span class="line">            LoggerFactory.getLogger(AbstractSink.class);</span><br><span class="line">    <span class="keyword">private</span> String prefix;</span><br><span class="line">    <span class="keyword">private</span> String suffix;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Status <span class="title function_">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//声明返回值状态信息</span></span><br><span class="line">        Status status;</span><br><span class="line">        <span class="comment">//获取当前 Sink 绑定的 Channel</span></span><br><span class="line">        <span class="type">Channel</span> <span class="variable">ch</span> <span class="operator">=</span> getChannel();</span><br><span class="line">        <span class="comment">//获取事务</span></span><br><span class="line">        <span class="type">Transaction</span> <span class="variable">txn</span> <span class="operator">=</span> ch.getTransaction();</span><br><span class="line">        <span class="comment">//声明事件</span></span><br><span class="line">        Event event;</span><br><span class="line">        <span class="comment">//开启事务</span></span><br><span class="line">        txn.begin();</span><br><span class="line">        <span class="comment">//读取 Channel 中的事件，直到读取到事件结束循环</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            event = ch.take();</span><br><span class="line">            <span class="keyword">if</span> (event != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//处理事件（打印）</span></span><br><span class="line">            LOG.info(prefix + <span class="keyword">new</span> <span class="title class_">String</span>(event.getBody()) +</span><br><span class="line">                    suffix);</span><br><span class="line">            <span class="comment">//事务提交</span></span><br><span class="line">            txn.commit();</span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="comment">//遇到异常，事务回滚</span></span><br><span class="line">            txn.rollback();</span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">//关闭事务</span></span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line">        <span class="comment">//读取配置文件内容，有默认值</span></span><br><span class="line">        prefix = context.getString(<span class="string">&quot;prefix&quot;</span>, <span class="string">&quot;hello:&quot;</span>);</span><br><span class="line">        <span class="comment">//读取配置文件内容，无默认值</span></span><br><span class="line">        suffix = context.getString(<span class="string">&quot;suffix&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（4）测试</p>
<p>将编写的代码打包，并放到&#x2F;opt&#x2F;module&#x2F;flume&#x2F;lib下。</p>
<p>编写配置文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 job]# vim mysink.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = com.atguigu.sink.MySink</span><br><span class="line">a1.sinks.k1.prefix = atguigu:</span><br><span class="line">a1.sinks.k1.suffix = :atguigu</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>开启服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/mysink.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>开启客户端：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 lib]# nc localhost 44444</span><br><span class="line">hello</span><br><span class="line">OK</span><br><span class="line">wyh</span><br><span class="line">OK</span><br><span class="line">alibaba</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>结果展示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2023-10-22 16:43:01,296 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - com.atguigu.sink.MySink.process(MySink.java:47)] atguigu:hello:atguigu</span><br><span class="line">2023-10-22 16:43:09,521 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - com.atguigu.sink.MySink.process(MySink.java:47)] atguigu:wyh:atguigu</span><br><span class="line">2023-10-22 16:43:12,871 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - com.atguigu.sink.MySink.process(MySink.java:47)] atguigu:alibaba:atguigu</span><br></pre></td></tr></table></figure>

<p>注意：此时这个flume进程用ctrl+C退出不了，可以再开一个窗口kill -9进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 lib]# jps</span><br><span class="line">4100 RunJar</span><br><span class="line">3942 JobHistoryServer</span><br><span class="line">4071 RunJar</span><br><span class="line">3240 NameNode</span><br><span class="line">3421 DataNode</span><br><span class="line">9901 Application</span><br><span class="line">10061 Jps</span><br><span class="line">3758 NodeManager</span><br><span class="line">[root@hadoop102 lib]# kill -9 9901</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">已杀死</span><br><span class="line">[root@hadoop102 flume]# </span><br></pre></td></tr></table></figure>

<h2 id="3-8-Flume数据流监控"><a href="#3-8-Flume数据流监控" class="headerlink" title="3.8 Flume数据流监控"></a>3.8 Flume数据流监控</h2><h3 id="3-8-1-Ganglia的安装与部署"><a href="#3-8-1-Ganglia的安装与部署" class="headerlink" title="3.8.1 Ganglia的安装与部署"></a>3.8.1 Ganglia的安装与部署</h3><p>Ganglia 由 gmond、gmetad 和 gweb 三部分组成。</p>
<p>gmond（Ganglia Monitoring Daemon）是一种轻量级服务，安装在每台需要收集指标数据的节点主机上。使用 gmond，你可以很容易收集很多系统指标数据，如 CPU、内存、磁盘、网络和活跃进程的数据等。</p>
<p>gmetad（Ganglia Meta Daemon）整合所有信息，并将其以 RRD 格式存储至磁盘的服务。</p>
<p>gweb（Ganglia Web）Ganglia 可视化工具，gweb 是一种利用浏览器显示 gmetad 所存储数据的 PHP 前端。在 Web 界面中以图表方式展现集群的运行状态下收集的多种不同指标数据。</p>
<p>（1）安装ganglia</p>
<p>①规划</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102: web gmetad gmod</span><br><span class="line">hadoop103: gmod</span><br><span class="line">hadoop104: gmod</span><br></pre></td></tr></table></figure>

<p>②在 102 103 104 分别安装 epel-release</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 lib]# yum -y install epel-release</span><br><span class="line">[root@hadoop103 ~]# yum -y install epel-release</span><br><span class="line">[root@hadoop104 ~]# yum -y install epel-release</span><br></pre></td></tr></table></figure>

<p>③在102安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 lib]# yum -y install ganglia-gmetad</span><br><span class="line">[root@hadoop102 lib]# yum -y install ganglia-web</span><br><span class="line">[root@hadoop102 lib]# yum -y install ganglia-gmond</span><br></pre></td></tr></table></figure>

<p>④在103和104上安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 ~]# yum -y install ganglia-gmond</span><br><span class="line">[root@hadoop104 ~]# yum -y install ganglia-gmond</span><br></pre></td></tr></table></figure>

<p>（2）在102修改配置文件&#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;ganglia.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 lib]# vim /etc/httpd/conf.d/ganglia.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"><span class="comment"># Ganglia monitoring system php web frontend</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"></span></span><br><span class="line">Alias /ganglia /usr/share/ganglia</span><br><span class="line"></span><br><span class="line">&lt;Location /ganglia&gt;</span><br><span class="line">  Require local</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">通过windows访问ganglia,需要配置 Linux 对应的主机(windows)ip 地址</span></span><br><span class="line">  Require ip 192.168.255.102</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Require ip 10.1.2.3</span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Require host example.org</span></span><br><span class="line">&lt;/Location&gt;</span><br></pre></td></tr></table></figure>

<p>（3）在102修改配置文件&#x2F;etc&#x2F;ganglia&#x2F;gmetad.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 lib]# vim /etc/ganglia/gmetad.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_source &quot;my cluster&quot; hadoop102</span><br></pre></td></tr></table></figure>

<p>（4）在102，103，104修改配置文件&#x2F;etc&#x2F;ganglia&#x2F;gmond.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 lib]# vim /etc/ganglia/gmond.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cluster &#123;</span><br><span class="line"> name = &quot;my cluster&quot;</span><br><span class="line"> owner = &quot;unspecified&quot;</span><br><span class="line"> latlong = &quot;unspecified&quot;</span><br><span class="line"> url = &quot;unspecified&quot;</span><br><span class="line">&#125;</span><br><span class="line">udp_send_channel &#123;</span><br><span class="line"><span class="meta prompt_"> #</span><span class="language-bash">bind_hostname = <span class="built_in">yes</span> <span class="comment"># Highly recommended, soon to be default.</span></span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">This option tells gmond to use a <span class="built_in">source</span></span> </span><br><span class="line">address</span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">that resolves to the machine<span class="string">&#x27;s hostname.</span></span> </span><br><span class="line">Without</span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash"><span class="string">this, the metrics may appear to come from</span></span> </span><br><span class="line">any</span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash"><span class="string">interface and the DNS names associated with</span></span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash"><span class="string">those IPs will be used to create the RRDs.</span></span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash"><span class="string">mcast_join = 239.2.11.71</span></span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash"><span class="string">数据发送给 hadoop102</span></span></span><br><span class="line"> host = hadoop102</span><br><span class="line"> port = 8649</span><br><span class="line"> ttl = 1</span><br><span class="line">&#125;</span><br><span class="line">udp_recv_channel &#123;</span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash"><span class="string">mcast_join = 239.2.11.71</span></span></span><br><span class="line"> port = 8649</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">接收来自任意连接的数据</span></span></span><br><span class="line"> bind = 0.0.0.0</span><br><span class="line"> retry_bind = true</span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash"><span class="string">Size of the UDP buffer. If you are handling lots of metrics</span></span> </span><br><span class="line">you really</span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash"><span class="string">should bump it up to e.g. 10MB or even higher.</span></span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash"><span class="string">buffer = 10485760</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>向103和014分发一下修改完的配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ganglia]# xsync gmond.conf</span><br></pre></td></tr></table></figure>

<p>（5）在102修改配置文件&#x2F;etc&#x2F;selinux&#x2F;config</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ganglia]# vim /etc/selinux/config</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">SELINUXTYPE= can take one of these two values:</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>

<p>临时生效：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ganglia]# setenforce 0</span><br></pre></td></tr></table></figure>

<p>（6）启动ganglia</p>
<p>修改&#x2F;var&#x2F;lib&#x2F;ganglia 目录的权限：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ganglia]# chmod -R 777 /var/lib/ganglia</span><br></pre></td></tr></table></figure>

<p>①在102，103，104启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ganglia]# systemctl start gmond</span><br><span class="line">[root@hadoop103 ganglia]# systemctl start gmond</span><br><span class="line">[root@hadoop104 ~]# systemctl start gmond</span><br></pre></td></tr></table></figure>

<p>②在102启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ganglia]# systemctl start httpd</span><br><span class="line">[root@hadoop102 ganglia]# systemctl start gmetad</span><br></pre></td></tr></table></figure>

<p>（7）用linux系统的火狐浏览器打开ganglia页面</p>
<p><a target="_blank" rel="noopener" href="http://hadoop102/ganglia">http://hadoop102/ganglia</a></p>
<p><img src="Snipaste_2023-10-22_22-11-32.png" alt="Snipaste_2023-10-22_22-11-32"></p>
<h3 id="3-8-2-操作Flume测试监控"><a href="#3-8-2-操作Flume测试监控" class="headerlink" title="3.8.2 操作Flume测试监控"></a>3.8.2 操作Flume测试监控</h3><p>（1）启动Flume任务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger==INFO,console -Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=hadoop102:8649</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# nc localhost 44444</span><br><span class="line">hello</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>数据是通的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2023-10-22 22:20:13,344 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F                                  hello &#125;</span><br></pre></td></tr></table></figure>

<p>（2）发送数据后观察ganglia监测图：</p>
<img src="Snipaste_2023-10-22_22-25-14.png" alt="Snipaste_2023-10-22_22-25-14" style="zoom:50%;">

<img src="Snipaste_2023-10-22_22-25-58.png" alt="Snipaste_2023-10-22_22-25-58" style="zoom:50%;">

<h1 id="第四章-企业真实面试题（重点）"><a href="#第四章-企业真实面试题（重点）" class="headerlink" title="第四章 企业真实面试题（重点）"></a>第四章 企业真实面试题（重点）</h1><p>见《大数据面试题总结》</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">Linux</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2023/10/20/flume%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2025 John Doe
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">随笔</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">常用算法</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">数据科学</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Linux</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">面试</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">java</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Java</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">聚宽</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">项目</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">推荐系统</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">数据仓库</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">贝叶斯</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">因子投资</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">NLP基础</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">考试</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://www.csdn.net/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>CSDN</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.zhihu.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>知乎</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.huaweicloud.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>华为云</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.aliyun.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>阿里云</a>
            </li>
          
            <li class="search-li">
              <a href="https://leetcode.cn/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>力扣</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.joinquant.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>聚宽</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">我叫王宇涵，东北人，本科和硕士分别毕业于哈尔滨工程大学和大连理工大学，2024年秋招拿到快手、百度、京东、科大讯飞、度小满、华为、荣耀、360等十余家企业offer，目前就职于快手数据平台部，担任数据研发工程师，专注于商业化广告业务。热爱大数据平台开发与数仓开发，技术栈包括但不限于Java、数仓建模、Hadoop、Hive、Spark、Flink、Clickhouse、Doris、数据湖、数据治理等。会不定期分享一些技术文章、业务知识、面试心得和读书笔记。</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>